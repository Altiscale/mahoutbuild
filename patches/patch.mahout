diff -uNar -x .git mahout/.gitignore mahout/.gitignore
--- mahout/.gitignore	2014-03-29 01:04:48.000000000 -0700
+++ mahout/.gitignore	2014-03-29 01:03:13.000000000 -0700
@@ -1,7 +1,11 @@
+output-asf-email-examples/
+.checkstyle
+.ruleset
+.pmd
 .classpath
 .project
 .settings/
-.idea/*
+.idea/
 *.iml
 target/
 examples/bin/tmp
diff -uNar -x .git mahout/CHANGELOG mahout/CHANGELOG
--- mahout/CHANGELOG	2014-03-29 01:04:48.000000000 -0700
+++ mahout/CHANGELOG	2014-03-29 01:03:13.000000000 -0700
@@ -1,6 +1,194 @@
 Mahout Change Log
 
-Release 0.8 - unreleased
+Release 1.0 - unreleased
+
+  MAHOUT-1491: Spectral KMeans Clustering doesn't clean its /tmp dir and fails when seeing it again (smarthi)
+
+  MAHOUT-1488: DisplaySpectralKMeans fails: examples/output/clusteredPoints/part-m-00000 does not exist (Saleem Ansari via smarthi)
+
+  MAHOUT-1476: Cleanup website on Hidden Markov Models
+
+  MAHOUT-1475: Cleanup website on Naive Bayes (smarthi)
+
+  MAHOUT-1472: Cleanup website on fuzzy kmeans (smarthi)
+
+  MAHOUT-1471: Cleanup website for Canopy clustering (smarthi)
+
+  MAHOUT-1467: ClusterClassifier readPolicy leaks file handles (Avi Shinnar, smarthi)
+
+  MAHOUT-1466: Cluster visualization fails to execute (ssc)
+  
+  MAHOUT-1465: Clean up README (akm)
+
+  MAHOUT-1463: Modify OnlineSummarizers to use the TDigest dependency from Maven Central (tdunning, smarthi)
+
+  MAHOUT-1460: Remove reference to Dirichlet in ClusterIterator (frankscholten)
+
+  MAHOUT-1459: Move Hadoop related code out of CanopyClusterer (frankscholten)
+
+  MAHOUT-1458: Remove KMeansConfigKeys and FuzzyKMeansConfigKeys (frankscholten)
+
+  MAHOUT-1457: Move EigenSeedGenerator into spectral kmeans package (frankscholten)
+
+  MAHOUT-1455: Forkcount config causes JVM crashes during build (frankscholten)
+
+  MAHOUT-1451: Cleaning up the examples for clustering on the website (Gaurav Misra via ssc)
+
+  MAHOUT-1449: Update the Known Issues in Random Forests Page (Manoj Awasthi via ssc)
+
+  MAHOUT-1448: In Random Forest, the training does not support multiple input files. The input dataset must be one single file. (Manoj Awasthi via ssc)
+
+  MAHOUT-1447: ImplicitFeedbackAlternatingLeastSquaresSolver tests and features (Adam Ilardi via ssc)
+
+  MAHOUT-1438: "quickstart" tutorial for building a simple recommender (Maciej Mazur and Steve Cook via ssc)
+
+  MAHOUT-1434: Dead links on the web ste (Kevin Moulart, smarthi)
+
+  MAHOUT-1433: Make SVDRecommender look at all unknown items of a user per default (ssc)
+
+  MAHOUT-1429: Parallelize YtransposeY in ImplicitFeedbackAlternatingLeastSquaresSolver (Adam Ilardi via ssc)
+
+  MAHOUT-1419: Random decision forest is excessively slow on numeric features (srowen)
+
+  MAHOUT-1417: Random decision forest implementation fails in Hadoop 2 (srowen)
+
+  MAHOUT-1416: Make access of DecisionForest.read(dataInput) less restricted (Manoj Awasthi via smarthi)
+  
+  MAHOUT-1415: Clone method on sparse matrices fails if there is an empty row which has not been set explicitly (till.rohrmann via ssc)
+
+  MAHOUT-1413: Rework Algorithms page (ssc)
+
+  MAHOUT-1356: Ensure unit tests fail fast when writing outside mvn target directory (isabel, smarthi, dweiss, frankscholten, akm)
+
+  MAHOUT-1329: Mahout for hadoop 2 (gcapan, Sergey Svinarchuk)
+
+Release 0.9 - 2014-02-01
+
+  MAHOUT-1411: Random test failures from TDigestTest (smarthi)
+
+  MAHOUT-1410: clusteredPoints do not contain a vector id (smarthi, Andrew Musselman)
+
+  MAHOUT-1409: MatrixVectorView has index check error (tdunning)
+
+  MAHOUT-1402: Zero clusters using streaming k-means option in cluster-reuters.sh (smarthi)
+
+  MAHOUT-1401: Resurrect Frequent Pattern mining (smarthi)
+
+  MAHOUT-1400: Remove references to deprecated and removed algorithms from examples scripts (ssc)
+
+  MAHOUT-1399: Fixed multiple slf4j bindings when running Mahout examples issue (sslavic)
+
+  MAHOUT-1398: FileDataModel should provide a constructor with a delimiterPattern (Roy Guo via ssc)
+
+  MAHOUT-1396: Accidental use of commons-math won't work with next Hadoop 2 release (srowen)
+
+  MAHOUT-1394: Undeprecate Lanczos (ssc)
+
+  MAHOUT-1393: Remove duplicated code from getTopTerms and getTopFeatures in AbstractClusterWriter (Diego Carrion via smarthi)
+
+  MAHOUT-1392: Streaming KMeans should write centroid output to a 'part-r-xxxx' file when executed in sequential mode (smarthi)
+
+  MAHOUT-1390: SVD hangs for certain inputs (tdunning)
+
+  MAHOUT-1389: Complementary Naive Bayes Classifier not getting called when "-c" option is activated (Gouri Shankar Majumdar via smarthi)
+
+  MAHOUT-1384: Executing the MR version of Naive Bayes/CNB of classify_20newgroups.sh fails in seqdirectory step (smarthi)
+
+  MAHOUT-1382: Upgrade Mahout third party jars for 0.9 Release (smarthi)
+
+  MAHOUT-1380: Streaming KMeans fails when executed in Sequential Mode (smarthi)
+
+  MAHOUT-1379: ClusterQualitySummarizer fails with the new T-Digest for clusters with 1 data point (smarthi)
+
+  MAHOUT-1378: Running Random Forest with Ignored features fails when loading feature descriptor from JSON file (Sam Wu via smarthi)
+
+  MAHOUT-1377: Exclude JUnit.jar from tarball (Sergey Svinarchuk via smarthi)
+
+  MAHOUT-1371: Arff loader can misinterpret nominals with integer, real or string (Mansur Iqbal via smarthi)
+
+  MAHOUT-1370: Vectordump doesn't write to output file in MapReduce Mode (smarthi)
+
+  MAHOUT-1368: Convert OnlineSummarizer to use the new TDigest (tdunning)
+
+  MAHOUT-1367: WikipediaXmlSplitter --> Exception in thread "main" java.lang.NullPointerException (smarthi)
+
+  MAHOUT-1364: Upgrade Mahout codebase to Lucene 4.6 (Frank Scholten)
+
+  MAHOUT-1363: Rebase packages in mahout-scala (dlyubimov)
+
+  MAHOUT-1362: Remove examples/bin/build-reuters.sh (smarthi)
+
+  MAHOUT-1361: Online algorithm for computing accurate Quantiles using 1-D clustering (tdunning)
+
+  MAHOUT-1358: StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true (smarthi)
+
+  MAHOUT-1355: InteractionValueEncoder produces wrong traceDictionary entries (Johannes Schulte via smarthi)
+
+  MAHOUT-1353: Visibility of preparePreferenceMatrix directory location (Pat Ferrel, ssc)
+
+  MAHOUT-1352: Option to change RecommenderJob output format (Pat Ferrel, ssc)
+
+  MAHOUT-1351: Adding DenseVector support to AbstractCluster (David DeBarr via smarthi)
+
+  MAHOUT-1349: Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size (Andrew Musselman via smarthi)
+
+  MAHOUT-1347: Add Streaming K-Means clustering algorithm to examples/bin/cluster-reuters.sh (smarthi)
+
+  MAHOUT-1345: Enable randomised testing for all Mahout modules (Dawid Weiss, Isabel, sslavic, Frank Scholten, smarthi)
+
+  MAHOUT-1343: JSON output format support in cluster dumper (Telvis Calhoun via sslavic)
+
+  MAHOUT-1333: Fixed examples bin directory permissions in distribution archives (Mike Percy via sslavic)
+
+  MAHOUT-1319: seqdirectory -filter argument silently ignored when run as MR (smarthi)
+
+  MAHOUT-1317: Clarify some of the messages in Preconditions.checkArgument (Nikolai Grinko, smarthi)
+
+  MAHOUT-1314: StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true (smarthi)
+
+  MAHOUT-1313: Fixed unwanted integral division bug in RowSimilarityJob downsampling code where precision should have been retained (sslavic)
+
+  MAHOUT-1312: LocalitySensitiveHashSearch does not limit search results (sslavic)
+
+  MAHOUT-1308: Cannot extend CandidateItemsStrategy due to restricted visibility (David Geiger, smarthi)
+
+  MAHOUT-1301: toString() method of SequentialAccessSparseVector has excess comma at the end (Alexander Senov, smarthi)
+
+  MAHOUT-1297: New module for linear algebra scala DSL (dlyubimov)
+
+  MAHOUT-1296: Remove deprecated algorithms (ssc)
+
+  MAHOUT-1295: Excluded all Maven's target directories from distribution archives (sslavic)
+
+  MAHOUT-1294: Cleanup previously installed artifacts from CI server local repository (sslavic)
+
+  MAHOUT-1293: Source distribution tar.gz archive cannot be unpacked on Linux (sslavic)
+
+  MAHOUT-1292: lucene2seq should validate the 'id' field (Frank Scholten via smarthi)
+
+  MAHOUT-1291: MahoutDriver yields cosmetically suboptimal exception when bin/mahout runs without args, on some Hadoop versions (srowen)
+
+  MAHOUT-1290: Issue when running Mahout Recommender Demo (Helder Garay Martins via smarthi)
+
+  MAHOUT-1289: Move downsampling code into RowSimilarityJob (ssc)
+
+  MAHOUT-1287: classifier.sgd.CsvRecordFactory incorrectly parses CSV format (Alex Franchuk via smarthi)
+
+  MAHOUT-1285: Arff loader can misparse string data as double (smarthi)
+
+  MAHOUT-1284: DummyRecordWriter's bug with reused Writables (Maysam Yabandeh via smarthi)
+
+  MAHOUT-1275: Dropped bz2 distribution format for source and binaries (sslavic)
+
+  MAHOUT-1265: Multilayer Perceptron (Yexi Jiang via smarthi)
+
+  MAHOUT-1261: TasteHadoopUtils.idToIndex can return an int that has size Integer.MAX_VALUE (Carl Clark, smarthi)
+
+  MAHOUT-1242: No key redistribution function for associative maps (Tharindu Rusira via smarthi)
+
+  MAHOUT-1030: Regression: Clustered Points Should be WeightedPropertyVectorWritable not WeightedVectorWritable (Andrew Musselman, Pat Ferrel, Jeff Eastman, Lars Norskog, smarthi)
+
+Release 0.8 - 2013-07-25
 
   MAHOUT-1272: Parallel SGD matrix factorizer for SVDrecommender (Peng Cheng via ssc)
 
diff -uNar -x .git mahout/README.txt mahout/README.txt
--- mahout/README.txt	2014-03-29 01:04:48.000000000 -0700
+++ mahout/README.txt	2014-03-29 01:03:13.000000000 -0700
@@ -1,33 +1,18 @@
 Welcome to Apache Mahout!
 
-Mahout is a scalable machine learning library that implements many different
-approaches to machine learning.  The project currently contains
-implementations of algorithms for classification, clustering, frequent item
-set mining, genetic programming and collaborative filtering. Mahout is 
-scalable along three dimensions: It scales to reasonably large data sets by 
-leveraging algorithm properties or implementing versions based on Apache 
-Hadoop. It scales to your perferred business case as it is distributed under 
-a commercially friendly license. In addition it scales in terms of support 
-by providing a vibrant, responsive and diverse community.
- 
 Getting Started
+  See http://mahout.apache.org
 
- See https://cwiki.apache.org/MAHOUT/quickstart.html
-
-To compile the sources run 'mvn -DskipTests clean install'
-To run all the tests run 'mvn test'
-To setup your ide run 'mvn eclipse:eclipse' or 'mvn idea:idea'
-For more info on maven see http://maven.apache.org
+From this directory
+  To compile, do `mvn -DskipTests clean install`
+  To run tests do `mvn test`
+  To set up your IDE, do `mvn eclipse:eclipse` or `mvn idea:idea`
 
 For more information on how to contribute see:
-
- https://cwiki.apache.org/confluence/display/MAHOUT/How+To+Contribute
-
+  https://mahout.apache.org/developers/how-to-contribute.html
 
 Legal
-
  Please see the NOTICE.txt included in this directory for more information.
 
 Documentation
-
-See http://mahout.apache.org/.
+  See http://mahout.apache.org/
diff -uNar -x .git mahout/bin/mahout mahout/bin/mahout
--- mahout/bin/mahout	2014-03-29 01:04:48.000000000 -0700
+++ mahout/bin/mahout	2014-03-29 01:03:13.000000000 -0700
@@ -24,13 +24,13 @@
 #   MAHOUT_CORE        set to anything other than an empty string to force
 #                      mahout to run in developer 'core' mode, just as if the
 #                      -core option was presented on the command-line
-# Commane-line Options
+# Command-line Options
 #
 #   -core              -core is used to switch into 'developer mode' when
 #                      running mahout locally. If specified, the classes
 #                      from the 'target/classes' directories in each project
-#                      are used. Otherwise classes will be retrived from
-#                      jars in the binary releas collection or *-job.jar files
+#                      are used. Otherwise classes will be retrieved from
+#                      jars in the binary release collection or *-job.jar files
 #                      found in build directories. When running on hadoop
 #                      the job files will always be used.
 
@@ -142,6 +142,17 @@
   for f in $MAHOUT_HOME/examples/target/mahout-examples-*-job.jar $MAHOUT_HOME/mahout-examples-*-job.jar ; do
     CLASSPATH=${CLASSPATH}:$f;
   done
+  
+  # add scala dev target since it is not currently used in examples
+  for f in $MAHOUT_HOME/math-scala/target/mahout-math-scala-*.jar ; do 
+    CLASSPATH=${CLASSPATH}:$f;
+  done
+
+  # add mahout-spark since it is currently not used in examples
+  for f in $MAHOUT_HOME/spark/target/mahout-spark-*.jar ; do 
+    CLASSPATH=${CLASSPATH}:$f;
+  done
+  
 
   # add release dependencies to CLASSPATH
   for f in $MAHOUT_HOME/lib/*.jar; do
@@ -152,6 +163,7 @@
   CLASSPATH=${CLASSPATH}:$MAHOUT_HOME/core/target/classes
   CLASSPATH=${CLASSPATH}:$MAHOUT_HOME/integration/target/classes
   CLASSPATH=${CLASSPATH}:$MAHOUT_HOME/examples/target/classes
+  CLASSPATH=${CLASSPATH}:$MAHOUT_HOME/math-scala/target/classes
   #CLASSPATH=${CLASSPATH}:$MAHOUT_HOME/core/src/main/resources
 fi
 
diff -uNar -x .git mahout/buildtools/Eclipse-Lucene-Codestyle.xml mahout/buildtools/Eclipse-Lucene-Codestyle.xml
--- mahout/buildtools/Eclipse-Lucene-Codestyle.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/buildtools/Eclipse-Lucene-Codestyle.xml	2014-03-29 01:03:13.000000000 -0700
@@ -1,4 +1,20 @@
 <?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
 <profiles version="12">
 <profile kind="CodeFormatterProfile" name="Apache Conventions" version="12">
 <setting id="org.eclipse.jdt.core.formatter.comment.insert_new_line_before_root_tags" value="insert"/>
diff -uNar -x .git mahout/buildtools/MahoutCleanUp.xml mahout/buildtools/MahoutCleanUp.xml
--- mahout/buildtools/MahoutCleanUp.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/buildtools/MahoutCleanUp.xml	2014-03-29 01:03:13.000000000 -0700
@@ -1,4 +1,20 @@
 <?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
 <profiles version="2">
 <profile kind="CleanUpProfile" name="CXF" version="2">
 <setting id="cleanup.always_use_blocks" value="true"/>
diff -uNar -x .git mahout/buildtools/pom.xml mahout/buildtools/pom.xml
--- mahout/buildtools/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/buildtools/pom.xml	2014-03-29 01:03:13.000000000 -0700
@@ -29,7 +29,7 @@
 
   <groupId>org.apache.mahout</groupId>
   <artifactId>mahout-buildtools</artifactId>
-  <version>0.8</version>
+  <version>1.0-SNAPSHOT</version>
   <name>Mahout Build Tools</name>
 
   <packaging>jar</packaging>
@@ -38,6 +38,10 @@
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
   </properties>
 
+  <prerequisites>
+    <maven>3.0</maven>
+  </prerequisites>
+
   <build>
     <plugins>
       <plugin>
@@ -117,11 +121,4 @@
       </build>
     </profile>
   </profiles>
-
-
-  <scm>
-    <connection>scm:svn:http://svn.apache.org/repos/asf/maven/pom/tags/mahout-0.8/mahout-buildtools</connection>
-    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/maven/pom/tags/mahout-0.8/mahout-buildtools</developerConnection>
-    <url>http://svn.apache.org/viewvc/maven/pom/tags/mahout-0.8/mahout-buildtools</url>
-  </scm>
 </project>
diff -uNar -x .git mahout/buildtools/src/test/resources/jaas.config mahout/buildtools/src/test/resources/jaas.config
--- mahout/buildtools/src/test/resources/jaas.config	1969-12-31 16:00:00.000000000 -0800
+++ mahout/buildtools/src/test/resources/jaas.config	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,3 @@
+hadoop-simple {
+  com.sun.security.auth.module.UnixLoginModule required ;
+}
diff -uNar -x .git mahout/buildtools/src/test/resources/java.policy mahout/buildtools/src/test/resources/java.policy
--- mahout/buildtools/src/test/resources/java.policy	1969-12-31 16:00:00.000000000 -0800
+++ mahout/buildtools/src/test/resources/java.policy	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,134 @@
+//
+// Licensed to the Apache Software Foundation (ASF) under one or more
+// contributor license agreements.  See the NOTICE file distributed with
+// this work for additional information regarding copyright ownership.
+// The ASF licenses this file to You under the Apache License, Version 2.0
+// (the "License"); you may not use this file except in compliance with
+// the License.  You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+
+// Allows unit tests to run with a Java Security Manager
+// Tested from Maven 3.0.3 with the Surfire 2.8.1 configuration:
+//
+// <argLine>-Djava.security.manager -Djava.security.policy=${basedir}/src/test/resources/java.policy</argLine>
+//
+// This policy file documents why each permission is granted by listing exceptions in comments.
+//
+// This policy file grants permission as narrowly as possible.
+ 
+grant {
+
+  permission java.io.FilePermission "${user.dir}/target/-", "read, write, delete";
+
+  permission java.util.PropertyPermission "user.dir", "write";
+  permission java.util.PropertyPermission "localRepository", "write";
+  permission java.util.PropertyPermission "mahout.test.directory", "write";
+  permission java.util.PropertyPermission "basedir", "write";
+  permission java.util.PropertyPermission "java.class.path", "read";
+  permission java.util.PropertyPermission "surefire.real.class.path", "write";
+  permission java.util.PropertyPermission "java.class.path", "write";
+  permission java.util.PropertyPermission "surefire.test.class.path", "write";
+  permission java.util.PropertyPermission "surefire.junit4.upgradecheck", "read";
+  permission java.util.PropertyPermission "*", "read,write";
+
+  permission java.lang.RuntimePermission "setIO";
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+  permission java.lang.RuntimePermission "exitVM.0";
+  permission java.lang.RuntimePermission "exitVM.1";
+  permission java.lang.RuntimePermission "exitVM.2";
+  permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
+  permission javax.security.auth.AuthPermission "getSubject";
+  permission javax.security.auth.AuthPermission "createLoginContext.*";
+  permission java.lang.RuntimePermission "setContextClassLoader";
+  permission java.lang.RuntimePermission "createClassLoader";
+  permission java.lang.RuntimePermission "getClassLoader";
+  permission java.lang.RuntimePermission "enableContextClasLoaderOverride";
+  permission java.lang.RuntimePermission "modifyThread";
+  permission java.lang.RuntimePermission "getProtectionDomain";
+  permission java.lang.RuntimePermission "getenv.*";
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
+  permission javax.security.auth.AuthPermission "doAs";
+  permission javax.security.auth.AuthPermission "doAsPriviledged";
+  permission javax.security.auth.AuthPermission "getSubject";
+  permission javax.security.auth.kerberos.ServicePermission "*", "initiate";
+  permission javax.security.auth.kerberos.ServicePermission "*", "accept";
+  permission java.lang.RuntimePermission "setDefaultUncaughtExceptionHandler";
+  permission java.lang.RuntimePermission "modifyThreadGroup";
+  permission java.lang.RuntimePermission "getStackTrace";
+};
+
+//
+// Licensed to the Apache Software Foundation (ASF) under one or more
+// contributor license agreements.  See the NOTICE file distributed with
+// this work for additional information regarding copyright ownership.
+// The ASF licenses this file to You under the Apache License, Version 2.0
+// (the "License"); you may not use this file except in compliance with
+// the License.  You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+
+// Allows unit tests to run with a Java Security Manager
+// Tested from Maven 3.0.3 with the Surfire 2.8.1 configuration:
+//
+// <argLine>-Djava.security.manager -Djava.security.policy=${basedir}/src/test/resources/java.policy</argLine>
+//
+// This policy file documents why each permission is granted by listing exceptions in comments.
+//
+// This policy file grants permission as narrowly as possible.
+ 
+grant {
+
+  permission java.io.FilePermission "${user.dir}/target/-", "read, write, delete";
+
+  permission java.util.PropertyPermission "user.dir", "write";
+  permission java.util.PropertyPermission "localRepository", "write";
+  permission java.util.PropertyPermission "mahout.test.directory", "write";
+  permission java.util.PropertyPermission "basedir", "write";
+  permission java.util.PropertyPermission "java.class.path", "read";
+  permission java.util.PropertyPermission "surefire.real.class.path", "write";
+  permission java.util.PropertyPermission "java.class.path", "write";
+  permission java.util.PropertyPermission "surefire.test.class.path", "write";
+  permission java.util.PropertyPermission "surefire.junit4.upgradecheck", "read";
+  permission java.util.PropertyPermission "*", "read,write";
+
+  permission java.lang.RuntimePermission "setIO";
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+  permission java.lang.RuntimePermission "exitVM.0";
+  permission java.lang.RuntimePermission "exitVM.1";
+  permission java.lang.RuntimePermission "exitVM.2";
+  permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
+  permission javax.security.auth.AuthPermission "getSubject";
+  permission javax.security.auth.AuthPermission "createLoginContext.*";
+  permission java.lang.RuntimePermission "setContextClassLoader";
+  permission java.lang.RuntimePermission "createClassLoader";
+  permission java.lang.RuntimePermission "getClassLoader";
+  permission java.lang.RuntimePermission "enableContextClasLoaderOverride";
+  permission java.lang.RuntimePermission "modifyThread";
+  permission java.lang.RuntimePermission "getProtectionDomain";
+  permission java.lang.RuntimePermission "getenv.*";
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
+  permission javax.security.auth.AuthPermission "doAs";
+  permission javax.security.auth.AuthPermission "doAsPriviledged";
+  permission javax.security.auth.AuthPermission "getSubject";
+  permission javax.security.auth.kerberos.ServicePermission "*", "initiate";
+  permission javax.security.auth.kerberos.ServicePermission "*", "accept";
+  permission java.lang.RuntimePermission "setDefaultUncaughtExceptionHandler";
+  permission java.lang.RuntimePermission "modifyThreadGroup";
+  permission java.lang.RuntimePermission "getStackTrace";
+};
+
diff -uNar -x .git mahout/core/pom.xml mahout/core/pom.xml
--- mahout/core/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/pom.xml	2014-03-29 01:03:13.000000000 -0700
@@ -23,7 +23,7 @@
   <parent>
     <groupId>org.apache.mahout</groupId>
     <artifactId>mahout</artifactId>
-    <version>0.8</version>
+    <version>1.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
 
@@ -147,11 +147,6 @@
     </dependency>
 
     <dependency>
-      <groupId>commons-io</groupId>
-      <artifactId>commons-io</artifactId>
-    </dependency>
-
-    <dependency>
       <groupId>com.thoughtworks.xstream</groupId>
       <artifactId>xstream</artifactId>
     </dependency>
@@ -183,6 +178,18 @@
     </dependency>
 
     <dependency>
+      <groupId>org.hamcrest</groupId>
+      <artifactId>hamcrest-all</artifactId>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>com.carrotsearch.randomizedtesting</groupId>
+      <artifactId>randomizedtesting-runner</artifactId>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
       <groupId>org.easymock</groupId>
       <artifactId>easymock</artifactId>
       <scope>test</scope>
@@ -193,23 +200,23 @@
       <artifactId>mrunit</artifactId>
       <version>1.0.0</version>
       <classifier>hadoop1</classifier>
+      <scope>test</scope>
     </dependency>
 
     <dependency>
-      <groupId>com.carrotsearch.randomizedtesting</groupId>
-      <artifactId>randomizedtesting-runner</artifactId>
-      <version>2.0.10</version>
-      <scope>test</scope>
+      <groupId>org.apache.solr</groupId>
+      <artifactId>solr-commons-csv</artifactId>
+      <version>3.5.0</version>
     </dependency>
 
   </dependencies>
   
   <profiles>
     <profile>
-      <id>hadoop-0.20</id>
+      <id>hadoop1</id>
       <activation>
         <property>
-          <name>!hadoop.version</name>
+          <name>!hadoop2.version</name>
         </property>
       </activation>
       <dependencies>
@@ -220,10 +227,10 @@
       </dependencies>
     </profile>
     <profile>
-      <id>hadoop-0.23</id>
+      <id>hadoop2</id>
       <activation>
         <property>
-          <name>hadoop.version</name>
+          <name>hadoop2.version</name>
         </property>
       </activation>
       <dependencies>
diff -uNar -x .git mahout/core/src/main/assembly/job.xml mahout/core/src/main/assembly/job.xml
--- mahout/core/src/main/assembly/job.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/assembly/job.xml	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
 <assembly
   xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
@@ -19,6 +36,7 @@
       </unpackOptions>
       <scope>runtime</scope>
       <outputDirectory>/</outputDirectory>
+      <useTransitiveFiltering>true</useTransitiveFiltering>
       <excludes>
         <exclude>org.apache.hadoop:hadoop-core</exclude>
       </excludes>
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/common/Refreshable.java mahout/core/src/main/java/org/apache/mahout/cf/taste/common/Refreshable.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/common/Refreshable.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/common/Refreshable.java	2014-03-29 01:03:13.000000000 -0700
@@ -38,13 +38,13 @@
   /**
    * <p>
    * Triggers "refresh" -- whatever that means -- of the implementation. The general contract is that any
-   *  should always leave itself in a consistent, operational state, and that the refresh
+   * {@link Refreshable} should always leave itself in a consistent, operational state, and that the refresh
    * atomically updates internal state from old to new.
    * </p>
    * 
    * @param alreadyRefreshed
-   *          s that are known to have already been
-   *          refreshed as a result of an initial call to a  method on some
+   *          {@link org.apache.mahout.cf.taste.common.Refreshable}s that are known to have already been
+   *          refreshed as a result of an initial call to a {@link #refresh(Collection)} method on some
    *          object. This ensure that objects in a refresh dependency graph aren't refreshed twice
    *          needlessly.
    */
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RecommenderEvaluator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RecommenderEvaluator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RecommenderEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RecommenderEvaluator.java	2014-03-29 01:03:13.000000000 -0700
@@ -44,7 +44,7 @@
    * recommendations, and the rest of the data is compared against estimated preference values to see how much
    * the {@link org.apache.mahout.cf.taste.recommender.Recommender}'s predicted preferences match the user's
    * real preferences. Specifically, for each user, this percentage of the user's ratings are used to produce
-   * recommendatinos, and for each user, the remaining preferences are compared against the user's real
+   * recommendations, and for each user, the remaining preferences are compared against the user's real
    * preferences.
    * </p>
    *
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RelevantItemsDataSplitter.java mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RelevantItemsDataSplitter.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RelevantItemsDataSplitter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/eval/RelevantItemsDataSplitter.java	2014-03-29 01:03:13.000000000 -0700
@@ -51,7 +51,7 @@
    * @param relevantItemIDs IDs of items considered relevant to that user
    * @param trainingUsers   the database of training preferences to which we will
    *                        append the ones for otherUserID.
-   * @param otherUserID     for whom we are adding preferences to the trianing model
+   * @param otherUserID     for whom we are adding preferences to the training model
    */
   void processOtherUser(long userID,
                         FastIDSet relevantItemIDs,
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtils.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtils.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtils.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtils.java	2014-03-29 01:03:13.000000000 -0700
@@ -51,10 +51,10 @@
   }
 
   /**
-   * Maps a long to an int
+   * Maps a long to an int with range of 0 to Integer.MAX_VALUE-1
    */
   public static int idToIndex(long id) {
-    return 0x7FFFFFFF & Longs.hashCode(id);
+    return 0x7FFFFFFF & Longs.hashCode(id) % 0x7FFFFFFE;
   }
 
   public static int readID(String token, boolean usesLongIDs) {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ALS.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ALS.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ALS.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ALS.java	2014-03-29 01:03:13.000000000 -0700
@@ -52,19 +52,6 @@
     return iterator.hasNext() ? iterator.next().get() : null;
   }
 
-  /**
-   * assumes that first entry always exists
-   *
-   * @param vectors
-   */
-  public static Vector sum(Iterator<VectorWritable> vectors) {
-    Vector sum = vectors.next().get();
-    while (vectors.hasNext()) {
-      sum.assign(vectors.next().get(), Functions.PLUS);
-    }
-    return sum;
-  }
-
   public static OpenIntObjectHashMap<Vector> readMatrixByRowsFromDistributedCache(int numEntities,
       Configuration conf) throws IOException {
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/FactorizationEvaluator.java	2014-03-29 01:03:13.000000000 -0700
@@ -50,7 +50,7 @@
 import org.apache.mahout.math.map.OpenIntObjectHashMap;
 
 /**
- * <p>Measures the root-mean-squared error of a ratring matrix factorization against a test set.</p>
+ * <p>Measures the root-mean-squared error of a rating matrix factorization against a test set.</p>
  *
  * <p>Command line arguments specific to this class are:</p>
  *
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/MultithreadedSharingMapper.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/MultithreadedSharingMapper.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/MultithreadedSharingMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/MultithreadedSharingMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,7 +17,10 @@
 
 package org.apache.mahout.cf.taste.hadoop.als;
 
+import com.google.common.base.Preconditions;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper;
 import org.apache.hadoop.util.ReflectionUtils;
 
@@ -34,17 +37,21 @@
  */
 public class MultithreadedSharingMapper<K1, V1, K2, V2> extends MultithreadedMapper<K1, V1, K2, V2> {
 
-  private static final String MAPPER_CLASS = "mapred.map.multithreadedrunner.class";
-
   @Override
   public void run(Context ctx) throws IOException, InterruptedException {
+    Class<Mapper<K1, V1, K2, V2>> mapperClass =
+        MultithreadedSharingMapper.getMapperClass((JobContext) ctx);
+    Preconditions.checkNotNull(mapperClass, "Could not find Multithreaded Mapper class.");
 
     Configuration conf = ctx.getConfiguration();
-
-    Class<? extends SharingMapper<K1,V1,K2,V2, ?>> mapperClass =
-        (Class<SharingMapper<K1,V1,K2,V2, ?>>) conf.getClass(MAPPER_CLASS, SharingMapper.class);
     // instantiate the mapper
-    SharingMapper<K1,V1,K2,V2, ?> mapper = ReflectionUtils.newInstance(mapperClass, conf);
+    Mapper<K1, V1, K2, V2> mapper1 = ReflectionUtils.newInstance(mapperClass, conf);
+    SharingMapper<K1, V1, K2, V2, ?> mapper = null;
+    if (mapper1 instanceof SharingMapper) {
+      mapper = (SharingMapper<K1, V1, K2, V2, ?>) mapper1;
+    }
+    Preconditions.checkNotNull(mapper, "Could not instantiate SharingMapper. Class was: %s",
+                               mapper1.getClass().getName());
 
     // single threaded call to setup the sharing mapper
     mapper.setupSharedInstance(ctx);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -51,6 +51,7 @@
 import org.apache.mahout.common.mapreduce.MergeVectorsCombiner;
 import org.apache.mahout.common.mapreduce.MergeVectorsReducer;
 import org.apache.mahout.common.mapreduce.TransposeMapper;
+import org.apache.mahout.common.mapreduce.VectorSumCombiner;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.RandomAccessSparseVector;
 import org.apache.mahout.math.SequentialAccessSparseVector;
@@ -58,6 +59,7 @@
 import org.apache.mahout.math.VarLongWritable;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -243,19 +245,6 @@
     }
   }
 
-  static class VectorSumCombiner
-      extends Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
-
-    private final VectorWritable result = new VectorWritable();
-
-    @Override
-    protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
-      throws IOException, InterruptedException {
-      result.set(ALS.sum(values.iterator()));
-      ctx.write(key, result);
-    }
-  }
-
   static class VectorSumReducer
       extends Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
 
@@ -264,7 +253,7 @@
     @Override
     protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
       throws IOException, InterruptedException {
-      Vector sum = ALS.sum(values.iterator());
+      Vector sum = Vectors.sum(values.iterator());
       result.set(new SequentialAccessSparseVector(sum));
       ctx.write(key, result);
     }
@@ -323,17 +312,16 @@
     // necessary for local execution in the same JVM only
     SharingMapper.reset();
 
-    int iterationNumber = currentIteration + 1;
     Class<? extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable>> solverMapperClassInternal;
     String name;
 
     if (implicitFeedback) {
       solverMapperClassInternal = SolveImplicitFeedbackMapper.class;
-      name = "Recompute " + matrixName + ", iteration (" + (iterationNumber + 1) + '/' + numIterations + "), "
+      name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), "
           + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, implicit feedback)";
     } else {
       solverMapperClassInternal = SolveExplicitFeedbackMapper.class;
-      name = "Recompute " + matrixName + ", iteration (" + (iterationNumber + 1) + '/' + numIterations + "), "
+      name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), "
           + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, explicit feedback)";
     }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveExplicitFeedbackMapper.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveExplicitFeedbackMapper.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveExplicitFeedbackMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveExplicitFeedbackMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -47,7 +47,7 @@
   protected void setup(Mapper.Context ctx) throws IOException, InterruptedException {
     lambda = Double.parseDouble(ctx.getConfiguration().get(ParallelALSFactorizationJob.LAMBDA));
     numFeatures = ctx.getConfiguration().getInt(ParallelALSFactorizationJob.NUM_FEATURES, -1);
-    Preconditions.checkArgument(numFeatures > 0, "numFeatures was not set correctly!");
+    Preconditions.checkArgument(numFeatures > 0, "numFeatures must be greater then 0!");
   }
 
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveImplicitFeedbackMapper.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveImplicitFeedbackMapper.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveImplicitFeedbackMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/als/SolveImplicitFeedbackMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -41,10 +41,10 @@
     int numFeatures = conf.getInt(ParallelALSFactorizationJob.NUM_FEATURES, -1);
     int numEntities = Integer.parseInt(conf.get(ParallelALSFactorizationJob.NUM_ENTITIES));
 
-    Preconditions.checkArgument(numFeatures > 0, "numFeatures was not set correctly!");
+    Preconditions.checkArgument(numFeatures > 0, "numFeatures must be greater then 0!");
 
     return new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha,
-        ALS.readMatrixByRowsFromDistributedCache(numEntities, conf));
+        ALS.readMatrixByRowsFromDistributedCache(numEntities, conf), 1);
   }
 
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -22,6 +22,7 @@
 import org.apache.hadoop.io.DoubleWritable;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.OutputFormat;
 import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
 import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
 import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
@@ -92,9 +93,10 @@
 public final class RecommenderJob extends AbstractJob {
 
   public static final String BOOLEAN_DATA = "booleanData";
+  public static final String DEFAULT_PREPARE_PATH = "preparePreferenceMatrix";
 
   private static final int DEFAULT_MAX_SIMILARITIES_PER_ITEM = 100;
-  private static final int DEFAULT_MAX_PREFS_PER_USER = 1000;
+  private static final int DEFAULT_MAX_PREFS = 500;
   private static final int DEFAULT_MIN_PREFS_PER_USER = 1;
 
   @Override
@@ -116,14 +118,16 @@
             + "(default: " + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));
     addOption("maxSimilaritiesPerItem", "m", "Maximum number of similarities considered per item ",
             String.valueOf(DEFAULT_MAX_SIMILARITIES_PER_ITEM));
-    addOption("maxPrefsPerUserInItemSimilarity", "mppuiis", "max number of preferences to consider per user in the " 
-            + "item similarity computation phase, users with more preferences will be sampled down (default: "
-        + DEFAULT_MAX_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MAX_PREFS_PER_USER));
+    addOption("maxPrefsInItemSimilarity", "mpiis", "max number of preferences to consider per user or item in the "
+            + "item similarity computation phase, users or items with more preferences will be sampled down (default: "
+        + DEFAULT_MAX_PREFS + ')', String.valueOf(DEFAULT_MAX_PREFS));
     addOption("similarityClassname", "s", "Name of distributed similarity measures class to instantiate, " 
             + "alternatively use one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')', true);
     addOption("threshold", "tr", "discard item pairs with a similarity value below this", false);
     addOption("outputPathForSimilarityMatrix", "opfsm", "write the item similarity matrix to this path (optional)",
         false);
+    addOption("randomSeed", null, "use this seed for sampling", false);
+    addFlag("sequencefileOutput", null, "write the output into a SequenceFile instead of a text file");
 
     Map<String, List<String>> parsedArgs = parseArguments(args);
     if (parsedArgs == null) {
@@ -138,14 +142,16 @@
     boolean booleanData = Boolean.valueOf(getOption("booleanData"));
     int maxPrefsPerUser = Integer.parseInt(getOption("maxPrefsPerUser"));
     int minPrefsPerUser = Integer.parseInt(getOption("minPrefsPerUser"));
-    int maxPrefsPerUserInItemSimilarity = Integer.parseInt(getOption("maxPrefsPerUserInItemSimilarity"));
+    int maxPrefsInItemSimilarity = Integer.parseInt(getOption("maxPrefsInItemSimilarity"));
     int maxSimilaritiesPerItem = Integer.parseInt(getOption("maxSimilaritiesPerItem"));
     String similarityClassname = getOption("similarityClassname");
     double threshold = hasOption("threshold")
         ? Double.parseDouble(getOption("threshold")) : RowSimilarityJob.NO_THRESHOLD;
+    long randomSeed = hasOption("randomSeed")
+        ? Long.parseLong(getOption("randomSeed")) : RowSimilarityJob.NO_FIXED_RANDOM_SEED;
 
 
-    Path prepPath = getTempPath("preparePreferenceMatrix");
+    Path prepPath = getTempPath(DEFAULT_PREPARE_PATH);
     Path similarityMatrixPath = getTempPath("similarityMatrix");
     Path explicitFilterPath = getTempPath("explicitFilterPath");
     Path partialMultiplyPath = getTempPath("partialMultiply");
@@ -158,7 +164,6 @@
       ToolRunner.run(getConf(), new PreparePreferenceMatrixJob(), new String[]{
         "--input", getInputPath().toString(),
         "--output", prepPath.toString(),
-        "--maxPrefsPerUser", String.valueOf(maxPrefsPerUserInItemSimilarity),
         "--minPrefsPerUser", String.valueOf(minPrefsPerUser),
         "--booleanData", String.valueOf(booleanData),
         "--tempDir", getTempPath().toString(),
@@ -176,17 +181,18 @@
                 PathType.LIST, null, getConf());
       }
 
-      /* Once DistributedRowMatrix uses the hadoop 0.20 API, we should refactor this call to something like
-       * new DistributedRowMatrix(...).rowSimilarity(...) */
       //calculate the co-occurrence matrix
       ToolRunner.run(getConf(), new RowSimilarityJob(), new String[]{
         "--input", new Path(prepPath, PreparePreferenceMatrixJob.RATING_MATRIX).toString(),
         "--output", similarityMatrixPath.toString(),
         "--numberOfColumns", String.valueOf(numberOfUsers),
         "--similarityClassname", similarityClassname,
+        "--maxObservationsPerRow", String.valueOf(maxPrefsInItemSimilarity),
+        "--maxObservationsPerColumn", String.valueOf(maxPrefsInItemSimilarity),
         "--maxSimilaritiesPerRow", String.valueOf(maxSimilaritiesPerItem),
         "--excludeSelfSimilarity", String.valueOf(Boolean.TRUE),
         "--threshold", String.valueOf(threshold),
+        "--randomSeed", String.valueOf(randomSeed),
         "--tempDir", getTempPath().toString(),
       });
 
@@ -255,12 +261,16 @@
       if (filterFile != null) {
         aggregateAndRecommendInput += "," + explicitFilterPath;
       }
+
+      Class<? extends OutputFormat> outputFormat = parsedArgs.containsKey("--sequencefileOutput")
+          ? SequenceFileOutputFormat.class : TextOutputFormat.class;
+
       //extract out the recommendations
       Job aggregateAndRecommend = prepareJob(
               new Path(aggregateAndRecommendInput), outputPath, SequenceFileInputFormat.class,
               PartialMultiplyMapper.class, VarLongWritable.class, PrefAndSimilarityColumnWritable.class,
               AggregateAndRecommendReducer.class, VarLongWritable.class, RecommendedItemsWritable.class,
-              TextOutputFormat.class);
+              outputFormat);
       Configuration aggregateAndRecommendConf = aggregateAndRecommend.getConfiguration();
       if (itemsFile != null) {
         aggregateAndRecommendConf.set(AggregateAndRecommendReducer.ITEMS_FILE, itemsFile);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/PreparePreferenceMatrixJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -56,8 +56,6 @@
 
     addInputOption();
     addOutputOption();
-    addOption("maxPrefsPerUser", "mppu", "max number of preferences to consider per user, " 
-            + "users with more preferences will be sampled down");
     addOption("minPrefsPerUser", "mp", "ignore users with less preferences than this "
             + "(default: " + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));
     addOption("booleanData", "b", "Treat input as without pref values", Boolean.FALSE.toString());
@@ -107,12 +105,6 @@
             IntWritable.class, VectorWritable.class);
     toItemVectors.setCombinerClass(ToItemVectorsReducer.class);
 
-    /* configure sampling regarding the uservectors */
-    if (hasOption("maxPrefsPerUser")) {
-      int samplingSize = Integer.parseInt(getOption("maxPrefsPerUser"));
-      toItemVectors.getConfiguration().setInt(ToItemVectorsMapper.SAMPLE_SIZE, samplingSize);
-    }
-
     succeeded = toItemVectors.waitForCompletion(true);
     if (!succeeded) {
       return -1;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsMapper.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsMapper.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -24,38 +24,20 @@
 import org.apache.mahout.math.VarLongWritable;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors;
 
 import java.io.IOException;
 
 public class ToItemVectorsMapper
     extends Mapper<VarLongWritable,VectorWritable,IntWritable,VectorWritable> {
 
-  public static final String SAMPLE_SIZE = ToItemVectorsMapper.class + ".sampleSize";
-
-  enum Elements {
-    USER_RATINGS_USED, USER_RATINGS_NEGLECTED
-  }
-
   private final IntWritable itemID = new IntWritable();
   private final VectorWritable itemVectorWritable = new VectorWritable();
 
-  private int sampleSize;
-
-  @Override
-  protected void setup(Context ctx) throws IOException, InterruptedException {
-    sampleSize = ctx.getConfiguration().getInt(SAMPLE_SIZE, Integer.MAX_VALUE);
-  }
-
   @Override
   protected void map(VarLongWritable rowIndex, VectorWritable vectorWritable, Context ctx)
     throws IOException, InterruptedException {
     Vector userRatings = vectorWritable.get();
 
-    int numElementsBeforeSampling = userRatings.getNumNondefaultElements();
-    userRatings = Vectors.maybeSample(userRatings, sampleSize);
-    int numElementsAfterSampling = userRatings.getNumNondefaultElements();
-
     int column = TasteHadoopUtils.idToIndex(rowIndex.get());
 
     itemVectorWritable.setWritesLaxPrecision(true);
@@ -69,9 +51,6 @@
       // reset vector for reuse
       itemVector.setQuick(elem.index(), 0.0);
     }
-
-    ctx.getCounter(Elements.USER_RATINGS_USED).increment(numElementsAfterSampling);
-    ctx.getCounter(Elements.USER_RATINGS_NEGLECTED).increment(numElementsBeforeSampling - numElementsAfterSampling);
   }
 
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsReducer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsReducer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/preparation/ToItemVectorsReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -23,12 +23,14 @@
 
 import java.io.IOException;
 
-class ToItemVectorsReducer extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
+public class ToItemVectorsReducer extends Reducer<IntWritable,VectorWritable,IntWritable,VectorWritable> {
 
   private final VectorWritable merged = new VectorWritable();
+
   @Override
   protected void reduce(IntWritable row, Iterable<VectorWritable> vectors, Context ctx)
     throws IOException, InterruptedException {
+
     merged.setWritesLaxPrecision(true);
     merged.set(VectorWritable.mergeToVector(vectors.iterator()));
     ctx.write(row, merged);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,152 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.pseudo;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.cf.taste.hadoop.RecommendedItemsWritable;
-import org.apache.mahout.math.VarLongWritable;
-
-/**
- * <p>
- * This job runs a "pseudo-distributed" recommendation process on Hadoop. It merely runs many
- * {@link org.apache.mahout.cf.taste.recommender.Recommender} instances on Hadoop,
- * where each instance is a normal non-distributed implementation.
- * </p>
- *
- * <p>This class configures and runs a {@link RecommenderReducer} using Hadoop.</p>
- *
- * <p>Command line arguments specific to this class are:</p>
- *
- * <ol>
- * <li>-Dmapred.input.dir=(path): Location of a data model file containing preference data, suitable for use with
- * {@link org.apache.mahout.cf.taste.impl.model.file.FileDataModel}</li>
- * <li>-Dmapred.output.dir=(path): output path where recommender output should go</li>
- * <li>--recommenderClassName (string): Fully-qualified class name of
- * {@link org.apache.mahout.cf.taste.recommender.Recommender} to use to make recommendations.
- * Note that it must have a constructor which takes a {@link org.apache.mahout.cf.taste.model.DataModel}
- * argument.</li>
- * <li>--numRecommendations (integer): Number of recommendations to compute per user</li>
- * <li>--usersFile (path): file containing user IDs to recommend for (optional)</li>
- * </ol>
- *
- * <p>General command line options are documented in {@link AbstractJob}.</p>
- *
- * <p>Note that because of how Hadoop parses arguments, all "-D" arguments must appear before all other
- * arguments.</p>
- *
- * <p>
- * For example, to get started trying this out, set up Hadoop in a pseudo-distributed manner:
- * http://hadoop.apache.org/common/docs/current/quickstart.html You can stop at the point where it instructs
- * you to copy files into HDFS.
- * </p>
- *
- * <p>
- * Assume your preference data file is {@code input.csv}. You will also need to create a file containing
- * all user IDs to write recommendations for, as something like {@code users.txt}. Place this input on
- * HDFS like so:
- * </p>
- *
- * {@code hadoop fs -put input.csv input/input.csv; hadoop fs -put users.txt input/users.txt * }
- *
- * <p>
- * Build Mahout code with {@code mvn package} in the core/ directory. Locate
- * {@code target/mahout-core-X.Y-SNAPSHOT.job}. This is a JAR file; copy it out to a convenient location
- * and name it {@code recommender.jar}.
- * </p>
- *
- * <p>
- * Now add your own custom recommender code and dependencies. Your IDE produced compiled .class files
- * somewhere and they need to be packaged up as well:
- * </p>
- *
- * {@code jar uf recommender.jar -C (your classes directory) . * }
- *
- * <p>
- * And launch:
- * </p>
- *
- * {@code hadoop jar recommender.jar \
- *   org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob \
- *   -Dmapred.input.dir=input/users.csv \
- *   -Dmapred.output.dir=output \
- *   --recommenderClassName your.project.Recommender \
- *   --numRecommendations 10  *
- * }
- */
-@Deprecated
-public final class RecommenderJob extends AbstractJob {
-  
-  @Override
-  public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
-
-    addInputOption();
-    addOutputOption();
-    addOption("recommenderClassName", "r", "Name of recommender class to instantiate");
-    addOption("numRecommendations", "n", "Number of recommendations per user", "10");
-    addOption("usersFile", "u", "File of users to recommend for", null);
-    
-    Map<String,List<String>> parsedArgs = parseArguments(args);
-    if (parsedArgs == null) {
-      return -1;
-    }
-
-    Path inputFile = getInputPath();
-    Path outputPath = getOutputPath();
-    Path usersFile = hasOption("usersFile") ? new Path(getOption("usersFile")) : inputFile;
-    
-    String recommendClassName = getOption("recommenderClassName");
-    int recommendationsPerUser = Integer.parseInt(getOption("numRecommendations"));
-    
-    Job job = prepareJob(usersFile,
-                         outputPath,
-                         TextInputFormat.class,
-                         UserIDsMapper.class,
-                         VarLongWritable.class,
-                         NullWritable.class,
-                         RecommenderReducer.class,
-                         VarLongWritable.class,
-                         RecommendedItemsWritable.class,
-                         TextOutputFormat.class);
-    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
-    Configuration jobConf = job.getConfiguration();
-    jobConf.set(RecommenderReducer.RECOMMENDER_CLASS_NAME, recommendClassName);
-    jobConf.setInt(RecommenderReducer.RECOMMENDATIONS_PER_USER, recommendationsPerUser);
-    jobConf.set(RecommenderReducer.DATA_MODEL_FILE, inputFile.toString());
-
-    boolean succeeded = job.waitForCompletion(true);
-    return succeeded ? 0 : -1;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new RecommenderJob(), args);
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderReducer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderReducer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/RecommenderReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,113 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.pseudo;
-
-import java.io.File;
-import java.io.IOException;
-import java.lang.reflect.Constructor;
-import java.lang.reflect.InvocationTargetException;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.hadoop.RecommendedItemsWritable;
-import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.apache.mahout.math.VarLongWritable;
-
-/**
- * <p>
- * The {@link Reducer} which takes as input the user IDs parsed out by the map phase, and for each unique user
- * ID, computes recommendations with the configured {@link Recommender}. The results are output as
- * {@link RecommendedItemsWritable}.
- * </p>
- * 
- * @see RecommenderJob
- */
-@Deprecated
-final class RecommenderReducer extends
-    Reducer<VarLongWritable,NullWritable,VarLongWritable,RecommendedItemsWritable> {
-  
-  static final String RECOMMENDER_CLASS_NAME = "recommenderClassName";
-  static final String RECOMMENDATIONS_PER_USER = "recommendationsPerUser";
-  static final String DATA_MODEL_FILE = "dataModelFile";
-
-  private Recommender recommender;
-  private int recommendationsPerUser;
-
-  @Override
-  protected void setup(Context context) throws IOException {
-    Configuration jobConf = context.getConfiguration();
-    String dataModelFile = jobConf.get(DATA_MODEL_FILE);
-    String recommenderClassName = jobConf.get(RECOMMENDER_CLASS_NAME);
-    Path dataModelPath = new Path(dataModelFile);
-    FileSystem fs = FileSystem.get(dataModelPath.toUri(), jobConf);
-    File tempDataFile = File.createTempFile("mahout-taste-hadoop", "txt");
-    tempDataFile.deleteOnExit();
-    fs.copyToLocalFile(dataModelPath, new Path(tempDataFile.getAbsolutePath()));
-    FileDataModel fileDataModel = new FileDataModel(tempDataFile);
-
-    try {
-      Class<? extends Recommender> recommenderClass = Class.forName(recommenderClassName).asSubclass(
-        Recommender.class);
-      Constructor<? extends Recommender> constructor = recommenderClass.getConstructor(DataModel.class);
-      recommender = constructor.newInstance(fileDataModel);
-    } catch (NoSuchMethodException nsme) {
-      throw new IllegalStateException(nsme);
-    } catch (ClassNotFoundException cnfe) {
-      throw new IllegalStateException(cnfe);
-    } catch (InstantiationException ie) {
-      throw new IllegalStateException(ie);
-    } catch (IllegalAccessException iae) {
-      throw new IllegalStateException(iae);
-    } catch (InvocationTargetException ite) {
-      throw new IllegalStateException(ite.getCause());
-    }
-    recommendationsPerUser = jobConf.getInt(RECOMMENDATIONS_PER_USER, 10);
-  }
-  
-  @Override
-  protected void reduce(VarLongWritable key,
-                        Iterable<NullWritable> values,
-                        Context context) throws IOException, InterruptedException {
-    long userID = key.get();
-    List<RecommendedItem> recommendedItems;
-    try {
-      recommendedItems = recommender.recommend(userID, recommendationsPerUser);
-    } catch (TasteException te) {
-      throw new IllegalStateException(te);
-    }
-    Iterator<RecommendedItem> it = recommendedItems.iterator();
-    while (it.hasNext()) {
-      if (Float.isNaN(it.next().getValue())) {
-        it.remove();
-      }
-    }
-    RecommendedItemsWritable writable = new RecommendedItemsWritable(recommendedItems);
-    context.write(key, writable);
-    context.getCounter(ReducerMetrics.USERS_PROCESSED).increment(1L);
-    context.getCounter(ReducerMetrics.RECOMMENDATIONS_MADE).increment(recommendedItems.size());
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/ReducerMetrics.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/ReducerMetrics.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/ReducerMetrics.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/ReducerMetrics.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.pseudo;
-
-/** Custom metrics collected by {@link RecommenderReducer}. */
-@Deprecated
-public enum ReducerMetrics {
-  
-  /** Number of unique users for which recommendations were produced */
-  USERS_PROCESSED,
-  /** Number of items recommended to those users */
-  RECOMMENDATIONS_MADE
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/UserIDsMapper.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/UserIDsMapper.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/UserIDsMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/pseudo/UserIDsMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,47 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.pseudo;
-
-import java.io.IOException;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.math.VarLongWritable;
-
-/**
- * Extracts and emits all user IDs from the users file, or input file.
- */
-@Deprecated
-final class UserIDsMapper extends
-    Mapper<LongWritable,Text, VarLongWritable,NullWritable> {
-
-  private static final Pattern DELIMITER = Pattern.compile("[\t,]");
-
-  @Override
-  protected void map(LongWritable key,
-                     Text value,
-                     Context context) throws IOException, InterruptedException {
-    String[] tokens = DELIMITER.split(value.toString());
-    long userID = Long.parseLong(tokens[0]);
-    context.write(new VarLongWritable(userID), NullWritable.get());
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/similarity/item/ItemSimilarityJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -86,7 +86,7 @@
   public static final String MAX_SIMILARITIES_PER_ITEM = ItemSimilarityJob.class.getName() + ".maxSimilarItemsPerItem";
 
   private static final int DEFAULT_MAX_SIMILAR_ITEMS_PER_ITEM = 100;
-  private static final int DEFAULT_MAX_PREFS_PER_USER = 1000;
+  private static final int DEFAULT_MAX_PREFS = 500;
   private static final int DEFAULT_MIN_PREFS_PER_USER = 1;
 
   public static void main(String[] args) throws Exception {
@@ -103,13 +103,14 @@
     addOption("maxSimilaritiesPerItem", "m", "try to cap the number of similar items per item to this number "
         + "(default: " + DEFAULT_MAX_SIMILAR_ITEMS_PER_ITEM + ')',
         String.valueOf(DEFAULT_MAX_SIMILAR_ITEMS_PER_ITEM));
-    addOption("maxPrefsPerUser", "mppu", "max number of preferences to consider per user, " 
-        + "users with more preferences will be sampled down (default: " + DEFAULT_MAX_PREFS_PER_USER + ')',
-        String.valueOf(DEFAULT_MAX_PREFS_PER_USER));
+    addOption("maxPrefs", "mppu", "max number of preferences to consider per user or item, " 
+        + "users or items with more preferences will be sampled down (default: " + DEFAULT_MAX_PREFS + ')',
+        String.valueOf(DEFAULT_MAX_PREFS));
     addOption("minPrefsPerUser", "mp", "ignore users with less preferences than this "
         + "(default: " + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));
     addOption("booleanData", "b", "Treat input as without pref values", String.valueOf(Boolean.FALSE));
     addOption("threshold", "tr", "discard item pairs with a similarity value below this", false);
+    addOption("randomSeed", null, "use this seed for sampling", false);
 
     Map<String,List<String>> parsedArgs = parseArguments(args);
     if (parsedArgs == null) {
@@ -118,12 +119,14 @@
 
     String similarityClassName = getOption("similarityClassname");
     int maxSimilarItemsPerItem = Integer.parseInt(getOption("maxSimilaritiesPerItem"));
-    int maxPrefsPerUser = Integer.parseInt(getOption("maxPrefsPerUser"));
+    int maxPrefs = Integer.parseInt(getOption("maxPrefs"));
     int minPrefsPerUser = Integer.parseInt(getOption("minPrefsPerUser"));
     boolean booleanData = Boolean.valueOf(getOption("booleanData"));
 
     double threshold = hasOption("threshold")
         ? Double.parseDouble(getOption("threshold")) : RowSimilarityJob.NO_THRESHOLD;
+    long randomSeed = hasOption("randomSeed")
+        ? Long.parseLong(getOption("randomSeed")) : RowSimilarityJob.NO_FIXED_RANDOM_SEED;
 
     Path similarityMatrixPath = getTempPath("similarityMatrix");
     Path prepPath = getTempPath("prepareRatingMatrix");
@@ -134,7 +137,6 @@
       ToolRunner.run(getConf(), new PreparePreferenceMatrixJob(), new String[] {
         "--input", getInputPath().toString(),
         "--output", prepPath.toString(),
-        "--maxPrefsPerUser", String.valueOf(maxPrefsPerUser),
         "--minPrefsPerUser", String.valueOf(minPrefsPerUser),
         "--booleanData", String.valueOf(booleanData),
         "--tempDir", getTempPath().toString(),
@@ -142,17 +144,19 @@
     }
 
     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
-      int numberOfUsers = HadoopUtil.readInt(new Path(prepPath, PreparePreferenceMatrixJob.NUM_USERS),
-          getConf());
+      int numberOfUsers = HadoopUtil.readInt(new Path(prepPath, PreparePreferenceMatrixJob.NUM_USERS), getConf());
 
       ToolRunner.run(getConf(), new RowSimilarityJob(), new String[] {
         "--input", new Path(prepPath, PreparePreferenceMatrixJob.RATING_MATRIX).toString(),
         "--output", similarityMatrixPath.toString(),
         "--numberOfColumns", String.valueOf(numberOfUsers),
         "--similarityClassname", similarityClassName,
+        "--maxObservationsPerRow", String.valueOf(maxPrefs),
+        "--maxObservationsPerColumn", String.valueOf(maxPrefs),
         "--maxSimilaritiesPerRow", String.valueOf(maxSimilarItemsPerItem),
         "--excludeSelfSimilarity", String.valueOf(Boolean.TRUE),
         "--threshold", String.valueOf(threshold),
+        "--randomSeed", String.valueOf(randomSeed),
         "--tempDir", getTempPath().toString(),
       });
     }
@@ -186,7 +190,7 @@
       maxSimilarItemsPerItem = conf.getInt(MAX_SIMILARITIES_PER_ITEM, -1);
       indexItemIDMap = TasteHadoopUtils.readIDIndexMap(conf.get(ITEM_ID_INDEX_PATH_STR), conf);
 
-      Preconditions.checkArgument(maxSimilarItemsPerItem > 0, "maxSimilarItemsPerItem was not correctly set!");
+      Preconditions.checkArgument(maxSimilarItemsPerItem > 0, "maxSimilarItemsPerItem must be greater then 0!");
     }
 
     @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/ByItemIDComparator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/ByItemIDComparator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/ByItemIDComparator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/ByItemIDComparator.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.slopeone;
-
-import java.io.Serializable;
-import java.util.Comparator;
-
-import org.apache.mahout.cf.taste.hadoop.EntityPrefWritable;
-
-@Deprecated
-final class ByItemIDComparator implements Comparator<EntityPrefWritable>, Serializable {
-  
-  private static final Comparator<EntityPrefWritable> INSTANCE = new ByItemIDComparator();
-  
-  public static Comparator<EntityPrefWritable> getInstance() {
-    return INSTANCE;
-  }
-  
-  @Override
-  public int compare(EntityPrefWritable a, EntityPrefWritable b) {
-    long idA = a.getID();
-    long idB = b.getID();
-    return idA < idB ? -1 : idA > idB ? 1 : 0;
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/FullRunningAverageAndStdDevWritable.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/FullRunningAverageAndStdDevWritable.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/FullRunningAverageAndStdDevWritable.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/FullRunningAverageAndStdDevWritable.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,60 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.slopeone;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverageAndStdDev;
-import org.apache.mahout.math.Varint;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-@Deprecated
-final class FullRunningAverageAndStdDevWritable implements Writable {
-  
-  private FullRunningAverageAndStdDev average;
-  
-  FullRunningAverageAndStdDevWritable(FullRunningAverageAndStdDev average) {
-    this.average = average;
-  }
-
-  @Override
-  public String toString() {
-    return String.valueOf(average.getAverage())
-        + '\t' + average.getCount() + '\t' + average.getMk() + '\t' + average.getSk();
-  }
-  
-  @Override
-  public void write(DataOutput dataOutput) throws IOException {
-    Varint.writeUnsignedVarInt(average.getCount(), dataOutput);
-    dataOutput.writeDouble(average.getAverage());
-    dataOutput.writeDouble(average.getMk());
-    dataOutput.writeDouble(average.getSk());
-  }
-
-  @Override
-  public void readFields(DataInput dataInput) throws IOException {
-    int count = Varint.readUnsignedVarInt(dataInput);
-    double diff = dataInput.readDouble();
-    double mk = dataInput.readDouble();
-    double sk = dataInput.readDouble();
-    average = new FullRunningAverageAndStdDev(count, diff, mk, sk);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneAverageDiffsJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,104 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.slopeone;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.cf.taste.hadoop.EntityEntityWritable;
-import org.apache.mahout.cf.taste.hadoop.EntityPrefWritable;
-import org.apache.mahout.cf.taste.hadoop.ToItemPrefsMapper;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.math.VarLongWritable;
-
-@Deprecated
-public final class SlopeOneAverageDiffsJob extends AbstractJob {
-  
-  @Override
-  public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
-    
-    addInputOption();
-    addOutputOption();
-
-    Map<String,List<String>> parsedArgs = parseArguments(args);
-    if (parsedArgs == null) {
-      return -1;
-    }
-    
-    Path prefsFile = getInputPath();
-    Path outputPath = getOutputPath();
-    Path averagesOutputPath = new Path(getOption("--tempDir"));
-
-    AtomicInteger currentPhase = new AtomicInteger();
-
-    if (shouldRunNextPhase(parsedArgs, currentPhase)) {
-      Job prefsToDiffsJob = prepareJob(prefsFile,
-                                       averagesOutputPath,
-                                       TextInputFormat.class,
-                                       ToItemPrefsMapper.class,
-                                       VarLongWritable.class,
-                                       EntityPrefWritable.class,
-                                       SlopeOnePrefsToDiffsReducer.class,
-                                       EntityEntityWritable.class,
-                                       FloatWritable.class,
-                                       SequenceFileOutputFormat.class);
-      boolean succeeded = prefsToDiffsJob.waitForCompletion(true);
-      if (!succeeded) {
-        return -1;
-      }
-    }
-
-
-    if (shouldRunNextPhase(parsedArgs, currentPhase)) {
-      Job diffsToAveragesJob = prepareJob(averagesOutputPath,
-                                          outputPath,
-                                          SequenceFileInputFormat.class,
-                                          Mapper.class,
-                                          EntityEntityWritable.class,
-                                          FloatWritable.class,
-                                          SlopeOneDiffsToAveragesReducer.class,
-                                          EntityEntityWritable.class,
-                                          FullRunningAverageAndStdDevWritable.class,
-                                          TextOutputFormat.class);
-      FileOutputFormat.setOutputCompressorClass(diffsToAveragesJob, GzipCodec.class);
-      boolean succeeded = diffsToAveragesJob.waitForCompletion(true);
-      if (!succeeded) {
-        return -1;
-      }
-    }
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new SlopeOneAverageDiffsJob(), args);
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneDiffsToAveragesReducer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneDiffsToAveragesReducer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneDiffsToAveragesReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOneDiffsToAveragesReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.slopeone;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.cf.taste.hadoop.EntityEntityWritable;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverageAndStdDev;
-
-@Deprecated
-final class SlopeOneDiffsToAveragesReducer extends
-    Reducer<EntityEntityWritable,FloatWritable,EntityEntityWritable,FullRunningAverageAndStdDevWritable> {
-  
-  @Override
-  protected void reduce(EntityEntityWritable key,
-                        Iterable<FloatWritable> values,
-                        Context context) throws IOException, InterruptedException {
-    FullRunningAverageAndStdDev average = new FullRunningAverageAndStdDev();
-    for (FloatWritable value : values) {
-      average.addDatum(value.get());
-    }
-    context.write(key, new FullRunningAverageAndStdDevWritable(average));
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOnePrefsToDiffsReducer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOnePrefsToDiffsReducer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOnePrefsToDiffsReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/hadoop/slopeone/SlopeOnePrefsToDiffsReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,58 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.hadoop.slopeone;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.cf.taste.hadoop.EntityEntityWritable;
-import org.apache.mahout.cf.taste.hadoop.EntityPrefWritable;
-import org.apache.mahout.math.VarLongWritable;
-
-@Deprecated
-final class SlopeOnePrefsToDiffsReducer extends
-    Reducer<VarLongWritable,EntityPrefWritable,EntityEntityWritable,FloatWritable> {
-  
-  @Override
-  protected void reduce(VarLongWritable key,
-                        Iterable<EntityPrefWritable> values,
-                        Context context) throws IOException, InterruptedException {
-    List<EntityPrefWritable> prefs = Lists.newArrayList();
-    for (EntityPrefWritable writable : values) {
-      prefs.add(new EntityPrefWritable(writable));
-    }
-    Collections.sort(prefs, ByItemIDComparator.getInstance());
-    int size = prefs.size();
-    for (int i = 0; i < size; i++) {
-      EntityPrefWritable first = prefs.get(i);
-      long itemAID = first.getID();
-      float itemAValue = first.getPrefValue();
-      for (int j = i + 1; j < size; j++) {
-        EntityPrefWritable second = prefs.get(j);
-        long itemBID = second.getID();
-        float itemBValue = second.getPrefValue();
-        context.write(new EntityEntityWritable(itemAID, itemBID), new FloatWritable(itemBValue - itemAValue));
-      }
-    }
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/BitSet.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/BitSet.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/BitSet.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/BitSet.java	2014-03-29 01:03:13.000000000 -0700
@@ -18,6 +18,7 @@
 package org.apache.mahout.cf.taste.impl.common;
 
 import java.io.Serializable;
+import java.util.Arrays;
 
 /** A simplified and streamlined version of {@link java.util.BitSet}. */
 final class BitSet implements Serializable, Cloneable {
@@ -60,7 +61,21 @@
   
   @Override
   public BitSet clone() {
-    return new BitSet(bits);
+    return new BitSet(bits.clone());
+  }
+
+  @Override
+  public int hashCode() {
+    return Arrays.hashCode(bits);
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (!(o instanceof BitSet)) {
+      return false;
+    }
+    BitSet other = (BitSet) o;
+    return Arrays.equals(bits, other.bits);
   }
   
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/Cache.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/Cache.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/Cache.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/Cache.java	2014-03-29 01:03:13.000000000 -0700
@@ -24,7 +24,7 @@
 
 /**
  * <p>
- * An efficient Map-like class which caches values for keys. Values are not "put" into a ;
+ * An efficient Map-like class which caches values for keys. Values are not "put" into a {@link Cache};
  * instead the caller supplies the instance with an implementation of {@link Retriever} which can load the
  * value for a given key.
  * </p>
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastByIDMap.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastByIDMap.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastByIDMap.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastByIDMap.java	2014-03-29 01:03:13.000000000 -0700
@@ -53,7 +53,7 @@
   private BitSet recentlyAccessed;
   private final boolean countingAccesses;
   
-  /** Creates a new  with default capacity. */
+  /** Creates a new {@link FastByIDMap} with default capacity. */
   public FastByIDMap() {
     this(2, NO_MAX_SIZE);
   }
@@ -71,7 +71,7 @@
   }
 
   /**
-   * Creates a new  whose capacity can accommodate the given number of entries without rehash.
+   * Creates a new {@link FastByIDMap} whose capacity can accommodate the given number of entries without rehash.
    * 
    * @param size desired capacity
    * @param maxSize max capacity
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastIDSet.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastIDSet.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastIDSet.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastIDSet.java	2014-03-29 01:03:13.000000000 -0700
@@ -42,7 +42,7 @@
   private int numEntries;
   private int numSlotsUsed;
   
-  /** Creates a new  with default capacity. */
+  /** Creates a new {@link FastIDSet} with default capacity. */
   public FastIDSet() {
     this(2);
   }
@@ -263,10 +263,10 @@
   }
   
   /**
-   * Convenience method to quickly compute just the size of the intersection with another .
+   * Convenience method to quickly compute just the size of the intersection with another {@link FastIDSet}.
    * 
    * @param other
-   *           to intersect with
+   *          {@link FastIDSet} to intersect with
    * @return number of elements in intersection
    */
   public int intersectionSize(FastIDSet other) {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastMap.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastMap.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastMap.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FastMap.java	2014-03-29 01:03:13.000000000 -0700
@@ -68,7 +68,7 @@
   private BitSet recentlyAccessed;
   private final boolean countingAccesses;
   
-  /** Creates a new  with default capacity. */
+  /** Creates a new {@link FastMap} with default capacity. */
   public FastMap() {
     this(2, NO_MAX_SIZE);
   }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FullRunningAverage.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FullRunningAverage.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FullRunningAverage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/FullRunningAverage.java	2014-03-29 01:03:13.000000000 -0700
@@ -21,7 +21,7 @@
 
 /**
  * <p>
- * A simple class that can keep track of a running avearage of a series of numbers. One can add to or remove
+ * A simple class that can keep track of a running average of a series of numbers. One can add to or remove
  * from the series, as well as update a datum in the series. The class does not actually keep track of the
  * series of values, just its running average, so it doesn't even matter if you remove/change a value that
  * wasn't added.
@@ -38,7 +38,7 @@
 
   public FullRunningAverage(int count, double average) {
     this.count = count;
-    this.average = average;    
+    this.average = average;
   }
 
   /**
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/LongPrimitiveArrayIterator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/LongPrimitiveArrayIterator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/LongPrimitiveArrayIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/LongPrimitiveArrayIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -32,7 +32,7 @@
   
   /**
    * <p>
-   * Creates an  over an entire array.
+   * Creates an {@link LongPrimitiveArrayIterator} over an entire array.
    * </p>
    * 
    * @param array
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/RefreshHelper.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/RefreshHelper.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/RefreshHelper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/RefreshHelper.java	2014-03-29 01:03:13.000000000 -0700
@@ -66,7 +66,7 @@
   }
   
   /**
-   * Typically this is called in  and is the entire body of
+   * Typically this is called in {@link Refreshable#refresh(java.util.Collection)} and is the entire body of
    * that method.
    */
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/SamplingLongPrimitiveIterator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/SamplingLongPrimitiveIterator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/SamplingLongPrimitiveIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/SamplingLongPrimitiveIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -41,7 +41,7 @@
 
   public SamplingLongPrimitiveIterator(RandomWrapper random, LongPrimitiveIterator delegate, double samplingRate) {
     Preconditions.checkNotNull(delegate);
-    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0);
+    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0, "Must be: 0.0 < samplingRate <= 1.0");
     // Geometric distribution is special case of negative binomial (aka Pascal) with r=1:
     geometricDistribution = new PascalDistribution(random.getRandomGenerator(), 1, samplingRate);
     this.delegate = delegate;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/WeightedRunningAverage.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/WeightedRunningAverage.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/WeightedRunningAverage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/common/WeightedRunningAverage.java	2014-03-29 01:03:13.000000000 -0700
@@ -68,7 +68,7 @@
   }
 
   public synchronized void changeDatum(double delta, double weight) {
-    Preconditions.checkArgument(weight <= totalWeight);
+    Preconditions.checkArgument(weight <= totalWeight, "weight must be <= totalWeight");
     average += delta * weight / totalWeight;
   }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/AbstractDifferenceRecommenderEvaluator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/AbstractDifferenceRecommenderEvaluator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/AbstractDifferenceRecommenderEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/AbstractDifferenceRecommenderEvaluator.java	2014-03-29 01:03:13.000000000 -0700
@@ -26,6 +26,7 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import com.google.common.collect.Lists;
@@ -98,9 +99,9 @@
     Preconditions.checkNotNull(recommenderBuilder);
     Preconditions.checkNotNull(dataModel);
     Preconditions.checkArgument(trainingPercentage >= 0.0 && trainingPercentage <= 1.0,
-      "Invalid trainingPercentage: " + trainingPercentage);
+      "Invalid trainingPercentage: " + trainingPercentage + ". Must be: 0.0 <= trainingPercentage <= 1.0");
     Preconditions.checkArgument(evaluationPercentage >= 0.0 && evaluationPercentage <= 1.0,
-      "Invalid evaluationPercentage: " + evaluationPercentage);
+      "Invalid evaluationPercentage: " + evaluationPercentage + ". Must be: 0.0 <= evaluationPercentage <= 1.0");
 
     log.info("Beginning evaluation using {} of {}", trainingPercentage, dataModel);
     
@@ -198,12 +199,19 @@
       for (Future<Void> future : futures) {
         future.get();
       }
+
     } catch (InterruptedException ie) {
       throw new TasteException(ie);
     } catch (ExecutionException ee) {
       throw new TasteException(ee.getCause());
     }
+    
     executor.shutdown();
+    try {
+      executor.awaitTermination(10, TimeUnit.SECONDS);
+    } catch (InterruptedException e) {
+      throw new TasteException(e.getCause());
+    }
   }
   
   private static Collection<Callable<Void>> wrapWithStatsCallables(Iterable<Callable<Void>> callables,
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluator.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluator.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluator.java	2014-03-29 01:03:13.000000000 -0700
@@ -94,7 +94,7 @@
     Preconditions.checkArgument(dataModel != null, "dataModel is null");
     Preconditions.checkArgument(at >= 1, "at must be at least 1");
     Preconditions.checkArgument(evaluationPercentage > 0.0 && evaluationPercentage <= 1.0,
-      "Invalid evaluationPercentage: %s", evaluationPercentage);
+        "Invalid evaluationPercentage: " + evaluationPercentage + ". Must be: 0.0 < evaluationPercentage <= 1.0");
 
     int numItems = dataModel.getNumItems();
     RunningAverage precision = new FullRunningAverage();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/IRStatisticsImpl.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/IRStatisticsImpl.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/IRStatisticsImpl.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/IRStatisticsImpl.java	2014-03-29 01:03:13.000000000 -0700
@@ -33,12 +33,15 @@
 
   IRStatisticsImpl(double precision, double recall, double fallOut, double ndcg, double reach) {
     Preconditions.checkArgument(Double.isNaN(precision) || (precision >= 0.0 && precision <= 1.0),
-        "Illegal precision: " + precision);
-    Preconditions.checkArgument(Double.isNaN(recall) || (recall >= 0.0 && recall <= 1.0), "Illegal recall: " + recall);
+        "Illegal precision: " + precision + ". Must be: 0.0 <= precision <= 1.0 or NaN");
+    Preconditions.checkArgument(Double.isNaN(recall) || (recall >= 0.0 && recall <= 1.0), 
+        "Illegal recall: " + recall + ". Must be: 0.0 <= recall <= 1.0 or NaN");
     Preconditions.checkArgument(Double.isNaN(fallOut) || (fallOut >= 0.0 && fallOut <= 1.0),
-        "Illegal fallOut: " + fallOut);
-    Preconditions.checkArgument(Double.isNaN(ndcg) || (ndcg >= 0.0 && ndcg <= 1.0), "Illegal nDCG: " + ndcg);
-    Preconditions.checkArgument(Double.isNaN(reach) || (reach >= 0.0 && reach <= 1.0), "Illegal reach: " + reach);
+        "Illegal fallOut: " + fallOut + ". Must be: 0.0 <= fallOut <= 1.0 or NaN");
+    Preconditions.checkArgument(Double.isNaN(ndcg) || (ndcg >= 0.0 && ndcg <= 1.0), 
+        "Illegal nDCG: " + ndcg + ". Must be: 0.0 <= nDCG <= 1.0 or NaN");
+    Preconditions.checkArgument(Double.isNaN(reach) || (reach >= 0.0 && reach <= 1.0), 
+        "Illegal reach: " + reach + ". Must be: 0.0 <= reach <= 1.0 or NaN");
     this.precision = precision;
     this.recall = recall;
     this.fallOut = fallOut;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/GenericDataModel.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/GenericDataModel.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/GenericDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/GenericDataModel.java	2014-03-29 01:03:13.000000000 -0700
@@ -57,7 +57,7 @@
   
   /**
    * <p>
-   * Creates a new  from the given users (and their preferences). This
+   * Creates a new {@link GenericDataModel} from the given users (and their preferences). This
    * {@link DataModel} retains all this information in memory and is effectively immutable.
    * </p>
    * 
@@ -69,7 +69,7 @@
 
   /**
    * <p>
-   * Creates a new  from the given users (and their preferences). This
+   * Creates a new {@link GenericDataModel} from the given users (and their preferences). This
    * {@link DataModel} retains all this information in memory and is effectively immutable.
    * </p>
    *
@@ -138,7 +138,7 @@
 
   /**
    * <p>
-   * Creates a new containing an immutable copy of the data from another given
+   * Creates a new {@link GenericDataModel} containing an immutable copy of the data from another given
    * {@link DataModel}.
    * </p>
    *
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/PlusAnonymousConcurrentUserDataModel.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/PlusAnonymousConcurrentUserDataModel.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/PlusAnonymousConcurrentUserDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/PlusAnonymousConcurrentUserDataModel.java	2014-03-29 01:03:13.000000000 -0700
@@ -44,7 +44,7 @@
  * <p>
  * To use it, you have to estimate the number of concurrent anonymous users of your application.
  * The pool of users with the given size will be created. For each anonymous recommendations request,
- * a user has to be taken from the pool and returned back immediately afterwars.
+ * a user has to be taken from the pool and returned back immediately afterwards.
  * </p>
  *
  * <p>
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModel.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModel.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModel.java	2014-03-29 01:03:13.000000000 -0700
@@ -28,9 +28,6 @@
 import java.util.TreeMap;
 import java.util.concurrent.locks.ReentrantLock;
 
-import com.google.common.base.Splitter;
-import com.google.common.collect.Lists;
-import com.google.common.io.Closeables;
 import org.apache.mahout.cf.taste.common.Refreshable;
 import org.apache.mahout.cf.taste.common.TasteException;
 import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
@@ -49,6 +46,9 @@
 import org.slf4j.LoggerFactory;
 
 import com.google.common.base.Preconditions;
+import com.google.common.base.Splitter;
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
 
 /**
  * <p>
@@ -93,7 +93,7 @@
  * This class will also look for update "delta" files in the same directory, with file names that start the
  * same way (up to the first period). These files have the same format, and provide updated data that
  * supersedes what is in the main data file. This is a mechanism that allows an application to push updates to
- *  without re-copying the entire data file.
+ * {@link FileDataModel} without re-copying the entire data file.
  * </p>
  *
  * <p>
@@ -148,7 +148,15 @@
   public FileDataModel(File dataFile) throws IOException {
     this(dataFile, false, DEFAULT_MIN_RELOAD_INTERVAL_MS);
   }
-
+  
+  /**
+   * @param delimiterRegex If your data file don't use '\t' or ',' as delimiter, you can specify 
+   * a custom regex pattern.
+   */
+  public FileDataModel(File dataFile, String delimiterRegex) throws IOException {
+    this(dataFile, false, DEFAULT_MIN_RELOAD_INTERVAL_MS, delimiterRegex);
+  }
+  
   /**
    * @param transpose
    *          transposes user IDs and item IDs -- convenient for 'flipping' the data model this way
@@ -158,6 +166,17 @@
    * @see #FileDataModel(File)
    */
   public FileDataModel(File dataFile, boolean transpose, long minReloadIntervalMS) throws IOException {
+    this(dataFile, transpose, minReloadIntervalMS, null);
+  }
+  
+  /**
+   * @param delimiterRegex If your data file don't use '\t' or ',' as delimiters, you can specify 
+   * user own using regex pattern.
+   * @throws IOException
+   */
+  public FileDataModel(File dataFile, boolean transpose, long minReloadIntervalMS, String delimiterRegex)
+    throws IOException {
+
     this.dataFile = Preconditions.checkNotNull(dataFile.getAbsoluteFile());
     if (!dataFile.exists() || dataFile.isDirectory()) {
       throw new FileNotFoundException(dataFile.toString());
@@ -178,8 +197,16 @@
     }
     Closeables.close(iterator, true);
 
-    delimiter = determineDelimiter(firstLine);
-    delimiterPattern = Splitter.on(delimiter);
+    if (delimiterRegex == null) {
+      delimiter = determineDelimiter(firstLine);
+      delimiterPattern = Splitter.on(delimiter);
+    } else {
+      delimiter = '\0';
+      delimiterPattern = Splitter.onPattern(delimiterRegex);
+      if (!delimiterPattern.split(firstLine).iterator().hasNext()) {
+        throw new IllegalArgumentException("Did not find a delimiter(pattern) in first line");
+      }
+    }
     List<String> firstLineSplit = Lists.newArrayList();
     for (String token : delimiterPattern.split(firstLine)) {
       firstLineSplit.add(token);
@@ -198,10 +225,6 @@
     return dataFile;
   }
 
-  public char getDelimiter() {
-    return delimiter;
-  }
-
   protected void reload() {
     if (reloadLock.tryLock()) {
       try {
@@ -688,7 +711,7 @@
   }
 
   /**
-   * Note that this method only updates the in-memory preference data that this
+   * Note that this method only updates the in-memory preference data that this {@link FileDataModel}
    * maintains; it does not modify any data on disk. Therefore any updates from this method are only
    * temporary, and lost when data is reloaded from a file. This method should also be considered relatively
    * slow.
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AbstractCandidateItemsStrategy.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AbstractCandidateItemsStrategy.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AbstractCandidateItemsStrategy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AbstractCandidateItemsStrategy.java	2014-03-29 01:03:13.000000000 -0700
@@ -44,7 +44,7 @@
     return doGetCandidateItems(itemIDs, dataModel);
   }
 
-  abstract FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException;
+  protected abstract FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException;
 
   @Override
   public void refresh(Collection<Refreshable> alreadyRefreshed) {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AllSimilarItemsCandidateItemsStrategy.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AllSimilarItemsCandidateItemsStrategy.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AllSimilarItemsCandidateItemsStrategy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/AllSimilarItemsCandidateItemsStrategy.java	2014-03-29 01:03:13.000000000 -0700
@@ -36,7 +36,7 @@
   }
 
   @Override
-  FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException {
+  protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException {
     FastIDSet candidateItemIDs = new FastIDSet();
     for (long itemID : preferredItemIDs) {
       candidateItemIDs.addAll(similarity.allSimilarItemIDs(itemID));
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,218 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import com.google.common.primitives.Doubles;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.similarity.ItemSimilarity;
-import org.apache.mahout.math.Sorting;
-import org.apache.mahout.math.Swapper;
-import org.apache.mahout.math.function.IntComparator;
-import org.apache.mahout.math.map.OpenLongDoubleHashMap;
-
-/**
- * item-based recommender that uses weighted sum estimation enhanced by baseline estimates, porting baseline estimation
- * from the "UserItemBaseline" rating predictor from "mymedialite" https://github.com/zenogantner/MyMediaLite/
- */
-@Deprecated
-public class BiasedItemBasedRecommender extends GenericItemBasedRecommender {
-  
-  private final int numSimilarItems;
-  
-  private final double averageRating;
-  private final OpenLongDoubleHashMap itemBiases;
-  private final OpenLongDoubleHashMap userBiases;
-
-  private static final int DEFAULT_NUM_SIMILAR_ITEMS = 50;
-  private static final int DEFAULT_NUM_OPTIMIZATION_PASSES = 5;
-  private static final double DEFAULT_USER_BIAS_REGULARIZATION = 10;
-  private static final double DEFAULT_ITEM_BIAS_REGULARIZATION = 5;
-
-  private final ItemSimilarity similarity;
-
-  public BiasedItemBasedRecommender(DataModel dataModel, ItemSimilarity similarity) throws TasteException {
-    this(dataModel, similarity, DEFAULT_NUM_SIMILAR_ITEMS, DEFAULT_NUM_OPTIMIZATION_PASSES,
-        DEFAULT_ITEM_BIAS_REGULARIZATION, DEFAULT_USER_BIAS_REGULARIZATION);
-  }
-
-  public BiasedItemBasedRecommender(DataModel dataModel, ItemSimilarity similarity, int numSimilarItems,
-      int numOptimizationPasses, double itemBiasRegularization, double userBiasRegularization) throws TasteException {
-    super(dataModel, similarity);
-    this.numSimilarItems = numSimilarItems;
-    this.similarity = similarity;
-
-    averageRating = averageRating();
-
-    itemBiases = new OpenLongDoubleHashMap(getDataModel().getNumItems());
-    userBiases = new OpenLongDoubleHashMap(getDataModel().getNumUsers());
-
-    for (int pass = 0; pass < numOptimizationPasses; pass++) {
-      optimizeItemBiases(itemBiasRegularization);
-      optimizeUserBiases(userBiasRegularization);
-    }
-  }
-
-  private void optimizeItemBiases(double itemBiasRegularization) throws TasteException {
-    LongPrimitiveIterator itemIDs = getDataModel().getItemIDs();
-    while (itemIDs.hasNext()) {
-      long itemID = itemIDs.nextLong();
-      PreferenceArray preferences = getDataModel().getPreferencesForItem(itemID);
-      double sum = 0;
-      for (Preference pref : preferences) {
-        sum += pref.getValue() - averageRating;
-      }
-      double bias = sum / (itemBiasRegularization + preferences.length());
-      itemBiases.put(itemID, bias);
-    }
-  }
-
-  private void optimizeUserBiases(double userBiasRegularization) throws TasteException {
-    LongPrimitiveIterator userIDs = getDataModel().getUserIDs();
-    while (userIDs.hasNext()) {
-      long userID = userIDs.nextLong();
-      PreferenceArray preferences = getDataModel().getPreferencesFromUser(userID);
-      double sum = 0;
-      for (Preference pref : preferences) {
-        sum += pref.getValue() - averageRating - itemBiases.get(pref.getItemID());
-      }
-      double bias = sum / (userBiasRegularization + preferences.length());
-      userBiases.put(userID, bias);
-    }
-  }
-
-  private double averageRating() throws TasteException {
-    RunningAverage averageRating = new FullRunningAverage();
-    LongPrimitiveIterator itemIDs = getDataModel().getItemIDs();
-    while (itemIDs.hasNext()) {
-      for (Preference pref : getDataModel().getPreferencesForItem(itemIDs.next())) {
-        averageRating.addDatum(pref.getValue());
-      }
-    }
-    return averageRating.getAverage();
-  }
-
-  @Override
-  public float estimatePreference(long userID, long itemID) throws TasteException {
-    PreferenceArray preferencesFromUser = getDataModel().getPreferencesFromUser(userID);
-    Float actualPref = getPreferenceForItem(preferencesFromUser, itemID);
-    if (actualPref != null) {
-      return actualPref;
-    }
-    return doEstimatePreference(userID, preferencesFromUser, itemID);
-  }
-
-  private static Float getPreferenceForItem(PreferenceArray preferencesFromUser, long itemID) {
-    int size = preferencesFromUser.length();
-    for (int i = 0; i < size; i++) {
-      if (preferencesFromUser.getItemID(i) == itemID) {
-        return preferencesFromUser.getValue(i);
-      }
-    }
-    return null;
-  }
-
-  protected double baselineEstimate(long userID, long itemID) {
-    return averageRating + userBiases.get(userID) + itemBiases.get(itemID);
-  }
-
-  @Override
-  protected float doEstimatePreference(long userID, PreferenceArray preferencesFromUser, long itemID)
-    throws TasteException {
-    long[] userIDs = preferencesFromUser.getIDs();
-    float[] ratings = new float[userIDs.length];
-    long[] itemIDs = new long[userIDs.length];
-            
-    double[] similarities = similarity.itemSimilarities(itemID, userIDs);
-
-    for (int n = 0; n < preferencesFromUser.length(); n++) {
-      ratings[n] = preferencesFromUser.get(n).getValue();
-      itemIDs[n] = preferencesFromUser.get(n).getItemID();
-    }
-
-    // sort, so that we can only use the top similarities
-    Sorting.quickSort(0, similarities.length, new SimilaritiesComparator(similarities),
-        new SimilaritiesRatingsItemIDsSwapper(similarities, ratings, itemIDs));
-
-    double preference = 0.0;
-    double totalSimilarity = 0.0;
-    int count = 0;
-    for (int i = 0; i < Math.min(numSimilarItems, similarities.length); i++) {
-      double theSimilarity = similarities[i];
-      if (!Double.isNaN(theSimilarity)) {
-        preference += theSimilarity * (ratings[i] - baselineEstimate(userID, itemIDs[i]));
-        totalSimilarity += Math.abs(theSimilarity);
-        count++;
-      }
-    }
-
-    if (count <= 1) {
-      return Float.NaN;
-    }
-
-    return (float) (baselineEstimate(userID, itemID) + (preference / totalSimilarity));
-  }
-
-  static class SimilaritiesComparator implements IntComparator {
-
-    private final double[] similarities;
-
-    SimilaritiesComparator(double[] similarities) {
-      this.similarities = similarities;
-    }
-
-    @Override
-    public int compare(int pos1, int pos2) {
-      return -1 * Doubles.compare(similarities[pos1], similarities[pos2]);
-    }
-  }
-
-  static class SimilaritiesRatingsItemIDsSwapper implements Swapper {
-
-    private final double[] similarities;
-    private final float[] ratings;
-    private final long[] itemIDs;
-
-    SimilaritiesRatingsItemIDsSwapper(double[] similarities, float[] ratings, long[] itemIDs) {
-      this.similarities = similarities;
-      this.ratings = ratings;
-      this.itemIDs = itemIDs;
-    }
-
-    @Override
-    public void swap(int a, int b) {
-      double tempDouble = similarities[b];
-      similarities[b] = similarities[a];
-      similarities[a] = tempDouble;
-
-      float tempFloat = ratings[b];
-      ratings[b] = ratings[a];
-      ratings[a] = tempFloat;
-
-      long tempLong = itemIDs[b];
-      itemIDs[b] = itemIDs[a];
-      itemIDs[a] = tempLong;
-    }
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/ClusterSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/ClusterSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/ClusterSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/ClusterSimilarity.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,47 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-
-/**
- * <p>
- * Returns the "similarity" between two clusters of users, according to some definition of similarity.
- * Subclassses define different notions of similarity.
- * </p>
- */
-@Deprecated
-public interface ClusterSimilarity extends Refreshable {
-  
-  /**
-   * @param cluster1
-   *          first cluster of user IDs
-   * @param cluster2
-   *          second cluster of user IDs
-   * @return "distance" between clusters; a bigger value means less similarity
-   * @throws TasteException
-   *           if an error occurs while computing similarity, such as errors accessing an underlying
-   *           {@link org.apache.mahout.cf.taste.model.DataModel}
-   * @throws IllegalArgumentException
-   *           if either argument is null or empty
-   */
-  double getSimilarity(FastIDSet cluster1, FastIDSet cluster2) throws TasteException;
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/FarthestNeighborClusterSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/FarthestNeighborClusterSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/FarthestNeighborClusterSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/FarthestNeighborClusterSimilarity.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,107 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import java.util.Collection;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator;
-import org.apache.mahout.cf.taste.similarity.UserSimilarity;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * Defines cluster similarity as the <em>smallest</em> similarity between any two users in the clusters --
- * that is, it says that clusters are close when <em>all pairs</em> of their members have relatively high
- * similarity.
- * </p>
- */
-@Deprecated
-public final class FarthestNeighborClusterSimilarity implements ClusterSimilarity {
-  
-  private final UserSimilarity similarity;
-  private final double samplingRate;
-  
-  /**
-   * <p>
-   * Constructs a  based on the given {@link UserSimilarity}. All
-   * user-user similarities are examined.
-   * </p>
-   */
-  public FarthestNeighborClusterSimilarity(UserSimilarity similarity) {
-    this(similarity, 1.0);
-  }
-  
-  /**
-   * <p>
-   * Constructs a  based on the given {@link UserSimilarity}. By
-   * setting {@code samplingRate} to a value less than 1.0, this implementation will only examine that
-   * fraction of all user-user similarities between two clusters, increasing performance at the expense of
-   * accuracy.
-   * </p>
-   */
-  public FarthestNeighborClusterSimilarity(UserSimilarity similarity, double samplingRate) {
-    Preconditions.checkArgument(similarity != null, "similarity is null");
-    Preconditions.checkArgument(!Double.isNaN(samplingRate) && samplingRate > 0.0 && samplingRate <= 1.0,
-                                "samplingRate is invalid: %.4f", samplingRate);
-    this.similarity = similarity;
-    this.samplingRate = samplingRate;
-  }
-  
-  @Override
-  public double getSimilarity(FastIDSet cluster1, FastIDSet cluster2) throws TasteException {
-    if (cluster1.isEmpty() || cluster2.isEmpty()) {
-      return Double.NaN;
-    }
-    double leastSimilarity = Double.POSITIVE_INFINITY;
-    LongPrimitiveIterator someUsers = SamplingLongPrimitiveIterator.maybeWrapIterator(cluster1.iterator(),
-      samplingRate);
-    while (someUsers.hasNext()) {
-      long userID1 = someUsers.next();
-      LongPrimitiveIterator it2 = cluster2.iterator();
-      while (it2.hasNext()) {
-        double theSimilarity = similarity.userSimilarity(userID1, it2.nextLong());
-        if (theSimilarity < leastSimilarity) {
-          leastSimilarity = theSimilarity;
-        }
-      }
-    }
-    // We skipped everything? well, at least try comparing the first Users to get some value
-    if (leastSimilarity == Double.POSITIVE_INFINITY) {
-      return similarity.userSimilarity(cluster1.iterator().next(), cluster2.iterator().next());
-    }
-    return leastSimilarity;
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);
-    RefreshHelper.maybeRefresh(alreadyRefreshed, similarity);
-  }
-  
-  @Override
-  public String toString() {
-    return "FarthestNeighborClusterSimilarity[similarity:" + similarity + ']';
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/NearestNeighborClusterSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/NearestNeighborClusterSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/NearestNeighborClusterSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/NearestNeighborClusterSimilarity.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,106 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import java.util.Collection;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator;
-import org.apache.mahout.cf.taste.similarity.UserSimilarity;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * Defines cluster similarity as the <em>largest</em> similarity between any two users in the clusters -- that
- * is, it says that clusters are close when <em>some pair</em> of their members has high similarity.
- * </p>
- */
-@Deprecated
-public final class NearestNeighborClusterSimilarity implements ClusterSimilarity {
-  
-  private final UserSimilarity similarity;
-  private final double samplingRate;
-  
-  /**
-   * <p>
-   * Constructs a  based on the given {@link UserSimilarity}. All
-   * user-user similarities are examined.
-   * </p>
-   */
-  public NearestNeighborClusterSimilarity(UserSimilarity similarity) {
-    this(similarity, 1.0);
-  }
-  
-  /**
-   * <p>
-   * Constructs a  based on the given {@link UserSimilarity}. By
-   * setting {@code samplingRate} to a value less than 1.0, this implementation will only examine that
-   * fraction of all user-user similarities between two clusters, increasing performance at the expense of
-   * accuracy.
-   * </p>
-   */
-  public NearestNeighborClusterSimilarity(UserSimilarity similarity, double samplingRate) {
-    Preconditions.checkArgument(similarity != null, "similarity is null");
-    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0, "samplingRate is invalid: %f", samplingRate);
-
-    this.similarity = similarity;
-    this.samplingRate = samplingRate;
-  }
-  
-  @Override
-  public double getSimilarity(FastIDSet cluster1, FastIDSet cluster2) throws TasteException {
-    if (cluster1.isEmpty() || cluster2.isEmpty()) {
-      return Double.NaN;
-    }
-    LongPrimitiveIterator someUsers = SamplingLongPrimitiveIterator.maybeWrapIterator(cluster1.iterator(),
-      samplingRate);
-    double greatestSimilarity = Double.NEGATIVE_INFINITY;
-    while (someUsers.hasNext()) {
-      long userID1 = someUsers.next();
-      LongPrimitiveIterator it2 = cluster2.iterator();
-      while (it2.hasNext()) {
-        double theSimilarity = similarity.userSimilarity(userID1, it2.nextLong());
-        if (theSimilarity > greatestSimilarity) {
-          greatestSimilarity = theSimilarity;
-        }
-      }
-    }
-    // We skipped everything? well, at least try comparing the first Users to get some value
-    if (greatestSimilarity == Double.NEGATIVE_INFINITY) {
-      return similarity.userSimilarity(cluster1.iterator().next(), cluster2.iterator().next());
-    }
-    return greatestSimilarity;
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);
-    RefreshHelper.maybeRefresh(alreadyRefreshed, similarity);
-  }
-  
-  @Override
-  public String toString() {
-    return "NearestNeighborClusterSimilarity[similarity:" + similarity + ']';
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/SamplingCandidateItemsStrategy.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/SamplingCandidateItemsStrategy.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/SamplingCandidateItemsStrategy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/SamplingCandidateItemsStrategy.java	2014-03-29 01:03:13.000000000 -0700
@@ -98,11 +98,11 @@
                                         int candidatesPerUserFactor,
                                         int numUsers,
                                         int numItems) {
-    Preconditions.checkArgument(itemsFactor > 0);
-    Preconditions.checkArgument(usersPerItemFactor > 0);
-    Preconditions.checkArgument(candidatesPerUserFactor > 0);
-    Preconditions.checkArgument(numUsers > 0);
-    Preconditions.checkArgument(numItems > 0);
+    Preconditions.checkArgument(itemsFactor > 0, "itemsFactor must be greater then 0!");
+    Preconditions.checkArgument(usersPerItemFactor > 0, "usersPerItemFactor must be greater then 0!");
+    Preconditions.checkArgument(candidatesPerUserFactor > 0, "candidatesPerUserFactor must be greater then 0!");
+    Preconditions.checkArgument(numUsers > 0, "numUsers must be greater then 0!");
+    Preconditions.checkArgument(numItems > 0, "numItems must be greater then 0!");
     maxItems = computeMaxFrom(itemsFactor, numItems);
     maxUsersPerItem = computeMaxFrom(usersPerItemFactor, numUsers);
     maxItemsPerUser = computeMaxFrom(candidatesPerUserFactor, numItems);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TopItems.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TopItems.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TopItems.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TopItems.java	2014-03-29 01:03:13.000000000 -0700
@@ -50,8 +50,8 @@
                                                   LongPrimitiveIterator possibleItemIDs,
                                                   IDRescorer rescorer,
                                                   Estimator<Long> estimator) throws TasteException {
-    Preconditions.checkArgument(possibleItemIDs != null, "argument is null");
-    Preconditions.checkArgument(estimator != null, "argument is null");
+    Preconditions.checkArgument(possibleItemIDs != null, "possibleItemIDs is null");
+    Preconditions.checkArgument(estimator != null, "estimator is null");
 
     Queue<RecommendedItem> topItems = new PriorityQueue<RecommendedItem>(howMany + 1,
       Collections.reverseOrder(ByValueRecommendedItemComparator.getInstance()));
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,414 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.Callable;
-
-import com.google.common.collect.Lists;
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.ClusteringRecommender;
-import org.apache.mahout.cf.taste.recommender.IDRescorer;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.RandomUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * A {@link org.apache.mahout.cf.taste.recommender.Recommender} that clusters users, then determines the
- * clusters' top recommendations. This implementation builds clusters by repeatedly merging clusters until
- * only a certain number remain, meaning that each cluster is sort of a tree of other clusters.
- * </p>
- * 
- * <p>
- * This {@link org.apache.mahout.cf.taste.recommender.Recommender} therefore has a few properties to note:
- * </p>
- * 
- * <ul>
- * <li>For all users in a cluster, recommendations will be the same</li>
- * <li>{@link #estimatePreference(long, long)} may well return {@link Double#NaN}; it does so when asked to
- * estimate preference for an item for which no preference is expressed in the users in the cluster.</li>
- * </ul>
- */
-@Deprecated
-public final class TreeClusteringRecommender extends AbstractRecommender implements ClusteringRecommender {
-  
-  private static final Logger log = LoggerFactory.getLogger(TreeClusteringRecommender.class);
-  
-  private static final FastIDSet[] NO_CLUSTERS = new FastIDSet[0];
-
-  private final Random random;
-  private final ClusterSimilarity clusterSimilarity;
-  private final int numClusters;
-  private final double clusteringThreshold;
-  private final boolean clusteringByThreshold;
-  private final double samplingRate;
-  private FastByIDMap<List<RecommendedItem>> topRecsByUserID;
-  private FastIDSet[] allClusters;
-  private FastByIDMap<FastIDSet> clustersByUserID;
-  private final RefreshHelper refreshHelper;
-  
-  /**
-   * @param dataModel
-   *          {@link DataModel} which provdes users
-   * @param clusterSimilarity
-   *          {@link ClusterSimilarity} used to compute cluster similarity
-   * @param numClusters
-   *          desired number of clusters to create
-   * @throws IllegalArgumentException
-   *           if arguments are {@code null}, or {@code numClusters} is less than 2
-   */
-  public TreeClusteringRecommender(DataModel dataModel, ClusterSimilarity clusterSimilarity, int numClusters)
-    throws TasteException {
-    this(dataModel, clusterSimilarity, numClusters, 1.0);
-  }
-  
-  /**
-   * @param dataModel
-   *          {@link DataModel} which provdes users
-   * @param clusterSimilarity
-   *          {@link ClusterSimilarity} used to compute cluster similarity
-   * @param numClusters
-   *          desired number of clusters to create
-   * @param samplingRate
-   *          percentage of all cluster-cluster pairs to consider when finding next-most-similar clusters.
-   *          Decreasing this value from 1.0 can increase performance at the cost of accuracy
-   * @throws IllegalArgumentException
-   *           if arguments are {@code null}, or {@code numClusters} is less than 2, or samplingRate
-   *           is {@link Double#NaN} or nonpositive or greater than 1.0
-   */
-  public TreeClusteringRecommender(DataModel dataModel,
-                                   ClusterSimilarity clusterSimilarity,
-                                   int numClusters,
-                                   double samplingRate) throws TasteException {
-    super(dataModel);
-    Preconditions.checkArgument(numClusters >= 2, "numClusters must be at least 2");
-    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0,
-      "samplingRate is invalid: %f", samplingRate);
-    random = RandomUtils.getRandom();
-    this.clusterSimilarity = Preconditions.checkNotNull(clusterSimilarity);
-    this.numClusters = numClusters;
-    this.clusteringThreshold = Double.NaN;
-    this.clusteringByThreshold = false;
-    this.samplingRate = samplingRate;
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        buildClusters();
-        return null;
-      }
-    });
-    refreshHelper.addDependency(dataModel);
-    refreshHelper.addDependency(clusterSimilarity);
-    buildClusters();
-  }
-  
-  /**
-   * @param dataModel
-   *          {@link DataModel} which provdes users
-   * @param clusterSimilarity
-   *          {@link ClusterSimilarity} used to compute cluster similarity
-   * @param clusteringThreshold
-   *          clustering similarity threshold; clusters will be aggregated into larger clusters until the next
-   *          two nearest clusters' similarity drops below this threshold
-   * @throws IllegalArgumentException
-   *           if arguments are {@code null}, or {@code clusteringThreshold} is {@link Double#NaN}
-   */
-  public TreeClusteringRecommender(DataModel dataModel,
-                                   ClusterSimilarity clusterSimilarity,
-                                   double clusteringThreshold) throws TasteException {
-    this(dataModel, clusterSimilarity, clusteringThreshold, 1.0);
-  }
-  
-  /**
-   * @param dataModel
-   *          {@link DataModel} which provides users
-   * @param clusterSimilarity
-   *          {@link ClusterSimilarity} used to compute cluster similarity
-   * @param clusteringThreshold
-   *          clustering similarity threshold; clusters will be aggregated into larger clusters until the next
-   *          two nearest clusters' similarity drops below this threshold
-   * @param samplingRate
-   *          percentage of all cluster-cluster pairs to consider when finding next-most-similar clusters.
-   *          Decreasing this value from 1.0 can increase performance at the cost of accuracy
-   * @throws IllegalArgumentException
-   *           if arguments are {@code null}, or {@code clusteringThreshold} is {@link Double#NaN},
-   *           or samplingRate is {@link Double#NaN} or nonpositive or greater than 1.0
-   */
-  public TreeClusteringRecommender(DataModel dataModel,
-                                   ClusterSimilarity clusterSimilarity,
-                                   double clusteringThreshold,
-                                   double samplingRate) throws TasteException {
-    super(dataModel);
-    Preconditions.checkArgument(!Double.isNaN(clusteringThreshold), "clusteringThreshold must not be NaN");
-    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0, "samplingRate is invalid: %f", samplingRate);
-    random = RandomUtils.getRandom();
-    this.clusterSimilarity = Preconditions.checkNotNull(clusterSimilarity);
-    this.numClusters = Integer.MIN_VALUE;
-    this.clusteringThreshold = clusteringThreshold;
-    this.clusteringByThreshold = true;
-    this.samplingRate = samplingRate;
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        buildClusters();
-        return null;
-      }
-    });
-    refreshHelper.addDependency(dataModel);
-    refreshHelper.addDependency(clusterSimilarity);
-    buildClusters();
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException {
-    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");
-    buildClusters();
-
-    log.debug("Recommending items for user ID '{}'", userID);
-
-    List<RecommendedItem> recommended = topRecsByUserID.get(userID);
-    if (recommended == null) {
-      return Collections.emptyList();
-    }
-
-    DataModel dataModel = getDataModel();
-    List<RecommendedItem> rescored = Lists.newArrayListWithCapacity(recommended.size());
-    // Only add items the user doesn't already have a preference for.
-    // And that the rescorer doesn't "reject".
-    for (RecommendedItem recommendedItem : recommended) {
-      long itemID = recommendedItem.getItemID();
-      if (rescorer != null && rescorer.isFiltered(itemID)) {
-        continue;
-      }
-      if (dataModel.getPreferenceValue(userID, itemID) == null
-          && (rescorer == null || !Double.isNaN(rescorer.rescore(itemID, recommendedItem.getValue())))) {
-        rescored.add(recommendedItem);
-      }
-    }
-    Collections.sort(rescored, new ByRescoreComparator(rescorer));
-
-    return rescored;
-  }
-  
-  @Override
-  public float estimatePreference(long userID, long itemID) throws TasteException {
-    DataModel model = getDataModel();
-    Float actualPref = model.getPreferenceValue(userID, itemID);
-    if (actualPref != null) {
-      return actualPref;
-    }
-    buildClusters();
-    List<RecommendedItem> topRecsForUser = topRecsByUserID.get(userID);
-    if (topRecsForUser != null) {
-      for (RecommendedItem item : topRecsForUser) {
-        if (itemID == item.getItemID()) {
-          return item.getValue();
-        }
-      }
-    }
-    // Hmm, we have no idea. The item is not in the user's cluster
-    return Float.NaN;
-  }
-  
-  @Override
-  public FastIDSet getCluster(long userID) throws TasteException {
-    buildClusters();
-    FastIDSet cluster = clustersByUserID.get(userID);
-    return cluster == null ? new FastIDSet() : cluster;
-  }
-  
-  @Override
-  public FastIDSet[] getClusters() throws TasteException {
-    buildClusters();
-    return allClusters;
-  }
-
-  private void buildClusters() throws TasteException {
-    DataModel model = getDataModel();
-    int numUsers = model.getNumUsers();
-    if (numUsers > 0) {
-      List<FastIDSet> newClusters = Lists.newArrayListWithCapacity(numUsers);
-      // Begin with a cluster for each user:
-      LongPrimitiveIterator it = model.getUserIDs();
-      while (it.hasNext()) {
-        FastIDSet newCluster = new FastIDSet();
-        newCluster.add(it.nextLong());
-        newClusters.add(newCluster);
-      }
-      if (numUsers > 1) {
-        findClusters(newClusters);
-      }
-      topRecsByUserID = computeTopRecsPerUserID(newClusters);
-      clustersByUserID = computeClustersPerUserID(newClusters);
-      allClusters = newClusters.toArray(new FastIDSet[newClusters.size()]);
-    } else {
-      topRecsByUserID = new FastByIDMap<List<RecommendedItem>>();
-      clustersByUserID = new FastByIDMap<FastIDSet>();
-      allClusters = NO_CLUSTERS;
-    }
-  }
-  
-  private void findClusters(List<FastIDSet> newClusters) throws TasteException {
-    if (clusteringByThreshold) {
-      Pair<FastIDSet,FastIDSet> nearestPair = findNearestClusters(newClusters);
-      if (nearestPair != null) {
-        FastIDSet cluster1 = nearestPair.getFirst();
-        FastIDSet cluster2 = nearestPair.getSecond();
-        while (clusterSimilarity.getSimilarity(cluster1, cluster2) >= clusteringThreshold) {
-          newClusters.remove(cluster1);
-          newClusters.remove(cluster2);
-          FastIDSet merged = new FastIDSet(cluster1.size() + cluster2.size());
-          merged.addAll(cluster1);
-          merged.addAll(cluster2);
-          newClusters.add(merged);
-          nearestPair = findNearestClusters(newClusters);
-          if (nearestPair == null) {
-            break;
-          }
-          cluster1 = nearestPair.getFirst();
-          cluster2 = nearestPair.getSecond();
-        }
-      }
-    } else {
-      while (newClusters.size() > numClusters) {
-        Pair<FastIDSet,FastIDSet> nearestPair = findNearestClusters(newClusters);
-        if (nearestPair == null) {
-          break;
-        }
-        FastIDSet cluster1 = nearestPair.getFirst();
-        FastIDSet cluster2 = nearestPair.getSecond();
-        newClusters.remove(cluster1);
-        newClusters.remove(cluster2);
-        FastIDSet merged = new FastIDSet(cluster1.size() + cluster2.size());
-        merged.addAll(cluster1);
-        merged.addAll(cluster2);
-        newClusters.add(merged);
-      }
-    }
-  }
-  
-  private Pair<FastIDSet,FastIDSet> findNearestClusters(List<FastIDSet> clusters) throws TasteException {
-    int size = clusters.size();
-    Pair<FastIDSet,FastIDSet> nearestPair = null;
-    double bestSimilarity = Double.NEGATIVE_INFINITY;
-    for (int i = 0; i < size; i++) {
-      FastIDSet cluster1 = clusters.get(i);
-      for (int j = i + 1; j < size; j++) {
-        if (samplingRate >= 1.0 || random.nextDouble() < samplingRate) {
-          FastIDSet cluster2 = clusters.get(j);
-          double similarity = clusterSimilarity.getSimilarity(cluster1, cluster2);
-          if (!Double.isNaN(similarity) && similarity > bestSimilarity) {
-            bestSimilarity = similarity;
-            nearestPair = new Pair<FastIDSet,FastIDSet>(cluster1, cluster2);
-          }
-        }
-      }
-    }
-    return nearestPair;
-  }
-  
-  private FastByIDMap<List<RecommendedItem>> computeTopRecsPerUserID(Iterable<FastIDSet> clusters)
-    throws TasteException {
-    FastByIDMap<List<RecommendedItem>> recsPerUser = new FastByIDMap<List<RecommendedItem>>();
-    for (FastIDSet cluster : clusters) {
-      List<RecommendedItem> recs = computeTopRecsForCluster(cluster);
-      LongPrimitiveIterator it = cluster.iterator();
-      while (it.hasNext()) {
-        recsPerUser.put(it.nextLong(), recs);
-      }
-    }
-    return recsPerUser;
-  }
-  
-  private List<RecommendedItem> computeTopRecsForCluster(FastIDSet cluster) throws TasteException {
-    DataModel dataModel = getDataModel();
-    FastIDSet possibleItemIDs = new FastIDSet();
-    LongPrimitiveIterator it = cluster.iterator();
-    while (it.hasNext()) {
-      possibleItemIDs.addAll(dataModel.getItemIDsFromUser(it.nextLong()));
-    }
-    
-    TopItems.Estimator<Long> estimator = new Estimator(cluster);
-    
-    List<RecommendedItem> topItems =
-        TopItems.getTopItems(possibleItemIDs.size(), possibleItemIDs.iterator(), null, estimator);
-    
-    log.debug("Recommendations are: {}", topItems);
-    return Collections.unmodifiableList(topItems);
-  }
-  
-  private static FastByIDMap<FastIDSet> computeClustersPerUserID(Collection<FastIDSet> clusters) {
-    FastByIDMap<FastIDSet> clustersPerUser = new FastByIDMap<FastIDSet>(clusters.size());
-    for (FastIDSet cluster : clusters) {
-      LongPrimitiveIterator it = cluster.iterator();
-      while (it.hasNext()) {
-        clustersPerUser.put(it.nextLong(), cluster);
-      }
-    }
-    return clustersPerUser;
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    refreshHelper.refresh(alreadyRefreshed);
-  }
-  
-  @Override
-  public String toString() {
-    return "TreeClusteringRecommender[clusterSimilarity:" + clusterSimilarity + ']';
-  }
-  
-  private final class Estimator implements TopItems.Estimator<Long> {
-    
-    private final FastIDSet cluster;
-    
-    private Estimator(FastIDSet cluster) {
-      this.cluster = cluster;
-    }
-    
-    @Override
-    public double estimate(Long itemID) throws TasteException {
-      DataModel dataModel = getDataModel();
-      RunningAverage average = new FullRunningAverage();
-      LongPrimitiveIterator it = cluster.iterator();
-      while (it.hasNext()) {
-        Float pref = dataModel.getPreferenceValue(it.nextLong(), itemID);
-        if (pref != null) {
-          average.addDatum(pref);
-        }
-      }
-      return average.getAverage();
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,480 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.List;
-import java.util.ListIterator;
-import java.util.PriorityQueue;
-import java.util.Queue;
-import java.util.concurrent.Callable;
-
-import com.google.common.collect.Lists;
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.ClusteringRecommender;
-import org.apache.mahout.cf.taste.recommender.IDRescorer;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.common.RandomUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * A {@link org.apache.mahout.cf.taste.recommender.Recommender} that clusters users, then determines the
- * clusters' top recommendations. This implementation builds clusters by repeatedly merging clusters until
- * only a certain number remain, meaning that each cluster is sort of a tree of other clusters.
- * </p>
- * 
- * <p>
- * This {@link org.apache.mahout.cf.taste.recommender.Recommender} therefore has a few properties to note:
- * </p>
- * <ul>
- * <li>For all users in a cluster, recommendations will be the same</li>
- * <li>{@link #estimatePreference(long, long)} may well return {@link Double#NaN}; it does so when asked to
- * estimate preference for an item for which no preference is expressed in the users in the cluster.</li>
- * </ul>
- * 
- * <p>
- * This is an <em>experimental</em> implementation which tries to gain a lot of speed at the cost of accuracy
- * in building clusters, compared to {@link TreeClusteringRecommender}. It will sometimes cluster two other
- * clusters together that may not be the exact closest two clusters in existence. This may not affect the
- * recommendation quality much, but it potentially speeds up the clustering process dramatically.
- * </p>
- */
-@Deprecated
-public final class TreeClusteringRecommender2 extends AbstractRecommender implements ClusteringRecommender {
-  
-  private static final Logger log = LoggerFactory.getLogger(TreeClusteringRecommender2.class);
-  
-  private static final int NUM_CLUSTER_RECS = 100;
-  
-  private final ClusterSimilarity clusterSimilarity;
-  private final int numClusters;
-  private final double clusteringThreshold;
-  private final boolean clusteringByThreshold;
-  private FastByIDMap<List<RecommendedItem>> topRecsByUserID;
-  private FastIDSet[] allClusters;
-  private FastByIDMap<FastIDSet> clustersByUserID;
-  private final RefreshHelper refreshHelper;
-  
-  /**
-   * @param dataModel
-   *          {@link DataModel} which provides users
-   * @param clusterSimilarity
-   *          {@link ClusterSimilarity} used to compute cluster similarity
-   * @param numClusters
-   *          desired number of clusters to create
-   * @throws IllegalArgumentException
-   *           if arguments are {@code null}, or {@code numClusters} is less than 2
-   */
-  public TreeClusteringRecommender2(DataModel dataModel, ClusterSimilarity clusterSimilarity, int numClusters)
-    throws TasteException {
-    super(dataModel);
-    Preconditions.checkArgument(numClusters >= 2, "numClusters must be at least 2");
-    this.clusterSimilarity = Preconditions.checkNotNull(clusterSimilarity);
-    this.numClusters = numClusters;
-    this.clusteringThreshold = Double.NaN;
-    this.clusteringByThreshold = false;
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        buildClusters();
-        return null;
-      }
-    });
-    refreshHelper.addDependency(dataModel);
-    refreshHelper.addDependency(clusterSimilarity);
-    buildClusters();
-  }
-  
-  /**
-   * @param dataModel
-   *          {@link DataModel} which provides users
-   * @param clusterSimilarity
-   *          {@link ClusterSimilarity} used to compute cluster
-   *          similarity
-   * @param clusteringThreshold
-   *          clustering similarity threshold; clusters will be aggregated into larger clusters until the next
-   *          two nearest clusters' similarity drops below this threshold
-   * @throws IllegalArgumentException
-   *           if arguments are {@code null}, or {@code clusteringThreshold} is {@link Double#NaN}
-   */
-  public TreeClusteringRecommender2(DataModel dataModel,
-                                    ClusterSimilarity clusterSimilarity,
-                                    double clusteringThreshold) throws TasteException {
-    super(dataModel);
-    Preconditions.checkArgument(!Double.isNaN(clusteringThreshold), "clusteringThreshold must not be NaN");
-    this.clusterSimilarity = Preconditions.checkNotNull(clusterSimilarity);
-    this.numClusters = Integer.MIN_VALUE;
-    this.clusteringThreshold = clusteringThreshold;
-    this.clusteringByThreshold = true;
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        buildClusters();
-        return null;
-      }
-    });
-    refreshHelper.addDependency(dataModel);
-    refreshHelper.addDependency(clusterSimilarity);
-    buildClusters();
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException {
-    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");
-    buildClusters();
-
-    log.debug("Recommending items for user ID '{}'", userID);
-
-    List<RecommendedItem> recommended = topRecsByUserID.get(userID);
-    if (recommended == null) {
-      return Collections.emptyList();
-    }
-
-    DataModel dataModel = getDataModel();
-    List<RecommendedItem> rescored = Lists.newArrayListWithCapacity(recommended.size());
-    // Only add items the user doesn't already have a preference for.
-    // And that the rescorer doesn't "reject".
-    for (RecommendedItem recommendedItem : recommended) {
-      long itemID = recommendedItem.getItemID();
-      if (rescorer != null && rescorer.isFiltered(itemID)) {
-        continue;
-      }
-      if (dataModel.getPreferenceValue(userID, itemID) == null
-          && (rescorer == null || !Double.isNaN(rescorer.rescore(itemID, recommendedItem.getValue())))) {
-        rescored.add(recommendedItem);
-      }
-    }
-    Collections.sort(rescored, new ByRescoreComparator(rescorer));
-
-    return rescored;
-  }
-  
-  @Override
-  public float estimatePreference(long userID, long itemID) throws TasteException {
-    Float actualPref = getDataModel().getPreferenceValue(userID, itemID);
-    if (actualPref != null) {
-      return actualPref;
-    }
-    buildClusters();
-    List<RecommendedItem> topRecsForUser = topRecsByUserID.get(userID);
-    if (topRecsForUser != null) {
-      for (RecommendedItem item : topRecsForUser) {
-        if (itemID == item.getItemID()) {
-          return item.getValue();
-        }
-      }
-    }
-    // Hmm, we have no idea. The item is not in the user's cluster
-    return Float.NaN;
-  }
-  
-  @Override
-  public FastIDSet getCluster(long userID) throws TasteException {
-    buildClusters();
-    FastIDSet cluster = clustersByUserID.get(userID);
-    return cluster == null ? new FastIDSet() : cluster;
-  }
-  
-  @Override
-  public FastIDSet[] getClusters() throws TasteException {
-    buildClusters();
-    return allClusters;
-  }
-  
-  private static final class ClusterClusterPair implements Comparable<ClusterClusterPair> {
-    
-    private final FastIDSet cluster1;
-    private final FastIDSet cluster2;
-    private final double similarity;
-    
-    private ClusterClusterPair(FastIDSet cluster1, FastIDSet cluster2, double similarity) {
-      this.cluster1 = cluster1;
-      this.cluster2 = cluster2;
-      this.similarity = similarity;
-    }
-    
-    FastIDSet getCluster1() {
-      return cluster1;
-    }
-    
-    FastIDSet getCluster2() {
-      return cluster2;
-    }
-    
-    double getSimilarity() {
-      return similarity;
-    }
-    
-    @Override
-    public int hashCode() {
-      return cluster1.hashCode() ^ cluster2.hashCode() ^ RandomUtils.hashDouble(similarity);
-    }
-    
-    @Override
-    public boolean equals(Object o) {
-      if (!(o instanceof ClusterClusterPair)) {
-        return false;
-      }
-      ClusterClusterPair other = (ClusterClusterPair) o;
-      return cluster1.equals(other.getCluster1())
-          && cluster2.equals(other.getCluster2())
-          && similarity == other.getSimilarity();
-    }
-    
-    @Override
-    public int compareTo(ClusterClusterPair other) {
-      double otherSimilarity = other.getSimilarity();
-      if (similarity > otherSimilarity) {
-        return -1;
-      } else if (similarity < otherSimilarity) {
-        return 1;
-      } else {
-        return 0;
-      }
-    }
-    
-  }
-  
-  private void buildClusters() throws TasteException {
-    DataModel model = getDataModel();
-    int numUsers = model.getNumUsers();
-
-    if (numUsers == 0) {
-
-      topRecsByUserID = new FastByIDMap<List<RecommendedItem>>();
-      clustersByUserID = new FastByIDMap<FastIDSet>();
-
-    } else {
-
-      List<FastIDSet> clusters = Lists.newArrayList();
-      // Begin with a cluster for each user:
-      LongPrimitiveIterator it = model.getUserIDs();
-      while (it.hasNext()) {
-        FastIDSet newCluster = new FastIDSet();
-        newCluster.add(it.nextLong());
-        clusters.add(newCluster);
-      }
-
-      boolean done = false;
-      while (!done) {
-        done = mergeClosestClusters(numUsers, clusters, done);
-      }
-
-      topRecsByUserID = computeTopRecsPerUserID(clusters);
-      clustersByUserID = computeClustersPerUserID(clusters);
-      allClusters = clusters.toArray(new FastIDSet[clusters.size()]);
-
-    }
-  }
-  
-  private boolean mergeClosestClusters(int numUsers, List<FastIDSet> clusters, boolean done) throws TasteException {
-    // We find a certain number of closest clusters...
-    List<ClusterClusterPair> queue = findClosestClusters(numUsers, clusters);
-    
-    // The first one is definitely the closest pair in existence so we can cluster
-    // the two together, put it back into the set of clusters, and start again. Instead
-    // we assume everything else in our list of closest cluster pairs is still pretty good,
-    // and we cluster them too.
-    
-    while (!queue.isEmpty()) {
-      
-      if (!clusteringByThreshold && clusters.size() <= numClusters) {
-        done = true;
-        break;
-      }
-      
-      ClusterClusterPair top = queue.remove(0);
-      
-      if (clusteringByThreshold && top.getSimilarity() < clusteringThreshold) {
-        done = true;
-        break;
-      }
-      
-      FastIDSet cluster1 = top.getCluster1();
-      FastIDSet cluster2 = top.getCluster2();
-      
-      // Pull out current two clusters from clusters
-      Iterator<FastIDSet> clusterIterator = clusters.iterator();
-      boolean removed1 = false;
-      boolean removed2 = false;
-      while (clusterIterator.hasNext() && !(removed1 && removed2)) {
-        FastIDSet current = clusterIterator.next();
-        // Yes, use == here
-        if (!removed1 && cluster1 == current) {
-          clusterIterator.remove();
-          removed1 = true;
-        } else if (!removed2 && cluster2 == current) {
-          clusterIterator.remove();
-          removed2 = true;
-        }
-      }
-      
-      // The only catch is if a cluster showed it twice in the list of best cluster pairs;
-      // have to remove the others. Pull out anything referencing these clusters from queue
-      for (Iterator<ClusterClusterPair> queueIterator = queue.iterator(); queueIterator.hasNext();) {
-        ClusterClusterPair pair = queueIterator.next();
-        FastIDSet pair1 = pair.getCluster1();
-        FastIDSet pair2 = pair.getCluster2();
-        if (pair1 == cluster1 || pair1 == cluster2 || pair2 == cluster1 || pair2 == cluster2) {
-          queueIterator.remove();
-        }
-      }
-      
-      // Make new merged cluster
-      FastIDSet merged = new FastIDSet(cluster1.size() + cluster2.size());
-      merged.addAll(cluster1);
-      merged.addAll(cluster2);
-      
-      // Compare against other clusters; update queue if needed
-      // That new pair we're just adding might be pretty close to something else, so
-      // catch that case here and put it back into our queue
-      for (FastIDSet cluster : clusters) {
-        double similarity = clusterSimilarity.getSimilarity(merged, cluster);
-        if (!queue.isEmpty() && similarity > queue.get(queue.size() - 1).getSimilarity()) {
-          ListIterator<ClusterClusterPair> queueIterator = queue.listIterator();
-          while (queueIterator.hasNext()) {
-            if (similarity > queueIterator.next().getSimilarity()) {
-              queueIterator.previous();
-              break;
-            }
-          }
-          queueIterator.add(new ClusterClusterPair(merged, cluster, similarity));
-        }
-      }
-      
-      // Finally add new cluster to list
-      clusters.add(merged);
-      
-    }
-    return done;
-  }
-  
-  private List<ClusterClusterPair> findClosestClusters(int numUsers,
-                                                       List<FastIDSet> clusters) throws TasteException {
-    Queue<ClusterClusterPair> queue =
-        new PriorityQueue<ClusterClusterPair>(numUsers + 1, Collections.<ClusterClusterPair>reverseOrder());
-    int size = clusters.size();
-    for (int i = 0; i < size; i++) {
-      FastIDSet cluster1 = clusters.get(i);
-      for (int j = i + 1; j < size; j++) {
-        FastIDSet cluster2 = clusters.get(j);
-        double similarity = clusterSimilarity.getSimilarity(cluster1, cluster2);
-        if (!Double.isNaN(similarity)) {
-          if (queue.size() < numUsers) {
-            queue.add(new ClusterClusterPair(cluster1, cluster2, similarity));
-          } else if (similarity > queue.poll().getSimilarity()) {
-            queue.add(new ClusterClusterPair(cluster1, cluster2, similarity));
-            queue.poll();
-          }
-        }
-      }
-    }
-    List<ClusterClusterPair> result = Lists.newArrayList(queue);
-    Collections.sort(result);
-    return result;
-  }
-  
-  private FastByIDMap<List<RecommendedItem>> computeTopRecsPerUserID(Iterable<FastIDSet> clusters)
-    throws TasteException {
-    FastByIDMap<List<RecommendedItem>> recsPerUser = new FastByIDMap<List<RecommendedItem>>();
-    for (FastIDSet cluster : clusters) {
-      List<RecommendedItem> recs = computeTopRecsForCluster(cluster);
-      LongPrimitiveIterator it = cluster.iterator();
-      while (it.hasNext()) {
-        recsPerUser.put(it.nextLong(), recs);
-      }
-    }
-    return recsPerUser;
-  }
-  
-  private List<RecommendedItem> computeTopRecsForCluster(FastIDSet cluster) throws TasteException {
-    
-    DataModel dataModel = getDataModel();
-    FastIDSet possibleItemIDs = new FastIDSet();
-    LongPrimitiveIterator it = cluster.iterator();
-    while (it.hasNext()) {
-      possibleItemIDs.addAll(dataModel.getItemIDsFromUser(it.nextLong()));
-    }
-    
-    TopItems.Estimator<Long> estimator = new Estimator(cluster);
-    
-    List<RecommendedItem> topItems = TopItems.getTopItems(NUM_CLUSTER_RECS,
-      possibleItemIDs.iterator(), null, estimator);
-    
-    log.debug("Recommendations are: {}", topItems);
-    return Collections.unmodifiableList(topItems);
-  }
-  
-  private static FastByIDMap<FastIDSet> computeClustersPerUserID(Collection<FastIDSet> clusters) {
-    FastByIDMap<FastIDSet> clustersPerUser = new FastByIDMap<FastIDSet>(clusters.size());
-    for (FastIDSet cluster : clusters) {
-      LongPrimitiveIterator it = cluster.iterator();
-      while (it.hasNext()) {
-        clustersPerUser.put(it.nextLong(), cluster);
-      }
-    }
-    return clustersPerUser;
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    refreshHelper.refresh(alreadyRefreshed);
-  }
-  
-  @Override
-  public String toString() {
-    return "TreeClusteringRecommender2[clusterSimilarity:" + clusterSimilarity + ']';
-  }
-  
-  private final class Estimator implements TopItems.Estimator<Long> {
-    
-    private final FastIDSet cluster;
-    
-    private Estimator(FastIDSet cluster) {
-      this.cluster = cluster;
-    }
-    
-    @Override
-    public double estimate(Long itemID) throws TasteException {
-      DataModel dataModel = getDataModel();
-      RunningAverage average = new FullRunningAverage();
-      LongPrimitiveIterator it = cluster.iterator();
-      while (it.hasNext()) {
-        Float pref = dataModel.getPreferenceValue(it.nextLong(), itemID);
-        if (pref != null) {
-          average.addDatum(pref);
-        }
-      }
-      return average.getAverage();
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/ConjugateGradientOptimizer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/ConjugateGradientOptimizer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/ConjugateGradientOptimizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/ConjugateGradientOptimizer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.knn;
-
-import java.util.Arrays;
-
-@Deprecated
-public final class ConjugateGradientOptimizer implements Optimizer {
-  
-  private static final double CONVERGENCE_LIMIT = 0.1;
-  private static final int MAX_ITERATIONS = 1000;
-  
-  /**
-   * <p>
-   * Conjugate gradient optimization. Matlab code:
-   * </p>
-   * 
-   * <p>
-   * 
-   * <pre>
-   * function [x] = conjgrad(A,b,x0)
-   *   x = x0;
-   *   r = b - A*x0;
-   *   w = -r;
-   *   for i = 1:size(A);
-   *      z = A*w;
-   *      a = (r'*w)/(w'*z);
-   *      x = x + a*w;
-   *      r = r - a*z;
-   *      if ( norm(r) < 1e-10 )
-   *           break;
-   *      end
-   *      B = (r'*z)/(w'*z);
-   *      w = -r + B*w;
-   *   end
-   * end
-   * </pre>
-   * 
-   * </p>
-   * 
-   * @param matrix
-   *          matrix nxn positions
-   * @param b
-   *          vector b, n positions
-   * @return vector of n weights
-   */
-  @Override
-  public double[] optimize(double[][] matrix, double[] b) {
-    
-    int k = b.length;
-    double[] x = new double[k];
-    double[] r = new double[k];
-    double[] w = new double[k];
-    double[] z = new double[k];
-    Arrays.fill(x, 3.0 / k);
-    
-    // r = b - A*x0;
-    // w = -r;
-    for (int i = 0; i < k; i++) {
-      double v = 0.0;
-      double[] ai = matrix[i];
-      for (int j = 0; j < k; j++) {
-        v += ai[j] * x[j];
-      }
-      double ri = b[i] - v;
-      r[i] = ri;
-      w[i] = -ri;
-    }
-    
-    for (int iteration = 0; iteration < MAX_ITERATIONS; iteration++) {
-      
-      // z = A*w;
-      for (int i = 0; i < k; i++) {
-        double v = 0.0;
-        double[] ai = matrix[i];
-        for (int j = 0; j < k; j++) {
-          v += ai[j] * w[j];
-        }
-        z[i] = v;
-      }
-      
-      // a = (r'*w)/(w'*z);
-      double anum = 0.0;
-      double aden = 0.0;
-      for (int i = 0; i < k; i++) {
-        anum += r[i] * w[i];
-        aden += w[i] * z[i];
-      }
-      double a = anum / aden;
-      
-      // x = x + a*w;
-      // r = r - a*z;
-      for (int i = 0; i < k; i++) {
-        x[i] += a * w[i];
-        r[i] -= a * z[i];
-      }
-      
-      // stop when residual is close to 0
-      double rdot = 0.0;
-      for (int i = 0; i < k; i++) {
-        double value = r[i];
-        rdot += value * value;
-      }
-      if (rdot <= CONVERGENCE_LIMIT) {
-        break;
-      }
-      
-      // B = (r'*z)/(w'*z);
-      double bnum = 0.0;
-      double bden = 0.0;
-      for (int i = 0; i < k; i++) {
-        double zi = z[i];
-        bnum += r[i] * zi;
-        bden += w[i] * zi;
-      }
-      double B = bnum / bden;
-      
-      // w = -r + B*w;
-      for (int i = 0; i < k; i++) {
-        w[i] = -r[i] + B * w[i];
-      }
-      
-    }
-    
-    return x;
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,252 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.knn;
-
-import java.util.Collection;
-import java.util.List;
-
-import com.google.common.collect.Lists;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender;
-import org.apache.mahout.cf.taste.impl.recommender.TopItems;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.recommender.CandidateItemsStrategy;
-import org.apache.mahout.cf.taste.recommender.MostSimilarItemsCandidateItemsStrategy;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Rescorer;
-import org.apache.mahout.cf.taste.similarity.ItemSimilarity;
-import org.apache.mahout.common.LongPair;
-
-/**
- * <p>
- * The weights to compute the final predicted preferences are calculated using linear interpolation, through
- * an {@link Optimizer}. This algorithm is based in the paper of Robert M. Bell and Yehuda Koren in ICDM '07.
- * </p>
- */
-@Deprecated
-public final class KnnItemBasedRecommender extends GenericItemBasedRecommender {
-  
-  private static final double BETA = 500.0;
-
-  private final Optimizer optimizer;
-  private final int neighborhoodSize;
-  
-  public KnnItemBasedRecommender(DataModel dataModel,
-                                 ItemSimilarity similarity,
-                                 Optimizer optimizer,
-                                 CandidateItemsStrategy candidateItemsStrategy,
-                                 MostSimilarItemsCandidateItemsStrategy mostSimilarItemsCandidateItemsStrategy,
-                                 int neighborhoodSize) {
-    super(dataModel, similarity, candidateItemsStrategy, mostSimilarItemsCandidateItemsStrategy);
-    this.optimizer = optimizer;
-    this.neighborhoodSize = neighborhoodSize;
-  }
-
-  public KnnItemBasedRecommender(DataModel dataModel,
-                                 ItemSimilarity similarity,
-                                 Optimizer optimizer,
-                                 int neighborhoodSize) {
-    this(dataModel, similarity, optimizer, getDefaultCandidateItemsStrategy(),
-        getDefaultMostSimilarItemsCandidateItemsStrategy(), neighborhoodSize);
-  }
-  
-  private List<RecommendedItem> mostSimilarItems(long itemID,
-                                                 LongPrimitiveIterator possibleItemIDs,
-                                                 int howMany,
-                                                 Rescorer<LongPair> rescorer) throws TasteException {
-    TopItems.Estimator<Long> estimator = new MostSimilarEstimator(itemID, getSimilarity(), rescorer);
-    return TopItems.getTopItems(howMany, possibleItemIDs, null, estimator);
-  }
-  
-  private double[] getInterpolations(long itemID, 
-                                     long[] itemNeighborhood,
-                                     Collection<Long> usersRatedNeighborhood) throws TasteException {
-    
-    int length = 0;
-    for (int i = 0; i < itemNeighborhood.length; i++) {
-      if (itemNeighborhood[i] == itemID) {
-        itemNeighborhood[i] = -1;
-        length = itemNeighborhood.length - 1;
-        break;
-      }
-    }
-    
-    int k = length;
-    double[][] aMatrix = new double[k][k];
-    double[] b = new double[k];
-    int i = 0;
-    
-    DataModel dataModel = getDataModel();
-    
-    int numUsers = usersRatedNeighborhood.size();
-    for (long iitem : itemNeighborhood) {
-      if (iitem == -1) {
-        break;
-      }
-      int j = 0;
-      double value = 0.0;
-      for (long jitem : itemNeighborhood) {
-        if (jitem == -1) {
-          continue;
-        }
-        for (long user : usersRatedNeighborhood) {
-          float prefVJ = dataModel.getPreferenceValue(user, iitem);
-          float prefVK = dataModel.getPreferenceValue(user, jitem);
-          value += prefVJ * prefVK;
-        }
-        aMatrix[i][j] = value / numUsers;
-        j++;
-      }
-      i++;
-    }
-    
-    i = 0;
-    for (long jitem : itemNeighborhood) {
-      if (jitem == -1) {
-        break;
-      }
-      double value = 0.0;
-      for (long user : usersRatedNeighborhood) {
-        float prefVJ = dataModel.getPreferenceValue(user, jitem);
-        float prefVI = dataModel.getPreferenceValue(user, itemID);
-        value += prefVJ * prefVI;
-      }
-      b[i] = value / numUsers;
-      i++;
-    }
-    
-    // Find the larger diagonal and calculate the average
-    double avgDiagonal = 0.0;
-    if (k > 1) {
-      double diagonalA = 0.0;
-      for (i = 0; i < k; i++) {
-        diagonalA += aMatrix[i][i];
-      }
-      double diagonalB = 0.0;
-      for (i = k - 1; i >= 0; i--) {
-        for (int j = 0; j < k; j++) {
-          diagonalB += aMatrix[i--][j];
-        }
-      }
-      avgDiagonal = Math.max(diagonalA, diagonalB) / k;
-    }
-    // Calculate the average of non-diagonal values
-    double avgMatrixA = 0.0;
-    double avgVectorB = 0.0;
-    for (i = 0; i < k; i++) {
-      for (int j = 0; j < k; j++) {
-        if (i != j || k <= 1) {
-          avgMatrixA += aMatrix[i][j];
-        }
-      }
-      avgVectorB += b[i];
-    }
-    if (k > 1) {
-      avgMatrixA /= k * k - k;
-    }
-    avgVectorB /= k;
-
-    double numUsersPlusBeta = numUsers + BETA;
-    for (i = 0; i < k; i++) {
-      for (int j = 0; j < k; j++) {
-        double average;
-        if (i == j && k > 1) {
-          average = avgDiagonal;
-        } else {
-          average = avgMatrixA;
-        }
-        aMatrix[i][j] = (numUsers * aMatrix[i][j] + BETA * average) / numUsersPlusBeta;
-      }
-      b[i] = (numUsers * b[i] + BETA * avgVectorB) / numUsersPlusBeta;
-    }
-
-    return optimizer.optimize(aMatrix, b);
-  }
-  
-  @Override
-  protected float doEstimatePreference(long theUserID, PreferenceArray preferencesFromUser, long itemID)
-    throws TasteException {
-    
-    DataModel dataModel = getDataModel();
-    int size = preferencesFromUser.length();
-    FastIDSet possibleItemIDs = new FastIDSet(size);
-    for (int i = 0; i < size; i++) {
-      possibleItemIDs.add(preferencesFromUser.getItemID(i));
-    }
-    possibleItemIDs.remove(itemID);
-    
-    List<RecommendedItem> mostSimilar = mostSimilarItems(itemID, possibleItemIDs.iterator(),
-      neighborhoodSize, null);
-    long[] theNeighborhood = new long[mostSimilar.size() + 1];
-    theNeighborhood[0] = -1;
-  
-    List<Long> usersRatedNeighborhood = Lists.newArrayList();
-    int nOffset = 0;
-    for (RecommendedItem rec : mostSimilar) {
-      theNeighborhood[nOffset++] = rec.getItemID();
-    }
-    
-    if (!mostSimilar.isEmpty()) {
-      theNeighborhood[mostSimilar.size()] = itemID;
-      for (int i = 0; i < theNeighborhood.length; i++) {
-        PreferenceArray usersNeighborhood = dataModel.getPreferencesForItem(theNeighborhood[i]);
-        int size1 = usersRatedNeighborhood.isEmpty() ? usersNeighborhood.length() : usersRatedNeighborhood.size();
-        for (int j = 0; j < size1; j++) {
-          if (i == 0) {
-            usersRatedNeighborhood.add(usersNeighborhood.getUserID(j));
-          } else {
-            if (j >= usersRatedNeighborhood.size()) {
-              break;
-            }
-            long index = usersRatedNeighborhood.get(j);
-            if (!usersNeighborhood.hasPrefWithUserID(index) || index == theUserID) {
-              usersRatedNeighborhood.remove(index);
-              j--;
-            }
-          }
-        }
-      }
-    }
-
-    double[] weights = null;
-    if (!mostSimilar.isEmpty()) {
-      weights = getInterpolations(itemID, theNeighborhood, usersRatedNeighborhood);
-    }
-    
-    int i = 0;
-    double preference = 0.0;
-    double totalSimilarity = 0.0;
-    for (long jitem : theNeighborhood) {
-      
-      Float pref = dataModel.getPreferenceValue(theUserID, jitem);
-      
-      if (pref != null) {
-        double weight = weights[i];
-        preference += pref * weight;
-        totalSimilarity += weight;
-      }
-      i++;
-      
-    }
-    return totalSimilarity == 0.0 ? Float.NaN : (float) (preference / totalSimilarity);
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/NonNegativeQuadraticOptimizer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/NonNegativeQuadraticOptimizer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/NonNegativeQuadraticOptimizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/NonNegativeQuadraticOptimizer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.knn;
-
-import java.util.Arrays;
-
-/**
- * Non-negative Quadratic Optimization. Based on the paper of Robert M. Bell and Yehuda Koren in ICDM '07.
- * Thanks to Dan Tillberg for the hints in the implementation.
- */
-@Deprecated
-public final class NonNegativeQuadraticOptimizer implements Optimizer {
-  
-  private static final double EPSILON = 1.0e-10;
-  private static final double CONVERGENCE_LIMIT = 0.1;
-  private static final int MAX_ITERATIONS = 1000;
-  private static final double DEFAULT_STEP = 0.001;
-  
-  /**
-   * Non-negative Quadratic Optimization.
-   * 
-   * @param matrix
-   *          matrix nxn positions
-   * @param b
-   *          vector b, n positions
-   * @return vector of n weights
-   */
-  @Override
-  public double[] optimize(double[][] matrix, double[] b) {
-    int k = b.length;
-    double[] r = new double[k];
-    double[] x = new double[k];
-    Arrays.fill(x, 3.0 / k);
-    
-    for (int iteration = 0; iteration < MAX_ITERATIONS; iteration++) {
-      
-      double rdot = 0.0;
-      for (int n = 0; n < k; n++) {
-        double sumAw = 0.0;
-        double[] rowAn = matrix[n];
-        for (int i = 0; i < k; i++) {
-          sumAw += rowAn[i] * x[i];
-        }
-        // r = b - Ax; // the residual, or 'steepest gradient'
-        double rn = b[n] - sumAw;
-        
-        // find active variables - those that are pinned due to
-        // nonnegativity constraints; set respective ri's to zero
-        if (x[n] < EPSILON && rn < 0.0) {
-          rn = 0.0;
-        } else {
-          // max step size numerator
-          rdot += rn * rn;
-        }
-        r[n] = rn;
-      }
-      
-      if (rdot <= CONVERGENCE_LIMIT) {
-        break;
-      }
-      
-      // max step size denominator
-      double rArdotSum = 0.0;
-      for (int n = 0; n < k; n++) {
-        double sumAr = 0.0;
-        double[] rowAn = matrix[n];
-        for (int i = 0; i < k; i++) {
-          sumAr += rowAn[i] * r[i];
-        }
-        rArdotSum += r[n] * sumAr;
-      }
-      
-      // max step size
-      double stepSize = rdot / rArdotSum;
-      
-      if (Double.isNaN(stepSize)) {
-        stepSize = DEFAULT_STEP;
-      }
-      
-      // adjust step size to prevent negative values
-      for (int n = 0; n < k; n++) {
-        if (r[n] < 0.0) {
-          double absStepSize = stepSize < 0.0 ? -stepSize : stepSize;
-          stepSize = Math.min(absStepSize, Math.abs(x[n] / r[n])) * stepSize / absStepSize;
-        }
-      }
-      
-      // update x values
-      for (int n = 0; n < k; n++) {
-        x[n] += stepSize * r[n];
-        if (x[n] < EPSILON) {
-          x[n] = 0.0;
-        }
-      }
-      
-      // TODO: do something in case of divergence
-    }
-    
-    return x;
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/Optimizer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/Optimizer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/Optimizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/knn/Optimizer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,25 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.knn;
-
-@Deprecated
-public interface Optimizer {
-  
-  double[] optimize(double[][] matrix, double[] b);
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorage.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorage.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorage.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,401 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone;
-
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.concurrent.Callable;
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.common.Weighting;
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.recommender.slopeone.DiffStorage;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * An implementation of {@link DiffStorage} that merely stores item-item diffs in memory. It is fast, but can
- * consume a great deal of memory.
- * </p>
- */
-@Deprecated
-public final class MemoryDiffStorage implements DiffStorage {
-  
-  private static final Logger log = LoggerFactory.getLogger(MemoryDiffStorage.class);
-  
-  private final DataModel dataModel;
-  private final boolean stdDevWeighted;
-  private final long maxEntries;
-  private final FastByIDMap<FastByIDMap<RunningAverage>> averageDiffs;
-  private final FastByIDMap<RunningAverage> averageItemPref;
-  private final FastIDSet allRecommendableItemIDs;
-  private final ReadWriteLock buildAverageDiffsLock;
-  private final RefreshHelper refreshHelper;
-  
-  /**
-   * <p>
-   * See {@link SlopeOneRecommender} for the meaning of
-   * {@code stdDevWeighted}. {@code maxEntries} controls the maximum number of
-   * item-item average preference differences that will be tracked internally. After the limit is reached, if
-   * a new item-item pair is observed in the data it will be ignored. This is recommended for large datasets.
-   * The first {@code maxEntries} item-item pairs observed in the data are tracked. Assuming that item
-   * ratings are reasonably distributed among users, this should only ignore item-item pairs that are very
-   * infrequently co-rated by a user. The intuition is that data on these infrequently co-rated item-item
-   * pairs is less reliable and should be the first that is ignored. This parameter can be used to limit the
-   * memory requirements of {@link SlopeOneRecommender}, which otherwise grow as the square of the number of
-   * items that exist in the {@link DataModel}. Memory requirements can reach gigabytes with only about 10000
-   * items, so this may be necessary on larger datasets.
-   *
-   * @param stdDevWeighted
-   *          see {@link SlopeOneRecommender}
-   * @param maxEntries
-   *          maximum number of item-item average preference differences to track internally
-   * @throws IllegalArgumentException
-   *           if {@code maxEntries} is not positive or {@code dataModel} is null
-   */
-  public MemoryDiffStorage(DataModel dataModel,
-                           Weighting stdDevWeighted,
-                           long maxEntries) throws TasteException {
-    Preconditions.checkArgument(dataModel != null, "dataModel is null");
-    Preconditions.checkArgument(dataModel.getNumItems() >= 1, "dataModel has no items");
-    Preconditions.checkArgument(maxEntries > 0L, "maxEntries must be positive");
-    this.dataModel = dataModel;
-    this.stdDevWeighted = stdDevWeighted == Weighting.WEIGHTED;
-    this.maxEntries = maxEntries;
-    this.averageDiffs = new FastByIDMap<FastByIDMap<RunningAverage>>();
-    this.averageItemPref = new FastByIDMap<RunningAverage>();
-    this.buildAverageDiffsLock = new ReentrantReadWriteLock();
-    this.allRecommendableItemIDs = new FastIDSet(dataModel.getNumItems());
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        buildAverageDiffs();
-        return null;
-      }
-    });
-    refreshHelper.addDependency(dataModel);
-    buildAverageDiffs();
-  }
-  
-  @Override
-  public RunningAverage getDiff(long itemID1, long itemID2) {
-    
-    boolean inverted = false;
-    if (itemID1 > itemID2) {
-      inverted = true;
-      long temp = itemID1;
-      itemID1 = itemID2;
-      itemID2 = temp;
-    }
-    
-    FastByIDMap<RunningAverage> level2Map;
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      level2Map = averageDiffs.get(itemID1);
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-    RunningAverage average = null;
-    if (level2Map != null) {
-      average = level2Map.get(itemID2);
-    }
-    if (inverted) {
-      return average == null ? null : average.inverse();
-    } else {
-      return average;
-    }
-  }
-  
-  @Override
-  public RunningAverage[] getDiffs(long userID, long itemID, PreferenceArray prefs) {
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      int size = prefs.length();
-      RunningAverage[] result = new RunningAverage[size];
-      for (int i = 0; i < size; i++) {
-        result[i] = getDiff(prefs.getItemID(i), itemID);
-      }
-      return result;
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-  }
-  
-  @Override
-  public RunningAverage getAverageItemPref(long itemID) {
-    return averageItemPref.get(itemID);
-  }
-
-  @Override
-  public void addItemPref(long userID, long itemIDA, float prefValue) throws TasteException {
-    PreferenceArray userPreferences = dataModel.getPreferencesFromUser(userID);
-    try {
-      buildAverageDiffsLock.writeLock().lock();
-
-      FastByIDMap<RunningAverage> aMap = averageDiffs.get(itemIDA);
-      if (aMap == null) {
-        aMap = new FastByIDMap<RunningAverage>();
-        averageDiffs.put(itemIDA, aMap);
-      }
-
-      int length = userPreferences.length();
-      for (int i = 0; i < length; i++) {
-        long itemIDB = userPreferences.getItemID(i);
-        float bValue = userPreferences.getValue(i);
-        if (itemIDA < itemIDB) {
-          RunningAverage average = aMap.get(itemIDB);
-          if (average == null) {
-            average = buildRunningAverage();
-            aMap.put(itemIDB, average);
-          }
-          average.addDatum(bValue - prefValue);
-        } else {
-          FastByIDMap<RunningAverage> bMap = averageDiffs.get(itemIDB);
-          if (bMap == null) {
-            bMap = new FastByIDMap<RunningAverage>();
-            averageDiffs.put(itemIDB, bMap);
-          }
-          RunningAverage average = bMap.get(itemIDA);
-          if (average == null) {
-            average = buildRunningAverage();
-            bMap.put(itemIDA, average);
-          }
-          average.addDatum(prefValue - bValue);
-        }
-      }
-
-    } finally {
-      buildAverageDiffsLock.writeLock().unlock();
-    }
-  }
-  
-  @Override
-  public void updateItemPref(long itemID, float prefDelta) {
-    if (stdDevWeighted) {
-      throw new UnsupportedOperationException("Can't update only when stdDevWeighted is set");
-    }
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      for (Map.Entry<Long,FastByIDMap<RunningAverage>> entry : averageDiffs.entrySet()) {
-        boolean matchesItemID1 = itemID == entry.getKey();
-        for (Map.Entry<Long,RunningAverage> entry2 : entry.getValue().entrySet()) {
-          RunningAverage average = entry2.getValue();
-          if (matchesItemID1) {
-            average.changeDatum(-prefDelta);
-          } else if (itemID == entry2.getKey()) {
-            average.changeDatum(prefDelta);
-          }
-        }
-      }
-      RunningAverage itemAverage = averageItemPref.get(itemID);
-      if (itemAverage != null) {
-        itemAverage.changeDatum(prefDelta);
-      }
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-  }
-
-  @Override
-  public void removeItemPref(long userID, long itemIDA, float prefValue) throws TasteException {
-    PreferenceArray userPreferences = dataModel.getPreferencesFromUser(userID);
-    try {
-      buildAverageDiffsLock.writeLock().lock();
-
-      FastByIDMap<RunningAverage> aMap = averageDiffs.get(itemIDA);
-
-      int length = userPreferences.length();
-      for (int i = 0; i < length; i++) {
-
-        long itemIDB = userPreferences.getItemID(i);
-        float bValue = userPreferences.getValue(i);
-
-        if (itemIDA < itemIDB) {
-
-          if (aMap != null) {
-            RunningAverage average = aMap.get(itemIDB);
-            if (average != null) {
-              if (average.getCount() <= 1) {
-                aMap.remove(itemIDB);
-              } else {
-                average.removeDatum(bValue - prefValue);
-              }
-            }
-          }
-
-        } else  if (itemIDA > itemIDB) {
-
-          FastByIDMap<RunningAverage> bMap = averageDiffs.get(itemIDB);
-          if (bMap != null) {
-            RunningAverage average = bMap.get(itemIDA);
-            if (average != null) {
-              if (average.getCount() <= 1) {
-                aMap.remove(itemIDA);
-              } else {
-                average.removeDatum(prefValue - bValue);
-              }
-            }
-          }
-
-        }
-      }
-
-    } finally {
-      buildAverageDiffsLock.writeLock().unlock();
-    }
-  }
-  
-  @Override
-  public FastIDSet getRecommendableItemIDs(long userID) throws TasteException {
-    FastIDSet result;
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      result = allRecommendableItemIDs.clone();
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-    Iterator<Long> it = result.iterator();
-    while (it.hasNext()) {
-      if (dataModel.getPreferenceValue(userID, it.next()) != null) {
-        it.remove();
-      }
-    }
-    return result;
-  }
-  
-  private void buildAverageDiffs() throws TasteException {
-    log.info("Building average diffs...");
-    try {
-      buildAverageDiffsLock.writeLock().lock();
-      averageDiffs.clear();
-      long averageCount = 0L;
-      LongPrimitiveIterator it = dataModel.getUserIDs();
-      while (it.hasNext()) {
-        averageCount = processOneUser(averageCount, it.nextLong());
-      }
-      
-      pruneInconsequentialDiffs();
-      updateAllRecommendableItems();
-      
-    } finally {
-      buildAverageDiffsLock.writeLock().unlock();
-    }
-  }
-  
-  private void pruneInconsequentialDiffs() {
-    // Go back and prune inconsequential diffs. "Inconsequential" means, here, only represented by one
-    // data point, so possibly unreliable
-    Iterator<Map.Entry<Long,FastByIDMap<RunningAverage>>> it1 = averageDiffs.entrySet().iterator();
-    while (it1.hasNext()) {
-      FastByIDMap<RunningAverage> map = it1.next().getValue();
-      Iterator<Map.Entry<Long,RunningAverage>> it2 = map.entrySet().iterator();
-      while (it2.hasNext()) {
-        RunningAverage average = it2.next().getValue();
-        if (average.getCount() <= 1) {
-          it2.remove();
-        }
-      }
-      if (map.isEmpty()) {
-        it1.remove();
-      } else {
-        map.rehash();
-      }
-    }
-    averageDiffs.rehash();
-  }
-  
-  private void updateAllRecommendableItems() throws TasteException {
-    FastIDSet ids = new FastIDSet(dataModel.getNumItems());
-    for (Map.Entry<Long,FastByIDMap<RunningAverage>> entry : averageDiffs.entrySet()) {
-      ids.add(entry.getKey());
-      LongPrimitiveIterator it = entry.getValue().keySetIterator();
-      while (it.hasNext()) {
-        ids.add(it.next());
-      }
-    }
-    allRecommendableItemIDs.clear();
-    allRecommendableItemIDs.addAll(ids);
-    allRecommendableItemIDs.rehash();
-  }
-  
-  private long processOneUser(long averageCount, long userID) throws TasteException {
-    log.debug("Processing prefs for user {}", userID);
-    // Save off prefs for the life of this loop iteration
-    PreferenceArray userPreferences = dataModel.getPreferencesFromUser(userID);
-    int length = userPreferences.length();
-    for (int i = 0; i < length; i++) { // Loop to length-1, not length-2, not for diffs but average item pref
-      float prefAValue = userPreferences.getValue(i);
-      long itemIDA = userPreferences.getItemID(i);
-      FastByIDMap<RunningAverage> aMap = averageDiffs.get(itemIDA);
-      if (aMap == null) {
-        aMap = new FastByIDMap<RunningAverage>();
-        averageDiffs.put(itemIDA, aMap);
-      }
-      for (int j = i + 1; j < length; j++) {
-        // This is a performance-critical block
-        long itemIDB = userPreferences.getItemID(j);
-        RunningAverage average = aMap.get(itemIDB);
-        if (average == null && averageCount < maxEntries) {
-          average = buildRunningAverage();
-          aMap.put(itemIDB, average);
-          averageCount++;
-        }
-        if (average != null) {
-          average.addDatum(userPreferences.getValue(j) - prefAValue);
-        }
-      }
-      RunningAverage itemAverage = averageItemPref.get(itemIDA);
-      if (itemAverage == null) {
-        itemAverage = buildRunningAverage();
-        averageItemPref.put(itemIDA, itemAverage);
-      }
-      itemAverage.addDatum(prefAValue);
-    }
-    return averageCount;
-  }
-  
-  private RunningAverage buildRunningAverage() {
-    return stdDevWeighted ? new FullRunningAverageAndStdDev() : new FullRunningAverage();
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    refreshHelper.refresh(alreadyRefreshed);
-  }
-  
-  @Override
-  public String toString() {
-    return "MemoryDiffStorage";
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,227 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone;
-
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.mahout.cf.taste.common.NoSuchUserException;
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.common.Weighting;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.impl.recommender.AbstractRecommender;
-import org.apache.mahout.cf.taste.impl.recommender.TopItems;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.recommender.IDRescorer;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.slopeone.DiffStorage;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * A basic "slope one" recommender. (See an <a href="http://www.daniel-lemire.com/fr/abstracts/SDM2005.html">
- * excellent summary here</a> for example.) This {@link org.apache.mahout.cf.taste.recommender.Recommender} is
- * especially suitable when user preferences are updating frequently as it can incorporate this information
- * without expensive recomputation.
- * </p>
- * 
- * <p>
- * This implementation can also be used as a "weighted slope one" recommender.
- * </p>
- */
-@Deprecated
-public final class SlopeOneRecommender extends AbstractRecommender {
-  
-  private static final Logger log = LoggerFactory.getLogger(SlopeOneRecommender.class);
-  
-  private final boolean weighted;
-  private final boolean stdDevWeighted;
-  private final DiffStorage diffStorage;
-  
-  /**
-   * <p>
-   * Creates a default (weighted)  based on the given {@link DataModel}.
-   * </p>
-   */
-  public SlopeOneRecommender(DataModel dataModel) throws TasteException {
-    this(dataModel,
-         Weighting.WEIGHTED,
-         Weighting.WEIGHTED,
-         new MemoryDiffStorage(dataModel, Weighting.WEIGHTED, Long.MAX_VALUE));
-  }
-  
-  /**
-   * <p>
-   * Creates a  based on the given {@link DataModel}.
-   * </p>
-   *
-   * <p>
-   * If {@code weighted} is set, acts as a weighted slope one recommender. This implementation also
-   * includes an experimental "standard deviation" weighting which weights item-item ratings diffs with lower
-   * standard deviation more highly, on the theory that they are more reliable.
-   * </p>
-   *
-   * @param weighting
-   *          if {@link Weighting#WEIGHTED}, acts as a weighted slope one recommender
-   * @param stdDevWeighting
-   *          use optional standard deviation weighting of diffs
-   * @throws IllegalArgumentException
-   *           if {@code diffStorage} is null, or stdDevWeighted is set when weighted is not set
-   */
-  public SlopeOneRecommender(DataModel dataModel,
-                             Weighting weighting,
-                             Weighting stdDevWeighting,
-                             DiffStorage diffStorage) {
-    super(dataModel);
-    Preconditions.checkArgument(stdDevWeighting != Weighting.WEIGHTED || weighting != Weighting.UNWEIGHTED,
-      "weighted required when stdDevWeighted is set");
-    Preconditions.checkArgument(diffStorage != null, "diffStorage is null");
-    this.weighted = weighting == Weighting.WEIGHTED;
-    this.stdDevWeighted = stdDevWeighting == Weighting.WEIGHTED;
-    this.diffStorage = diffStorage;
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException {
-    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");
-    log.debug("Recommending items for user ID '{}'", userID);
-
-    FastIDSet possibleItemIDs = diffStorage.getRecommendableItemIDs(userID);
-
-    TopItems.Estimator<Long> estimator = new Estimator(userID);
-
-    List<RecommendedItem> topItems = TopItems.getTopItems(howMany, possibleItemIDs.iterator(), rescorer,
-      estimator);
-
-    log.debug("Recommendations are: {}", topItems);
-    return topItems;
-  }
-  
-  @Override
-  public float estimatePreference(long userID, long itemID) throws TasteException {
-    DataModel model = getDataModel();
-    Float actualPref = model.getPreferenceValue(userID, itemID);
-    if (actualPref != null) {
-      return actualPref;
-    }
-    return doEstimatePreference(userID, itemID);
-  }
-  
-  private float doEstimatePreference(long userID, long itemID) throws TasteException {
-    double count = 0.0;
-    double totalPreference = 0.0;
-    PreferenceArray prefs = getDataModel().getPreferencesFromUser(userID);
-    RunningAverage[] averages = diffStorage.getDiffs(userID, itemID, prefs);
-    int size = prefs.length();
-    for (int i = 0; i < size; i++) {
-      RunningAverage averageDiff = averages[i];
-      if (averageDiff != null) {
-        double averageDiffValue = averageDiff.getAverage();
-        if (weighted) {
-          double weight = averageDiff.getCount();
-          if (stdDevWeighted) {
-            double stdev = ((RunningAverageAndStdDev) averageDiff).getStandardDeviation();
-            if (!Double.isNaN(stdev)) {
-              weight /= 1.0 + stdev;
-            }
-            // If stdev is NaN, then it is because count is 1. Because we're weighting by count,
-            // the weight is already relatively low. We effectively assume stdev is 0.0 here and
-            // that is reasonable enough. Otherwise, dividing by NaN would yield a weight of NaN
-            // and disqualify this pref entirely
-            // (Thanks Daemmon)
-          }
-          totalPreference += weight * (prefs.getValue(i) + averageDiffValue);
-          count += weight;
-        } else {
-          totalPreference += prefs.getValue(i) + averageDiffValue;
-          count += 1.0;
-        }
-      }
-    }
-    if (count <= 0.0) {
-      RunningAverage itemAverage = diffStorage.getAverageItemPref(itemID);
-      return itemAverage == null ? Float.NaN : (float) itemAverage.getAverage();
-    } else {
-      return (float) (totalPreference / count);
-    }
-  }
-  
-  @Override
-  public void setPreference(long userID, long itemID, float value) throws TasteException {
-    DataModel dataModel = getDataModel();
-    Float oldPref;
-    try {
-      oldPref = dataModel.getPreferenceValue(userID, itemID);
-    } catch (NoSuchUserException nsee) {
-      oldPref = null;
-    }
-    super.setPreference(userID, itemID, value);
-    if (oldPref == null) {
-      // Add new preference
-      diffStorage.addItemPref(userID, itemID, value);
-    } else {
-      // Update preference
-      diffStorage.updateItemPref(itemID, value - oldPref);
-    }
-  }
-  
-  @Override
-  public void removePreference(long userID, long itemID) throws TasteException {
-    DataModel dataModel = getDataModel();
-    Float oldPref = dataModel.getPreferenceValue(userID, itemID);
-    super.removePreference(userID, itemID);
-    if (oldPref != null) {
-      diffStorage.removeItemPref(userID, itemID, oldPref);
-    }
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);
-    RefreshHelper.maybeRefresh(alreadyRefreshed, diffStorage);
-  }
-  
-  @Override
-  public String toString() {
-    return "SlopeOneRecommender[weighted:" + weighted + ", stdDevWeighted:" + stdDevWeighted
-           + ", diffStorage:" + diffStorage + ']';
-  }
-  
-  private final class Estimator implements TopItems.Estimator<Long> {
-    
-    private final long userID;
-    
-    private Estimator(long userID) {
-      this.userID = userID;
-    }
-    
-    @Override
-    public double estimate(Long itemID) throws TasteException {
-      return doEstimatePreference(userID, itemID);
-    }
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/file/FileDiffStorage.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/file/FileDiffStorage.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/file/FileDiffStorage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/file/FileDiffStorage.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,324 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone.file;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-import java.util.regex.Pattern;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.recommender.slopeone.DiffStorage;
-import org.apache.mahout.common.iterator.FileLineIterator;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * {@link DiffStorage} which reads pre-computed diffs from a file and stores in memory. The file should have
- * one diff per line:
- * </p>
- * 
- * {@code itemID1,itemID2,diff[,count[,mk,sk]]}
- * 
- * <p>
- * The fourth column is optional, and is a count representing the number of occurrences of the item-item pair
- * that contribute to the diff. It is assumed to be 1 if not present. The fifth and sixth arguments are
- * computed values used by {@link FullRunningAverageAndStdDev} implementations to compute a running standard deviation.
- * They are required if using {@link org.apache.mahout.cf.taste.common.Weighting#WEIGHTED}
- * with {@link org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender}.
- * </p>
- *
- * <p>
- * Commas or tabs can be delimiters. This is intended for use in conjuction with the output of
- * {@link org.apache.mahout.cf.taste.hadoop.slopeone.SlopeOneAverageDiffsJob}.
- * </p>
- *
- * <p>Note that the same item-item pair should not appear on multiple lines -- one line per item-item pair.</p>
- */
-@Deprecated
-public final class FileDiffStorage implements DiffStorage {
-  
-  private static final Logger log = LoggerFactory.getLogger(FileDiffStorage.class);
-  
-  private static final long MIN_RELOAD_INTERVAL_MS = 60 * 1000L; // 1 minute?
-  private static final char COMMENT_CHAR = '#';
-  private static final Pattern SEPARATOR = Pattern.compile("[\t,]");
-
-  private final File dataFile;
-  private long lastModified;
-  private final long maxEntries;
-  private final FastByIDMap<FastByIDMap<RunningAverage>> averageDiffs;
-  private final FastIDSet allRecommendableItemIDs;
-  private final ReadWriteLock buildAverageDiffsLock;
-  
-  /**
-   * @param dataFile
-   *          diffs file
-   * @param maxEntries
-   *          maximum number of diffs to store
-   * @throws FileNotFoundException
-   *           if data file does not exist or is a directory
-   */
-  public FileDiffStorage(File dataFile, long maxEntries) throws FileNotFoundException {
-    Preconditions.checkArgument(dataFile != null, "dataFile is null");
-    if (!dataFile.exists() || dataFile.isDirectory()) {
-      throw new FileNotFoundException(dataFile.toString());
-    }
-    Preconditions.checkArgument(maxEntries > 0L, "maxEntries must be positive");
-    log.info("Creating FileDataModel for file {}", dataFile);
-    this.dataFile = dataFile.getAbsoluteFile();
-    this.lastModified = dataFile.lastModified();
-    this.maxEntries = maxEntries;
-    this.averageDiffs = new FastByIDMap<FastByIDMap<RunningAverage>>();
-    this.allRecommendableItemIDs = new FastIDSet();
-    this.buildAverageDiffsLock = new ReentrantReadWriteLock();
-
-    buildDiffs();
-  }
-  
-  private void buildDiffs() {
-    if (buildAverageDiffsLock.writeLock().tryLock()) {
-      try {
-
-        averageDiffs.clear();
-        allRecommendableItemIDs.clear();
-        
-        FileLineIterator iterator = new FileLineIterator(dataFile, false);
-        String firstLine = iterator.peek();
-        while (firstLine.isEmpty() || firstLine.charAt(0) == COMMENT_CHAR) {
-          iterator.next();
-          firstLine = iterator.peek();
-        }
-        long averageCount = 0L;
-        while (iterator.hasNext()) {
-          averageCount = processLine(iterator.next(), averageCount);
-        }
-        
-        pruneInconsequentialDiffs();
-        updateAllRecommendableItems();
-        
-      } catch (IOException ioe) {
-        log.warn("Exception while reloading", ioe);
-      } finally {
-        buildAverageDiffsLock.writeLock().unlock();
-      }
-    }
-  }
-  
-  private long processLine(String line, long averageCount) {
-
-    if (line.isEmpty() || line.charAt(0) == COMMENT_CHAR) {
-      return averageCount;
-    }
-    
-    String[] tokens = SEPARATOR.split(line);
-    Preconditions.checkArgument(tokens.length >= 3 && tokens.length != 5, "Bad line: %s", line);
-
-    long itemID1 = Long.parseLong(tokens[0]);
-    long itemID2 = Long.parseLong(tokens[1]);
-    double diff = Double.parseDouble(tokens[2]);
-    int count = tokens.length >= 4 ? Integer.parseInt(tokens[3]) : 1;
-    boolean hasMkSk = tokens.length >= 5;
-    
-    if (itemID1 > itemID2) {
-      long temp = itemID1;
-      itemID1 = itemID2;
-      itemID2 = temp;
-    }
-    
-    FastByIDMap<RunningAverage> level1Map = averageDiffs.get(itemID1);
-    if (level1Map == null) {
-      level1Map = new FastByIDMap<RunningAverage>();
-      averageDiffs.put(itemID1, level1Map);
-    }
-    RunningAverage average = level1Map.get(itemID2);
-    if (average != null) {
-      throw new IllegalArgumentException("Duplicated line for item-item pair " + itemID1 + " / " + itemID2);
-    }
-    if (averageCount < maxEntries) {
-      if (hasMkSk) {
-        double mk = Double.parseDouble(tokens[4]);
-        double sk = Double.parseDouble(tokens[5]);
-        average = new FullRunningAverageAndStdDev(count, diff, mk, sk);
-      } else {
-        average = new FullRunningAverage(count, diff);
-      }
-      level1Map.put(itemID2, average);
-      averageCount++;
-    }
-
-    allRecommendableItemIDs.add(itemID1);
-    allRecommendableItemIDs.add(itemID2);
-    
-    return averageCount;
-  }
-  
-  private void pruneInconsequentialDiffs() {
-    // Go back and prune inconsequential diffs. "Inconsequential" means, here, only represented by one
-    // data point, so possibly unreliable
-    Iterator<Map.Entry<Long,FastByIDMap<RunningAverage>>> it1 = averageDiffs.entrySet().iterator();
-    while (it1.hasNext()) {
-      FastByIDMap<RunningAverage> map = it1.next().getValue();
-      Iterator<Map.Entry<Long,RunningAverage>> it2 = map.entrySet().iterator();
-      while (it2.hasNext()) {
-        RunningAverage average = it2.next().getValue();
-        if (average.getCount() <= 1) {
-          it2.remove();
-        }
-      }
-      if (map.isEmpty()) {
-        it1.remove();
-      } else {
-        map.rehash();
-      }
-    }
-    averageDiffs.rehash();
-  }
-  
-  private void updateAllRecommendableItems() {
-    for (Map.Entry<Long,FastByIDMap<RunningAverage>> entry : averageDiffs.entrySet()) {
-      allRecommendableItemIDs.add(entry.getKey());
-      LongPrimitiveIterator it = entry.getValue().keySetIterator();
-      while (it.hasNext()) {
-        allRecommendableItemIDs.add(it.next());
-      }
-    }
-    allRecommendableItemIDs.rehash();
-  }
-  
-  @Override
-  public RunningAverage getDiff(long itemID1, long itemID2) {
-
-    boolean inverted = false;
-    if (itemID1 > itemID2) {
-      inverted = true;
-      long temp = itemID1;
-      itemID1 = itemID2;
-      itemID2 = temp;
-    }
-    
-    FastByIDMap<RunningAverage> level2Map;
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      level2Map = averageDiffs.get(itemID1);
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-    RunningAverage average = null;
-    if (level2Map != null) {
-      average = level2Map.get(itemID2);
-    }
-    if (inverted) {
-      return average == null ? null : average.inverse();
-    } else {
-      return average;
-    }
-  }
-  
-  @Override
-  public RunningAverage[] getDiffs(long userID, long itemID, PreferenceArray prefs) {
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      int size = prefs.length();
-      RunningAverage[] result = new RunningAverage[size];
-      for (int i = 0; i < size; i++) {
-        result[i] = getDiff(prefs.getItemID(i), itemID);
-      }
-      return result;
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-  }
-  
-  @Override
-  public RunningAverage getAverageItemPref(long itemID) {
-    return null; // TODO can't do this without a DataModel
-  }
-
-  @Override
-  public void addItemPref(long userID, long itemIDA, float prefValue) {
-    // Can't do this without a DataModel; should it just be a no-op?
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public void updateItemPref(long itemID, float prefDelta) {
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      for (Map.Entry<Long,FastByIDMap<RunningAverage>> entry : averageDiffs.entrySet()) {
-        boolean matchesItemID1 = itemID == entry.getKey();
-        for (Map.Entry<Long,RunningAverage> entry2 : entry.getValue().entrySet()) {
-          RunningAverage average = entry2.getValue();
-          if (matchesItemID1) {
-            average.changeDatum(-prefDelta);
-          } else if (itemID == entry2.getKey()) {
-            average.changeDatum(prefDelta);
-          }
-        }
-      }
-      // RunningAverage itemAverage = averageItemPref.get(itemID);
-      // if (itemAverage != null) {
-      // itemAverage.changeDatum(prefDelta);
-      // }
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-  }
-
-  @Override
-  public void removeItemPref(long userID, long itemIDA, float prefValue) {
-    // Can't do this without a DataModel; should it just be a no-op?
-    throw new UnsupportedOperationException();
-  }
-  
-  @Override
-  public FastIDSet getRecommendableItemIDs(long userID) {
-    try {
-      buildAverageDiffsLock.readLock().lock();
-      return allRecommendableItemIDs.clone();
-    } finally {
-      buildAverageDiffsLock.readLock().unlock();
-    }
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    long mostRecentModification = dataFile.lastModified();
-    if (mostRecentModification > lastModified + MIN_RELOAD_INTERVAL_MS) {
-      log.debug("File has changed; reloading...");
-      lastModified = mostRecentModification;
-      buildDiffs();
-    }
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizer.java	2014-03-29 01:03:13.000000000 -0700
@@ -181,7 +181,8 @@
       try {
 
         final ImplicitFeedbackAlternatingLeastSquaresSolver implicitFeedbackSolver = usesImplicitFeedback
-            ? new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, itemY) : null;
+            ? new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, itemY, numTrainingThreads)
+            : null;
 
         while (userIDsIterator.hasNext()) {
           final long userID = userIDsIterator.nextLong();
@@ -219,7 +220,8 @@
       try {
 
         final ImplicitFeedbackAlternatingLeastSquaresSolver implicitFeedbackSolver = usesImplicitFeedback
-            ? new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, userY) : null;
+            ? new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, userY, numTrainingThreads)
+            : null;
 
         while (itemIDsIterator.hasNext()) {
           final long itemID = itemIDsIterator.nextLong();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/Factorization.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/Factorization.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/Factorization.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/Factorization.java	2014-03-29 01:03:13.000000000 -0700
@@ -24,6 +24,7 @@
 import org.apache.mahout.cf.taste.common.NoSuchItemException;
 import org.apache.mahout.cf.taste.common.NoSuchUserException;
 import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
+import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
 
 /**
  * a factorization of the rating matrix
@@ -83,6 +84,10 @@
   public Iterable<Map.Entry<Long,Integer>> getUserIDMappings() {
     return userIDMapping.entrySet();
   }
+  
+  public LongPrimitiveIterator getUserIDMappingKeys() {
+    return userIDMapping.keySetIterator();
+  }
 
   public int itemIndex(long itemID) throws NoSuchItemException {
     Integer index = itemIDMapping.get(itemID);
@@ -95,6 +100,10 @@
   public Iterable<Map.Entry<Long,Integer>> getItemIDMappings() {
     return itemIDMapping.entrySet();
   }
+  
+  public LongPrimitiveIterator getItemIDMappingKeys() {
+    return itemIDMapping.keySetIterator();
+  }
 
   public int numFeatures() {
     return userFeatures.length > 0 ? userFeatures[0].length : 0;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/FunkSVDFactorizer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/FunkSVDFactorizer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/FunkSVDFactorizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/FunkSVDFactorizer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,177 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.svd;
-
-import java.util.Collections;
-import java.util.List;
-import java.util.Random;
-
-import com.google.common.collect.Lists;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.common.RandomUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Implementation of Simon Funk's famous algorithm from the Netflix prize,,
- * see http://sifter.org/~simon/journal/20061211.html for details
- */
-@Deprecated
-public final class FunkSVDFactorizer extends AbstractFactorizer {
-
-  private static final Logger log = LoggerFactory.getLogger(FunkSVDFactorizer.class);
-
-  private final double learningRate;
-  /** used to prevent overfitting.*/
-  private final double regularization;
-  /** number of features used to compute this factorization */
-  private final int numFeatures;
-  /** number of iterations */
-  private final int numIterations;
-  private final double randomNoise;
-  private double[][] userVectors;
-  private double[][] itemVectors;
-  private final DataModel dataModel;
-  private List<SVDPreference> cachedPreferences;
-  private double defaultValue;
-  private double interval;
-
-  private static final double DEFAULT_LEARNING_RATE = 0.005;
-  private static final double DEFAULT_REGULARIZATION = 0.02;
-  private static final double DEFAULT_RANDOM_NOISE = 0.005;
-
-  public FunkSVDFactorizer(DataModel dataModel, int numFeatures, int numIterations) throws TasteException {
-    this(dataModel, numFeatures, DEFAULT_LEARNING_RATE, DEFAULT_REGULARIZATION, DEFAULT_RANDOM_NOISE,
-        numIterations);
-  }
-
-  public FunkSVDFactorizer(DataModel dataModel, int numFeatures, double learningRate, double regularization,
-      double randomNoise, int numIterations) throws TasteException {
-    super(dataModel);
-    this.dataModel = dataModel;
-    this.numFeatures = numFeatures;
-    this.numIterations = numIterations;
-
-    this.learningRate = learningRate;
-    this.regularization = regularization;
-    this.randomNoise = randomNoise;
-  }
-
-  @Override
-  public Factorization factorize() throws TasteException {
-    Random random = RandomUtils.getRandom();
-    userVectors = new double[dataModel.getNumUsers()][numFeatures];
-    itemVectors = new double[dataModel.getNumItems()][numFeatures];
-
-    double average = getAveragePreference();
-
-    double prefInterval = dataModel.getMaxPreference() - dataModel.getMinPreference();
-    defaultValue = Math.sqrt((average - prefInterval * 0.1) / numFeatures);
-    interval = prefInterval * 0.1 / numFeatures;
-
-    for (int feature = 0; feature < numFeatures; feature++) {
-      for (int userIndex = 0; userIndex < dataModel.getNumUsers(); userIndex++) {
-        userVectors[userIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * randomNoise;
-      }
-      for (int itemIndex = 0; itemIndex < dataModel.getNumItems(); itemIndex++) {
-        itemVectors[itemIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * randomNoise;
-      }
-    }
-    cachedPreferences = Lists.newArrayListWithCapacity(dataModel.getNumUsers());
-    cachePreferences();
-    double rmse = dataModel.getMaxPreference() - dataModel.getMinPreference();
-    for (int feature = 0; feature < numFeatures; feature++) {
-      Collections.shuffle(cachedPreferences, random);
-      for (int i = 0; i < numIterations; i++) {
-        double err = 0.0;
-        for (SVDPreference pref : cachedPreferences) {
-          int useridx = userIndex(pref.getUserID());
-          int itemidx = itemIndex(pref.getItemID());
-          err += Math.pow(train(useridx, itemidx, feature, pref), 2.0);
-        }
-        rmse = Math.sqrt(err / cachedPreferences.size());
-      }
-      if (feature < numFeatures - 1) {
-        for (SVDPreference preference : cachedPreferences) {
-          int useridx = userIndex(preference.getUserID());
-          int itemidx = itemIndex(preference.getItemID());
-          buildCache(useridx, itemidx, feature, preference);
-        }
-      }
-      log.info("Finished training feature {} with RMSE {}.", feature, rmse);
-    }
-    return createFactorization(userVectors, itemVectors);
-  }
-
-  double getAveragePreference() throws TasteException {
-    RunningAverage average = new FullRunningAverage();
-    LongPrimitiveIterator userIDs = dataModel.getUserIDs();
-    while (userIDs.hasNext()) {
-      for (Preference preference : dataModel.getPreferencesFromUser(userIDs.nextLong())) {
-        average.addDatum(preference.getValue());
-      }
-    }
-    return average.getAverage();
-  }
-
-  private double train(int userIndex, int itemIndex, int feature, SVDPreference pref) {
-    double[] userVector = userVectors[userIndex];
-    double[] itemVector = itemVectors[itemIndex];
-    double prediction = predictRating(userIndex, itemIndex, feature, pref, true);
-    double err = pref.getValue() - prediction;
-    userVector[feature] += learningRate * (err * itemVector[feature] - regularization * userVector[feature]);
-    itemVector[feature] += learningRate * (err * userVector[feature] - regularization * itemVector[feature]);
-    return err;
-  }
-
-  private void buildCache(int userIndex, int itemIndex, int k, SVDPreference pref) {
-    pref.setCache(predictRating(userIndex, itemIndex, k, pref, false));
-  }
-
-  private double predictRating(int userIndex, int itemIndex, int feature, SVDPreference pref, boolean trailing) {
-    float minPreference = dataModel.getMinPreference();
-    float maxPreference = dataModel.getMaxPreference();
-    double sum = pref.getCache();
-    sum += userVectors[userIndex][feature] * itemVectors[itemIndex][feature];
-    if (trailing) {
-      sum += (numFeatures - feature - 1) * (defaultValue + interval) * (defaultValue + interval);
-      if (sum > maxPreference) {
-        sum = maxPreference;
-      } else if (sum < minPreference) {
-        sum = minPreference;
-      }
-    }
-    return sum;
-  }
-
-  private void cachePreferences() throws TasteException {
-    cachedPreferences.clear();
-    LongPrimitiveIterator userIDs = dataModel.getUserIDs();
-    while (userIDs.hasNext()) {
-      for (Preference pref : dataModel.getPreferencesFromUser(userIDs.nextLong())) {
-        cachedPreferences.add(new SVDPreference(pref.getUserID(), pref.getItemID(), pref.getValue(), 0.0));
-      }
-    }
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ImplicitLinearRegressionFactorizer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ImplicitLinearRegressionFactorizer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ImplicitLinearRegressionFactorizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/ImplicitLinearRegressionFactorizer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,391 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.cf.taste.impl.recommender.svd;
-
-import java.util.Collection;
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
-
-import com.google.common.collect.Lists;
-import org.apache.mahout.cf.taste.common.NoSuchUserException;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DiagonalMatrix;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.QRDecomposition;
-import org.apache.mahout.math.SparseMatrix;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-@Deprecated
-public final class ImplicitLinearRegressionFactorizer extends AbstractFactorizer {
-
-  private static final Logger log = LoggerFactory.getLogger(ImplicitLinearRegressionFactorizer.class);
-  private final double preventOverfitting;
-  /** number of features used to compute this factorization */
-  private final int numFeatures;
-  /** number of iterations */
-  private final int numIterations;
-  private final DataModel dataModel;
-  /** User singular vector. */
-  private double[][] userMatrix;
-  /** Item singular vector. */
-  private double[][] itemMatrix;
-  private Matrix userTransUser;
-  private Matrix itemTransItem;
-  private Collection<Callable<Void>> fVectorCallables;
-  private boolean recomputeUserFeatures;
-  private RunningAverage avrChange;
-
-  public ImplicitLinearRegressionFactorizer(DataModel dataModel) throws TasteException {
-    this(dataModel, 64, 10, 0.1);
-  }
-
-  public ImplicitLinearRegressionFactorizer(DataModel dataModel, int numFeatures, int numIterations,
-                                            double preventOverfitting) throws TasteException {
-
-    super(dataModel);
-    this.dataModel = dataModel;
-    this.numFeatures = numFeatures;
-    this.numIterations = numIterations;
-    this.preventOverfitting = preventOverfitting;
-    fVectorCallables = Lists.newArrayList();
-    avrChange = new FullRunningAverage();
-  }
-
-  @Override
-  public Factorization factorize() throws TasteException {
-    Random random = RandomUtils.getRandom();
-    userMatrix = new double[dataModel.getNumUsers()][numFeatures];
-    itemMatrix = new double[dataModel.getNumItems()][numFeatures];
-
-    /* start with the user side */
-    recomputeUserFeatures = true;
-
-    double average = getAveragePreference();
-
-    double prefInterval = dataModel.getMaxPreference() - dataModel.getMinPreference();
-    double defaultValue = Math.sqrt((average - prefInterval * 0.1) / numFeatures);
-    double interval = prefInterval * 0.1 / numFeatures;
-
-    for (int feature = 0; feature < numFeatures; feature++) {
-      for (int userIndex = 0; userIndex < dataModel.getNumUsers(); userIndex++) {
-        userMatrix[userIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * random.nextDouble();
-      }
-      for (int itemIndex = 0; itemIndex < dataModel.getNumItems(); itemIndex++) {
-        itemMatrix[itemIndex][feature] = defaultValue + (random.nextDouble() - 0.5) * interval * random.nextDouble();
-      }
-    }
-    train();
-    return createFactorization(userMatrix, itemMatrix);
-  }
-
-  public void train() throws TasteException {
-    for (int i = 0; i < numIterations; i++) {
-      if (recomputeUserFeatures) {
-        LongPrimitiveIterator userIds = dataModel.getUserIDs();
-        /* start with calculating X^TX or Y^TX */
-        log.info("Calculating Y^TY");
-        reCalculateTrans(recomputeUserFeatures);
-        log.info("Building callables for users.");
-        while (userIds.hasNext()) {
-          long userId = userIds.nextLong();
-          int useridx = userIndex(userId);
-          buildCallables(buildConfidenceMatrixForUser(userId), buildPreferenceVectorForUser(userId), useridx);
-        }
-        finishProcessing();
-      } else {
-        LongPrimitiveIterator itemIds = dataModel.getItemIDs();
-        /* start with calculating X^TX or Y^TX */
-        log.info("Calculating X^TX");
-        reCalculateTrans(recomputeUserFeatures);
-        log.info("Building callables for items.");
-        while (itemIds.hasNext()) {
-          long itemId = itemIds.nextLong();
-          int itemidx = itemIndex(itemId);
-          buildCallables(buildConfidenceMatrixForItem(itemId), buildPreferenceVectorForItem(itemId), itemidx);
-        }
-        finishProcessing();
-      }
-    }
-  }
-
-  public Matrix buildPreferenceVectorForUser(long realId) throws TasteException {
-    Matrix ids = new SparseMatrix(1, dataModel.getNumItems());
-    for (Preference pref : dataModel.getPreferencesFromUser(realId)) {
-      int itemidx = itemIndex(pref.getItemID());
-      ids.setQuick(0, itemidx, pref.getValue());
-    }
-    return ids;
-  }
-
-  private Matrix buildConfidenceMatrixForItem(long itemId) throws TasteException {
-    PreferenceArray prefs = dataModel.getPreferencesForItem(itemId);
-    Matrix confidenceMatrix = new SparseMatrix(dataModel.getNumUsers(), dataModel.getNumUsers());
-    for (Preference pref : prefs) {
-      long userId = pref.getUserID();
-      int userIdx = userIndex(userId);
-      confidenceMatrix.setQuick(userIdx, userIdx, 1);
-    }
-    return new DiagonalMatrix(confidenceMatrix);
-  }
-
-  private Matrix buildConfidenceMatrixForUser(long userId) throws TasteException {
-    PreferenceArray prefs = dataModel.getPreferencesFromUser(userId);
-    Matrix confidenceMatrix = new SparseMatrix(dataModel.getNumItems(), dataModel.getNumItems());
-    for (Preference pref : prefs) {
-      long itemId = pref.getItemID();
-      int itemIdx = itemIndex(itemId);
-      confidenceMatrix.setQuick(itemIdx, itemIdx, 1);
-    }
-    return new DiagonalMatrix(confidenceMatrix);
-  }
-
-  private Matrix buildPreferenceVectorForItem(long realId) throws TasteException {
-    Matrix ids = new SparseMatrix(1, dataModel.getNumUsers());
-    for (Preference pref : dataModel.getPreferencesForItem(realId)) {
-      int useridx = userIndex(pref.getUserID());
-      ids.setQuick(0, useridx, pref.getValue());
-    }
-    return ids;
-  }
-
-  private static Matrix ones(int size) {
-    double[] vector = new double[size];
-    for (int i = 0; i < size; i++) {
-      vector[i] = 1;
-    }
-    return new DiagonalMatrix(vector);
-  }
-
-  private double getAveragePreference() throws TasteException {
-    RunningAverage average = new FullRunningAverage();
-    LongPrimitiveIterator it = dataModel.getUserIDs();
-    while (it.hasNext()) {
-      int count = 0;
-      try {
-        PreferenceArray prefs = dataModel.getPreferencesFromUser(it.nextLong());
-        for (Preference pref : prefs) {
-          average.addDatum(pref.getValue());
-          count++;
-        }
-      } catch (NoSuchUserException ex) {
-        continue;
-      }
-      /* add the remaining zeros */
-      for (int i = 0; i < (dataModel.getNumItems() - count); i++) {
-        average.addDatum(0);
-      }
-    }
-    return average.getAverage();
-  }
-
-  /**
-   * Recalculating Y^TY or X^TX which is needed for further calculations
-   * @param recomputeUserFeatures
-   */
-  public void reCalculateTrans(boolean recomputeUserFeatures) {
-    if (recomputeUserFeatures) {
-      Matrix iMatrix = new DenseMatrix(itemMatrix);
-      itemTransItem = iMatrix.transpose().times(iMatrix);
-    } else {
-      Matrix uMatrix = new DenseMatrix(userMatrix);
-      userTransUser = uMatrix.transpose().times(uMatrix);
-    }
-  }
-
-  private synchronized void updateMatrix(int id, Matrix m) {
-    double normA = 0;
-    double normB = 0;
-    double aTb = 0;
-    for (int feature = 0; feature < numFeatures; feature++) {
-      if (recomputeUserFeatures) {
-        normA += userMatrix[id][feature] * userMatrix[id][feature];
-        normB += m.get(feature, 0) * m.get(feature, 0);
-        aTb += userMatrix[id][feature] * m.get(feature, 0);
-        userMatrix[id][feature] = m.get(feature, 0);
-      } else {
-        normA += itemMatrix[id][feature] * itemMatrix[id][feature];
-        normB += m.get(feature, 0) * m.get(feature, 0);
-        aTb += itemMatrix[id][feature] * m.get(feature, 0);
-        itemMatrix[id][feature] = m.get(feature, 0);
-      }
-    }
-    /* calculating cosine similarity to determine when to stop the algorithm,
-    this could be used to detect convergence */
-    double cosine = aTb / (Math.sqrt(normA) * Math.sqrt(normB));
-    if (Double.isNaN(cosine)) {
-      log.info("Cosine similarity is NaN, recomputeUserFeatures={} id={}", recomputeUserFeatures, id);
-    } else {
-      avrChange.addDatum(cosine);
-    }
-  }
-
-  public void resetCallables() {
-    fVectorCallables = Lists.newArrayList();
-  }
-
-  private void resetAvrChange() {
-    log.info("Avr Change: {}", avrChange.getAverage());
-    avrChange = new FullRunningAverage();
-  }
-
-  public void buildCallables(Matrix C, Matrix prefVector, int id) {
-    fVectorCallables.add(new FeatureVectorCallable(C, prefVector, id));
-    if (fVectorCallables.size() % (200 * Runtime.getRuntime().availableProcessors()) == 0) {
-      execute(fVectorCallables);
-      resetCallables();
-    }
-  }
-
-  public void finishProcessing() {
-    /* run the remaining part */
-    if (fVectorCallables != null) {
-      execute(fVectorCallables);
-    }
-    resetCallables();
-    if ((recomputeUserFeatures && avrChange.getCount() != userMatrix.length)
-        || (!recomputeUserFeatures && avrChange.getCount() != itemMatrix.length)) {
-      log.info("Matrix length is not equal to count");
-    }
-    resetAvrChange();
-    recomputeUserFeatures = !recomputeUserFeatures;
-  }
-
-  public static Matrix identityV(int size) {
-    return ones(size);
-  }
-
-  static void execute(Collection<Callable<Void>> callables) {
-    callables = wrapWithStatsCallables(callables);
-    int numProcessors = Runtime.getRuntime().availableProcessors();
-    ExecutorService executor = Executors.newFixedThreadPool(numProcessors);
-    log.info("Starting timing of {} tasks in {} threads", callables.size(), numProcessors);
-    try {
-      List<Future<Void>> futures = executor.invokeAll(callables);
-      //TODO go look for exceptions here, really
-      for (Future<Void> future : futures) {
-        future.get();
-      }
-    } catch (InterruptedException ie) {
-      log.warn("error in factorization", ie);
-    } catch (ExecutionException ee) {
-      log.warn("error in factorization", ee);
-    }
-    executor.shutdown();
-  }
-
-  private static Collection<Callable<Void>> wrapWithStatsCallables(Collection<Callable<Void>> callables) {
-    int size = callables.size();
-    Collection<Callable<Void>> wrapped = Lists.newArrayListWithExpectedSize(size);
-    int count = 1;
-    RunningAverageAndStdDev timing = new FullRunningAverageAndStdDev();
-    for (Callable<Void> callable : callables) {
-      boolean logStats = count++ % 1000 == 0;
-      wrapped.add(new StatsCallable(callable, logStats, timing));
-    }
-    return wrapped;
-  }
-
-  private final class FeatureVectorCallable implements Callable<Void> {
-
-    private final Matrix C;
-    private final Matrix prefVector;
-    private final int id;
-
-    private FeatureVectorCallable(Matrix C, Matrix prefVector, int id) {
-      this.C = C;
-      this.prefVector = prefVector;
-      this.id = id;
-    }
-
-    @Override
-    public Void call() throws Exception {
-      Matrix XTCX;
-      if (recomputeUserFeatures) {
-        Matrix I = identityV(dataModel.getNumItems());
-        Matrix I2 = identityV(numFeatures);
-        Matrix iTi = itemTransItem.clone();
-        Matrix itemM = new DenseMatrix(itemMatrix);
-        XTCX = iTi.plus(itemM.transpose().times(C.minus(I)).times(itemM));
-
-        Matrix diag = solve(XTCX.plus(I2.times(preventOverfitting)), I2);
-        Matrix results = diag.times(itemM.transpose().times(C)).times(prefVector.transpose());
-        updateMatrix(id, results);
-      } else {
-        Matrix I = identityV(dataModel.getNumUsers());
-        Matrix I2 = identityV(numFeatures);
-        Matrix uTu = userTransUser.clone();
-        Matrix userM = new DenseMatrix(userMatrix);
-        XTCX = uTu.plus(userM.transpose().times(C.minus(I)).times(userM));
-
-        Matrix diag = solve(XTCX.plus(I2.times(preventOverfitting)), I2);
-        Matrix results = diag.times(userM.transpose().times(C)).times(prefVector.transpose());
-        updateMatrix(id, results);
-      }
-      return null;
-    }
-  }
-
-  private static Matrix solve(Matrix A, Matrix y) {
-    return new QRDecomposition(A).solve(y);
-  }
-
-  private static final class StatsCallable implements Callable<Void> {
-
-    private final Callable<Void> delegate;
-    private final boolean logStats;
-    private final RunningAverageAndStdDev timing;
-
-    private StatsCallable(Callable<Void> delegate, boolean logStats, RunningAverageAndStdDev timing) {
-      this.delegate = delegate;
-      this.logStats = logStats;
-      this.timing = timing;
-    }
-
-    @Override
-    public Void call() throws Exception {
-      long start = System.currentTimeMillis();
-      delegate.call();
-      long end = System.currentTimeMillis();
-      timing.addDatum(end - start);
-      if (logStats) {
-        Runtime runtime = Runtime.getRuntime();
-        int average = (int) timing.getAverage();
-        log.info("Average time per task: {}ms", average);
-        long totalMemory = runtime.totalMemory();
-        long memory = totalMemory - runtime.freeMemory();
-        log.info("Approximate memory used: {}MB / {}MB", memory / 1000000L, totalMemory / 1000000L);
-      }
-      return null;
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/SVDRecommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/SVDRecommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/SVDRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/svd/SVDRecommender.java	2014-03-29 01:03:13.000000000 -0700
@@ -28,6 +28,7 @@
 import org.apache.mahout.cf.taste.impl.common.FastIDSet;
 import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
 import org.apache.mahout.cf.taste.impl.recommender.AbstractRecommender;
+import org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategy;
 import org.apache.mahout.cf.taste.impl.recommender.TopItems;
 import org.apache.mahout.cf.taste.model.DataModel;
 import org.apache.mahout.cf.taste.model.PreferenceArray;
@@ -51,7 +52,7 @@
   private static final Logger log = LoggerFactory.getLogger(SVDRecommender.class);
 
   public SVDRecommender(DataModel dataModel, Factorizer factorizer) throws TasteException {
-    this(dataModel, factorizer, getDefaultCandidateItemsStrategy(), getDefaultPersistenceStrategy());
+    this(dataModel, factorizer, new AllUnknownItemsCandidateItemsStrategy(), getDefaultPersistenceStrategy());
   }
 
   public SVDRecommender(DataModel dataModel, Factorizer factorizer, CandidateItemsStrategy candidateItemsStrategy)
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/AbstractSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/AbstractSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/AbstractSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/AbstractSimilarity.java	2014-03-29 01:03:13.000000000 -0700
@@ -28,8 +28,6 @@
 import org.apache.mahout.cf.taste.model.PreferenceArray;
 import org.apache.mahout.cf.taste.similarity.PreferenceInferrer;
 import org.apache.mahout.cf.taste.similarity.UserSimilarity;
-import org.apache.mahout.cf.taste.transforms.PreferenceTransform;
-import org.apache.mahout.cf.taste.transforms.SimilarityTransform;
 
 import com.google.common.base.Preconditions;
 
@@ -37,8 +35,6 @@
 abstract class AbstractSimilarity extends AbstractItemSimilarity implements UserSimilarity {
 
   private PreferenceInferrer inferrer;
-  private PreferenceTransform prefTransform;
-  private SimilarityTransform similarityTransform;
   private final boolean weighted;
   private final boolean centerData;
   private int cachedNumItems;
@@ -47,7 +43,7 @@
 
   /**
    * <p>
-   * Creates a possibly weighted AbstractSimilarity.
+   * Creates a possibly weighted {@link AbstractSimilarity}.
    * </p>
    */
   AbstractSimilarity(final DataModel dataModel, Weighting weighting, boolean centerData) throws TasteException {
@@ -78,26 +74,6 @@
     this.inferrer = inferrer;
   }
   
-  public final PreferenceTransform getPrefTransform() {
-    return prefTransform;
-  }
-  
-  public final void setPrefTransform(PreferenceTransform prefTransform) {
-    refreshHelper.addDependency(prefTransform);
-    refreshHelper.removeDependency(this.prefTransform);
-    this.prefTransform = prefTransform;
-  }
-  
-  public final SimilarityTransform getSimilarityTransform() {
-    return similarityTransform;
-  }
-  
-  public final void setSimilarityTransform(SimilarityTransform similarityTransform) {
-    refreshHelper.addDependency(similarityTransform);
-    refreshHelper.removeDependency(this.similarityTransform);
-    this.similarityTransform = similarityTransform;
-  }
-  
   final boolean isWeighted() {
     return weighted;
   }
@@ -117,7 +93,7 @@
    * @param n
    *          total number of users or items
    * @param sumXY
-   *          sum of product of user/item preference values, over all items/users prefererred by both
+   *          sum of product of user/item preference values, over all items/users preferred by both
    *          users/items
    * @param sumX2
    *          sum of the square of user/item preference values, over the first item/user
@@ -126,7 +102,7 @@
    * @param sumXYdiff2
    *          sum of squares of differences in X and Y values
    * @return similarity value between -1.0 and 1.0, inclusive, or {@link Double#NaN} if no similarity can be
-   *         computed (e.g. when no items have been rated by both uesrs
+   *         computed (e.g. when no items have been rated by both users
    */
   abstract double computeResult(int n, double sumXY, double sumX2, double sumY2, double sumXYdiff2);
   
@@ -156,7 +132,6 @@
     int count = 0;
     
     boolean hasInferrer = inferrer != null;
-    boolean hasPrefTransform = prefTransform != null;
     
     while (true) {
       int compare = xIndex < yIndex ? -1 : xIndex > yIndex ? 1 : 0;
@@ -165,29 +140,20 @@
         double y;
         if (xIndex == yIndex) {
           // Both users expressed a preference for the item
-          if (hasPrefTransform) {
-            x = prefTransform.getTransformedValue(xPrefs.get(xPrefIndex));
-            y = prefTransform.getTransformedValue(yPrefs.get(yPrefIndex));
-          } else {
-            x = xPrefs.getValue(xPrefIndex);
-            y = yPrefs.getValue(yPrefIndex);
-          }
+          x = xPrefs.getValue(xPrefIndex);
+          y = yPrefs.getValue(yPrefIndex);
         } else {
           // Only one user expressed a preference, but infer the other one's preference and tally
           // as if the other user expressed that preference
           if (compare < 0) {
             // X has a value; infer Y's
-            x = hasPrefTransform
-                ? prefTransform.getTransformedValue(xPrefs.get(xPrefIndex))
-                : xPrefs.getValue(xPrefIndex);
+            x = xPrefs.getValue(xPrefIndex);
             y = inferrer.inferPreference(userID2, xIndex);
           } else {
             // compare > 0
             // Y has a value; infer X's
             x = inferrer.inferPreference(userID1, yIndex);
-            y = hasPrefTransform
-                ? prefTransform.getTransformedValue(yPrefs.get(yPrefIndex))
-                : yPrefs.getValue(yPrefIndex);
+            y = yPrefs.getValue(yPrefIndex);
           }
         }
         sumXY += x * y;
@@ -218,7 +184,7 @@
       if (compare >= 0) {
         if (++yPrefIndex >= yLength) {
           if (hasInferrer) {
-            // Must count other Xs; pretend next Y is far away            
+            // Must count other Xs; pretend next Y is far away
             if (xIndex == Long.MAX_VALUE) {
               // ... but stop if both are done!
               break;
@@ -249,10 +215,6 @@
       result = computeResult(count, sumXY, sumX2, sumY2, sumXYdiff2);
     }
     
-    if (similarityTransform != null) {
-      result = similarityTransform.transformSimilarity(userID1, userID2, result);
-    }
-    
     if (!Double.isNaN(result)) {
       result = normalizeWeightResult(result, count, cachedNumItems);
     }
@@ -284,7 +246,7 @@
     double sumXYdiff2 = 0.0;
     int count = 0;
     
-    // No, pref inferrers and transforms don't appy here. I think.
+    // No, pref inferrers and transforms don't apply here. I think.
     
     while (true) {
       int compare = xIndex < yIndex ? -1 : xIndex > yIndex ? 1 : 0;
@@ -332,10 +294,6 @@
       result = computeResult(count, sumXY, sumX2, sumY2, sumXYdiff2);
     }
     
-    if (similarityTransform != null) {
-      result = similarityTransform.transformSimilarity(itemID1, itemID2, result);
-    }
-    
     if (!Double.isNaN(result)) {
       result = normalizeWeightResult(result, count, cachedNumUsers);
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericItemSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericItemSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericItemSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericItemSimilarity.java	2014-03-29 01:03:13.000000000 -0700
@@ -55,7 +55,7 @@
 
   /**
    * <p>
-   * Creates a  from a precomputed list of {@link ItemItemSimilarity}s. Each
+   * Creates a {@link GenericItemSimilarity} from a precomputed list of {@link ItemItemSimilarity}s. Each
    * represents the similarity between two distinct items. Since similarity is assumed to be symmetric, it is
    * not necessary to specify similarity between item1 and item2, and item2 and item1. Both are the same. It
    * is also not necessary to specify a similarity between any item and itself; these are assumed to be 1.0.
@@ -101,7 +101,7 @@
    * </p>
    * 
    * <p>
-   * It's valid to build a  this way, but perhaps missing some of the point of an
+   * It's valid to build a {@link GenericItemSimilarity} this way, but perhaps missing some of the point of an
    * item-based recommender. Item-based recommenders use the assumption that item-item similarities are
    * relatively fixed, and might be known already independent of user preferences. Hence it is useful to
    * inject that information, using {@link #GenericItemSimilarity(Iterable)}.
@@ -261,7 +261,7 @@
      *           if value is NaN, less than -1.0 or greater than 1.0
      */
     public ItemItemSimilarity(long itemID1, long itemID2, double value) {
-      Preconditions.checkArgument(value >= -1.0 && value <= 1.0, "Illegal value: %s", value);
+      Preconditions.checkArgument(value >= -1.0 && value <= 1.0, "Illegal value: " + value + ". Must be: -1.0 <= value <= 1.0");
       this.itemID1 = itemID1;
       this.itemID2 = itemID2;
       this.value = value;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericUserSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericUserSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericUserSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/similarity/GenericUserSimilarity.java	2014-03-29 01:03:13.000000000 -0700
@@ -146,7 +146,7 @@
     private final double value;
     
     public UserUserSimilarity(long userID1, long userID2, double value) {
-      Preconditions.checkArgument(value >= -1.0 && value <= 1.0, "Illegal value: %s", value);
+      Preconditions.checkArgument(value >= -1.0 && value <= 1.0, "Illegal value: " + value + ". Must be: -1.0 <= value <= 1.0");
       this.userID1 = userID1;
       this.userID2 = userID2;
       this.value = value;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplification.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplification.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplification.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplification.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import java.util.Collection;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.transforms.SimilarityTransform;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * Applies "case amplification" to similarities. This essentially makes big values bigger and small values
- * smaller by raising each score to a power. It could however be used to achieve the opposite effect.
- * </p>
- */
-@Deprecated
-public final class CaseAmplification implements SimilarityTransform {
-  
-  private final double factor;
-  
-  /**
-   * <p>
-   * Creates a  transformation based on the given factor.
-   * </p>
-   * 
-   * @param factor
-   *          transformation factor
-   * @throws IllegalArgumentException
-   *           if factor is 0.0 or {@link Double#NaN}
-   */
-  public CaseAmplification(double factor) {
-    Preconditions.checkArgument(factor != 0.0 && !Double.isNaN(factor), "factor is 0 or NaN");
-    this.factor = factor;
-  }
-  
-  /**
-   * <p>
-   * Transforms one similarity value. This implementation is such that it's possible to define this
-   * transformation on one value in isolation. The "thing" parameters are therefore unused.
-   * </p>
-   *
-   * @param id1
-   *          unused
-   * @param id2
-   *          unused
-   * @param value
-   *          similarity to transform
-   * @return {@code value<sup>factor</sup>} if value is nonnegative;
-   *         {@code -value<sup>-factor</sup>} otherwise
-   */
-  @Override
-  public double transformSimilarity(long id1, long id2, double value) {
-    return value < 0.0 ? -Math.pow(-value, factor) : Math.pow(value, factor);
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-  // do nothing
-  }
-  
-  @Override
-  public String toString() {
-    return "CaseAmplification[factor:" + factor + ']';
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/Counters.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/Counters.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/Counters.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/Counters.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import java.util.Map;
-
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-
-/**
- * <p>
- * A simple, fast utility class that maps keys to counts.
- * </p>
- */
-@Deprecated
-final class Counters {
-  
-  private final FastByIDMap<int[]> counts = new FastByIDMap<int[]>();
-  
-  void increment(long key) {
-    int[] count = counts.get(key);
-    if (count == null) {
-      int[] newCount = new int[1];
-      newCount[0] = 1;
-      counts.put(key, newCount);
-    } else {
-      count[0]++;
-    }
-  }
-  
-  int getCount(long key) {
-    int[] count = counts.get(key);
-    return count == null ? 0 : count[0];
-  }
-  
-  int size() {
-    return counts.size();
-  }
-  
-  Iterable<Map.Entry<Long,int[]>> getEntrySet() {
-    return counts.entrySet();
-  }
-  
-  @Override
-  public String toString() {
-    return "Counters[" + counts + ']';
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequency.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequency.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequency.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequency.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import java.util.Collection;
-import java.util.Map;
-import java.util.concurrent.Callable;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-import org.apache.mahout.cf.taste.impl.common.LongPrimitiveIterator;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.transforms.PreferenceTransform;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * Implements an "inverse user frequency" transformation, which boosts preference values for items for which
- * few users have expressed a preference, and reduces preference values for items for which many users have
- * expressed a preference. The idea is that these "rare" items are more useful in deciding how similar two
- * users' tastes are, and so should be emphasized in other calculatioons. This idea is mentioned in <a
- * href="ftp://ftp.research.microsoft.com/pub/tr/tr-98-12.pdf">Empirical Analysis of Predictive Algorithms for
- * Collaborative Filtering</a>.
- * </p>
- * 
- * <p>
- * A scaling factor is computed for each item by dividing the total number of users by the number of users
- * expressing a preference for that item, and taking the log of that value. The log base of this calculation
- * can be controlled in the constructor. Intuitively, the right value for the base is equal to the average
- * number of users who express a preference for each item in your model. If each item has about 100
- * preferences on average, 100.0 is a good log base.
- * </p>
- */
-@Deprecated
-public final class InverseUserFrequency implements PreferenceTransform {
-  
-  private final DataModel dataModel;
-  private final RefreshHelper refreshHelper;
-  private final double logBase;
-  private FastByIDMap<Double> iufFactors;
-  
-  /**
-   * <p>
-   * Creates a  transformation. Computations use the given log base.
-   * </p>
-   *
-   * @param dataModel
-   *          {@link DataModel} from which to calculate user frequencies
-   * @param logBase
-   *          calculation logarithm base
-   * @throws IllegalArgumentException
-   *           if dataModel is {@code null} or logBase is {@link Double#NaN} or &lt;= 1.0
-   */
-  public InverseUserFrequency(DataModel dataModel, double logBase) throws TasteException {
-    Preconditions.checkArgument(logBase > 1.0, "logBase should be > 1.0");
-    this.dataModel = Preconditions.checkNotNull(dataModel);
-    this.logBase = logBase;
-    this.iufFactors = new FastByIDMap<Double>();
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        recompute();
-        return null;
-      }
-    });
-    this.refreshHelper.addDependency(this.dataModel);
-    recompute();
-  }
-  
-  /** @return log base used in this object's calculations */
-  public double getLogBase() {
-    return logBase;
-  }
-  
-  @Override
-  public float getTransformedValue(Preference pref) {
-    Double factor = iufFactors.get(pref.getItemID());
-    if (factor != null) {
-      return (float) (pref.getValue() * factor);
-    }
-    return pref.getValue();
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    refreshHelper.refresh(alreadyRefreshed);
-  }
-  
-  private void recompute() throws TasteException {
-    Counters itemPreferenceCounts = new Counters();
-    int numUsers = 0;
-    LongPrimitiveIterator it = dataModel.getUserIDs();
-    while (it.hasNext()) {
-      PreferenceArray prefs = dataModel.getPreferencesFromUser(it.nextLong());
-      int size = prefs.length();
-      for (int i = 0; i < size; i++) {
-        itemPreferenceCounts.increment(prefs.getItemID(i));
-      }
-      numUsers++;
-    }
-    FastByIDMap<Double> newIufFactors = new FastByIDMap<Double>(itemPreferenceCounts.size());
-    double logFactor = Math.log(logBase);
-    for (Map.Entry<Long,int[]> entry : itemPreferenceCounts.getEntrySet()) {
-      newIufFactors.put(entry.getKey(), Math.log((double) numUsers / (double) entry.getValue()[0])
-                                        / logFactor);
-    }
-    iufFactors = newIufFactors;
-  }
-  
-  @Override
-  public String toString() {
-    return "InverseUserFrequency[logBase:" + logBase + ']';
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/ZScore.java mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/ZScore.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/ZScore.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/impl/transforms/ZScore.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,98 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import java.util.Collection;
-
-import com.google.common.base.Preconditions;
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.Cache;
-import org.apache.mahout.cf.taste.impl.common.FullRunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.Retriever;
-import org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.transforms.PreferenceTransform;
-
-/**
- * <p>
- * Normalizes preference values for a user by converting them to <a
- * href="http://mathworld.wolfram.com/z-Score.html">"z-scores"</a>. This process normalizes preference values
- * to adjust for variation in mean and variance of a user's preferences.
- * </p>
- * 
- * <p>
- * Imagine two users, one who tends to rate every movie he/she sees four or five stars, and another who uses
- * the full one to five star range when assigning ratings. This transform normalizes away the difference in
- * scale used by the two users so that both have a mean preference of 0.0 and a standard deviation of 1.0.
- * </p>
- */
-@Deprecated
-public final class ZScore implements PreferenceTransform {
-  
-  private final DataModel dataModel;
-  private final Cache<Long,RunningAverageAndStdDev> meanAndStdevs;
-  
-  public ZScore(DataModel dataModel) {
-    this.dataModel = Preconditions.checkNotNull(dataModel);
-    this.meanAndStdevs = new Cache<Long,RunningAverageAndStdDev>(new MeanStdevRetriever());
-    refresh(null);
-  }
-  
-  @Override
-  public float getTransformedValue(Preference pref) throws TasteException {
-    RunningAverageAndStdDev meanAndStdev = meanAndStdevs.get(pref.getUserID());
-    if (meanAndStdev.getCount() > 1) {
-      double stdev = meanAndStdev.getStandardDeviation();
-      if (stdev > 0.0) {
-        return (float) ((pref.getValue() - meanAndStdev.getAverage()) / stdev);
-      }
-    }
-    return 0.0f;
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    meanAndStdevs.clear();
-    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);
-    RefreshHelper.maybeRefresh(alreadyRefreshed, dataModel);
-  }
-  
-  @Override
-  public String toString() {
-    return "ZScore";
-  }
-  
-  private class MeanStdevRetriever implements Retriever<Long,RunningAverageAndStdDev> {
-    
-    @Override
-    public RunningAverageAndStdDev get(Long userID) throws TasteException {
-      RunningAverageAndStdDev running = new FullRunningAverageAndStdDev();
-      PreferenceArray prefs = dataModel.getPreferencesFromUser(userID);
-      int size = prefs.length();
-      for (int i = 0; i < size; i++) {
-        running.addDatum(prefs.getValue(i));
-      }
-      return running;
-    }
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/model/DataModel.java mahout/core/src/main/java/org/apache/mahout/cf/taste/model/DataModel.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/model/DataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/model/DataModel.java	2014-03-29 01:03:13.000000000 -0700
@@ -175,7 +175,7 @@
   void removePreference(long userID, long itemID) throws TasteException;
 
   /**
-   * @return true iff this implementation actually stores and returns distinct preference values;
+   * @return true if this implementation actually stores and returns distinct preference values;
    *  that is, if it is not a 'boolean' DataModel
    */
   boolean hasPreferenceValues();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/model/Preference.java mahout/core/src/main/java/org/apache/mahout/cf/taste/model/Preference.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/model/Preference.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/model/Preference.java	2014-03-29 01:03:13.000000000 -0700
@@ -19,8 +19,8 @@
 
 /**
  * <p>
- * A  encapsulates an item and a preference value, which indicates the strength of the
- * preference for it. s are associated to users.
+ * A {@link Preference} encapsulates an item and a preference value, which indicates the strength of the
+ * preference for it. {@link Preference}s are associated to users.
  * </p>
  */
 public interface Preference {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/ClusteringRecommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/ClusteringRecommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/ClusteringRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/ClusteringRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.recommender;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-
-/**
- * <p>
- * Interface implemented by "clustering" recommenders.
- * </p>
- */
-@Deprecated
-public interface ClusteringRecommender extends Recommender {
-  
-  /**
-   * <p>
-   * Returns the cluster of users to which the given user, denoted by user ID, belongs.
-   * </p>
-   * 
-   * @param userID
-   *          user ID for which to find a cluster
-   * @return {@link FastIDSet} of IDs of users in the requested user's cluster
-   * @throws TasteException
-   *           if an error occurs while accessing the {@link org.apache.mahout.cf.taste.model.DataModel}
-   */
-  FastIDSet getCluster(long userID) throws TasteException;
-  
-  /**
-   * <p>
-   * Returns all clusters of users.
-   * </p>
-   * 
-   * @return array of {@link FastIDSet}s of user IDs
-   * @throws TasteException
-   *           if an error occurs while accessing the {@link org.apache.mahout.cf.taste.model.DataModel}
-   */
-  FastIDSet[] getClusters() throws TasteException;
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Recommender.java mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Recommender.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Recommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Recommender.java	2014-03-29 01:03:13.000000000 -0700
@@ -93,7 +93,7 @@
   void removePreference(long userID, long itemID) throws TasteException;
 
   /**
-   * @return underlying {@link DataModel} used by this implementation
+   * @return underlying {@link DataModel} used by this {@link Recommender} implementation
    */
   DataModel getDataModel();
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Rescorer.java mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Rescorer.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Rescorer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/Rescorer.java	2014-03-29 01:03:13.000000000 -0700
@@ -19,14 +19,14 @@
 
 /**
  * <p>
- * A  simply assigns a new "score" to a thing like an ID of an item or user which a
+ * A {@link Rescorer} simply assigns a new "score" to a thing like an ID of an item or user which a
  * {@link Recommender} is considering returning as a top recommendation. It may be used to arbitrarily re-rank
  * the results according to application-specific logic before returning recommendations. For example, an
  * application may want to boost the score of items in a certain category just for one request.
  * </p>
  *
  * <p>
- * A  can also exclude a thing from consideration entirely by returning {@code true} from
+ * A {@link Rescorer} can also exclude a thing from consideration entirely by returning {@code true} from
  * {@link #isFiltered(Object)}.
  * </p>
  */
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/slopeone/DiffStorage.java mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/slopeone/DiffStorage.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/slopeone/DiffStorage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/recommender/slopeone/DiffStorage.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,92 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.recommender.slopeone;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-
-/**
- * <p>
- * Implementations store item-item preference diffs for a
- * {@link org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender}. It actually does a bit
- * more for this implementation, like listing all items that may be considered for recommendation, in order to
- * maximize what implementations can do to optimize the slope-one algorithm.
- * </p>
- * 
- * @see org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender
- */
-@Deprecated
-public interface DiffStorage extends Refreshable {
-  
-  /**
-   * @return {@link RunningAverage} encapsulating the average difference in preferences between items
-   *         corresponding to {@code itemID1} and {@code itemID2}, in that direction; that is, it's
-   *         the average of item 2's preferences minus item 1's preferences
-   */
-  RunningAverage getDiff(long itemID1, long itemID2) throws TasteException;
-  
-  /**
-   * @param userID
-   *          user ID to get diffs for
-   * @param itemID
-   *          itemID to assess
-   * @param prefs
-   *          user's preferendces
-   * @return {@link RunningAverage}s for that user's item-item diffs
-   */
-  RunningAverage[] getDiffs(long userID, long itemID, PreferenceArray prefs) throws TasteException;
-  
-  /** @return {@link RunningAverage} encapsulating the average preference for the given item */
-  RunningAverage getAverageItemPref(long itemID) throws TasteException;
-
-  /**
-   * <p>Updates internal data structures to reflect a new preference value for an item.</p>
-   *
-   * @param userID user whose pref is being added
-   * @param itemID item to add preference value for
-   * @param prefValue new preference value
-   */
-  void addItemPref(long userID, long itemID, float prefValue) throws TasteException;
-
-  /**
-   * <p>Updates internal data structures to reflect an update in a preference value for an item.</p>
-   * 
-   * @param itemID item to update preference value for
-   * @param prefDelta amount by which preference value changed
-   */
-  void updateItemPref(long itemID, float prefDelta) throws TasteException;
-
-  /**
-   * <p>Updates internal data structures to reflect an update in a preference value for an item.</p>
-   *
-   * @param userID user whose pref is being removed
-   * @param itemID item to update preference value for
-   * @param prefValue old preference value
-   */
-  void removeItemPref(long userID, long itemID, float prefValue) throws TasteException;
-  
-  /**
-   * @return item IDs that may possibly be recommended to the given user, which may not be all items since the
-   *         item-item diff matrix may be sparse
-   */
-  FastIDSet getRecommendableItemIDs(long userID) throws TasteException;
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/similarity/UserSimilarity.java mahout/core/src/main/java/org/apache/mahout/cf/taste/similarity/UserSimilarity.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/similarity/UserSimilarity.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/similarity/UserSimilarity.java	2014-03-29 01:03:13.000000000 -0700
@@ -48,7 +48,7 @@
   
   /**
    * <p>
-   * Attaches a {@link PreferenceInferrer} to the  implementation.
+   * Attaches a {@link PreferenceInferrer} to the {@link UserSimilarity} implementation.
    * </p>
    * 
    * @param inferrer {@link PreferenceInferrer}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/PreferenceTransform.java mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/PreferenceTransform.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/PreferenceTransform.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/PreferenceTransform.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.transforms;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.model.Preference;
-
-/**
- * <p>
- * Implementations encapsulate a transform on a {@link Preference}'s value. These transformations are
- * typically applied to values before they are used to compute a similarity value. They are typically not
- * applied elsewhere; in particular {@link org.apache.mahout.cf.taste.model.DataModel}s no longer use a
- * transform like this to transform all of their preference values at the source.
- * </p>
- */
-@Deprecated
-public interface PreferenceTransform extends Refreshable {
-  
-  float getTransformedValue(Preference pref) throws TasteException;
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/SimilarityTransform.java mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/SimilarityTransform.java
--- mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/SimilarityTransform.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/cf/taste/transforms/SimilarityTransform.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.transforms;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-
-/**
- * <p>
- * Implementations encapsulate some transformation on similarity values between two things, where things might
- * be IDs of users or items or something else.
- * </p>
- */
-@Deprecated
-public interface SimilarityTransform extends Refreshable {
-  
-  /**
-   * @param value
-   *          original similarity between thing1 and thing2 (should be in [-1,1])
-   * @return transformed similarity (should be in [-1,1])
-   */
-  double transformSimilarity(long id1, long id2, double value);
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java mahout/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/DecisionForest.java	2014-03-29 01:03:13.000000000 -0700
@@ -198,7 +198,13 @@
     }
   }
 
-  private static DecisionForest read(DataInput dataInput) throws IOException {
+  /**
+   * Read the forest from inputStream
+   * @param dataInput - input forest
+   * @return {@link org.apache.mahout.classifier.df.DecisionForest}
+   * @throws IOException
+   */
+  public static DecisionForest read(DataInput dataInput) throws IOException {
     DecisionForest forest = new DecisionForest();
     forest.readFields(dataInput);
     return forest;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataConverter.java mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataConverter.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataConverter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataConverter.java	2014-03-29 01:03:13.000000000 -0700
@@ -42,29 +42,27 @@
 
     String[] tokens = COMMA_SPACE.split(string);
     Preconditions.checkArgument(tokens.length == nball,
-        "Wrong number of attributes in the string");
+        "Wrong number of attributes in the string: " + tokens.length + ". Must be " + nball);
 
     int nbattrs = dataset.nbAttributes();
     DenseVector vector = new DenseVector(nbattrs);
 
     int aId = 0;
     for (int attr = 0; attr < nball; attr++) {
-      if (ArrayUtils.contains(dataset.getIgnored(), attr)) {
-        continue; // IGNORED
-      }
-
-      String token = tokens[attr].trim();
-
-      if ("?".equals(token)) {
-        // missing value
-        return null;
-      }
+      if (!ArrayUtils.contains(dataset.getIgnored(), attr)) {
+        String token = tokens[attr].trim();
 
-      if (dataset.isNumerical(aId)) {
-        vector.set(aId++, Double.parseDouble(token));
-      } else { // CATEGORICAL
-        vector.set(aId, dataset.valueOf(aId, token));
-        aId++;
+        if ("?".equals(token)) {
+          // missing value
+          return null;
+        }
+
+        if (dataset.isNumerical(aId)) {
+          vector.set(aId++, Double.parseDouble(token));
+        } else { // CATEGORICAL
+          vector.set(aId, dataset.valueOf(aId, token));
+          aId++;
+        }
       }
     }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataLoader.java mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataLoader.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataLoader.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/data/DataLoader.java	2014-03-29 01:03:13.000000000 -0700
@@ -48,9 +48,9 @@
 
   private static final Logger log = LoggerFactory.getLogger(DataLoader.class);
 
-  private static final Pattern COMMA_SPACE = Pattern.compile("[, ]");
+  private static final Pattern SEPARATORS = Pattern.compile("[, ]");
 
-  private DataLoader() { }
+  private DataLoader() {}
 
   /**
    * Converts a comma-separated String to a Vector.
@@ -63,37 +63,32 @@
    */
   private static boolean parseString(Attribute[] attrs, Set<String>[] values, CharSequence string,
     boolean regression) {
-    String[] tokens = COMMA_SPACE.split(string);
-    Preconditions.checkArgument(tokens.length == attrs.length, "Wrong number of attributes in the string");
+    String[] tokens = SEPARATORS.split(string);
+    Preconditions.checkArgument(tokens.length == attrs.length,
+        "Wrong number of attributes in the string: " + tokens.length + ". Must be: " + attrs.length);
 
     // extract tokens and check is there is any missing value
     for (int attr = 0; attr < attrs.length; attr++) {
-      if (attrs[attr].isIgnored()) {
-        continue;
-      }
-      if ("?".equals(tokens[attr])) {
+      if (!attrs[attr].isIgnored() && "?".equals(tokens[attr])) {
         return false; // missing value
       }
     }
 
     for (int attr = 0; attr < attrs.length; attr++) {
-      if (attrs[attr].isIgnored()) {
-        continue;
-      }
-
-      String token = tokens[attr];
-
-      if (attrs[attr].isCategorical() || (!regression && attrs[attr].isLabel())) {
-        // update values
-        if (values[attr] == null) {
-          values[attr] = Sets.newHashSet();
-        }
-        values[attr].add(token);
-      } else {
-        try {
-          Double.parseDouble(token);
-        } catch (NumberFormatException e) {
-          return false;
+      if (!attrs[attr].isIgnored()) {
+        String token = tokens[attr];
+        if (attrs[attr].isCategorical() || (!regression && attrs[attr].isLabel())) {
+          // update values
+          if (values[attr] == null) {
+            values[attr] = Sets.newHashSet();
+          }
+          values[attr].add(token);
+        } else {
+          try {
+            Double.parseDouble(token);
+          } catch (NumberFormatException e) {
+            return false;
+          }
         }
       }
     }
@@ -122,48 +117,55 @@
 
     while (scanner.hasNextLine()) {
       String line = scanner.nextLine();
-      if (line.isEmpty()) {
+      if (!line.isEmpty()) {
+        Instance instance = converter.convert(line);
+        if (instance != null) {
+          instances.add(instance);
+        } else {
+          // missing values found
+          log.warn("{}: missing values", instances.size());
+        }
+      } else {
         log.warn("{}: empty string", instances.size());
-        continue;
       }
-
-      Instance instance = converter.convert(line);
-      if (instance == null) {
-        // missing values found
-        log.warn("{}: missing values", instances.size());
-        continue;
-      }
-
-      instances.add(instance);
     }
 
     scanner.close();
+    return new Data(dataset, instances);
+  }
+
+
+  /** Loads the data from multiple paths specified by pathes */
+  public static Data loadData(Dataset dataset, FileSystem fs, Path[] pathes) throws IOException {
+    List<Instance> instances = Lists.newArrayList();
 
+    for (Path path : pathes) {
+      Data loadedData = loadData(dataset, fs, path);
+      for (int index = 0; index <= loadedData.size(); index++) {
+        instances.add(loadedData.get(index));
+      }
+    }
     return new Data(dataset, instances);
   }
 
-  /**
-   * Loads the data from a String array
-   */
+  /** Loads the data from a String array */
   public static Data loadData(Dataset dataset, String[] data) {
     List<Instance> instances = Lists.newArrayList();
 
     DataConverter converter = new DataConverter(dataset);
 
     for (String line : data) {
-      if (line.isEmpty()) {
+      if (!line.isEmpty()) {
+        Instance instance = converter.convert(line);
+        if (instance != null) {
+          instances.add(instance);
+        } else {
+          // missing values found
+          log.warn("{}: missing values", instances.size());
+        }
+      } else {
         log.warn("{}: empty string", instances.size());
-        continue;
-      }
-
-      Instance instance = converter.convert(line);
-      if (instance == null) {
-        // missing values found
-        log.warn("{}: missing values", instances.size());
-        continue;
       }
-
-      instances.add(instance);
     }
 
     return new Data(dataset, instances);
@@ -193,12 +195,10 @@
     int size = 0;
     while (scanner.hasNextLine()) {
       String line = scanner.nextLine();
-      if (line.isEmpty()) {
-        continue;
-      }
-
-      if (parseString(attrs, valsets, line, regression)) {
-        size++;
+      if (!line.isEmpty()) {
+        if (parseString(attrs, valsets, line, regression)) {
+          size++;
+        }
       }
     }
 
@@ -232,12 +232,10 @@
 
     int size = 0;
     for (String aData : data) {
-      if (aData.isEmpty()) {
-        continue;
-      }
-
-      if (parseString(attrs, valsets, aData, regression)) {
-        size++;
+      if (!aData.isEmpty()) {
+        if (parseString(attrs, valsets, aData, regression)) {
+          size++;
+        }
       }
     }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/data/Dataset.java mahout/core/src/main/java/org/apache/mahout/classifier/df/data/Dataset.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/data/Dataset.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/data/Dataset.java	2014-03-29 01:03:13.000000000 -0700
@@ -37,7 +37,7 @@
 import java.util.Map;
 
 /**
- * Contains informations about the attributes.
+ * Contains information about the attributes.
  */
 public class Dataset {
 
@@ -67,7 +67,6 @@
     }
     
     private static Attribute fromString(String from) {
-      
       Attribute toReturn = LABEL;
       if (NUMERICAL.toString().equalsIgnoreCase(from)) {
         toReturn = NUMERICAL;
@@ -225,7 +224,7 @@
    */
   public int valueOf(int attr, String token) {
     Preconditions.checkArgument(!isNumerical(attr), "Only for CATEGORICAL attributes");
-    Preconditions.checkArgument(values != null, "Values not found");
+    Preconditions.checkArgument(values != null, "Values not found (equals null)");
     return ArrayUtils.indexOf(values[attr], token);
   }
 
@@ -238,13 +237,11 @@
    */
   private static int countAttributes(Attribute[] attrs) {
     int nbattrs = 0;
-
     for (Attribute attr : attrs) {
       if (!attr.isIgnored()) {
         nbattrs++;
       }
     }
-
     return nbattrs;
   }
 
@@ -320,7 +317,6 @@
    * @throws java.io.IOException
    */
   public static Dataset load(Configuration conf, Path path) throws IOException {
-
     FileSystem fs = path.getFileSystem(conf);
     long bytesToRead = fs.getFileStatus(path).getLen();
     byte[] buff = new byte[Long.valueOf(bytesToRead).intValue()];
@@ -340,12 +336,11 @@
    * @return some JSON
    */
   public String toJSON() {
-
     List<Map<String, Object>> toWrite = Lists.newLinkedList();
     // attributes does not include ignored columns and it does include the class label
     int ignoredCount = 0;
     for (int i = 0; i < attributes.length + ignored.length; i++) {
-      Map<String, Object> attribute = null;
+      Map<String, Object> attribute;
       int attributesIndex = i - ignoredCount;
       if (ignoredCount < ignored.length && i == ignored[ignoredCount]) {
         // fill in ignored atttribute
@@ -370,10 +365,9 @@
   /**
    * De-serialize an instance from a string
    * @param json From which an instance is created
-   * @return A shinny new Dataset
+   * @return A shiny new Dataset
    */
   public static Dataset fromJSON(String json) {
-
     List<Map<String, Object>> fromJSON;
     try {
       fromJSON = OBJECT_MAPPER.readValue(json, new TypeReference<List<Map<String, Object>>>() {});
@@ -397,7 +391,7 @@
         if (attribute.get(VALUES) != null) {
           List<String> get = (List<String>) attribute.get(VALUES);
           String[] array = get.toArray(new String[get.size()]);
-          nominalValues[i] = array;
+          nominalValues[i - ignored.size()] = array;
         }
       }
     }
@@ -413,17 +407,15 @@
   /**
    * Generate a map to describe an attribute
    * @param type The type
-   * @param values
-   * @param isLabel
-   * @return 
+   * @param values - values
+   * @param isLabel - is a label
+   * @return map of (AttributeTypes, Values)
    */
   private Map<String, Object> getMap(Attribute type, String[] values, boolean isLabel) {
-
     Map<String, Object> attribute = Maps.newHashMap();
     attribute.put(TYPE, type.toString().toLowerCase(Locale.getDefault()));
     attribute.put(VALUES, values);
     attribute.put(LABEL, isLabel);
     return attribute;
   }
-
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilder.java mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilder.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilder.java	2014-03-29 01:03:13.000000000 -0700
@@ -35,15 +35,20 @@
 import org.apache.mahout.classifier.df.node.Node;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.List;
 
 /**
  * Builds a random forest using partial data. Each mapper uses only the data given by its InputSplit
  */
 public class PartialBuilder extends Builder {
 
+  private static final Logger log = LoggerFactory.getLogger(PartialBuilder.class);
+
   public PartialBuilder(TreeBuilder treeBuilder, Path dataPath, Path datasetPath, Long seed) {
     this(treeBuilder, dataPath, datasetPath, seed, new Configuration());
   }
@@ -73,6 +78,18 @@
     
     job.setInputFormatClass(TextInputFormat.class);
     job.setOutputFormatClass(SequenceFileOutputFormat.class);
+
+    // For this implementation to work, mapred.map.tasks needs to be set to the actual
+    // number of mappers Hadoop will use:
+    TextInputFormat inputFormat = new TextInputFormat();
+    List<?> splits = inputFormat.getSplits(job);
+    if (splits == null || splits.isEmpty()) {
+      log.warn("Unable to compute number of splits?");
+    } else {
+      int numSplits = splits.size();
+      log.info("Setting mapred.map.tasks = {}", numSplits);
+      conf.setInt("mapred.map.tasks", numSplits);
+    }
   }
   
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1Mapper.java mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1Mapper.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1Mapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1Mapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -98,7 +98,7 @@
     }
     
     // mapper's partition
-    Preconditions.checkArgument(partition >= 0, "Wrong partition ID");
+    Preconditions.checkArgument(partition >= 0, "Wrong partition ID: " + partition + ". Partition must be >= 0!");
     this.partition = partition;
     
     // compute number of trees to build
@@ -116,8 +116,8 @@
   }
   
   /**
-   * Compute the number of trees for a given partition. The first partition (0) may be longer than the rest of
-   * partition because of the remainder.
+   * Compute the number of trees for a given partition. The first partitions may be longer
+   * than the rest because of the remainder.
    * 
    * @param numMaps
    *          total number of maps (partitions)
@@ -127,12 +127,9 @@
    *          partition to compute the number of trees for
    */
   public static int nbTrees(int numMaps, int numTrees, int partition) {
-    int nbTrees = numTrees / numMaps;
-    if (partition == 0) {
-      nbTrees += numTrees - nbTrees * numMaps;
-    }
-    
-    return nbTrees;
+    int treesPerMapper = numTrees / numMaps;
+    int remainder = numTrees - numMaps * treesPerMapper;
+    return treesPerMapper + (partition < remainder ? 1 : 0);
   }
   
   @Override
@@ -162,6 +159,8 @@
         MapredOutput emOut = new MapredOutput(tree);
         context.write(key, emOut);
       }
+
+      context.progress();
     }
   }
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/TreeID.java mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/TreeID.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/TreeID.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/mapreduce/partial/TreeID.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,8 +30,8 @@
   public TreeID() { }
   
   public TreeID(int partition, int treeId) {
-    Preconditions.checkArgument(partition >= 0, "partition < 0");
-    Preconditions.checkArgument(treeId >= 0, "treeId < 0");
+    Preconditions.checkArgument(partition >= 0, "Wrong partition: " + partition + ". Partition must be >= 0!");
+    Preconditions.checkArgument(treeId >= 0, "Wrong treeId: " + treeId + ". TreeId must be >= 0!");
     set(partition, treeId);
   }
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/df/split/OptIgSplit.java mahout/core/src/main/java/org/apache/mahout/classifier/df/split/OptIgSplit.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/df/split/OptIgSplit.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/df/split/OptIgSplit.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,178 +1,231 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.classifier.df.split;
-
-import org.apache.commons.lang3.ArrayUtils;
-import org.apache.mahout.classifier.df.data.Data;
-import org.apache.mahout.classifier.df.data.DataUtils;
-import org.apache.mahout.classifier.df.data.Dataset;
-import org.apache.mahout.classifier.df.data.Instance;
-
-import java.util.Arrays;
-
-/**
- * Optimized implementation of IgSplit<br>
- * This class can be used when the criterion variable is the categorical attribute.
- */
-public class OptIgSplit extends IgSplit {
-
-  private int[][] counts;
-
-  private int[] countAll;
-
-  private int[] countLess;
-
-  @Override
-  public Split computeSplit(Data data, int attr) {
-    if (data.getDataset().isNumerical(attr)) {
-      return numericalSplit(data, attr);
-    } else {
-      return categoricalSplit(data, attr);
-    }
-  }
-
-  /**
-   * Computes the split for a CATEGORICAL attribute
-   */
-  private static Split categoricalSplit(Data data, int attr) {
-    double[] values = data.values(attr);
-    int[][] counts = new int[values.length][data.getDataset().nblabels()];
-    int[] countAll = new int[data.getDataset().nblabels()];
-
-    Dataset dataset = data.getDataset();
-
-    // compute frequencies
-    for (int index = 0; index < data.size(); index++) {
-      Instance instance = data.get(index);
-      counts[ArrayUtils.indexOf(values, instance.get(attr))][(int) dataset.getLabel(instance)]++;
-      countAll[(int) dataset.getLabel(instance)]++;
-    }
-
-    int size = data.size();
-    double hy = entropy(countAll, size); // H(Y)
-    double hyx = 0.0; // H(Y|X)
-    double invDataSize = 1.0 / size;
-
-    for (int index = 0; index < values.length; index++) {
-      size = DataUtils.sum(counts[index]);
-      hyx += size * invDataSize * entropy(counts[index], size);
-    }
-
-    double ig = hy - hyx;
-    return new Split(attr, ig);
-  }
-
-  /**
-   * Return the sorted list of distinct values for the given attribute
-   */
-  private static double[] sortedValues(Data data, int attr) {
-    double[] values = data.values(attr);
-    Arrays.sort(values);
-
-    return values;
-  }
-
-  /**
-   * Instantiates the counting arrays
-   */
-  void initCounts(Data data, double[] values) {
-    counts = new int[values.length][data.getDataset().nblabels()];
-    countAll = new int[data.getDataset().nblabels()];
-    countLess = new int[data.getDataset().nblabels()];
-  }
-
-  void computeFrequencies(Data data, int attr, double[] values) {
-    Dataset dataset = data.getDataset();
-
-    for (int index = 0; index < data.size(); index++) {
-      Instance instance = data.get(index);
-      counts[ArrayUtils.indexOf(values, instance.get(attr))][(int) dataset.getLabel(instance)]++;
-      countAll[(int) dataset.getLabel(instance)]++;
-    }
-  }
-
-  /**
-   * Computes the best split for a NUMERICAL attribute
-   */
-  Split numericalSplit(Data data, int attr) {
-    double[] values = sortedValues(data, attr);
-
-    initCounts(data, values);
-
-    computeFrequencies(data, attr, values);
-
-    int size = data.size();
-    double hy = entropy(countAll, size);
-    double invDataSize = 1.0 / size;
-
-    int best = -1;
-    double bestIg = -1.0;
-
-    // try each possible split value
-    for (int index = 0; index < values.length; index++) {
-      double ig = hy;
-
-      // instance with attribute value < values[index]
-      size = DataUtils.sum(countLess);
-      ig -= size * invDataSize * entropy(countLess, size);
-
-      // instance with attribute value >= values[index]
-      size = DataUtils.sum(countAll);
-      ig -= size * invDataSize * entropy(countAll, size);
-
-      if (ig > bestIg) {
-        bestIg = ig;
-        best = index;
-      }
-
-      DataUtils.add(countLess, counts[index]);
-      DataUtils.dec(countAll, counts[index]);
-    }
-
-    if (best == -1) {
-      throw new IllegalStateException("no best split found !");
-    }
-    return new Split(attr, bestIg, values[best]);
-  }
-
-  /**
-   * Computes the Entropy
-   *
-   * @param counts   counts[i] = numInstances with label i
-   * @param dataSize numInstances
-   */
-  private static double entropy(int[] counts, int dataSize) {
-    if (dataSize == 0) {
-      return 0.0;
-    }
-
-    double entropy = 0.0;
-    double invDataSize = 1.0 / dataSize;
-
-    for (int count : counts) {
-      if (count == 0) {
-        continue; // otherwise we get a NaN
-      }
-      double p = count * invDataSize;
-      entropy += -p * Math.log(p) / LOG2;
-    }
-
-    return entropy;
-  }
-
-}
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.classifier.df.split;
+
+import org.apache.commons.math3.stat.descriptive.rank.Percentile;
+import org.apache.mahout.classifier.df.data.Data;
+import org.apache.mahout.classifier.df.data.DataUtils;
+import org.apache.mahout.classifier.df.data.Dataset;
+import org.apache.mahout.classifier.df.data.Instance;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.TreeSet;
+
+/**
+ * <p>Optimized implementation of IgSplit.
+ * This class can be used when the criterion variable is the categorical attribute.</p>
+ *
+ * <p>This code was changed in MAHOUT-1419 to deal in sampled splits among numeric
+ * features to fix a performance problem. To generate some synthetic data that exercises
+ * the issue, try for example generating 4 features of Normal(0,1) values with a random
+ * boolean 0/1 categorical feature. In Scala:</p>
+ *
+ * {@code
+ *  val r = new scala.util.Random()
+ *  val pw = new java.io.PrintWriter("random.csv")
+ *  (1 to 10000000).foreach(e =>
+ *    pw.println(r.nextDouble() + "," +
+ *               r.nextDouble() + "," +
+ *               r.nextDouble() + "," +
+ *               r.nextDouble() + "," +
+ *               (if (r.nextBoolean()) 1 else 0))
+ *   )
+ *   pw.close()
+ * }
+ */
+public class OptIgSplit extends IgSplit {
+
+  private static final int MAX_NUMERIC_SPLITS = 16;
+
+  @Override
+  public Split computeSplit(Data data, int attr) {
+    if (data.getDataset().isNumerical(attr)) {
+      return numericalSplit(data, attr);
+    } else {
+      return categoricalSplit(data, attr);
+    }
+  }
+
+  /**
+   * Computes the split for a CATEGORICAL attribute
+   */
+  private static Split categoricalSplit(Data data, int attr) {
+    double[] values = data.values(attr).clone();
+
+    double[] splitPoints = chooseCategoricalSplitPoints(values);
+
+    int numLabels = data.getDataset().nblabels();
+    int[][] counts = new int[splitPoints.length][numLabels];
+    int[] countAll = new int[numLabels];
+
+    computeFrequencies(data, attr, splitPoints, counts, countAll);
+
+    int size = data.size();
+    double hy = entropy(countAll, size); // H(Y)
+    double hyx = 0.0; // H(Y|X)
+    double invDataSize = 1.0 / size;
+
+    for (int index = 0; index < splitPoints.length; index++) {
+      size = DataUtils.sum(counts[index]);
+      hyx += size * invDataSize * entropy(counts[index], size);
+    }
+
+    double ig = hy - hyx;
+    return new Split(attr, ig);
+  }
+
+  static void computeFrequencies(Data data,
+                                 int attr,
+                                 double[] splitPoints,
+                                 int[][] counts,
+                                 int[] countAll) {
+    Dataset dataset = data.getDataset();
+
+    for (int index = 0; index < data.size(); index++) {
+      Instance instance = data.get(index);
+      int label = (int) dataset.getLabel(instance);
+      double value = instance.get(attr);
+      int split = 0;
+      while (split < splitPoints.length && value > splitPoints[split]) {
+        split++;
+      }
+      if (split < splitPoints.length) {
+        counts[split][label]++;
+      } // Otherwise it's in the last split, which we don't need to count
+      countAll[label]++;
+    }
+  }
+
+  /**
+   * Computes the best split for a NUMERICAL attribute
+   */
+  static Split numericalSplit(Data data, int attr) {
+    double[] values = data.values(attr).clone();
+    Arrays.sort(values);
+
+    double[] splitPoints = chooseNumericSplitPoints(values);
+
+    int numLabels = data.getDataset().nblabels();
+    int[][] counts = new int[splitPoints.length][numLabels];
+    int[] countAll = new int[numLabels];
+    int[] countLess = new int[numLabels];
+
+    computeFrequencies(data, attr, splitPoints, counts, countAll);
+
+    int size = data.size();
+    double hy = entropy(countAll, size);
+    double invDataSize = 1.0 / size;
+
+    int best = -1;
+    double bestIg = -1.0;
+
+    // try each possible split value
+    for (int index = 0; index < splitPoints.length; index++) {
+      double ig = hy;
+
+      DataUtils.add(countLess, counts[index]);
+      DataUtils.dec(countAll, counts[index]);
+
+      // instance with attribute value < values[index]
+      size = DataUtils.sum(countLess);
+      ig -= size * invDataSize * entropy(countLess, size);
+      // instance with attribute value >= values[index]
+      size = DataUtils.sum(countAll);
+      ig -= size * invDataSize * entropy(countAll, size);
+
+      if (ig > bestIg) {
+        bestIg = ig;
+        best = index;
+      }
+    }
+
+    if (best == -1) {
+      throw new IllegalStateException("no best split found !");
+    }
+    return new Split(attr, bestIg, splitPoints[best]);
+  }
+
+  /**
+   * @return an array of values to split the numeric feature's values on when
+   *  building candidate splits. When input size is <= MAX_NUMERIC_SPLITS + 1, it will
+   *  return the averages between success values as split points. When larger, it will
+   *  return MAX_NUMERIC_SPLITS approximate percentiles through the data.
+   */
+  private static double[] chooseNumericSplitPoints(double[] values) {
+    if (values.length <= 1) {
+      return values;
+    }
+    if (values.length <= MAX_NUMERIC_SPLITS + 1) {
+      double[] splitPoints = new double[values.length - 1];
+      for (int i = 1; i < values.length; i++) {
+        splitPoints[i-1] = (values[i] + values[i-1]) / 2.0;
+      }
+      return splitPoints;
+    }
+    Percentile distribution = new Percentile();
+    distribution.setData(values);
+    double[] percentiles = new double[MAX_NUMERIC_SPLITS];
+    for (int i = 0 ; i < percentiles.length; i++) {
+      double p = 100.0 * ((i + 1.0) / (MAX_NUMERIC_SPLITS + 1.0));
+      percentiles[i] = distribution.evaluate(p);
+    }
+    return percentiles;
+  }
+
+  private static double[] chooseCategoricalSplitPoints(double[] values) {
+    // There is no great reason to believe that categorical value order matters,
+    // but the original code worked this way, and it's not terrible in the absence
+    // of more sophisticated analysis
+    Collection<Double> uniqueOrderedCategories = new TreeSet<Double>();
+    for (double v : values) {
+      uniqueOrderedCategories.add(v);
+    }
+    double[] uniqueValues = new double[uniqueOrderedCategories.size()];
+    Iterator<Double> it = uniqueOrderedCategories.iterator();
+    for (int i = 0; i < uniqueValues.length; i++) {
+      uniqueValues[i] = it.next();
+    }
+    return uniqueValues;
+  }
+
+  /**
+   * Computes the Entropy
+   *
+   * @param counts   counts[i] = numInstances with label i
+   * @param dataSize numInstances
+   */
+  private static double entropy(int[] counts, int dataSize) {
+    if (dataSize == 0) {
+      return 0.0;
+    }
+
+    double entropy = 0.0;
+
+    for (int count : counts) {
+      if (count > 0) {
+        double p = count / (double) dataSize;
+        entropy -= p * Math.log(p);
+      }
+    }
+
+    return entropy / LOG2;
+  }
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearModel.java mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearModel.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearModel.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,104 +0,0 @@
-/* Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.math.Vector;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Classifies a data point using a hyperplane.
- */
-@Deprecated
-public class LinearModel {
-
-  private static final Logger log = LoggerFactory.getLogger(LinearModel.class);
-
-  /** Represents the direction of the hyperplane found during training.*/
-  private Vector hyperplane;
-  /** Displacement of hyperplane from origin.*/
-  private double bias;
-  /** Classification threshold. */
-  private final double threshold;
-  
-  /**
-   * Init a linear model with a hyperplane, distance and displacement.
-   */
-  public LinearModel(Vector hyperplane, double displacement, double threshold) {
-    this.hyperplane = hyperplane;
-    this.bias = displacement;
-    this.threshold = threshold;
-  }
-  
-  /**
-   * Init a linear model with zero displacement and a threshold of 0.5.
-   */
-  public LinearModel(Vector hyperplane) {
-    this(hyperplane, 0, 0.5);
-  }
-  
-  /**
-   * Classify a point to either belong to the class modeled by this linear model or not.
-   * @param dataPoint the data point to classify.
-   * @return returns true if data point should be classified as belonging to this model.
-   */
-  public boolean classify(Vector dataPoint) {
-    double product = this.hyperplane.dot(dataPoint);
-    if (log.isDebugEnabled()) {
-      log.debug("model: {} product: {} Bias: {} threshold: {}",
-                this, product, bias, threshold);
-    }
-    return product + this.bias > this.threshold;
-  }
-  
-  /**
-   * Update the hyperplane by adding delta.
-   * @param delta the delta to add to the hyperplane vector.
-   */
-  public void addDelta(Vector delta) {
-    this.hyperplane = this.hyperplane.plus(delta);
-  }
-  
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder("Model: ");
-    for (int i = 0; i < this.hyperplane.size(); i++) {
-      builder.append("  ").append(this.hyperplane.get(i));
-    }
-    builder.append(" C: ").append(this.bias);
-    return builder.toString();
-  }
-  
-  /**
-   * Shift the bias of the model.
-   * @param factor factor to multiply the bias by.
-   */
-  public void shiftBias(double factor) {
-    this.bias += factor;
-  }
-  
-  /**
-   * Multiply the weight at index by delta.
-   * @param index the index of the element to update.
-   * @param delta the delta to multiply the element with.
-   */
-  public void timesDelta(int index, double delta) {
-    double element = this.hyperplane.get(index);
-    element *= delta;
-    this.hyperplane.setQuick(index, element);
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearTrainer.java mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearTrainer.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearTrainer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/LinearTrainer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,127 +0,0 @@
-/* Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.math.CardinalityException;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.Vector;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Implementors of this class need to provide a way to train linear
- * discriminative classifiers.
- * 
- * As this is just the reference implementation we assume that the dataset fits
- * into main memory - this should be the first thing to change when switching to
- * Hadoop.
- */
-@Deprecated
-public abstract class LinearTrainer {
-  
-  private static final Logger log = LoggerFactory.getLogger(LinearTrainer.class);
-
-  /** The model to train. */
-  private final LinearModel model;
-  
-  /**
-   * Initialize the trainer. Distance is initialized to cosine distance, all
-   * weights are represented through a dense vector.
-   * 
-   * 
-   * @param dimension
-   *          number of expected features.
-   * @param threshold
-   *          threshold to use for classification.
-   * @param init
-   *          initial value of weight vector.
-   * @param initBias
-   *          initial classification bias.
-   */
-  protected LinearTrainer(int dimension, double threshold,
-                          double init, double initBias) {
-    DenseVector initialWeights = new DenseVector(dimension);
-    initialWeights.assign(init);
-    this.model = new LinearModel(initialWeights, initBias, threshold);
-  }
-  
-  /**
-   * Initializes training. Runs through all data points in the training set and
-   * updates the weight vector whenever a classification error occurs.
-   * 
-   * Can be called multiple times.
-   * 
-   * @param dataset
-   *          the dataset to train on. Each column is treated as point.
-   * @param labelset
-   *          the set of labels, one for each data point. If the cardinalities
-   *          of data- and labelset do not match, a CardinalityException is
-   *          thrown
-   */
-  public void train(Vector labelset, Matrix dataset) throws TrainingException {
-    if (labelset.size() != dataset.columnSize()) {
-      throw new CardinalityException(labelset.size(), dataset.columnSize());
-    }
-    
-    boolean converged = false;
-    int iteration = 0;
-    while (!converged) {
-      if (iteration > 1000) {
-        throw new TrainingException("Too many iterations needed to find hyperplane.");
-      }
-      
-      converged = true;
-      int columnCount = dataset.columnSize();
-      for (int i = 0; i < columnCount; i++) {
-        Vector dataPoint = dataset.viewColumn(i);
-        log.debug("Training point: {}", dataPoint);
-        
-        synchronized (this.model) {
-          boolean prediction = model.classify(dataPoint);
-          double label = labelset.get(i);
-          if (label <= 0 && prediction || label > 0 && !prediction) {
-            log.debug("updating");
-            converged = false;
-            update(label, dataPoint, this.model);
-          }
-        }
-      }
-      iteration++;
-    }
-  }
-  
-  /**
-   * Retrieves the trained model if called after train, otherwise the raw model.
-   */
-  public LinearModel getModel() {
-    return this.model;
-  }
-  
-  /**
-   * Implement this method to match your training strategy.
-   * 
-   * @param model
-   *          the model to update.
-   * @param label
-   *          the target label of the wrongly classified data point.
-   * @param dataPoint
-   *          the data point that was classified incorrectly.
-   */
-  protected abstract void update(double label, Vector dataPoint, LinearModel model);
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/PerceptronTrainer.java mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/PerceptronTrainer.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/PerceptronTrainer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/PerceptronTrainer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,63 +0,0 @@
-/* Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.math.Vector;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Implements training according to the perceptron update rule.
- */
-@Deprecated
-public class PerceptronTrainer extends LinearTrainer {
-  
-  private static final Logger log = LoggerFactory.getLogger(PerceptronTrainer.class);
-
-  /** Rate the model is to be updated with at each step. */
-  private final double learningRate;
-  
-  public PerceptronTrainer(int dimension, double threshold,
-                           double learningRate, double init, double initBias) {
-    super(dimension, threshold, init, initBias);
-    this.learningRate = learningRate;
-  }
-  
-  /**
-   * {@inheritDoc} Perceptron update works such that in case the predicted label
-   * does not match the real label, the weight vector is updated as follows: In
-   * case the prediction was positive but should have been negative, the weight vector
-   * is set to the sum of weight vector and example (multiplied by the learning rate).
-   * 
-   * In case the prediction was negative but should have been positive, the example
-   * vector (multiplied by the learning rate) is subtracted from the weight vector.
-   */
-  @Override
-  protected void update(double label, Vector dataPoint, LinearModel model) {
-    double factor = 1.0;
-    if (label == 0.0) {
-      factor = -1.0;
-    }
-    
-    Vector updateVector = dataPoint.times(factor).times(this.learningRate);
-    log.debug("Updatevec: {}", updateVector);
-    
-    model.addDelta(updateVector);
-    model.shiftBias(factor * this.learningRate);
-    log.debug("{}", model);
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/TrainingException.java mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/TrainingException.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/TrainingException.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/TrainingException.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.classifier.discriminative;
-
-/**
- * This exception is thrown in case training fails. E.g. training with an algorithm
- * that can find linear separating hyperplanes only on a training set that is not
- * linearly separable.
- * */
-@Deprecated
-public class TrainingException extends Exception {
-  
-  /**
-   * Init with message string describing the cause of the exception.
-   * */
-  public TrainingException(String message) {
-    super(message);
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/WinnowTrainer.java mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/WinnowTrainer.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/WinnowTrainer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/discriminative/WinnowTrainer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,89 +0,0 @@
-/* Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.math.Vector;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * This class implements training according to the winnow update algorithm.
- */
-@Deprecated
-public class WinnowTrainer extends LinearTrainer {
-  
-  private static final Logger log = LoggerFactory.getLogger(WinnowTrainer.class);
-  
-  /** Promotion step to multiply weights with on update. */
-  private final double promotionStep;
-  
-  public WinnowTrainer(int dimension, double promotionStep, double threshold, double init, double initBias) {
-    super(dimension, threshold, init, initBias);
-    this.promotionStep = promotionStep;
-  }
-  
-  public WinnowTrainer(int dimension, double promotionStep) {
-    this(dimension, promotionStep, 0.5, 1, 0);
-  }
-  
-  /**
-   * Initializes with dimension and promotionStep of 2.
-   * 
-   * @param dimension
-   *          number of features.
-   */
-  public WinnowTrainer(int dimension) {
-    this(dimension, 2);
-  }
-  
-  /**
-   * {@inheritDoc} Winnow update works such that in case the predicted label
-   * does not match the real label, the weight vector is updated as follows: In
-   * case the prediction was positive but should have been negative, all entries
-   * in the weight vector that correspond to non null features in the example
-   * are doubled.
-   * 
-   * In case the prediction was negative but should have been positive, all
-   * entries in the weight vector that correspond to non null features in the
-   * example are halved.
-   *
-   * NOTE: case 1 and case 2 below look _exactly_ the same.  Is this broken?
-   */
-  @Override
-  protected void update(double label, Vector dataPoint, LinearModel model) {
-    if (label > 0) {
-      // case one
-      Vector updateVector = dataPoint.times(1 / this.promotionStep);
-      log.info("Winnow update positive: {}", updateVector);
-      for (Vector.Element element : updateVector.nonZeroes()) {
-        if (element.get() != 0) {
-          model.timesDelta(element.index(), element.get());
-        }
-      }
-    } else {
-      // case two
-      Vector updateVector = dataPoint.times(1 / this.promotionStep);
-      log.info("Winnow update negative: {}", updateVector);
-      for (Vector.Element element : updateVector.nonZeroes()) {
-        if (element.get() != 0) {
-          model.timesDelta(element.index(), element.get());
-        }
-      }
-    }
-    log.info(model.toString());
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/mlp/MultilayerPerceptron.java mahout/core/src/main/java/org/apache/mahout/classifier/mlp/MultilayerPerceptron.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/mlp/MultilayerPerceptron.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/classifier/mlp/MultilayerPerceptron.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,88 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.classifier.mlp;
+
+import org.apache.mahout.classifier.OnlineLearner;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Vector;
+
+/**
+ * A Multilayer Perceptron (MLP) is a kind of feed-forward artificial neural
+ * network, which is a mathematical model inspired by the biological neural
+ * network. The Multilayer Perceptron can be used for various machine learning
+ * tasks such as classification and regression.
+ * 
+ * A detailed introduction about MLP can be found at
+ * http://ufldl.stanford.edu/wiki/index.php/Neural_Networks.
+ * 
+ * For this particular implementation, the users can freely control the topology
+ * of the MLP, including: 1. The size of the input layer; 2. The number of
+ * hidden layers; 3. The size of each hidden layer; 4. The size of the output
+ * layer. 5. The cost function. 6. The squashing function.
+ * 
+ * The model is trained in an online learning approach, where the weights of
+ * neurons in the MLP is updated incremented using backPropagation algorithm
+ * proposed by (Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986)
+ * Learning representations by back-propagating errors. Nature, 323, 533--536.)
+ */
+public class MultilayerPerceptron extends NeuralNetwork implements OnlineLearner {
+
+  /**
+   * The default constructor.
+   */
+  public MultilayerPerceptron() {
+    super();
+  }
+
+  /**
+   * Initialize the MLP by specifying the location of the model.
+   * 
+   * @param modelPath The path of the model.
+   */
+  public MultilayerPerceptron(String modelPath) {
+    super(modelPath);
+  }
+
+  @Override
+  public void train(int actual, Vector instance) {
+    // construct the training instance, where append the actual to instance
+    Vector trainingInstance = new DenseVector(instance.size() + 1);
+    for (int i = 0; i < instance.size(); ++i) {
+      trainingInstance.setQuick(i, instance.getQuick(i));
+    }
+    trainingInstance.setQuick(instance.size(), actual);
+    this.trainOnline(trainingInstance);
+  }
+
+  @Override
+  public void train(long trackingKey, String groupKey, int actual,
+      Vector instance) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public void train(long trackingKey, int actual, Vector instance) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public void close() {
+    // DO NOTHING
+  }
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetwork.java mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetwork.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetwork.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetwork.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,740 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.classifier.mlp;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.common.RandomWrapper;
+import org.apache.mahout.math.DenseMatrix;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Matrix;
+import org.apache.mahout.math.MatrixWritable;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.function.DoubleDoubleFunction;
+import org.apache.mahout.math.function.DoubleFunction;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
+
+/**
+ * AbstractNeuralNetwork defines the general operations for a neural network
+ * based model. Typically, all derivative models such as Multilayer Perceptron
+ * and Autoencoder consist of neurons and the weights between neurons.
+ */
+public abstract class NeuralNetwork {
+
+  /* The default learning rate */
+  private static final double DEFAULT_LEARNING_RATE = 0.5;
+  /* The default regularization weight */
+  private static final double DEFAULT_REGULARIZATION_WEIGHT = 0;
+  /* The default momentum weight */
+  private static final double DEFAULT_MOMENTUM_WEIGHT = 0.1;
+
+  public static enum TrainingMethod {
+    GRADIENT_DESCENT
+  }
+
+  /* the name of the model */
+  protected String modelType;
+
+  /* the path to store the model */
+  protected String modelPath;
+
+  protected double learningRate;
+
+  /* The weight of regularization */
+  protected double regularizationWeight;
+
+  /* The momentum weight */
+  protected double momentumWeight;
+
+  /* The cost function of the model */
+  protected String costFunctionName;
+
+  /* Record the size of each layer */
+  protected List<Integer> layerSizeList;
+
+  /* Training method used for training the model */
+  protected TrainingMethod trainingMethod;
+
+  /* Weights between neurons at adjacent layers */
+  protected List<Matrix> weightMatrixList;
+
+  /* Previous weight updates between neurons at adjacent layers */
+  protected List<Matrix> prevWeightUpdatesList;
+
+  /* Different layers can have different squashing function */
+  protected List<String> squashingFunctionList;
+
+  /* The index of the final layer */
+  protected int finalLayerIdx;
+
+  /**
+   * The default constructor that initializes the learning rate, regularization
+   * weight, and momentum weight by default.
+   */
+  public NeuralNetwork() {
+    this.learningRate = DEFAULT_LEARNING_RATE;
+    this.regularizationWeight = DEFAULT_REGULARIZATION_WEIGHT;
+    this.momentumWeight = DEFAULT_MOMENTUM_WEIGHT;
+    this.trainingMethod = TrainingMethod.GRADIENT_DESCENT;
+    this.costFunctionName = "Minus_Squared";
+    this.modelType = this.getClass().getSimpleName();
+
+    this.layerSizeList = Lists.newArrayList();
+    this.layerSizeList = Lists.newArrayList();
+    this.weightMatrixList = Lists.newArrayList();
+    this.prevWeightUpdatesList = Lists.newArrayList();
+    this.squashingFunctionList = Lists.newArrayList();
+  }
+
+  /**
+   * Initialize the NeuralNetwork by specifying learning rate, momentum weight
+   * and regularization weight.
+   * 
+   * @param learningRate The learning rate.
+   * @param momentumWeight The momentum weight.
+   * @param regularizationWeight The regularization weight.
+   */
+  public NeuralNetwork(double learningRate, double momentumWeight, double regularizationWeight) {
+    this();
+    this.setLearningRate(learningRate);
+    this.setMomentumWeight(momentumWeight);
+    this.setRegularizationWeight(regularizationWeight);
+  }
+
+  /**
+   * Initialize the NeuralNetwork by specifying the location of the model.
+   * 
+   * @param modelPath The location that the model is stored.
+   */
+  public NeuralNetwork(String modelPath) {
+    try {
+      this.modelPath = modelPath;
+      this.readFromModel();
+    } catch (IOException e) {
+      e.printStackTrace();
+    }
+  }
+
+  /**
+   * Get the type of the model.
+   * 
+   * @return The name of the model.
+   */
+  public String getModelType() {
+    return this.modelType;
+  }
+
+  /**
+   * Set the degree of aggression during model training, a large learning rate
+   * can increase the training speed, but it also decreases the chance of model
+   * converge.
+   * 
+   * @param learningRate Learning rate must be a non-negative value. Recommend in range (0, 0.5).
+   * @return The model instance.
+   */
+  public NeuralNetwork setLearningRate(double learningRate) {
+    Preconditions.checkArgument(learningRate > 0, "Learning rate must be larger than 0.");
+    this.learningRate = learningRate;
+    return this;
+  }
+
+  /**
+   * Get the value of learning rate.
+   * 
+   * @return The value of learning rate.
+   */
+  public double getLearningRate() {
+    return this.learningRate;
+  }
+
+  /**
+   * Set the regularization weight. More complex the model is, less weight the
+   * regularization is.
+   * 
+   * @param regularizationWeight regularization must be in the range [0, 0.1).
+   * @return The model instance.
+   */
+  public NeuralNetwork setRegularizationWeight(double regularizationWeight) {
+    Preconditions.checkArgument(regularizationWeight >= 0
+        && regularizationWeight < 0.1, "Regularization weight must be in range [0, 0.1)");
+    this.regularizationWeight = regularizationWeight;
+    return this;
+  }
+
+  /**
+   * Get the weight of the regularization.
+   * 
+   * @return The weight of regularization.
+   */
+  public double getRegularizationWeight() {
+    return this.regularizationWeight;
+  }
+
+  /**
+   * Set the momentum weight for the model.
+   * 
+   * @param momentumWeight momentumWeight must be in range [0, 0.5].
+   * @return The model instance.
+   */
+  public NeuralNetwork setMomentumWeight(double momentumWeight) {
+    Preconditions.checkArgument(momentumWeight >= 0 && momentumWeight <= 1.0,
+        "Momentum weight must be in range [0, 1.0]");
+    this.momentumWeight = momentumWeight;
+    return this;
+  }
+
+  /**
+   * Get the momentum weight.
+   * 
+   * @return The value of momentum.
+   */
+  public double getMomentumWeight() {
+    return this.momentumWeight;
+  }
+
+  /**
+   * Set the training method.
+   * 
+   * @param method The training method, currently supports GRADIENT_DESCENT.
+   * @return The instance of the model.
+   */
+  public NeuralNetwork setTrainingMethod(TrainingMethod method) {
+    this.trainingMethod = method;
+    return this;
+  }
+
+  /**
+   * Get the training method.
+   * 
+   * @return The training method enumeration.
+   */
+  public TrainingMethod getTrainingMethod() {
+    return this.trainingMethod;
+  }
+
+  /**
+   * Set the cost function for the model.
+   * 
+   * @param costFunction the name of the cost function. Currently supports
+   *          "Minus_Squared", "Cross_Entropy".
+   */
+  public NeuralNetwork setCostFunction(String costFunction) {
+    this.costFunctionName = costFunction;
+    return this;
+  }
+
+  /**
+   * Add a layer of neurons with specified size. If the added layer is not the
+   * first layer, it will automatically connect the neurons between with the
+   * previous layer.
+   * 
+   * @param size The size of the layer. (bias neuron excluded)
+   * @param isFinalLayer If false, add a bias neuron.
+   * @param squashingFunctionName The squashing function for this layer, input
+   *          layer is f(x) = x by default.
+   * @return The layer index, starts with 0.
+   */
+  public int addLayer(int size, boolean isFinalLayer, String squashingFunctionName) {
+    Preconditions.checkArgument(size > 0, "Size of layer must be larger than 0.");
+    int actualSize = size;
+    if (!isFinalLayer) {
+      actualSize += 1;
+    }
+
+    this.layerSizeList.add(actualSize);
+    int layerIdx = this.layerSizeList.size() - 1;
+    if (isFinalLayer) {
+      this.finalLayerIdx = layerIdx;
+    }
+
+    // add weights between current layer and previous layer, and input layer has
+    // no squashing function
+    if (layerIdx > 0) {
+      int sizePrevLayer = this.layerSizeList.get(layerIdx - 1);
+      // row count equals to size of current size and column count equal to
+      // size of previous layer
+      int row = isFinalLayer ? actualSize : actualSize - 1;
+      Matrix weightMatrix = new DenseMatrix(row, sizePrevLayer);
+      // initialize weights
+      final RandomWrapper rnd = RandomUtils.getRandom();
+      weightMatrix.assign(new DoubleFunction() {
+        @Override
+        public double apply(double value) {
+          return rnd.nextDouble() - 0.5;
+        }
+      });
+      this.weightMatrixList.add(weightMatrix);
+      this.prevWeightUpdatesList.add(new DenseMatrix(row, sizePrevLayer));
+      this.squashingFunctionList.add(squashingFunctionName);
+    }
+    return layerIdx;
+  }
+
+  /**
+   * Get the size of a particular layer.
+   * 
+   * @param layer The index of the layer, starting from 0.
+   * @return The size of the corresponding layer.
+   */
+  public int getLayerSize(int layer) {
+    Preconditions.checkArgument(layer >= 0 && layer < this.layerSizeList.size(),
+        String.format("Input must be in range [0, %d]\n", this.layerSizeList.size() - 1));
+    return this.layerSizeList.get(layer);
+  }
+
+  /**
+   * Get the layer size list.
+   * 
+   * @return The sizes of the layers.
+   */
+  protected List<Integer> getLayerSizeList() {
+    return this.layerSizeList;
+  }
+
+  /**
+   * Get the weights between layer layerIdx and layerIdx + 1
+   * 
+   * @param layerIdx The index of the layer.
+   * @return The weights in form of {@link Matrix}.
+   */
+  public Matrix getWeightsByLayer(int layerIdx) {
+    return this.weightMatrixList.get(layerIdx);
+  }
+
+  /**
+   * Update the weight matrices with given matrices.
+   * 
+   * @param matrices The weight matrices, must be the same dimension as the
+   *          existing matrices.
+   */
+  public void updateWeightMatrices(Matrix[] matrices) {
+    for (int i = 0; i < matrices.length; ++i) {
+      Matrix matrix = this.weightMatrixList.get(i);
+      this.weightMatrixList.set(i, matrix.plus(matrices[i]));
+    }
+  }
+
+  /**
+   * Set the weight matrices.
+   * 
+   * @param matrices The weight matrices, must be the same dimension of the
+   *          existing matrices.
+   */
+  public void setWeightMatrices(Matrix[] matrices) {
+    this.weightMatrixList = Lists.newArrayList();
+    Collections.addAll(this.weightMatrixList, matrices);
+  }
+
+  /**
+   * Set the weight matrix for a specified layer.
+   * 
+   * @param index The index of the matrix, starting from 0 (between layer 0 and 1).
+   * @param matrix The instance of {@link Matrix}.
+   */
+  public void setWeightMatrix(int index, Matrix matrix) {
+    Preconditions.checkArgument(0 <= index && index < this.weightMatrixList.size(),
+        String.format("index [%s] should be in range [%s, %s).", index, 0, this.weightMatrixList.size()));
+    this.weightMatrixList.set(index, matrix);
+  }
+
+  /**
+   * Get all the weight matrices.
+   * 
+   * @return The weight matrices.
+   */
+  public Matrix[] getWeightMatrices() {
+    Matrix[] matrices = new Matrix[this.weightMatrixList.size()];
+    this.weightMatrixList.toArray(matrices);
+    return matrices;
+  }
+
+  /**
+   * Get the output calculated by the model.
+   * 
+   * @param instance The feature instance in form of {@link Vector}, each dimension contains the value of the corresponding feature.
+   * @return The output vector.
+   */
+  public Vector getOutput(Vector instance) {
+    Preconditions.checkArgument(this.layerSizeList.get(0) == instance.size() + 1,
+        String.format("The dimension of input instance should be %d, but the input has dimension %d.",
+            this.layerSizeList.get(0) - 1, instance.size()));
+
+    // add bias feature
+    Vector instanceWithBias = new DenseVector(instance.size() + 1);
+    // set bias to be a little bit less than 1.0
+    instanceWithBias.set(0, 0.99999);
+    for (int i = 1; i < instanceWithBias.size(); ++i) {
+      instanceWithBias.set(i, instance.get(i - 1));
+    }
+
+    List<Vector> outputCache = getOutputInternal(instanceWithBias);
+    // return the output of the last layer
+    Vector result = outputCache.get(outputCache.size() - 1);
+    // remove bias
+    return result.viewPart(1, result.size() - 1);
+  }
+
+  /**
+   * Calculate output internally, the intermediate output of each layer will be
+   * stored.
+   * 
+   * @param instance The feature instance in form of {@link Vector}, each dimension contains the value of the corresponding feature.
+   * @return Cached output of each layer.
+   */
+  protected List<Vector> getOutputInternal(Vector instance) {
+    List<Vector> outputCache = Lists.newArrayList();
+    // fill with instance
+    Vector intermediateOutput = instance;
+    outputCache.add(intermediateOutput);
+
+    for (int i = 0; i < this.layerSizeList.size() - 1; ++i) {
+      intermediateOutput = forward(i, intermediateOutput);
+      outputCache.add(intermediateOutput);
+    }
+    return outputCache;
+  }
+
+  /**
+   * Forward the calculation for one layer.
+   * 
+   * @param fromLayer The index of the previous layer.
+   * @param intermediateOutput The intermediate output of previous layer.
+   * @return The intermediate results of the current layer.
+   */
+  protected Vector forward(int fromLayer, Vector intermediateOutput) {
+    Matrix weightMatrix = this.weightMatrixList.get(fromLayer);
+
+    Vector vec = weightMatrix.times(intermediateOutput);
+    vec = vec.assign(NeuralNetworkFunctions.getDoubleFunction(this.squashingFunctionList.get(fromLayer)));
+
+    // add bias
+    Vector vecWithBias = new DenseVector(vec.size() + 1);
+    vecWithBias.set(0, 1);
+    for (int i = 0; i < vec.size(); ++i) {
+      vecWithBias.set(i + 1, vec.get(i));
+    }
+    return vecWithBias;
+  }
+
+  /**
+   * Train the neural network incrementally with given training instance.
+   * 
+   * @param trainingInstance An training instance, including the features and the label(s). Its dimension must equals
+   *          to the size of the input layer (bias neuron excluded) + the size
+   *          of the output layer (a.k.a. the dimension of the labels).
+   */
+  public void trainOnline(Vector trainingInstance) {
+    Matrix[] matrices = this.trainByInstance(trainingInstance);
+    this.updateWeightMatrices(matrices);
+  }
+
+  /**
+   * Get the updated weights using one training instance.
+   * 
+   * @param trainingInstance An training instance, including the features and the label(s). Its dimension must equals
+   *          to the size of the input layer (bias neuron excluded) + the size
+   *          of the output layer (a.k.a. the dimension of the labels).
+   * @return The update of each weight, in form of {@link Matrix} list.
+   */
+  public Matrix[] trainByInstance(Vector trainingInstance) {
+    // validate training instance
+    int inputDimension = this.layerSizeList.get(0) - 1;
+    int outputDimension = this.layerSizeList.get(this.layerSizeList.size() - 1);
+    Preconditions.checkArgument(inputDimension + outputDimension == trainingInstance.size(),
+        String.format("The dimension of training instance is %d, but requires %d.", trainingInstance.size(),
+            inputDimension + outputDimension));
+
+    if (this.trainingMethod.equals(TrainingMethod.GRADIENT_DESCENT)) {
+      return this.trainByInstanceGradientDescent(trainingInstance);
+    }
+    throw new IllegalArgumentException(String.format("Training method is not supported."));
+  }
+
+  /**
+   * Train by gradient descent. Get the updated weights using one training
+   * instance.
+   * 
+   * @param trainingInstance An training instance, including the features and the label(s). Its dimension must equals
+   *          to the size of the input layer (bias neuron excluded) + the size
+   *          of the output layer (a.k.a. the dimension of the labels).
+   * @return The weight update matrices.
+   */
+  private Matrix[] trainByInstanceGradientDescent(Vector trainingInstance) {
+    int inputDimension = this.layerSizeList.get(0) - 1;
+
+    Vector inputInstance = new DenseVector(this.layerSizeList.get(0));
+    inputInstance.set(0, 1); // add bias
+    for (int i = 0; i < inputDimension; ++i) {
+      inputInstance.set(i + 1, trainingInstance.get(i));
+    }
+
+    Vector labels = trainingInstance.viewPart(inputInstance.size() - 1, trainingInstance.size() - inputInstance.size() + 1);
+
+    // initialize weight update matrices
+    Matrix[] weightUpdateMatrices = new Matrix[this.weightMatrixList.size()];
+    for (int m = 0; m < weightUpdateMatrices.length; ++m) {
+      weightUpdateMatrices[m] = new DenseMatrix(this.weightMatrixList.get(m).rowSize(), this.weightMatrixList.get(m).columnSize());
+    }
+
+    List<Vector> internalResults = this.getOutputInternal(inputInstance);
+
+    Vector deltaVec = new DenseVector(this.layerSizeList.get(this.layerSizeList.size() - 1));
+    Vector output = internalResults.get(internalResults.size() - 1);
+
+    final DoubleFunction derivativeSquashingFunction =
+        NeuralNetworkFunctions.getDerivativeDoubleFunction(this.squashingFunctionList.get(this.squashingFunctionList.size() - 1));
+
+    final DoubleDoubleFunction costFunction = NeuralNetworkFunctions.getDerivativeDoubleDoubleFunction(this.costFunctionName);
+
+    Matrix lastWeightMatrix = this.weightMatrixList.get(this.weightMatrixList.size() - 1);
+
+    for (int i = 0; i < deltaVec.size(); ++i) {
+      double costFuncDerivative = costFunction.apply(labels.get(i), output.get(i + 1));
+      // add regularization
+      costFuncDerivative += this.regularizationWeight * lastWeightMatrix.viewRow(i).zSum();
+      deltaVec.set(i, costFuncDerivative);
+      deltaVec.set(i, deltaVec.get(i) * derivativeSquashingFunction.apply(output.get(i + 1)));
+    }
+
+    // start from previous layer of output layer
+    for (int layer = this.layerSizeList.size() - 2; layer >= 0; --layer) {
+      deltaVec = backPropagate(layer, deltaVec, internalResults, weightUpdateMatrices[layer]);
+    }
+
+    this.prevWeightUpdatesList = Arrays.asList(weightUpdateMatrices);
+
+    return weightUpdateMatrices;
+  }
+
+  /**
+   * Back-propagate the errors to from next layer to current layer. The weight
+   * updated information will be stored in the weightUpdateMatrices, and the
+   * delta of the prevLayer will be returned.
+   * 
+   * @param curLayerIdx Index of current layer.
+   * @param nextLayerDelta Delta of next layer.
+   * @param outputCache The output cache to store intermediate results.
+   * @param weightUpdateMatrix  The weight update, in form of {@link Matrix}.
+   * @return The weight updates.
+   */
+  private Vector backPropagate(int curLayerIdx, Vector nextLayerDelta,
+                               List<Vector> outputCache, Matrix weightUpdateMatrix) {
+
+    // get layer related information
+    final DoubleFunction derivativeSquashingFunction =
+        NeuralNetworkFunctions.getDerivativeDoubleFunction(this.squashingFunctionList.get(curLayerIdx));
+    Vector curLayerOutput = outputCache.get(curLayerIdx);
+    Matrix weightMatrix = this.weightMatrixList.get(curLayerIdx);
+    Matrix prevWeightMatrix = this.prevWeightUpdatesList.get(curLayerIdx);
+
+    // next layer is not output layer, remove the delta of bias neuron
+    if (curLayerIdx != this.layerSizeList.size() - 2) {
+      nextLayerDelta = nextLayerDelta.viewPart(1, nextLayerDelta.size() - 1);
+    }
+
+    Vector delta = weightMatrix.transpose().times(nextLayerDelta);
+
+    delta = delta.assign(curLayerOutput, new DoubleDoubleFunction() {
+      @Override
+      public double apply(double deltaElem, double curLayerOutputElem) {
+        return deltaElem * derivativeSquashingFunction.apply(curLayerOutputElem);
+      }
+    });
+
+    // update weights
+    for (int i = 0; i < weightUpdateMatrix.rowSize(); ++i) {
+      for (int j = 0; j < weightUpdateMatrix.columnSize(); ++j) {
+        weightUpdateMatrix.set(i, j, -learningRate * nextLayerDelta.get(i) *
+            curLayerOutput.get(j) + this.momentumWeight * prevWeightMatrix.get(i, j));
+      }
+    }
+
+    return delta;
+  }
+
+  /**
+   * Read the model meta-data from the specified location.
+   * 
+   * @throws IOException
+   */
+  protected void readFromModel() throws IOException {
+    Preconditions.checkArgument(this.modelPath != null, "Model path has not been set.");
+    FSDataInputStream is = null;
+    try {
+      Path path = new Path(this.modelPath);
+      FileSystem fs = path.getFileSystem(new Configuration());
+      is = new FSDataInputStream(fs.open(path));
+      this.readFields(is);
+    } finally {
+      Closeables.close(is, true);
+    }
+  }
+
+  /**
+   * Write the model data to specified location.
+   * 
+   * @throws IOException
+   */
+  public void writeModelToFile() throws IOException {
+    Preconditions.checkArgument(this.modelPath != null, "Model path has not been set.");
+    FSDataOutputStream stream = null;
+    try {
+      Path path = new Path(this.modelPath);
+      FileSystem fs = path.getFileSystem(new Configuration());
+      stream = fs.create(path, true);
+      this.write(stream);
+    } finally {
+      Closeables.close(stream, false);
+    }
+  }
+
+  /**
+   * Set the model path.
+   * 
+   * @param modelPath The path of the model.
+   */
+  public void setModelPath(String modelPath) {
+    this.modelPath = modelPath;
+  }
+
+  /**
+   * Get the model path.
+   * 
+   * @return The path of the model.
+   */
+  public String getModelPath() {
+    return this.modelPath;
+  }
+
+  /**
+   * Write the fields of the model to output.
+   * 
+   * @param output The output instance.
+   * @throws IOException
+   */
+  public void write(DataOutput output) throws IOException {
+    // write model type
+    WritableUtils.writeString(output, modelType);
+    // write learning rate
+    output.writeDouble(learningRate);
+    // write model path
+    if (this.modelPath != null) {
+      WritableUtils.writeString(output, modelPath);
+    } else {
+      WritableUtils.writeString(output, "null");
+    }
+
+    // write regularization weight
+    output.writeDouble(this.regularizationWeight);
+    // write momentum weight
+    output.writeDouble(this.momentumWeight);
+
+    // write cost function
+    WritableUtils.writeString(output, this.costFunctionName);
+
+    // write layer size list
+    output.writeInt(this.layerSizeList.size());
+    for (Integer aLayerSizeList : this.layerSizeList) {
+      output.writeInt(aLayerSizeList);
+    }
+
+    WritableUtils.writeEnum(output, this.trainingMethod);
+
+    // write squashing functions
+    output.writeInt(this.squashingFunctionList.size());
+    for (String aSquashingFunctionList : this.squashingFunctionList) {
+      WritableUtils.writeString(output, aSquashingFunctionList);
+    }
+
+    // write weight matrices
+    output.writeInt(this.weightMatrixList.size());
+    for (Matrix aWeightMatrixList : this.weightMatrixList) {
+      MatrixWritable.writeMatrix(output, aWeightMatrixList);
+    }
+  }
+
+  /**
+   * Read the fields of the model from input.
+   * 
+   * @param input The input instance.
+   * @throws IOException
+   */
+  public void readFields(DataInput input) throws IOException {
+    // read model type
+    this.modelType = WritableUtils.readString(input);
+    if (!this.modelType.equals(this.getClass().getSimpleName())) {
+      throw new IllegalArgumentException("The specified location does not contains the valid NeuralNetwork model.");
+    }
+    // read learning rate
+    this.learningRate = input.readDouble();
+    // read model path
+    this.modelPath = WritableUtils.readString(input);
+    if (this.modelPath.equals("null")) {
+      this.modelPath = null;
+    }
+
+    // read regularization weight
+    this.regularizationWeight = input.readDouble();
+    // read momentum weight
+    this.momentumWeight = input.readDouble();
+
+    // read cost function
+    this.costFunctionName = WritableUtils.readString(input);
+
+    // read layer size list
+    int numLayers = input.readInt();
+    this.layerSizeList = Lists.newArrayList();
+    for (int i = 0; i < numLayers; i++) {
+      this.layerSizeList.add(input.readInt());
+    }
+
+    this.trainingMethod = WritableUtils.readEnum(input, TrainingMethod.class);
+
+    // read squash functions
+    int squashingFunctionSize = input.readInt();
+    this.squashingFunctionList = Lists.newArrayList();
+    for (int i = 0; i < squashingFunctionSize; i++) {
+      this.squashingFunctionList.add(WritableUtils.readString(input));
+    }
+
+    // read weights and construct matrices of previous updates
+    int numOfMatrices = input.readInt();
+    this.weightMatrixList = Lists.newArrayList();
+    this.prevWeightUpdatesList = Lists.newArrayList();
+    for (int i = 0; i < numOfMatrices; i++) {
+      Matrix matrix = MatrixWritable.readMatrix(input);
+      this.weightMatrixList.add(matrix);
+      this.prevWeightUpdatesList.add(new DenseMatrix(matrix.rowSize(), matrix.columnSize()));
+    }
+  }
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetworkFunctions.java mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetworkFunctions.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetworkFunctions.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/classifier/mlp/NeuralNetworkFunctions.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.classifier.mlp;
+
+import org.apache.mahout.math.function.DoubleDoubleFunction;
+import org.apache.mahout.math.function.DoubleFunction;
+import org.apache.mahout.math.function.Functions;
+
+/**
+ * The functions that will be used by NeuralNetwork.
+ */
+public class NeuralNetworkFunctions {
+
+  /**
+   * The derivation of identity function (f(x) = x).
+   */
+  public static DoubleFunction derivativeIdentityFunction = new DoubleFunction() {
+    @Override
+    public double apply(double x) {
+      return 1;
+    }
+  };
+
+  /**
+   * The derivation of minus squared function (f(t, o) = (o - t)^2).
+   */
+  public static DoubleDoubleFunction derivativeMinusSquared = new DoubleDoubleFunction() {
+    @Override
+    public double apply(double target, double output) {
+      return 2 * (output - target);
+    }
+  };
+
+  /**
+   * The cross entropy function (f(t, o) = -t * log(o) - (1 - t) * log(1 - o)).
+   */
+  public static DoubleDoubleFunction crossEntropy = new DoubleDoubleFunction() {
+    @Override
+    public double apply(double target, double output) {
+      return -target * Math.log(output) - (1 - target) * Math.log(1 - output);
+    }
+  };
+
+  /**
+   * The derivation of cross entropy function (f(t, o) = -t * log(o) - (1 - t) *
+   * log(1 - o)).
+   */
+  public static DoubleDoubleFunction derivativeCrossEntropy = new DoubleDoubleFunction() {
+    @Override
+    public double apply(double target, double output) {
+      double adjustedTarget = target;
+      double adjustedActual = output;
+      if (adjustedActual == 1) {
+        adjustedActual = 0.999;
+      } else if (output == 0) {
+        adjustedActual = 0.001;
+      }
+      if (adjustedTarget == 1) {
+        adjustedTarget = 0.999;
+      } else if (adjustedTarget == 0) {
+        adjustedTarget = 0.001;
+      }
+      return -adjustedTarget / adjustedActual + (1 - adjustedTarget) / (1 - adjustedActual);
+    }
+  };
+
+  /**
+   * Get the corresponding function by its name.
+   * Currently supports: "Identity", "Sigmoid".
+   * 
+   * @param function The name of the function.
+   * @return The corresponding double function.
+   */
+  public static DoubleFunction getDoubleFunction(String function) {
+    if (function.equalsIgnoreCase("Identity")) {
+      return Functions.IDENTITY;
+    } else if (function.equalsIgnoreCase("Sigmoid")) {
+      return Functions.SIGMOID;
+    } else {
+      throw new IllegalArgumentException("Function not supported.");
+    }
+  }
+
+  /**
+   * Get the derivation double function by the name.
+   * Currently supports: "Identity", "Sigmoid".
+   * 
+   * @param function The name of the function.
+   * @return The double function.
+   */
+  public static DoubleFunction getDerivativeDoubleFunction(String function) {
+    if (function.equalsIgnoreCase("Identity")) {
+      return derivativeIdentityFunction;
+    } else if (function.equalsIgnoreCase("Sigmoid")) {
+      return Functions.SIGMOIDGRADIENT;
+    } else {
+      throw new IllegalArgumentException("Function not supported.");
+    }
+  }
+
+  /**
+   * Get the corresponding double-double function by the name.
+   * Currently supports: "Minus_Squared", "Cross_Entropy".
+   * 
+   * @param function The name of the function.
+   * @return The double-double function.
+   */
+  public static DoubleDoubleFunction getDoubleDoubleFunction(String function) {
+    if (function.equalsIgnoreCase("Minus_Squared")) {
+      return Functions.MINUS_SQUARED;
+    } else if (function.equalsIgnoreCase("Cross_Entropy")) {
+      return derivativeCrossEntropy;
+    } else {
+      throw new IllegalArgumentException("Function not supported.");
+    }
+  }
+
+  /**
+   * Get the corresponding derivation of double double function by the name.
+   * Currently supports: "Minus_Squared", "Cross_Entropy".
+   * 
+   * @param function The name of the function.
+   * @return The double-double-function.
+   */
+  public static DoubleDoubleFunction getDerivativeDoubleDoubleFunction(String function) {
+    if (function.equalsIgnoreCase("Minus_Squared")) {
+      return derivativeMinusSquared;
+    } else if (function.equalsIgnoreCase("Cross_Entropy")) {
+      return derivativeCrossEntropy;
+    } else {
+      throw new IllegalArgumentException("Function not supported.");
+    }
+  }
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/test/TestNaiveBayesDriver.java mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/test/TestNaiveBayesDriver.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/test/TestNaiveBayesDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/test/TestNaiveBayesDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -136,7 +136,9 @@
     Job testJob = prepareJob(getInputPath(), getOutputPath(), SequenceFileInputFormat.class, BayesTestMapper.class,
             Text.class, VectorWritable.class, SequenceFileOutputFormat.class);
     //testJob.getConfiguration().set(LABEL_KEY, getOption("--labels"));
-    boolean complementary = parsedArgs.containsKey("testComplementary");
+    
+    //boolean complementary = parsedArgs.containsKey("testComplementary"); //always result to false as key in hash map is "--testComplementary"
+    boolean complementary = hasOption("testComplementary"); //or  complementary = parsedArgs.containsKey("--testComplementary");
     testJob.getConfiguration().set(COMPLEMENTARY, String.valueOf(complementary));
     return testJob.waitForCompletion(true);
   }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -93,8 +93,8 @@
     }
     long labelSize = createLabelIndex(labPath);
     float alphaI = Float.parseFloat(getOption(ALPHA_I));
-    boolean trainComplementary = Boolean.parseBoolean(getOption(TRAIN_COMPLEMENTARY));
-
+    //boolean trainComplementary = Boolean.parseBoolean(getOption(TRAIN_COMPLEMENTARY)); //always result to false
+    boolean trainComplementary = hasOption(TRAIN_COMPLEMENTARY);
 
     HadoopUtil.setSerializations(getConf());
     HadoopUtil.cacheFiles(labPath, getConf());
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/WeightsMapper.java mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/WeightsMapper.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/WeightsMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/WeightsMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -41,7 +41,7 @@
   protected void setup(Context ctx) throws IOException, InterruptedException {
     super.setup(ctx);
     int numLabels = Integer.parseInt(ctx.getConfiguration().get(NUM_LABELS));
-    Preconditions.checkArgument(numLabels > 0);
+    Preconditions.checkArgument(numLabels > 0, "Wrong numLabels: " + numLabels + ". Must be > 0!");
     weightsPerLabel = new DenseVector(numLabels);
   }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegression.java mahout/core/src/main/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegression.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegression.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegression.java	2014-03-29 01:03:13.000000000 -0700
@@ -99,7 +99,7 @@
    * @param numFeatures The number of features used in creating the vectors (i.e. the cardinality of the vector)
    * @param prior The {@link org.apache.mahout.classifier.sgd.PriorFunction} to use
    *
-   * @see {@link #AdaptiveLogisticRegression(int, int, org.apache.mahout.classifier.sgd.PriorFunction, int, int)}
+   * @see #AdaptiveLogisticRegression(int, int, org.apache.mahout.classifier.sgd.PriorFunction, int, int)
    */
   public AdaptiveLogisticRegression(int numCategories, int numFeatures, PriorFunction prior) {
     this(numCategories, numFeatures, prior, DEFAULT_THREAD_COUNT, DEFAULT_POOL_SIZE);
@@ -234,11 +234,12 @@
           return learner.logLikelihood();
         }
       });
-      ep.close();
     } catch (InterruptedException e) {
       log.warn("Ignoring exception", e);
     } catch (ExecutionException e) {
       throw new IllegalStateException(e);
+    } finally {
+      ep.close();
     }
   }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/sgd/CsvRecordFactory.java mahout/core/src/main/java/org/apache/mahout/classifier/sgd/CsvRecordFactory.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/sgd/CsvRecordFactory.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/sgd/CsvRecordFactory.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,14 +17,14 @@
 
 package org.apache.mahout.classifier.sgd;
 
-import com.google.common.base.CharMatcher;
 import com.google.common.base.Function;
 import com.google.common.base.Preconditions;
-import com.google.common.base.Splitter;
 import com.google.common.collect.Collections2;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
+
+import org.apache.commons.csv.CSVUtils;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.vectorizer.encoders.ConstantValueEncoder;
 import org.apache.mahout.vectorizer.encoders.ContinuousValueEncoder;
@@ -33,15 +33,17 @@
 import org.apache.mahout.vectorizer.encoders.StaticWordValueEncoder;
 import org.apache.mahout.vectorizer.encoders.TextValueEncoder;
 
+import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
 /**
- * Converts csv data lines to vectors.
+ * Converts CSV data lines to vectors.
  *
  * Use of this class proceeds in a few steps.
  * <ul>
@@ -68,10 +70,6 @@
 public class CsvRecordFactory implements RecordFactory {
   private static final String INTERCEPT_TERM = "Intercept Term";
 
-  // crude CSV value splitter.  This will fail if any double quoted strings have
-  // commas inside.  Also, escaped quotes will not be unescaped.  Good enough for now.
-  private static final Splitter COMMA = Splitter.on(',').trimResults(CharMatcher.is('"'));
-
   private static final Map<String, Class<? extends FeatureVectorEncoder>> TYPE_DICTIONARY =
           ImmutableMap.<String, Class<? extends FeatureVectorEncoder>>builder()
                   .put("continuous", ContinuousValueEncoder.class)
@@ -103,6 +101,29 @@
       "Unable to construct type converter... shouldn't be possible";
 
   /**
+   * Parse a single line of CSV-formatted text.
+   *
+   * Separated to make changing this functionality for the entire class easier
+   * in the future.
+   * @param line - CSV formatted text
+   * @return List<String>
+   */
+  private List<String> parseCsvLine(String line) {
+    try {
+      return Arrays.asList(CSVUtils.parseLine(line));
+	   }
+	   catch (IOException e) {
+      List<String> list = Lists.newArrayList();
+      list.add(line);
+      return list;
+   	}
+  }
+
+  private List<String> parseCsvLine(CharSequence line) {
+    return parseCsvLine(line.toString());
+  }
+
+  /**
    * Construct a parser for CSV lines that encodes the parsed data in vector form.
    * @param targetName            The name of the target variable.
    * @param typeMap               A map describing the types of the predictor variables.
@@ -142,7 +163,7 @@
   /**
    * Defines the number of target variable categories, but allows this parser to
    * pick encodings for them as they appear.
-   * @param max  The number of categories that will be excpeted.  Once this many have been
+   * @param max  The number of categories that will be expected.  Once this many have been
    * seen, all others will get the encoding max-1.
    */
   @Override
@@ -166,7 +187,7 @@
   public void firstLine(String line) {
     // read variable names, build map of name -> column
     final Map<String, Integer> vars = Maps.newHashMap();
-    variableNames = Lists.newArrayList(COMMA.split(line));
+    variableNames = parseCsvLine(line);
     int column = 0;
     for (String var : variableNames) {
       vars.put(var, column++);
@@ -229,7 +250,7 @@
 
 
   /**
-   * Decodes a single line of csv data and records the target and predictor variables in a record.
+   * Decodes a single line of CSV data and records the target and predictor variables in a record.
    * As a side effect, features are added into the featureVector.  Returns the value of the target
    * variable.
    *
@@ -240,7 +261,7 @@
    */
   @Override
   public int processLine(String line, Vector featureVector) {
-    List<String> values = Lists.newArrayList(COMMA.split(line));
+    List<String> values = parseCsvLine(line);
 
     int targetValue = targetDictionary.intern(values.get(target));
     if (targetValue >= maxTargetValue) {
@@ -260,7 +281,7 @@
   }
   
   /***
-   * Decodes a single line of csv data and records the target(if retrunTarget is true)
+   * Decodes a single line of CSV data and records the target(if retrunTarget is true)
    * and predictor variables in a record. As a side effect, features are added into the featureVector.
    * Returns the value of the target variable. When used during classify against production data without
    * target value, the method will be called with returnTarget = false. 
@@ -271,7 +292,7 @@
    * @return The value of the target variable.
    */
   public int processLine(CharSequence line, Vector featureVector, boolean returnTarget) {
-    List<String> values = Lists.newArrayList(COMMA.split(line));
+    List<String> values = parseCsvLine(line);
     int targetValue = -1;
     if (returnTarget) {
       targetValue = targetDictionary.intern(values.get(target));
@@ -293,7 +314,7 @@
    * @return the raw target value in the corresponding column of CSV line 
    */
   public String getTargetString(CharSequence line) {
-    List<String> values = Lists.newArrayList(COMMA.split(line));
+    List<String> values = parseCsvLine(line);
     return values.get(target);
 
   }
@@ -318,7 +339,7 @@
    * @return the id value of the CSV record
    */
   public String getIdString(CharSequence line) {
-    List<String> values = Lists.newArrayList(COMMA.split(line));
+    List<String> values = parseCsvLine(line);
     return values.get(id);
   }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/classifier/sgd/TPrior.java mahout/core/src/main/java/org/apache/mahout/classifier/sgd/TPrior.java
--- mahout/core/src/main/java/org/apache/mahout/classifier/sgd/TPrior.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/classifier/sgd/TPrior.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,7 +17,7 @@
 
 package org.apache.mahout.classifier.sgd;
 
-import org.apache.commons.math.special.Gamma;
+import org.apache.commons.math3.special.Gamma;
 
 import java.io.DataInput;
 import java.io.DataOutput;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/AbstractCluster.java mahout/core/src/main/java/org/apache/mahout/clustering/AbstractCluster.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/AbstractCluster.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/AbstractCluster.java	2014-03-29 01:03:13.000000000 -0700
@@ -60,7 +60,7 @@
   protected AbstractCluster(Vector point, int id2) {
     setNumObservations(0);
     setTotalObservations(0);
-    setCenter(new RandomAccessSparseVector(point));
+    setCenter(point.clone());
     setRadius(center.like());
     setS0(0);
     setS1(center.like());
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/ClusteringUtils.java mahout/core/src/main/java/org/apache/mahout/clustering/ClusteringUtils.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/ClusteringUtils.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/ClusteringUtils.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.clustering;
 
 import java.util.List;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/UncommonDistributions.java mahout/core/src/main/java/org/apache/mahout/clustering/UncommonDistributions.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/UncommonDistributions.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/UncommonDistributions.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,136 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering;
+
+import org.apache.commons.math3.distribution.NormalDistribution;
+import org.apache.commons.math3.distribution.RealDistribution;
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.common.RandomWrapper;
+
+public final class UncommonDistributions {
+
+  private static final RandomWrapper RANDOM = RandomUtils.getRandom();
+  
+  private UncommonDistributions() {}
+  
+  // =============== start of BSD licensed code. See LICENSE.txt
+  /**
+   * Returns a double sampled according to this distribution. Uniformly fast for all k > 0. (Reference:
+   * Non-Uniform Random Variate Generation, Devroye http://cgm.cs.mcgill.ca/~luc/rnbookindex.html) Uses
+   * Cheng's rejection algorithm (GB) for k>=1, rejection from Weibull distribution for 0 < k < 1.
+   */
+  public static double rGamma(double k, double lambda) {
+    boolean accept = false;
+    if (k >= 1.0) {
+      // Cheng's algorithm
+      double b = k - Math.log(4.0);
+      double c = k + Math.sqrt(2.0 * k - 1.0);
+      double lam = Math.sqrt(2.0 * k - 1.0);
+      double cheng = 1.0 + Math.log(4.5);
+      double x;
+      do {
+        double u = RANDOM.nextDouble();
+        double v = RANDOM.nextDouble();
+        double y = 1.0 / lam * Math.log(v / (1.0 - v));
+        x = k * Math.exp(y);
+        double z = u * v * v;
+        double r = b + c * y - x;
+        if (r >= 4.5 * z - cheng || r >= Math.log(z)) {
+          accept = true;
+        }
+      } while (!accept);
+      return x / lambda;
+    } else {
+      // Weibull algorithm
+      double c = 1.0 / k;
+      double d = (1.0 - k) * Math.pow(k, k / (1.0 - k));
+      double x;
+      do {
+        double u = RANDOM.nextDouble();
+        double v = RANDOM.nextDouble();
+        double z = -Math.log(u);
+        double e = -Math.log(v);
+        x = Math.pow(z, c);
+        if (z + e >= d + x) {
+          accept = true;
+        }
+      } while (!accept);
+      return x / lambda;
+    }
+  }
+  
+  // ============= end of BSD licensed code
+  
+  /**
+   * Returns a random sample from a beta distribution with the given shapes
+   * 
+   * @param shape1
+   *          a double representing shape1
+   * @param shape2
+   *          a double representing shape2
+   * @return a Vector of samples
+   */
+  public static double rBeta(double shape1, double shape2) {
+    double gam1 = rGamma(shape1, 1.0);
+    double gam2 = rGamma(shape2, 1.0);
+    return gam1 / (gam1 + gam2);
+    
+  }
+  
+  /**
+   * Return a random value from a normal distribution with the given mean and standard deviation
+   * 
+   * @param mean
+   *          a double mean value
+   * @param sd
+   *          a double standard deviation
+   * @return a double sample
+   */
+  public static double rNorm(double mean, double sd) {
+    RealDistribution dist = new NormalDistribution(RANDOM.getRandomGenerator(),
+                                                   mean,
+                                                   sd,
+                                                   NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);
+    return dist.sample();
+  }
+  
+  /**
+   * Returns an integer sampled according to this distribution. Takes time proportional to np + 1. (Reference:
+   * Non-Uniform Random Variate Generation, Devroye http://cgm.cs.mcgill.ca/~luc/rnbookindex.html) Second
+   * time-waiting algorithm.
+   */
+  public static int rBinomial(int n, double p) {
+    if (p >= 1.0) {
+      return n; // needed to avoid infinite loops and negative results
+    }
+    double q = -Math.log1p(-p);
+    double sum = 0.0;
+    int x = 0;
+    while (sum <= q) {
+      double u = RANDOM.nextDouble();
+      double e = -Math.log(u);
+      sum += e / (n - x);
+      x++;
+    }
+    if (x == 0) {
+      return 0;
+    }
+    return x - 1;
+  }
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyClusterer.java mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyClusterer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyClusterer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyClusterer.java	2014-03-29 01:03:13.000000000 -0700
@@ -21,9 +21,7 @@
 import java.util.Iterator;
 import java.util.List;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.mahout.clustering.AbstractCluster;
-import org.apache.mahout.common.ClassUtils;
 import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.math.Vector;
 import org.slf4j.Logger;
@@ -76,35 +74,6 @@
     return t4;
   }
 
-  public CanopyClusterer(Configuration config) {
-    this.configure(config);
-  }
-
-  /**
-   * Configure the Canopy and its distance measure
-   * 
-   * @param configuration
-   *            the Configuration
-   */
-  public void configure(Configuration configuration) {
-    measure = ClassUtils.instantiateAs(configuration.get(CanopyConfigKeys.DISTANCE_MEASURE_KEY),
-                                       DistanceMeasure.class);
-    measure.configure(configuration);
-    t1 = Double.parseDouble(configuration.get(CanopyConfigKeys.T1_KEY));
-    t2 = Double.parseDouble(configuration.get(CanopyConfigKeys.T2_KEY));
-    t3 = t1;
-    String d = configuration.get(CanopyConfigKeys.T3_KEY);
-    if (d != null) {
-      t3 = Double.parseDouble(d);
-    }
-    t4 = t2;
-    d = configuration.get(CanopyConfigKeys.T4_KEY);
-    if (d != null) {
-      t4 = Double.parseDouble(d);
-    }
-    nextCanopyId = 0;
-  }
-
   /**
    * Used by CanopyReducer to set t1=t3 and t2=t4 configuration values
    */
@@ -114,24 +83,6 @@
   }
 
   /**
-   * Configure the Canopy for unit tests
-   * 
-   * @param aMeasure
-   *            the DistanceMeasure
-   * @param aT1
-   *            the T1 distance threshold
-   * @param aT2
-   *            the T2 distance threshold
-   * */
-  public void config(DistanceMeasure aMeasure, double aT1, double aT2) {
-    measure = aMeasure;
-    t1 = aT1;
-    t2 = aT2;
-    t3 = t1;
-    t4 = t2;
-  }
-
-  /**
    * This is the same algorithm as the reference but inverted to iterate over
    * existing canopies instead of the points. Because of this it does not need
    * to actually store the points, instead storing a total points vector and
@@ -258,4 +209,11 @@
     }
   }
 
+  public void setT3(double t3) {
+    this.t3 = t3;
+  }
+
+  public void setT4(double t4) {
+    this.t4 = t4;
+  }
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyConfigKeys.java mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyConfigKeys.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyConfigKeys.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyConfigKeys.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,6 +17,10 @@
 
 package org.apache.mahout.clustering.canopy;
 
+import org.apache.hadoop.conf.Configuration;
+import org.apache.mahout.common.ClassUtils;
+import org.apache.mahout.common.distance.DistanceMeasure;
+
 public final class CanopyConfigKeys {
 
   private CanopyConfigKeys() {}
@@ -34,4 +38,32 @@
 
   public static final String CF_KEY = "org.apache.mahout.clustering.canopy.canopyFilter";
 
+  /**
+   * Create a {@link CanopyClusterer} from the Hadoop configuration.
+   *
+   * @param configuration Hadoop configuration
+   *
+   * @return CanopyClusterer
+   */
+  public static CanopyClusterer configureCanopyClusterer(Configuration configuration) {
+    double t1 = Double.parseDouble(configuration.get(T1_KEY));
+    double t2 = Double.parseDouble(configuration.get(T2_KEY));
+
+    DistanceMeasure measure = ClassUtils.instantiateAs(configuration.get(DISTANCE_MEASURE_KEY), DistanceMeasure.class);
+    measure.configure(configuration);
+
+    CanopyClusterer canopyClusterer = new CanopyClusterer(measure, t1, t2);
+
+    String d = configuration.get(T3_KEY);
+    if (d != null) {
+      canopyClusterer.setT3(Double.parseDouble(d));
+    }
+
+    d = configuration.get(T4_KEY);
+    if (d != null) {
+      canopyClusterer.setT4(Double.parseDouble(d));
+    }
+    return canopyClusterer;
+  }
+
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -45,7 +45,7 @@
   protected void setup(Context context) throws IOException,
       InterruptedException {
     super.setup(context);
-    canopyClusterer = new CanopyClusterer(context.getConfiguration());
+    canopyClusterer = CanopyConfigKeys.configureCanopyClusterer(context.getConfiguration());
     clusterFilter = Integer.parseInt(context.getConfiguration().get(
         CanopyConfigKeys.CF_KEY));
   }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -60,7 +60,7 @@
   protected void setup(Context context) throws IOException,
       InterruptedException {
     super.setup(context);
-    canopyClusterer = new CanopyClusterer(context.getConfiguration());
+    canopyClusterer = CanopyConfigKeys.configureCanopyClusterer(context.getConfiguration());
     canopyClusterer.useT3T4();
     clusterFilter = Integer.parseInt(context.getConfiguration().get(
         CanopyConfigKeys.CF_KEY));
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -20,14 +20,17 @@
 import java.io.IOException;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
 
 import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
@@ -38,12 +41,16 @@
 import org.apache.mahout.clustering.Cluster;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.clustering.iterator.ClusteringPolicy;
+import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
 import org.apache.mahout.common.AbstractJob;
+import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
+import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
+import org.apache.mahout.math.NamedVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.Vector.Element;
 import org.apache.mahout.math.VectorWritable;
@@ -122,15 +129,8 @@
    * @throws InterruptedException
    * @throws ClassNotFoundException
    */
-  public static void run(Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
+  public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold,
       boolean emitMostLikely, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
-    Configuration conf = new Configuration();
-    run(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely, runSequential);
-  }
-
-  public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output,
-                         Double clusterClassificationThreshold, boolean emitMostLikely, boolean runSequential)
-    throws IOException, InterruptedException, ClassNotFoundException {
     if (runSequential) {
       classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);
     } else {
@@ -190,20 +190,34 @@
    * @param output
    *          the path to store classified data
    * @param clusterClassificationThreshold
+   *          the threshold value of probability distribution function from 0.0
+   *          to 1.0. Any vector with pdf less that this threshold will not be
+   *          classified for the cluster
    * @param emitMostLikely
-   *          TODO
+   *          emit the vectors with the max pdf values per cluster
    * @throws IOException
    */
   private static void selectCluster(Path input, List<Cluster> clusterModels, ClusterClassifier clusterClassifier,
       Path output, Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException {
     Configuration conf = new Configuration();
     SequenceFile.Writer writer = new SequenceFile.Writer(input.getFileSystem(conf), conf, new Path(output,
-        "part-m-" + 0), IntWritable.class, WeightedVectorWritable.class);
-    for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(input, PathType.LIST,
+        "part-m-" + 0), IntWritable.class, WeightedPropertyVectorWritable.class);
+    for (Pair<Writable, VectorWritable> vw : new SequenceFileDirIterable<Writable, VectorWritable>(input, PathType.LIST,
         PathFilters.logsCRCFilter(), conf)) {
-      Vector pdfPerCluster = clusterClassifier.classify(vw.get());
+      // Converting to NamedVectors to preserve the vectorId else its not obvious as to which point
+      // belongs to which cluster - fix for MAHOUT-1410
+      Class<? extends Writable> keyClass = vw.getFirst().getClass();
+      Vector vector = vw.getSecond().get();
+      if (!keyClass.equals(NamedVector.class)) {
+        if (keyClass.equals(Text.class)) {
+          vector = new NamedVector(vector, vw.getFirst().toString());
+        } else if (keyClass.equals(IntWritable.class)) {
+          vector = new NamedVector(vector, Integer.toString(((IntWritable) vw.getFirst()).get()));
+        }
+      }
+      Vector pdfPerCluster = clusterClassifier.classify(vector);
       if (shouldClassify(pdfPerCluster, clusterClassificationThreshold)) {
-        classifyAndWrite(clusterModels, clusterClassificationThreshold, emitMostLikely, writer, vw, pdfPerCluster);
+        classifyAndWrite(clusterModels, clusterClassificationThreshold, emitMostLikely, writer, new VectorWritable(vector), pdfPerCluster);
       }
     }
     writer.close();
@@ -211,10 +225,12 @@
   
   private static void classifyAndWrite(List<Cluster> clusterModels, Double clusterClassificationThreshold,
       boolean emitMostLikely, SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException {
+    Map<Text, Text> props = Maps.newHashMap();
     if (emitMostLikely) {
       int maxValueIndex = pdfPerCluster.maxValueIndex();
-      WeightedVectorWritable wvw = new WeightedVectorWritable(pdfPerCluster.maxValue(), vw.get());
-      write(clusterModels, writer, wvw, maxValueIndex);
+      WeightedPropertyVectorWritable weightedPropertyVectorWritable =
+          new WeightedPropertyVectorWritable(pdfPerCluster.maxValue(), vw.get(), props);
+      write(clusterModels, writer, weightedPropertyVectorWritable, maxValueIndex);
     } else {
       writeAllAboveThreshold(clusterModels, clusterClassificationThreshold, writer, vw, pdfPerCluster);
     }
@@ -222,19 +238,27 @@
   
   private static void writeAllAboveThreshold(List<Cluster> clusterModels, Double clusterClassificationThreshold,
       SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException {
+    Map<Text, Text> props = Maps.newHashMap();
     for (Element pdf : pdfPerCluster.nonZeroes()) {
       if (pdf.get() >= clusterClassificationThreshold) {
-        WeightedVectorWritable wvw = new WeightedVectorWritable(pdf.get(), vw.get());
+        WeightedPropertyVectorWritable wvw = new WeightedPropertyVectorWritable(pdf.get(), vw.get(), props);
         int clusterIndex = pdf.index();
         write(clusterModels, writer, wvw, clusterIndex);
       }
     }
   }
-  
-  private static void write(List<Cluster> clusterModels, SequenceFile.Writer writer, WeightedVectorWritable wvw,
+
+  private static void write(List<Cluster> clusterModels, SequenceFile.Writer writer,
+      WeightedPropertyVectorWritable weightedPropertyVectorWritable,
       int maxValueIndex) throws IOException {
     Cluster cluster = clusterModels.get(maxValueIndex);
-    writer.append(new IntWritable(cluster.getId()), wvw);
+
+    DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;
+    DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();
+    double distance = distanceMeasure.distance(cluster.getCenter(), weightedPropertyVectorWritable.getVector());
+
+    weightedPropertyVectorWritable.getProperties().put(new Text("distance"), new Text(Double.toString(distance)));
+    writer.append(new IntWritable(cluster.getId()), weightedPropertyVectorWritable);
   }
   
   /**
@@ -266,7 +290,7 @@
     job.setNumReduceTasks(0);
     
     job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(WeightedVectorWritable.class);
+    job.setOutputValueClass(WeightedPropertyVectorWritable.class);
     
     FileInputFormat.addInputPath(job, input);
     FileOutputFormat.setOutputPath(job, output);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassificationMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -20,22 +20,28 @@
 import java.io.IOException;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
 
 import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.mahout.clustering.Cluster;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.clustering.iterator.ClusteringPolicy;
+import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
+import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.PathType;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
+import org.apache.mahout.math.NamedVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.Vector.Element;
 import org.apache.mahout.math.VectorWritable;
@@ -80,13 +86,24 @@
   protected void map(WritableComparable<?> key, VectorWritable vw, Context context)
     throws IOException, InterruptedException {
     if (!clusterModels.isEmpty()) {
-      Vector pdfPerCluster = clusterClassifier.classify(vw.get());
+      // Converting to NamedVectors to preserve the vectorId else its not obvious as to which point
+      // belongs to which cluster - fix for MAHOUT-1410
+      Class<? extends Vector> vectorClass = vw.get().getClass();
+      Vector vector = vw.get();
+      if (!vectorClass.equals(NamedVector.class)) {
+        if (key.getClass().equals(Text.class)) {
+          vector = new NamedVector(vector, key.toString());
+        } else if (key.getClass().equals(IntWritable.class)) {
+          vector = new NamedVector(vector, Integer.toString(((IntWritable) key).get()));
+        }
+      }
+      Vector pdfPerCluster = clusterClassifier.classify(vector);
       if (shouldClassify(pdfPerCluster)) {
         if (emitMostLikely) {
           int maxValueIndex = pdfPerCluster.maxValueIndex();
-          write(vw, context, maxValueIndex, 1.0);
+          write(new VectorWritable(vector), context, maxValueIndex, 1.0);
         } else {
-          writeAllAboveThreshold(vw, context, pdfPerCluster);
+          writeAllAboveThreshold(new VectorWritable(vector), context, pdfPerCluster);
         }
       }
     }
@@ -106,7 +123,14 @@
     throws IOException, InterruptedException {
     Cluster cluster = clusterModels.get(clusterIndex);
     clusterId.set(cluster.getId());
-    context.write(clusterId, new WeightedVectorWritable(weight, vw.get()));
+
+    DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;
+    DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();
+    double distance = distanceMeasure.distance(cluster.getCenter(), vw.get());
+
+    Map<Text, Text> props = Maps.newHashMap();
+    props.put(new Text("distance"), new Text(Double.toString(distance)));
+    context.write(clusterId, new WeightedPropertyVectorWritable(weight, vw.get(), props));
   }
   
   public static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassifier.java mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassifier.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassifier.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/classify/ClusterClassifier.java	2014-03-29 01:03:13.000000000 -0700
@@ -55,7 +55,7 @@
  * causes all the models to computeParameters.
  * 
  * Because a ClusterClassifier implements Writable, it can be written-to and
- * read-from a sequence file as a single entity. For sequential and mapreduce
+ * read-from a sequence file as a single entity. For sequential and MapReduce
  * clustering in conjunction with a ClusterIterator; however, it utilizes an
  * exploded file format. In this format, the iterator writes the policy to a
  * single POLICY_FILE_NAME file in the clustersOut directory and the models are
@@ -86,7 +86,7 @@
     this.policy = policy;
   }
   
-  // needed for serialization/deserialization
+  // needed for serialization/De-serialization
   public ClusterClassifier() {}
   
   // only used by MR ClusterIterator
@@ -224,6 +224,7 @@
     Text key = new Text();
     ClusteringPolicyWritable cpw = new ClusteringPolicyWritable();
     reader.next(key, cpw);
+    Closeables.close(reader, true);
     return cpw.getValue();
   }
   
@@ -234,6 +235,6 @@
     SequenceFile.Writer writer = new SequenceFile.Writer(fs, config, policyPath, Text.class,
         ClusteringPolicyWritable.class);
     writer.append(new Text(), new ClusteringPolicyWritable(policy));
-    writer.close();
+    Closeables.close(writer, false);
   }
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/DirichletDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/DirichletDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/DirichletDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/DirichletDriver.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,236 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.Model;
-import org.apache.mahout.clustering.ModelDistribution;
-import org.apache.mahout.clustering.classify.ClusterClassificationDriver;
-import org.apache.mahout.clustering.classify.ClusterClassifier;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
-import org.apache.mahout.clustering.iterator.ClusterIterator;
-import org.apache.mahout.clustering.iterator.DirichletClusteringPolicy;
-import org.apache.mahout.clustering.topdown.PathDirectory;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.VectorWritable;
-
-import com.google.common.collect.Lists;
-
-@Deprecated
-public class DirichletDriver extends AbstractJob {
-  
-  public static final String STATE_IN_KEY = "org.apache.mahout.clustering.dirichlet.stateIn";
-  public static final String MODEL_DISTRIBUTION_KEY = "org.apache.mahout.clustering.dirichlet.modelFactory";
-  public static final String NUM_CLUSTERS_KEY = "org.apache.mahout.clustering.dirichlet.numClusters";
-  public static final String ALPHA_0_KEY = "org.apache.mahout.clustering.dirichlet.alpha_0";
-  public static final String EMIT_MOST_LIKELY_KEY = "org.apache.mahout.clustering.dirichlet.emitMostLikely";
-  public static final String THRESHOLD_KEY = "org.apache.mahout.clustering.dirichlet.threshold";
-  public static final String MODEL_PROTOTYPE_CLASS_OPTION = "modelPrototype";
-  public static final String MODEL_DISTRIBUTION_CLASS_OPTION = "modelDist";
-  public static final String ALPHA_OPTION = "alpha";
-  
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new Configuration(), new DirichletDriver(), args);
-  }
-  
-  @Override
-  public int run(String[] args) throws Exception {
-    addInputOption();
-    addOutputOption();
-    addOption(DefaultOptionCreator.maxIterationsOption().create());
-    addOption(DefaultOptionCreator.numClustersOption().withRequired(true).create());
-    addOption(DefaultOptionCreator.overwriteOption().create());
-    addOption(DefaultOptionCreator.clusteringOption().create());
-    addOption(ALPHA_OPTION, "a0", "The alpha0 value for the DirichletDistribution. Defaults to 1.0", "1.0");
-    addOption(MODEL_DISTRIBUTION_CLASS_OPTION, "md",
-        "The ModelDistribution class name. Defaults to GaussianClusterDistribution",
-        GaussianClusterDistribution.class.getName());
-    addOption(MODEL_PROTOTYPE_CLASS_OPTION, "mp",
-        "The ModelDistribution prototype Vector class name. Defaults to RandomAccessSparseVector",
-        RandomAccessSparseVector.class.getName());
-    addOption(DefaultOptionCreator.distanceMeasureOption().withRequired(false).create());
-    addOption(DefaultOptionCreator.emitMostLikelyOption().create());
-    addOption(DefaultOptionCreator.thresholdOption().create());
-    addOption(DefaultOptionCreator.methodOption().create());
-    
-    if (parseArguments(args) == null) {
-      return -1;
-    }
-    
-    Path input = getInputPath();
-    Path output = getOutputPath();
-    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(getConf(), output);
-    }
-    String modelFactory = getOption(MODEL_DISTRIBUTION_CLASS_OPTION);
-    String modelPrototype = getOption(MODEL_PROTOTYPE_CLASS_OPTION);
-    String distanceMeasure = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
-    int numModels = Integer.parseInt(getOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION));
-    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
-    boolean emitMostLikely = Boolean.parseBoolean(getOption(DefaultOptionCreator.EMIT_MOST_LIKELY_OPTION));
-    double threshold = Double.parseDouble(getOption(DefaultOptionCreator.THRESHOLD_OPTION));
-    double alpha0 = Double.parseDouble(getOption(ALPHA_OPTION));
-    boolean runClustering = hasOption(DefaultOptionCreator.CLUSTERING_OPTION);
-    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(
-        DefaultOptionCreator.SEQUENTIAL_METHOD);
-    int prototypeSize = readPrototypeSize(input);
-    
-    DistributionDescription description = new DistributionDescription(modelFactory, modelPrototype, distanceMeasure,
-        prototypeSize);
-    
-    run(getConf(), input, output, description, numModels, maxIterations, alpha0, runClustering, emitMostLikely,
-        threshold, runSequential);
-    return 0;
-  }
-  
-  /**
-   * Iterate over the input vectors to produce clusters and, if requested, use the results of the final iteration to
-   * cluster the input vectors.
-   * 
-   * @param conf
-   *          the Configuration to use
-   * @param input
-   *          the directory Path for input points
-   * @param output
-   *          the directory Path for output points
-   * @param description
-   *          model distribution parameters
-   * @param maxIterations
-   *          the maximum number of iterations
-   * @param alpha0
-   *          the alpha_0 value for the DirichletDistribution
-   * @param runClustering
-   *          true if clustering of points to be done after iterations
-   * @param emitMostLikely
-   *          a boolean if true emit only most likely cluster for each point
-   * @param threshold
-   *          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
-   * @param runSequential
-   *          execute sequentially if true
-   */
-  public static void run(Configuration conf, Path input, Path output, DistributionDescription description,
-      int numModels, int maxIterations, double alpha0, boolean runClustering, boolean emitMostLikely, double threshold,
-      boolean runSequential) throws IOException, ClassNotFoundException, InterruptedException {
-    Path clustersOut = buildClusters(conf, input, output, description, numModels, maxIterations, alpha0, runSequential);
-    if (runClustering) {
-      clusterData(conf, input, clustersOut, output, alpha0, numModels, emitMostLikely, threshold, runSequential);
-    }
-  }
-  
-  /**
-   * Read the first input vector to determine the prototype size for the modelPrototype
-   */
-  public static int readPrototypeSize(Path input) throws IOException {
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    FileStatus[] status = fs.listStatus(input, PathFilters.logsCRCFilter());
-    int protoSize = 0;
-    if (status.length > 0) {
-      FileStatus s = status[0];
-      for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(s.getPath(), true, conf)) {
-        protoSize = value.get().size();
-      }
-    }
-    return protoSize;
-  }
-  
-  /**
-   * Iterate over the input vectors to produce cluster directories for each iteration
-   * 
-   * @param conf
-   *          the hadoop configuration
-   * @param input
-   *          the directory Path for input points
-   * @param output
-   *          the directory Path for output points
-   * @param description
-   *          model distribution parameters
-   * @param numClusters
-   *          the number of models to iterate over
-   * @param maxIterations
-   *          the maximum number of iterations
-   * @param alpha0
-   *          the alpha_0 value for the DirichletDistribution
-   * @param runSequential
-   *          execute sequentially if true
-   * 
-   * @return the Path of the final clusters directory
-   */
-  public static Path buildClusters(Configuration conf, Path input, Path output, DistributionDescription description,
-      int numClusters, int maxIterations, double alpha0, boolean runSequential) throws IOException,
-      ClassNotFoundException, InterruptedException {
-    Path clustersIn = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);
-    ModelDistribution<VectorWritable> modelDist = description.createModelDistribution(conf);
-    
-    List<Cluster> models = Lists.newArrayList();
-    for (Model<VectorWritable> cluster : modelDist.sampleFromPrior(numClusters)) {
-      models.add((Cluster) cluster);
-    }
-    
-    ClusterClassifier prior = new ClusterClassifier(models, new DirichletClusteringPolicy(numClusters, alpha0));
-    prior.writeToSeqFiles(clustersIn);
-    
-    if (runSequential) {
-      ClusterIterator.iterateSeq(conf, input, clustersIn, output, maxIterations);
-    } else {
-      ClusterIterator.iterateMR(conf, input, clustersIn, output, maxIterations);
-    }
-    return output;
-    
-  }
-  
-  /**
-   * Run the job using supplied arguments
-   * 
-   * @param conf
-   * @param input
-   *          the directory pathname for input points
-   * @param stateIn
-   *          the directory pathname for input state
-   * @param output
-   *          the directory pathname for output points
-   * @param emitMostLikely
-   *          a boolean if true emit only most likely cluster for each point
-   * @param threshold
-   *          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
-   * @param runSequential
-   *          execute sequentially if true
-   */
-  public static void clusterData(Configuration conf, Path input, Path stateIn, Path output, double alpha0,
-      int numModels, boolean emitMostLikely, double threshold, boolean runSequential) throws IOException,
-      InterruptedException, ClassNotFoundException {
-    ClusterClassifier.writePolicy(new DirichletClusteringPolicy(numModels, alpha0), stateIn);
-    ClusterClassificationDriver.run(conf, input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY),
-        threshold, emitMostLikely, runSequential);
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/UncommonDistributions.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/UncommonDistributions.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/UncommonDistributions.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/UncommonDistributions.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,271 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet;
-
-import org.apache.commons.math3.distribution.NormalDistribution;
-import org.apache.commons.math3.distribution.RealDistribution;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.common.RandomWrapper;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-
-@Deprecated
-public final class UncommonDistributions {
-  
-  public static final double SQRT2PI = Math.sqrt(2.0 * Math.PI);
-  
-  private static final RandomWrapper RANDOM = RandomUtils.getRandom();
-  
-  private UncommonDistributions() {
-  }
-  
-  // =============== start of BSD licensed code. See LICENSE.txt
-  /**
-   * Returns a double sampled according to this distribution. Uniformly fast for all k > 0. (Reference:
-   * Non-Uniform Random Variate Generation, Devroye http://cgm.cs.mcgill.ca/~luc/rnbookindex.html) Uses
-   * Cheng's rejection algorithm (GB) for k>=1, rejection from Weibull distribution for 0 < k < 1.
-   */
-  public static double rGamma(double k, double lambda) {
-    boolean accept = false;
-    if (k >= 1.0) {
-      // Cheng's algorithm
-      double b = k - Math.log(4.0);
-      double c = k + Math.sqrt(2.0 * k - 1.0);
-      double lam = Math.sqrt(2.0 * k - 1.0);
-      double cheng = 1.0 + Math.log(4.5);
-      double x;
-      do {
-        double u = RANDOM.nextDouble();
-        double v = RANDOM.nextDouble();
-        double y = 1.0 / lam * Math.log(v / (1.0 - v));
-        x = k * Math.exp(y);
-        double z = u * v * v;
-        double r = b + c * y - x;
-        if (r >= 4.5 * z - cheng || r >= Math.log(z)) {
-          accept = true;
-        }
-      } while (!accept);
-      return x / lambda;
-    } else {
-      // Weibull algorithm
-      double c = 1.0 / k;
-      double d = (1.0 - k) * Math.pow(k, k / (1.0 - k));
-      double x;
-      do {
-        double u = RANDOM.nextDouble();
-        double v = RANDOM.nextDouble();
-        double z = -Math.log(u);
-        double e = -Math.log(v);
-        x = Math.pow(z, c);
-        if (z + e >= d + x) {
-          accept = true;
-        }
-      } while (!accept);
-      return x / lambda;
-    }
-  }
-  
-  // ============= end of BSD licensed code
-  
-  /**
-   * Returns a random sample from a beta distribution with the given shapes
-   * 
-   * @param shape1
-   *          a double representing shape1
-   * @param shape2
-   *          a double representing shape2
-   * @return a Vector of samples
-   */
-  public static double rBeta(double shape1, double shape2) {
-    double gam1 = rGamma(shape1, 1.0);
-    double gam2 = rGamma(shape2, 1.0);
-    return gam1 / (gam1 + gam2);
-    
-  }
-  
-  /**
-   * Returns a vector of random samples from a beta distribution with the given shapes
-   * 
-   * @param k
-   *          the number of samples to return
-   * @param shape1
-   *          a double representing shape1
-   * @param shape2
-   *          a double representing shape2
-   * @return a Vector of samples
-   */
-  public static Vector rBeta(int k, double shape1, double shape2) {
-    // List<Double> params = new ArrayList<Double>(2);
-    // params.add(shape1);
-    // params.add(Math.max(0, shape2));
-    Vector result = new DenseVector(k);
-    for (int i = 0; i < k; i++) {
-      result.set(i, rBeta(shape1, shape2));
-    }
-    return result;
-  }
-  
-  /**
-   * Return a random sample from the chi-squared (chi^2) distribution with df degrees of freedom.
-   * 
-   * @return a double sample
-   */
-  public static double rChisq(double df) {
-    double result = 0.0;
-    for (int i = 0; i < df; i++) {
-      double sample = rNorm(0.0, 1.0);
-      result += sample * sample;
-    }
-    return result;
-  }
-  
-  /**
-   * Return a random value from a normal distribution with the given mean and standard deviation
-   * 
-   * @param mean
-   *          a double mean value
-   * @param sd
-   *          a double standard deviation
-   * @return a double sample
-   */
-  public static double rNorm(double mean, double sd) {
-    RealDistribution dist = new NormalDistribution(RANDOM.getRandomGenerator(),
-                                                   mean,
-                                                   sd,
-                                                   NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);
-    return dist.sample();
-  }
-  
-  /**
-   * Return the normal density function value for the sample x
-   * 
-   * pdf = 1/[sqrt(2*p)*s] * e^{-1/2*[(x-m)/s]^2}
-   * 
-   * @param x
-   *          a double sample value
-   * @param m
-   *          a double mean value
-   * @param s
-   *          a double standard deviation
-   * @return a double probability value
-   */
-  public static double dNorm(double x, double m, double s) {
-    double xms = (x - m) / s;
-    double ex = xms * xms / 2.0;
-    double exp = Math.exp(-ex);
-    return exp / (SQRT2PI * s);
-  }
-  
-  /** Returns one sample from a multinomial. */
-  public static int rMultinom(Vector probabilities) {
-    // our probability argument are not normalized.
-    double total = probabilities.zSum();
-    double nextDouble = RANDOM.nextDouble();
-    double p = nextDouble * total;
-    for (int i = 0; i < probabilities.size(); i++) {
-      double pi = probabilities.get(i);
-      if (p < pi) {
-        return i;
-      } else {
-        p -= pi;
-      }
-    }
-    // can't happen except for round-off error so we don't care what we return here
-    return 0;
-  }
-  
-  /**
-   * Returns a multinomial vector sampled from the given probabilities
-   * 
-   * rmultinom should be implemented as successive binomial sampling.
-   * 
-   * Keep a normalizing amount that starts with 1 (I call it total).
-   * 
-   * For each i k[i] = rbinom(p[i] / total, size); total -= p[i]; size -= k[i];
-   * 
-   * @param size
-   *          the size parameter of the binomial distribution
-   * @param probabilities
-   *          a Vector of probabilities
-   * @return a multinomial distribution Vector
-   */
-  public static Vector rMultinom(int size, Vector probabilities) {
-    // our probability argument may not be normalized.
-    double total = probabilities.zSum();
-    int cardinality = probabilities.size();
-    Vector result = new DenseVector(cardinality);
-    for (int i = 0; total > 0 && i < cardinality; i++) {
-      double p = probabilities.get(i);
-      int ki = rBinomial(size, p / total);
-      total -= p;
-      size -= ki;
-      result.set(i, ki);
-    }
-    return result;
-  }
-  
-  /**
-   * Returns an integer sampled according to this distribution. Takes time proportional to np + 1. (Reference:
-   * Non-Uniform Random Variate Generation, Devroye http://cgm.cs.mcgill.ca/~luc/rnbookindex.html) Second
-   * time-waiting algorithm.
-   */
-  public static int rBinomial(int n, double p) {
-    if (p >= 1.0) {
-      return n; // needed to avoid infinite loops and negative results
-    }
-    double q = -Math.log1p(-p);
-    double sum = 0.0;
-    int x = 0;
-    while (sum <= q) {
-      double u = RANDOM.nextDouble();
-      double e = -Math.log(u);
-      sum += e / (n - x);
-      x++;
-    }
-    if (x == 0) {
-      return 0;
-    }
-    return x - 1;
-  }
-  
-  /**
-   * Sample from a Dirichlet distribution, returning a vector of probabilities using a stick-breaking
-   * algorithm
-   * 
-   * @param totalCounts
-   *          an unnormalized count Vector
-   * @param alpha0
-   *          a double
-   * @return a Vector of probabilities
-   */
-  public static Vector rDirichlet(Vector totalCounts, double alpha0) {
-    Vector pi = totalCounts.like();
-    double total = totalCounts.zSum();
-    double remainder = 1.0;
-    for (int k = 0; k < pi.size(); k++) {
-      double countK = totalCounts.get(k);
-      total -= countK;
-      double betaK = rBeta(1.0 + countK, Math.max(0.0, alpha0 + total));
-      double piK = betaK * remainder;
-      pi.set(k, piK);
-      remainder -= piK;
-    }
-    return pi;
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/AbstractVectorModelDistribution.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/AbstractVectorModelDistribution.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/AbstractVectorModelDistribution.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/AbstractVectorModelDistribution.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,51 +0,0 @@
-/* Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet.models;
-
-import org.apache.mahout.clustering.ModelDistribution;
-import org.apache.mahout.math.VectorWritable;
-
-@Deprecated
-public abstract class AbstractVectorModelDistribution implements ModelDistribution<VectorWritable> {
-
-  // a prototype instance used for creating prior model distributions using like(). It
-  // should be of the class and cardinality desired for the particular application.
-  private VectorWritable modelPrototype;
-
-  protected AbstractVectorModelDistribution() {
-  }
-
-  protected AbstractVectorModelDistribution(VectorWritable modelPrototype) {
-    this.modelPrototype = modelPrototype;
-  }
-
-  /**
-   * @return the modelPrototype
-   */
-  public VectorWritable getModelPrototype() {
-    return modelPrototype;
-  }
-
-  /**
-   * @param modelPrototype
-   *          the modelPrototype to set
-   */
-  public void setModelPrototype(VectorWritable modelPrototype) {
-    this.modelPrototype = modelPrototype;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistanceMeasureClusterDistribution.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistanceMeasureClusterDistribution.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistanceMeasureClusterDistribution.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistanceMeasureClusterDistribution.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet.models;
-
-import org.apache.mahout.clustering.Model;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
-import org.apache.mahout.clustering.iterator.DistanceMeasureCluster;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-/**
- * An implementation of the ModelDistribution interface suitable for testing the
- * DirichletCluster algorithm. Models use a DistanceMeasure to calculate pdf
- * values.
- */
-@Deprecated
-public class DistanceMeasureClusterDistribution extends AbstractVectorModelDistribution {
-
-  private DistanceMeasure measure;
-
-  public DistanceMeasureClusterDistribution() {
-  }
-
-  public DistanceMeasureClusterDistribution(VectorWritable modelPrototype) {
-    super(modelPrototype);
-    this.measure = new ManhattanDistanceMeasure();
-  }
-
-  public DistanceMeasureClusterDistribution(VectorWritable modelPrototype, DistanceMeasure measure) {
-    super(modelPrototype);
-    this.measure = measure;
-  }
-
-  @Override
-  public Model<VectorWritable>[] sampleFromPrior(int howMany) {
-    Model<VectorWritable>[] result = new DistanceMeasureCluster[howMany];
-    Vector prototype = getModelPrototype().get().like();
-    for (int i = 0; i < prototype.size(); i++) {
-      prototype.setQuick(i, UncommonDistributions.rNorm(0, 1));
-    }
-    for (int i = 0; i < howMany; i++) {
-      result[i] = new DistanceMeasureCluster(prototype, i, measure);
-    }
-    return result;
-  }
-
-  @Override
-  public Model<VectorWritable>[] sampleFromPosterior(Model<VectorWritable>[] posterior) {
-    Model<VectorWritable>[] result = new DistanceMeasureCluster[posterior.length];
-    for (int i = 0; i < posterior.length; i++) {
-      result[i] = posterior[i].sampleFromPosterior();
-    }
-    return result;
-  }
-
-  public void setMeasure(DistanceMeasure measure) {
-    this.measure = measure;
-  }
-
-  public DistanceMeasure getMeasure() {
-    return measure;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistributionDescription.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistributionDescription.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistributionDescription.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/DistributionDescription.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet.models;
-
-import java.util.Iterator;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.mahout.clustering.ModelDistribution;
-import org.apache.mahout.common.ClassUtils;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-import com.google.common.base.Splitter;
-
-/**
- * Simply describes parameters needs to create a {@link org.apache.mahout.clustering.ModelDistribution}.
- */
-@Deprecated
-public final class DistributionDescription {
-  
-  private final String modelFactory;
-  private final String modelPrototype;
-  private final String distanceMeasure;
-  private final int prototypeSize;
-  
-  public DistributionDescription(String modelFactory,
-                                 String modelPrototype,
-                                 String distanceMeasure,
-                                 int prototypeSize) {
-    this.modelFactory = modelFactory;
-    this.modelPrototype = modelPrototype;
-    this.distanceMeasure = distanceMeasure;
-    this.prototypeSize = prototypeSize;
-  }
-  
-  public String getModelFactory() {
-    return modelFactory;
-  }
-  
-  public String getModelPrototype() {
-    return modelPrototype;
-  }
-  
-  public String getDistanceMeasure() {
-    return distanceMeasure;
-  }
-  
-  public int getPrototypeSize() {
-    return prototypeSize;
-  }
-  
-  /**
-   * Create an instance of AbstractVectorModelDistribution from the given command line arguments
-   */
-  public ModelDistribution<VectorWritable> createModelDistribution(Configuration conf) {
-    AbstractVectorModelDistribution modelDistribution =
-        ClassUtils.instantiateAs(modelFactory, AbstractVectorModelDistribution.class);
-
-    Vector prototype = ClassUtils.instantiateAs(modelPrototype,
-                                                Vector.class,
-                                                new Class<?>[] {int.class},
-                                                new Object[] {prototypeSize});
-      
-    modelDistribution.setModelPrototype(new VectorWritable(prototype));
-
-    if (modelDistribution instanceof DistanceMeasureClusterDistribution) {
-      DistanceMeasure measure = ClassUtils.instantiateAs(distanceMeasure, DistanceMeasure.class);
-      measure.configure(conf);
-      ((DistanceMeasureClusterDistribution) modelDistribution).setMeasure(measure);
-    }
-
-    return modelDistribution;
-  }
-  
-  @Override
-  public String toString() {
-    return modelFactory + ',' + modelPrototype + ',' + distanceMeasure + ',' + prototypeSize;
-  }
-  
-  public static DistributionDescription fromString(CharSequence s) {
-    Iterator<String> tokens = Splitter.on(',').split(s).iterator();
-    String modelFactory = tokens.next();
-    String modelPrototype = tokens.next();
-    String distanceMeasure = tokens.next();
-    int prototypeSize = Integer.parseInt(tokens.next());
-    return new DistributionDescription(modelFactory, modelPrototype, distanceMeasure, prototypeSize);
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianCluster.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianCluster.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianCluster.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianCluster.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet.models;
-
-import org.apache.mahout.clustering.AbstractCluster;
-import org.apache.mahout.clustering.Model;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.Vector.Element;
-import org.apache.mahout.math.VectorWritable;
-
-@Deprecated
-public class GaussianCluster extends AbstractCluster {
-  
-  public GaussianCluster() {}
-  
-  public GaussianCluster(Vector point, int id2) {
-    super(point, id2);
-  }
-  
-  public GaussianCluster(Vector center, Vector radius, int id) {
-    super(center, radius, id);
-  }
-  
-  @Override
-  public String getIdentifier() {
-    return "GC:" + getId();
-  }
-  
-  @Override
-  public Model<VectorWritable> sampleFromPosterior() {
-    return new GaussianCluster(getCenter(), getRadius(), getId());
-  }
-
-  @Override
-  protected void setRadius(Vector s2) {
-    super.setRadius(s2);
-    computeProd2piR();
-  }
-
-  // the value of the zProduct(S*2pi) term. Calculated below.
-  private double zProd2piR;
-  
-  /**
-   * Compute the product(r[i]*SQRT2PI) over all i. Note that the cluster Radius
-   * corresponds to the Stdev of a Gaussian and the Center to its Mean.
-   */
-  private void computeProd2piR() {
-    zProd2piR = 1.0;
-    for (Element radius : getRadius().nonZeroes()) {
-      zProd2piR *= radius.get() * UncommonDistributions.SQRT2PI;
-    }
-  }
-
-  @Override
-  public double pdf(VectorWritable vw) {
-    return Math.exp(-(sumXminusCdivRsquared(vw.get()) / 2)) / zProd2piR;
-  }
-  
-  /**
-   * @param x
-   *          a Vector
-   * @return the zSum(((x[i]-c[i])/r[i])^2) over all i
-   */
-  private double sumXminusCdivRsquared(Vector x) {
-    double result = 0;
-    for (Element radiusElem : getRadius().nonZeroes()) {
-      int index = radiusElem.index();
-      double quotient = (x.get(index) - getCenter().get(index))
-          / radiusElem.get();
-      result += quotient * quotient;
-    }
-    return result;
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianClusterDistribution.java mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianClusterDistribution.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianClusterDistribution.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/dirichlet/models/GaussianClusterDistribution.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,67 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet.models;
-
-import org.apache.mahout.clustering.Model;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-/**
- * An implementation of the ModelDistribution interface suitable for testing the DirichletCluster algorithm.
- * Uses a Normal Distribution to sample the prior model values. Model values have a vector standard deviation,
- * allowing assymetrical regions to be covered by a model.
- */
-@Deprecated
-public class GaussianClusterDistribution extends AbstractVectorModelDistribution {
-
-  public GaussianClusterDistribution() {
-  }
-
-  public GaussianClusterDistribution(VectorWritable modelPrototype) {
-    super(modelPrototype);
-  }
-
-  @Override
-  public Model<VectorWritable>[] sampleFromPrior(int howMany) {
-    Model<VectorWritable>[] result = new GaussianCluster[howMany];
-    for (int i = 0; i < howMany; i++) {
-      Vector prototype = getModelPrototype().get();
-      Vector mean = prototype.like();
-      for (int j = 0; j < prototype.size(); j++) {
-        mean.set(j, UncommonDistributions.rNorm(0, 1));
-      }
-      Vector sd = prototype.like();
-      for (int j = 0; j < prototype.size(); j++) {
-        sd.set(j, UncommonDistributions.rNorm(1, 1));
-      }
-      result[i] = new GaussianCluster(mean, sd, i);
-    }
-    return result;
-  }
-
-  @Override
-  public Model<VectorWritable>[] sampleFromPosterior(Model<VectorWritable>[] posterior) {
-    Model<VectorWritable>[] result = new GaussianCluster[posterior.length];
-    for (int i = 0; i < posterior.length; i++) {
-      result[i] = posterior[i].sampleFromPosterior();
-    }
-    return result;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansConfigKeys.java mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansConfigKeys.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansConfigKeys.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansConfigKeys.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,26 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.fuzzykmeans;
-
-public final class FuzzyKMeansConfigKeys {
-
-  private FuzzyKMeansConfigKeys() {}
-
-  public static final  String DISTANCE_MEASURE_KEY = "org.apache.mahout.clustering.kmeans.measure";
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/fuzzykmeans/FuzzyKMeansDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -110,7 +110,6 @@
         input,
         clusters,
         output,
-        measure,
         convergenceDelta,
         maxIterations,
         fuzziness,
@@ -124,32 +123,31 @@
   /**
    * Iterate over the input vectors to produce clusters and, if requested, use the
    * results of the final iteration to cluster the input vectors.
-   * 
+   *
    * @param input
    *          the directory pathname for input points
    * @param clustersIn
    *          the directory pathname for initial & computed clusters
    * @param output
-   *          the directory pathname for output points
+ *          the directory pathname for output points
    * @param convergenceDelta
-   *          the convergence delta value
+*          the convergence delta value
    * @param maxIterations
-   *          the maximum number of iterations
+*          the maximum number of iterations
    * @param m
-   *          the fuzzification factor, see
-   *          http://en.wikipedia.org/wiki/Data_clustering#Fuzzy_c-means_clustering
-   * @param runClustering 
-   *          true if points are to be clustered after iterations complete
+*          the fuzzification factor, see
+*          http://en.wikipedia.org/wiki/Data_clustering#Fuzzy_c-means_clustering
+   * @param runClustering
+*          true if points are to be clustered after iterations complete
    * @param emitMostLikely
-   *          a boolean if true emit only most likely cluster for each point
-   * @param threshold 
-   *          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
+*          a boolean if true emit only most likely cluster for each point
+   * @param threshold
+*          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
    * @param runSequential if true run in sequential execution mode
    */
   public static void run(Path input,
                          Path clustersIn,
                          Path output,
-                         DistanceMeasure measure,
                          double convergenceDelta,
                          int maxIterations,
                          float m,
@@ -162,7 +160,6 @@
                                      input,
                                      clustersIn,
                                      output,
-                                     measure,
                                      convergenceDelta,
                                      maxIterations,
                                      m,
@@ -172,7 +169,6 @@
       clusterData(conf, input,
                   clustersOut,
                   output,
-                  measure,
                   convergenceDelta,
                   m,
                   emitMostLikely,
@@ -189,27 +185,26 @@
    * @param clustersIn
    *          the directory pathname for initial & computed clusters
    * @param output
-   *          the directory pathname for output points
+ *          the directory pathname for output points
    * @param convergenceDelta
-   *          the convergence delta value
+*          the convergence delta value
    * @param maxIterations
-   *          the maximum number of iterations
+*          the maximum number of iterations
    * @param m
-   *          the fuzzification factor, see
-   *          http://en.wikipedia.org/wiki/Data_clustering#Fuzzy_c-means_clustering
-   * @param runClustering 
-   *          true if points are to be clustered after iterations complete
+*          the fuzzification factor, see
+*          http://en.wikipedia.org/wiki/Data_clustering#Fuzzy_c-means_clustering
+   * @param runClustering
+*          true if points are to be clustered after iterations complete
    * @param emitMostLikely
-   *          a boolean if true emit only most likely cluster for each point
-   * @param threshold 
-   *          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
+*          a boolean if true emit only most likely cluster for each point
+   * @param threshold
+*          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
    * @param runSequential if true run in sequential execution mode
    */
   public static void run(Configuration conf,
                          Path input,
                          Path clustersIn,
                          Path output,
-                         DistanceMeasure measure,
                          double convergenceDelta,
                          int maxIterations,
                          float m,
@@ -219,14 +214,13 @@
                          boolean runSequential)
     throws IOException, ClassNotFoundException, InterruptedException {
     Path clustersOut =
-        buildClusters(conf, input, clustersIn, output, measure, convergenceDelta, maxIterations, m, runSequential);
+        buildClusters(conf, input, clustersIn, output, convergenceDelta, maxIterations, m, runSequential);
     if (runClustering) {
       log.info("Clustering");
       clusterData(conf, 
                   input,
                   clustersOut,
                   output,
-                  measure,
                   convergenceDelta,
                   m,
                   emitMostLikely,
@@ -237,14 +231,13 @@
 
   /**
    * Iterate over the input vectors to produce cluster directories for each iteration
+   *
    * @param input
    *          the directory pathname for input points
    * @param clustersIn
    *          the file pathname for initial cluster centers
    * @param output
    *          the directory pathname for output points
-   * @param measure
-   *          the classname of the DistanceMeasure
    * @param convergenceDelta
    *          the convergence delta value
    * @param maxIterations
@@ -253,14 +246,13 @@
    *          the fuzzification factor, see
    *          http://en.wikipedia.org/wiki/Data_clustering#Fuzzy_c-means_clustering
    * @param runSequential if true run in sequential execution mode
-   * 
+   *
    * @return the Path of the final clusters directory
    */
   public static Path buildClusters(Configuration conf,
                                    Path input,
                                    Path clustersIn,
                                    Path output,
-                                   DistanceMeasure measure,
                                    double convergenceDelta,
                                    int maxIterations,
                                    float m,
@@ -293,28 +285,25 @@
 
   /**
    * Run the job using supplied arguments
-   * 
+   *
    * @param input
    *          the directory pathname for input points
    * @param clustersIn
    *          the directory pathname for input clusters
    * @param output
-   *          the directory pathname for output points
-   * @param measure
-   *          the classname of the DistanceMeasure
+ *          the directory pathname for output points
    * @param convergenceDelta
-   *          the convergence delta value
+*          the convergence delta value
    * @param emitMostLikely
-   *          a boolean if true emit only most likely cluster for each point
+*          a boolean if true emit only most likely cluster for each point
    * @param threshold
-   *          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
+*          a double threshold value emits all clusters having greater pdf (emitMostLikely = false)
    * @param runSequential if true run in sequential execution mode
    */
   public static void clusterData(Configuration conf,
                                  Path input,
                                  Path clustersIn,
                                  Path output,
-                                 DistanceMeasure measure,
                                  double convergenceDelta,
                                  float m,
                                  boolean emitMostLikely,
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/iterator/CIMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/iterator/CIMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/iterator/CIMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/iterator/CIMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.clustering.iterator;
 
 import java.io.IOException;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/iterator/ClusterIterator.java mahout/core/src/main/java/org/apache/mahout/clustering/iterator/ClusterIterator.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/iterator/ClusterIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/iterator/ClusterIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -42,7 +42,7 @@
 /**
  * This is a clustering iterator which works with a set of Vector data and a prior ClusterClassifier which has been
  * initialized with a set of models. Its implementation is algorithm-neutral and works for any iterative clustering
- * algorithm (currently k-means, fuzzy-k-means and Dirichlet) that processes all the input vectors in each iteration.
+ * algorithm (currently k-means and fuzzy-k-means) that processes all the input vectors in each iteration.
  * The cluster classifier is configured with a ClusteringPolicy to select the desired clustering algorithm.
  */
 public final class ClusterIterator {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/iterator/DirichletClusteringPolicy.java mahout/core/src/main/java/org/apache/mahout/clustering/iterator/DirichletClusteringPolicy.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/iterator/DirichletClusteringPolicy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/iterator/DirichletClusteringPolicy.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.clustering.iterator;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.mahout.clustering.classify.ClusterClassifier;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.SequentialAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-public class DirichletClusteringPolicy extends AbstractClusteringPolicy {
-  
-  public DirichletClusteringPolicy() {
-  }
-  
-  /**
-   * 
-   * @param k
-   *          The number of models to create from prior
-   * @param alpha0
-   *          The alpha_0 parameter to the Dirichlet Distribution.
-   */
-  public DirichletClusteringPolicy(int k, double alpha0) {
-    this.alpha0 = alpha0;
-    this.mixture = UncommonDistributions.rDirichlet(new DenseVector(k), alpha0);
-  }
-  
-  // The mixture is the Dirichlet distribution of the total Cluster counts over
-  // all iterations
-  private Vector mixture;
-  
-  // Alpha_0 primes the Dirichlet distribution
-  private double alpha0;
-
-  @Override
-  public Vector select(Vector probabilities) {
-    int rMultinom = UncommonDistributions.rMultinom(probabilities.times(mixture));
-    Vector weights = new SequentialAccessSparseVector(probabilities.size());
-    weights.set(rMultinom, 1.0);
-    return weights;
-  }
-  
-  // update the total counts and then the mixture
-
-  @Override
-  public void update(ClusterClassifier prior) {
-    Vector totalCounts = new DenseVector(prior.getModels().size());
-    for (int i = 0; i < prior.getModels().size(); i++) {
-      totalCounts.set(i, prior.getModels().get(i).getTotalObservations());
-    }
-    mixture = UncommonDistributions.rDirichlet(totalCounts, alpha0);
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    out.writeDouble(alpha0);
-    VectorWritable.writeVector(out, mixture);
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    this.alpha0 = in.readDouble();
-    this.mixture = VectorWritable.readVector(in);
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/iterator/MeanShiftClusteringPolicy.java mahout/core/src/main/java/org/apache/mahout/clustering/iterator/MeanShiftClusteringPolicy.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/iterator/MeanShiftClusteringPolicy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/iterator/MeanShiftClusteringPolicy.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,51 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.clustering.iterator;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-/**
- * This is a simple maximum likelihood clustering policy, suitable for k-means
- * clustering
- * 
- */
-public class MeanShiftClusteringPolicy extends AbstractClusteringPolicy {
-  
-  private double t1;
-  private double t2;
-  private double t3;
-  private double t4;
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    out.writeDouble(t1);
-    out.writeDouble(t2);
-    out.writeDouble(t3);
-    out.writeDouble(t4);
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    this.t1 = in.readDouble();
-    this.t2 = in.readDouble();
-    this.t3 = in.readDouble();
-    this.t4 = in.readDouble();
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/EigenSeedGenerator.java mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/EigenSeedGenerator.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/EigenSeedGenerator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/EigenSeedGenerator.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,123 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.kmeans;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.collect.Maps;
-import com.google.common.io.Closeables;
-
-/**
- * Given an Input Path containing a {@link org.apache.hadoop.io.SequenceFile}, select k vectors and write them to the
- * output file as a {@link org.apache.mahout.clustering.kmeans.Kluster} representing the initial centroid to use. The
- * selection criterion is the rows with max value in that respective column
- */
-public final class EigenSeedGenerator {
-
-  private static final Logger log = LoggerFactory.getLogger(EigenSeedGenerator.class);
-
-  public static final String K = "k";
-
-  private EigenSeedGenerator() {}
-
-  public static Path buildFromEigens(Configuration conf, Path input, Path output, int k, DistanceMeasure measure)
-      throws IOException {
-    // delete the output directory
-    FileSystem fs = FileSystem.get(output.toUri(), conf);
-    HadoopUtil.delete(conf, output);
-    Path outFile = new Path(output, "part-eigenSeed");
-    boolean newFile = fs.createNewFile(outFile);
-    if (newFile) {
-      Path inputPathPattern;
-
-      if (fs.getFileStatus(input).isDir()) {
-        inputPathPattern = new Path(input, "*");
-      } else {
-        inputPathPattern = input;
-      }
-
-      FileStatus[] inputFiles = fs.globStatus(inputPathPattern, PathFilters.logsCRCFilter());
-      SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, outFile, Text.class, ClusterWritable.class);
-      Map<Integer,Double> maxEigens = Maps.newHashMapWithExpectedSize(k); // store
-                                                                          // max
-                                                                          // value
-                                                                          // of
-                                                                          // each
-                                                                          // column
-      Map<Integer,Text> chosenTexts = Maps.newHashMapWithExpectedSize(k);
-      Map<Integer,ClusterWritable> chosenClusters = Maps.newHashMapWithExpectedSize(k);
-
-      for (FileStatus fileStatus : inputFiles) {
-        if (!fileStatus.isDir()) {
-          for (Pair<Writable,VectorWritable> record : new SequenceFileIterable<Writable,VectorWritable>(
-              fileStatus.getPath(), true, conf)) {
-            Writable key = record.getFirst();
-            VectorWritable value = record.getSecond();
-
-            for (Vector.Element e : value.get().nonZeroes()) {
-              int index = e.index();
-              double v = Math.abs(e.get());
-
-              if (!maxEigens.containsKey(index) || v > maxEigens.get(index)) {
-                maxEigens.put(index, v);
-                Text newText = new Text(key.toString());
-                chosenTexts.put(index, newText);
-                Kluster newCluster = new Kluster(value.get(), index, measure);
-                newCluster.observe(value.get(), 1);
-                ClusterWritable clusterWritable = new ClusterWritable();
-                clusterWritable.setValue(newCluster);
-                chosenClusters.put(index, clusterWritable);
-              }
-            }
-          }
-        }
-      }
-
-      try {
-        for (Integer key : maxEigens.keySet()) {
-          writer.append(chosenTexts.get(key), chosenClusters.get(key));
-        }
-        log.info("EigenSeedGenerator:: Wrote {} Klusters to {}", chosenTexts.size(), outFile);
-      } finally {
-        Closeables.close(writer, false);
-      }
-    }
-
-    return outFile;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansConfigKeys.java mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansConfigKeys.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansConfigKeys.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansConfigKeys.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,30 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.kmeans;
-
-/**
- * This class holds all config keys that are relevant to be used in the KMeans MapReduce configuration.
- * */
-public final class KMeansConfigKeys {
-
-  private KMeansConfigKeys() {}
-
-  /** Configuration key for distance measure to use. */
-  public static final String DISTANCE_MEASURE_KEY = "org.apache.mahout.clustering.kmeans.measure";
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/KMeansDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -100,7 +100,7 @@
     if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {
       clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));
     }
-    run(getConf(), input, clusters, output, measure, convergenceDelta, maxIterations, runClustering,
+    run(getConf(), input, clusters, output, convergenceDelta, maxIterations, runClustering,
         clusterClassificationThreshold, runSequential);
     return 0;
   }
@@ -108,15 +108,13 @@
   /**
    * Iterate over the input vectors to produce clusters and, if requested, use the results of the final iteration to
    * cluster the input vectors.
-   * 
+   *
    * @param input
    *          the directory pathname for input points
    * @param clustersIn
    *          the directory pathname for initial & computed clusters
    * @param output
    *          the directory pathname for output points
-   * @param measure
-   *          the DistanceMeasure to use
    * @param convergenceDelta
    *          the convergence delta value
    * @param maxIterations
@@ -129,36 +127,33 @@
    * @param runSequential
    *          if true execute sequential algorithm
    */
-  public static void run(Configuration conf, Path input, Path clustersIn, Path output, DistanceMeasure measure,
-      double convergenceDelta, int maxIterations, boolean runClustering, double clusterClassificationThreshold,
-      boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
+  public static void run(Configuration conf, Path input, Path clustersIn, Path output,
+    double convergenceDelta, int maxIterations, boolean runClustering, double clusterClassificationThreshold,
+    boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException {
     
     // iterate until the clusters converge
     String delta = Double.toString(convergenceDelta);
     if (log.isInfoEnabled()) {
-      log.info("Input: {} Clusters In: {} Out: {} Distance: {}", input, clustersIn, output,
-               measure.getClass().getName());
+      log.info("Input: {} Clusters In: {} Out: {}", input, clustersIn, output);
       log.info("convergence: {} max Iterations: {}", convergenceDelta, maxIterations);
     }
-    Path clustersOut = buildClusters(conf, input, clustersIn, output, measure, maxIterations, delta, runSequential);
+    Path clustersOut = buildClusters(conf, input, clustersIn, output, maxIterations, delta, runSequential);
     if (runClustering) {
       log.info("Clustering data");
-      clusterData(conf, input, clustersOut, output, measure, clusterClassificationThreshold, runSequential);
+      clusterData(conf, input, clustersOut, output, clusterClassificationThreshold, runSequential);
     }
   }
   
   /**
    * Iterate over the input vectors to produce clusters and, if requested, use the results of the final iteration to
    * cluster the input vectors.
-   * 
+   *
    * @param input
    *          the directory pathname for input points
    * @param clustersIn
    *          the directory pathname for initial & computed clusters
    * @param output
    *          the directory pathname for output points
-   * @param measure
-   *          the DistanceMeasure to use
    * @param convergenceDelta
    *          the convergence delta value
    * @param maxIterations
@@ -166,21 +161,22 @@
    * @param runClustering
    *          true if points are to be clustered after iterations are completed
    * @param clusterClassificationThreshold
-   *          Is a clustering strictness / outlier removal parrameter. Its value should be between 0 and 1. Vectors
+   *          Is a clustering strictness / outlier removal parameter. Its value should be between 0 and 1. Vectors
    *          having pdf below this value will not be clustered.
    * @param runSequential
    *          if true execute sequential algorithm
    */
-  public static void run(Path input, Path clustersIn, Path output, DistanceMeasure measure, double convergenceDelta,
-      int maxIterations, boolean runClustering, double clusterClassificationThreshold, boolean runSequential)
+  public static void run(Path input, Path clustersIn, Path output, double convergenceDelta,
+    int maxIterations, boolean runClustering, double clusterClassificationThreshold, boolean runSequential)
     throws IOException, InterruptedException, ClassNotFoundException {
-    run(new Configuration(), input, clustersIn, output, measure, convergenceDelta, maxIterations, runClustering,
+    run(new Configuration(), input, clustersIn, output, convergenceDelta, maxIterations, runClustering,
         clusterClassificationThreshold, runSequential);
   }
   
   /**
    * Iterate over the input vectors to produce cluster directories for each iteration
    * 
+   *
    * @param conf
    *          the Configuration to use
    * @param input
@@ -189,20 +185,18 @@
    *          the directory pathname for initial & computed clusters
    * @param output
    *          the directory pathname for output points
-   * @param measure
-   *          the classname of the DistanceMeasure
    * @param maxIterations
    *          the maximum number of iterations
    * @param delta
    *          the convergence delta value
    * @param runSequential
    *          if true execute sequential algorithm
-   * 
+   *
    * @return the Path of the final clusters directory
    */
   public static Path buildClusters(Configuration conf, Path input, Path clustersIn, Path output,
-      DistanceMeasure measure, int maxIterations, String delta, boolean runSequential) throws IOException,
-      InterruptedException, ClassNotFoundException {
+    int maxIterations, String delta, boolean runSequential) throws IOException,
+    InterruptedException, ClassNotFoundException {
     
     double convergenceDelta = Double.parseDouble(delta);
     List<Cluster> clusters = Lists.newArrayList();
@@ -227,28 +221,26 @@
   
   /**
    * Run the job using supplied arguments
-   * 
+   *
    * @param input
    *          the directory pathname for input points
    * @param clustersIn
    *          the directory pathname for input clusters
    * @param output
    *          the directory pathname for output points
-   * @param measure
-   *          the classname of the DistanceMeasure
    * @param clusterClassificationThreshold
-   *          Is a clustering strictness / outlier removal parrameter. Its value should be between 0 and 1. Vectors
+   *          Is a clustering strictness / outlier removal parameter. Its value should be between 0 and 1. Vectors
    *          having pdf below this value will not be clustered.
    * @param runSequential
    *          if true execute sequential algorithm
    */
-  public static void clusterData(Configuration conf, Path input, Path clustersIn, Path output, DistanceMeasure measure,
-      double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException,
-      ClassNotFoundException {
+  public static void clusterData(Configuration conf, Path input, Path clustersIn, Path output,
+    double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException,
+    ClassNotFoundException {
     
     if (log.isInfoEnabled()) {
       log.info("Running Clustering");
-      log.info("Input: {} Clusters In: {} Out: {} Distance: {}", input, clustersIn, output, measure);
+      log.info("Input: {} Clusters In: {} Out: {}", input, clustersIn, output);
     }
     ClusterClassifier.writePolicy(new KMeansClusteringPolicy(), clustersIn);
     ClusterClassificationDriver.run(conf, input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY),
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/RandomSeedGenerator.java mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/RandomSeedGenerator.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/RandomSeedGenerator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/kmeans/RandomSeedGenerator.java	2014-03-29 01:03:13.000000000 -0700
@@ -64,7 +64,7 @@
                                  int k,
                                  DistanceMeasure measure) throws IOException {
 
-    Preconditions.checkArgument(k > 0);
+    Preconditions.checkArgument(k > 0, "Must be: k > 0, but k = " + k);
     // delete the output directory
     FileSystem fs = FileSystem.get(output.toUri(), conf);
     HadoopUtil.delete(conf, output);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0Mapper.java mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0Mapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0Mapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0Mapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -34,21 +34,21 @@
  * Run ensemble learning via loading the {@link ModelTrainer} with two {@link TopicModel} instances:
  * one from the previous iteration, the other empty.  Inference is done on the first, and the
  * learning updates are stored in the second, and only emitted at cleanup().
- *
+ * <p/>
  * In terms of obvious performance improvements still available, the memory footprint in this
  * Mapper could be dropped by half if we accumulated model updates onto the model we're using
  * for inference, which might also speed up convergence, as we'd be able to take advantage of
  * learning <em>during</em> iteration, not just after each one is done.  Most likely we don't
  * really need to accumulate double values in the model either, floats would most likely be
  * sufficient.  Between these two, we could squeeze another factor of 4 in memory efficiency.
- *
+ * <p/>
  * In terms of CPU, we're re-learning the p(topic|doc) distribution on every iteration, starting
  * from scratch.  This is usually only 10 fixed-point iterations per doc, but that's 10x more than
  * only 1.  To avoid having to do this, we would need to do a map-side join of the unchanging
  * corpus with the continually-improving p(topic|doc) matrix, and then emit multiple outputs
  * from the mappers to make sure we can do the reduce model averaging as well.  Tricky, but
  * possibly worth it.
- *
+ * <p/>
  * {@link ModelTrainer} already takes advantage (in maybe the not-nice way) of multi-core
  * availability by doing multithreaded learning, see that class for details.
  */
@@ -58,17 +58,19 @@
   private static final Logger log = LoggerFactory.getLogger(CachingCVB0Mapper.class);
 
   private ModelTrainer modelTrainer;
+  private TopicModel readModel;
+  private TopicModel writeModel;
   private int maxIters;
   private int numTopics;
 
   protected ModelTrainer getModelTrainer() {
     return modelTrainer;
   }
-  
+
   protected int getMaxIters() {
     return maxIters;
   }
-  
+
   protected int getNumTopics() {
     return numTopics;
   }
@@ -88,7 +90,6 @@
     float modelWeight = conf.getFloat(CVB0Driver.MODEL_WEIGHT, 1.0f);
 
     log.info("Initializing read model");
-    TopicModel readModel;
     Path[] modelPaths = CVB0Driver.getModelPaths(conf);
     if (modelPaths != null && modelPaths.length > 0) {
       readModel = new TopicModel(conf, eta, alpha, null, numUpdateThreads, modelWeight, modelPaths);
@@ -99,7 +100,7 @@
     }
 
     log.info("Initializing write model");
-    TopicModel writeModel = modelWeight == 1
+    writeModel = modelWeight == 1
         ? new TopicModel(numTopics, numTerms, eta, alpha, null, numUpdateThreads)
         : readModel;
 
@@ -110,7 +111,7 @@
 
   @Override
   public void map(IntWritable docId, VectorWritable document, Context context)
-    throws IOException, InterruptedException {
+      throws IOException, InterruptedException {
     /* where to get docTopics? */
     Vector topicVector = new DenseVector(numTopics).assign(1.0 / numTopics);
     modelTrainer.train(document.get(), topicVector, true, maxIters);
@@ -122,9 +123,11 @@
     modelTrainer.stop();
 
     log.info("Writing model");
-    TopicModel model = modelTrainer.getReadModel();
-    for (MatrixSlice topic : model) {
+    TopicModel readFrom = modelTrainer.getReadModel();
+    for (MatrixSlice topic : readFrom) {
       context.write(new IntWritable(topic.index()), new VectorWritable(topic.vector()));
     }
+    readModel.stop();
+    writeModel.stop();
   }
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0PerplexityMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0PerplexityMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0PerplexityMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/CachingCVB0PerplexityMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -44,6 +44,7 @@
   private static final Logger log = LoggerFactory.getLogger(CachingCVB0PerplexityMapper.class);
 
   private ModelTrainer modelTrainer;
+  private TopicModel readModel;
   private int maxIters;
   private int numTopics;
   private float testFraction;
@@ -71,7 +72,6 @@
     testFraction = conf.getFloat(CVB0Driver.TEST_SET_FRACTION, 0.1f);
 
     log.info("Initializing read model");
-    TopicModel readModel;
     Path[] modelPaths = CVB0Driver.getModelPaths(conf);
     if (modelPaths != null && modelPaths.length > 0) {
       readModel = new TopicModel(conf, eta, alpha, null, numUpdateThreads, modelWeight, modelPaths);
@@ -90,6 +90,7 @@
 
   @Override
   protected void cleanup(Context context) throws IOException, InterruptedException {
+    readModel.stop();
     MemoryUtil.stopMemoryLogger();
   }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/TopicModel.java mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/TopicModel.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/TopicModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/lda/cvb/TopicModel.java	2014-03-29 01:03:13.000000000 -0700
@@ -153,7 +153,15 @@
     return v;
   }
 
-  private void initializeThreadPool() {
+  private synchronized void initializeThreadPool() {
+    if (threadPool != null) {
+      threadPool.shutdown();
+      try {
+        threadPool.awaitTermination(100, TimeUnit.SECONDS);
+      } catch (InterruptedException e) {
+        log.error("Could not terminate all threads for TopicModel in time.", e);
+      }
+    }
     threadPool = new ThreadPoolExecutor(numThreads, numThreads, 0, TimeUnit.SECONDS,
                                                            new ArrayBlockingQueue<Runnable>(numThreads * 10));
     threadPool.allowCoreThreadTimeOut(false);
@@ -242,7 +250,7 @@
     return sampler.sample(topicTermCounts.viewRow(topic));
   }
 
-  public void reset() {
+  public synchronized void reset() {
     for (int x = 0; x < numTopics; x++) {
       topicTermCounts.assignRow(x, new SequentialAccessSparseVector(numTerms));
     }
@@ -252,7 +260,7 @@
     }
   }
 
-  public void stop() {
+  public synchronized void stop() {
     for (Updater updater : updaters) {
       updater.shutdown();
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopy.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopy.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopy.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,183 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.mahout.clustering.kmeans.Kluster;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.list.IntArrayList;
-
-/**
- * This class models a canopy as a center point, the number of points that are
- * contained within it according to the application of some distance metric, and
- * a point total which is the sum of all the points and is used to compute the
- * centroid when needed.
- */
-@Deprecated
-public class MeanShiftCanopy extends Kluster {
-
-  // TODO: this is still problematic from a scalability perspective, but how
-  // else to encode membership?
-  private IntArrayList boundPoints = new IntArrayList();
-
-  private int mass = 0;
-
-  public int getMass() {
-    return mass;
-  }
-
-  void setMass(int num) {
-    mass = num;
-  }
-
-  /**
-   * Used for Writable
-   */
-  public MeanShiftCanopy() {
-  }
-
-  /**
-   * Create a new Canopy containing the given point
-   * 
-   * @param point
-   *          a Vector
-   * @param id
-   *          an int canopy id
-   * @param measure
-   *          a DistanceMeasure
-   */
-  public MeanShiftCanopy(Vector point, int id, DistanceMeasure measure) {
-    super(point, id, measure);
-    boundPoints.add(id);
-    mass = 1;
-  }
-
-  /**
-   * Create an initial Canopy, retaining the original type of the given point
-   * (e.g. NamedVector)
-   * 
-   * @param point
-   *          a Vector
-   * @param id
-   *          an int
-   * @param measure
-   *          a DistanceMeasure
-   * @return a MeanShiftCanopy
-   */
-  public static MeanShiftCanopy initialCanopy(Vector point, int id,
-      DistanceMeasure measure) {
-    MeanShiftCanopy result = new MeanShiftCanopy(point, id, measure);
-    // overwrite center so original point type is retained
-    result.setCenter(point);
-    return result;
-  }
-
-  public IntArrayList getBoundPoints() {
-    return boundPoints;
-  }
-
-  /**
-   * The receiver overlaps the given canopy. Add my bound points to it.
-   * 
-   * @param canopy
-   *          an existing MeanShiftCanopy
-   * @param accumulateBoundPoints
-   *          true to accumulate bound points from the canopy
-   */
-  void merge(MeanShiftCanopy canopy, boolean accumulateBoundPoints) {
-    if (accumulateBoundPoints) {
-      boundPoints.addAllOf(canopy.boundPoints);
-    }
-    mass += canopy.mass;
-  }
-
-  /**
-   * The receiver touches the given canopy. Add respective centers with the
-   * given weights.
-   * 
-   * @param canopy
-   *          an existing MeanShiftCanopy
-   * @param weight
-   *          double weight of the touching
-   */
-  void touch(MeanShiftCanopy canopy, double weight) {
-    canopy.observe(getCenter(), weight * mass);
-    observe(canopy.getCenter(), weight * canopy.mass);
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    this.mass = in.readInt();
-    int numpoints = in.readInt();
-    this.boundPoints = new IntArrayList();
-    for (int i = 0; i < numpoints; i++) {
-      this.boundPoints.add(in.readInt());
-    }
-    this.mass = boundPoints.size();
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    out.writeInt(mass);
-    out.writeInt(boundPoints.size());
-    for (int v : boundPoints.elements()) {
-      out.writeInt(v);
-    }
-  }
-
-  public MeanShiftCanopy shallowCopy() {
-    MeanShiftCanopy result = new MeanShiftCanopy();
-    result.setMeasure(this.getMeasure());
-    result.setId(this.getId());
-    result.setCenter(this.getCenter());
-    result.setRadius(this.getRadius());
-    result.setNumObservations(this.getNumObservations());
-    result.setBoundPoints(boundPoints);
-    result.setMass(mass);
-    return result;
-  }
-
-  @Override
-  public String asFormatString() {
-    return toString();
-  }
-
-  public void setBoundPoints(IntArrayList boundPoints) {
-    this.boundPoints = boundPoints;
-  }
-
-  @Override
-  public String getIdentifier() {
-    return (isConverged() ? "MSV-" : "MSC-") + getId();
-  }
-
-  @Override
-  public double pdf(VectorWritable vw) {
-    // MSCanopy membership is explicit via membership in boundPoints. Can't
-    // compute pdf for Arbitrary point
-    throw new UnsupportedOperationException();
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
-
-import com.google.common.collect.Lists;
-
-@Deprecated
-public class MeanShiftCanopyClusterMapper
-  extends Mapper<WritableComparable<?>, ClusterWritable, IntWritable, WeightedVectorWritable> {
-
-  private List<MeanShiftCanopy> canopies;
-
-  @Override
-  protected void map(WritableComparable<?> key, ClusterWritable clusterWritable, Context context)
-    throws IOException, InterruptedException {
-    // canopies use canopyIds assigned when input vectors are processed as vectorIds too
-    MeanShiftCanopy canopy = (MeanShiftCanopy)clusterWritable.getValue();
-    int vectorId = canopy.getId();
-    for (MeanShiftCanopy msc : canopies) {
-      for (int containedId : msc.getBoundPoints().toList()) {
-        if (vectorId == containedId) {
-          context.write(new IntWritable(msc.getId()),
-                         new WeightedVectorWritable(1, canopy.getCenter()));
-        }
-      }
-    }
-  }
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    canopies = getCanopies(context.getConfiguration());
-  }
-
-  public static List<MeanShiftCanopy> getCanopies(Configuration conf) {
-    String statePath = conf.get(MeanShiftCanopyDriver.STATE_IN_KEY);
-    List<MeanShiftCanopy> canopies = Lists.newArrayList();
-    Path path = new Path(statePath);
-    for (ClusterWritable clusterWritable 
-         : new SequenceFileDirValueIterable<ClusterWritable>(path, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {
-      MeanShiftCanopy canopy = (MeanShiftCanopy)clusterWritable.getValue();
-      canopies.add(canopy);
-    }
-    return canopies;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterer.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyClusterer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,217 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.util.Collection;
-import java.util.List;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.common.ClassUtils;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.math.Vector;
-
-@Deprecated
-public class MeanShiftCanopyClusterer {
-
-  private final double convergenceDelta;
-
-  // the T1 distance threshold
-  private final double t1;
-
-  // the T2 distance threshold
-  private final double t2;
-
-  // the distance measure
-  private final DistanceMeasure measure;
-
-  private final IKernelProfile kernelProfile;
-
-  // if true accumulate clusters during merge so clusters can be produced later
-  private final boolean runClustering;
-
-  public MeanShiftCanopyClusterer(Configuration configuration) {
-    measure = ClassUtils.instantiateAs(configuration.get(MeanShiftCanopyConfigKeys.DISTANCE_MEASURE_KEY), 
-                                       DistanceMeasure.class);
-    measure.configure(configuration);
-    runClustering = configuration.getBoolean(MeanShiftCanopyConfigKeys.CLUSTER_POINTS_KEY, true);
-    kernelProfile = ClassUtils.instantiateAs(configuration.get(MeanShiftCanopyConfigKeys.KERNEL_PROFILE_KEY),
-                                             IKernelProfile.class);
-    // nextCanopyId = 0; // never read?
-    t1 = Double
-        .parseDouble(configuration.get(MeanShiftCanopyConfigKeys.T1_KEY));
-    t2 = Double
-        .parseDouble(configuration.get(MeanShiftCanopyConfigKeys.T2_KEY));
-    convergenceDelta = Double.parseDouble(configuration
-        .get(MeanShiftCanopyConfigKeys.CLUSTER_CONVERGENCE_KEY));
-  }
-
-  public MeanShiftCanopyClusterer(DistanceMeasure aMeasure,
-      IKernelProfile aKernelProfileDerivative, double aT1, double aT2,
-      double aDelta, boolean runClustering) {
-    // nextCanopyId = 100; // so canopyIds will sort properly // never read?
-    measure = aMeasure;
-    t1 = aT1;
-    t2 = aT2;
-    convergenceDelta = aDelta;
-    kernelProfile = aKernelProfileDerivative;
-    this.runClustering = runClustering;
-  }
-
-  public double getT1() {
-    return t1;
-  }
-
-  public double getT2() {
-    return t2;
-  }
-
-  /**
-   * Merge the given canopy into the canopies list. If it touches any existing
-   * canopy (norm<T1) then add the center of each to the other. If it covers any
-   * other canopies (norm<T2), then merge the given canopy with the closest
-   * covering canopy. If the given canopy does not cover any other canopies, add
-   * it to the canopies list.
-   * 
-   * @param aCanopy
-   *          a MeanShiftCanopy to be merged
-   * @param canopies
-   *          the List<Canopy> to be appended
-   */
-  public void mergeCanopy(MeanShiftCanopy aCanopy,
-      Collection<MeanShiftCanopy> canopies) {
-    MeanShiftCanopy closestCoveringCanopy = null;
-    double closestNorm = Double.MAX_VALUE;
-    for (MeanShiftCanopy canopy : canopies) {
-      double norm = measure.distance(canopy.getCenter(), aCanopy.getCenter());
-      double weight = kernelProfile.calculateDerivativeValue(norm, t1);
-      if (weight > 0.0) {
-        aCanopy.touch(canopy, weight);
-      }
-      if (norm < t2 && (closestCoveringCanopy == null || norm < closestNorm)) {
-        closestNorm = norm;
-        closestCoveringCanopy = canopy;
-      }
-    }
-    if (closestCoveringCanopy == null) {
-      canopies.add(aCanopy);
-    } else {
-      closestCoveringCanopy.merge(aCanopy, runClustering);
-    }
-  }
-
-  /**
-   * Shift the center to the new centroid of the cluster
-   * 
-   * @param canopy
-   *          the canopy to shift.
-   * @return if the cluster is converged
-   */
-  public boolean shiftToMean(MeanShiftCanopy canopy) {
-    canopy.observe(canopy.getCenter(), canopy.getMass());
-    canopy.computeConvergence(measure, convergenceDelta);
-    canopy.computeParameters();
-    return canopy.isConverged();
-  }
-
-  /**
-   * Return if the point is covered by this canopy
-   * 
-   * @param canopy
-   *          a canopy.
-   * @param point
-   *          a Vector point
-   * @return if the point is covered
-   */
-  boolean covers(MeanShiftCanopy canopy, Vector point) {
-    return measure.distance(canopy.getCenter(), point) < t1;
-  }
-
-  /**
-   * Return if the point is closely covered by the canopy
-   * 
-   * @param canopy
-   *          a canopy.
-   * @param point
-   *          a Vector point
-   * @return if the point is covered
-   */
-  public boolean closelyBound(MeanShiftCanopy canopy, Vector point) {
-    return measure.distance(canopy.getCenter(), point) < t2;
-  }
-
-  /**
-   * This is the reference mean-shift implementation. Given its inputs it
-   * iterates over the points and clusters until their centers converge or until
-   * the maximum number of iterations is exceeded.
-   * 
-   * @param points
-   *          the input List<Vector> of points
-   * @param measure
-   *          the DistanceMeasure to use
-   * @param numIter
-   *          the maximum number of iterations
-   */
-  public static List<MeanShiftCanopy> clusterPoints(Iterable<Vector> points,
-      DistanceMeasure measure, IKernelProfile aKernelProfileDerivative,
-      double convergenceThreshold, double t1, double t2, int numIter) {
-    MeanShiftCanopyClusterer clusterer = new MeanShiftCanopyClusterer(measure,
-        aKernelProfileDerivative, t1, t2, convergenceThreshold, true);
-    int nextCanopyId = 0;
-
-    List<MeanShiftCanopy> canopies = Lists.newArrayList();
-    for (Vector point : points) {
-      clusterer.mergeCanopy(
-          new MeanShiftCanopy(point, nextCanopyId++, measure), canopies);
-    }
-    List<MeanShiftCanopy> newCanopies = canopies;
-    boolean[] converged = { false };
-    for (int iter = 0; !converged[0] && iter < numIter; iter++) {
-      newCanopies = clusterer.iterate(newCanopies, converged);
-    }
-    return newCanopies;
-  }
-
-  protected List<MeanShiftCanopy> iterate(Iterable<MeanShiftCanopy> canopies,
-      boolean[] converged) {
-    converged[0] = true;
-    List<MeanShiftCanopy> migratedCanopies = Lists.newArrayList();
-    for (MeanShiftCanopy canopy : canopies) {
-      converged[0] = shiftToMean(canopy) && converged[0];
-      mergeCanopy(canopy, migratedCanopies);
-    }
-    return migratedCanopies;
-  }
-
-  protected static MeanShiftCanopy findCoveringCanopy(MeanShiftCanopy canopy,
-      Iterable<MeanShiftCanopy> clusters) {
-    // canopies use canopyIds assigned when input vectors are processed as
-    // vectorIds too
-    int vectorId = canopy.getId();
-    for (MeanShiftCanopy msc : clusters) {
-      for (int containedId : msc.getBoundPoints().toList()) {
-        if (vectorId == containedId) {
-          return msc;
-        }
-      }
-    }
-    return null;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyConfigKeys.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyConfigKeys.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyConfigKeys.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyConfigKeys.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-@Deprecated
-public final class MeanShiftCanopyConfigKeys {
-
-  private MeanShiftCanopyConfigKeys() {}
-
-  // keys used by Driver, Mapper, Combiner & Reducer
-  public static final String DISTANCE_MEASURE_KEY = "org.apache.mahout.clustering.canopy.measure";
-  public static final String KERNEL_PROFILE_KEY = "org.apache.mahout.clustering.canopy.kernelprofile";
-  public static final String T1_KEY = "org.apache.mahout.clustering.canopy.t1";
-  public static final String T2_KEY = "org.apache.mahout.clustering.canopy.t2";
-  public static final String CONTROL_PATH_KEY = "org.apache.mahout.clustering.control.path";
-  public static final String CLUSTER_CONVERGENCE_KEY = "org.apache.mahout.clustering.canopy.convergence";
-  public static final String CLUSTER_POINTS_KEY = "org.apache.mahout.clustering.meanshift.clusterPointsKey";
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyCreatorMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyCreatorMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyCreatorMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyCreatorMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,69 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.io.IOException;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.clustering.kmeans.KMeansConfigKeys;
-import org.apache.mahout.common.ClassUtils;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.math.VectorWritable;
-
-import com.google.common.base.Preconditions;
-
-@Deprecated
-public class MeanShiftCanopyCreatorMapper extends Mapper<WritableComparable<?>, VectorWritable, Text, ClusterWritable> {
-
-  private static final Pattern UNDERSCORE_PATTERN = Pattern.compile("_");
-
-  private static int nextCanopyId = -1;
-
-  private DistanceMeasure measure;
-
-  @Override
-  protected void map(WritableComparable<?> key, VectorWritable point, Context context)
-    throws IOException, InterruptedException {
-    MeanShiftCanopy canopy = MeanShiftCanopy.initialCanopy(point.get(), nextCanopyId++, measure);
-    ClusterWritable clusterWritable = new ClusterWritable();
-    clusterWritable.setValue(canopy);
-    context.write(new Text(key.toString()), clusterWritable);
-  }
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    String measureClass = context.getConfiguration().get(KMeansConfigKeys.DISTANCE_MEASURE_KEY);
-    measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
-
-    if (nextCanopyId == -1) {
-      String taskId = context.getConfiguration().get("mapred.task.id");
-      String[] parts = UNDERSCORE_PATTERN.split(taskId);
-      Preconditions.checkArgument(parts.length == 6
-          && "attempt".equals(parts[0])
-          && ("m".equals(parts[3]) || "r".equals(parts[3])),
-          "TaskAttemptId string: %d is not properly formed", taskId);
-      nextCanopyId = ((1 << 31) / 50000) * Integer.parseInt(parts[4]);
-      //each mapper has 42,949 ids to give.
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyDriver.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,527 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.AbstractCluster;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.clustering.kmeans.KMeansConfigKeys;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.ClassUtils;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
-import org.apache.mahout.math.VectorWritable;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.collect.Lists;
-import com.google.common.io.Closeables;
-
-/**
- * This class implements the driver for Mean Shift Canopy clustering
- * 
- */
-@Deprecated
-public class MeanShiftCanopyDriver extends AbstractJob {
-
-  public static final String MAPRED_REDUCE_TASKS = "mapred.reduce.tasks";
-
-  private static final Logger log = LoggerFactory
-      .getLogger(MeanShiftCanopyDriver.class);
-
-  public static final String INPUT_IS_CANOPIES_OPTION = "inputIsCanopies";
-
-  public static final String STATE_IN_KEY = "org.apache.mahout.clustering.meanshift.stateInKey";
-
-  private static final String CONTROL_CONVERGED = "control/converged";
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new Configuration(), new MeanShiftCanopyDriver(), args);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    addInputOption();
-    addOutputOption();
-    addOption(DefaultOptionCreator.convergenceOption().create());
-    addOption(DefaultOptionCreator.maxIterationsOption().create());
-    addOption(DefaultOptionCreator.overwriteOption().create());
-    addOption(DefaultOptionCreator.inputIsCanopiesOption().create());
-    addOption(DefaultOptionCreator.distanceMeasureOption().create());
-    addOption(DefaultOptionCreator.kernelProfileOption().create());
-    addOption(DefaultOptionCreator.t1Option().create());
-    addOption(DefaultOptionCreator.t2Option().create());
-    addOption(DefaultOptionCreator.clusteringOption().create());
-    addOption(DefaultOptionCreator.methodOption().create());
-
-    if (parseArguments(args) == null) {
-      return -1;
-    }
-
-    Path input = getInputPath();
-    Path output = getOutputPath();
-    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(getConf(), output);
-    }
-    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
-    String kernelProfileClass = getOption(DefaultOptionCreator.KERNEL_PROFILE_OPTION);
-    double t1 = Double.parseDouble(getOption(DefaultOptionCreator.T1_OPTION));
-    double t2 = Double.parseDouble(getOption(DefaultOptionCreator.T2_OPTION));
-    boolean runClustering = hasOption(DefaultOptionCreator.CLUSTERING_OPTION);
-    double convergenceDelta = Double
-        .parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
-    int maxIterations = Integer
-        .parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
-    boolean inputIsCanopies = hasOption(INPUT_IS_CANOPIES_OPTION);
-    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION)
-        .equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);
-    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
-    IKernelProfile kernelProfile = ClassUtils.instantiateAs(kernelProfileClass, IKernelProfile.class);
-    run(getConf(), input, output, measure, kernelProfile, t1, t2,
-        convergenceDelta, maxIterations, inputIsCanopies, runClustering,
-        runSequential);
-
-    return 0;
-  }
-
-  /**
-   * Run the job where the input format can be either Vectors or Canopies. If
-   * requested, cluster the input data using the computed Canopies
-   * 
-   * @param conf
-   *          the Configuration to use
-   * @param input
-   *          the input pathname String
-   * @param output
-   *          the output pathname String
-   * @param measure
-   *          the DistanceMeasure
-   * @param kernelProfile
-   *          the IKernelProfile
-   * @param t1
-   *          the T1 distance threshold
-   * @param t2
-   *          the T2 distance threshold
-   * @param convergenceDelta
-   *          the double convergence criteria
-   * @param maxIterations
-   *          an int number of iterations
-   * @param inputIsCanopies
-   *          true if the input path already contains MeanShiftCanopies and does
-   *          not need to be converted from Vectors
-   * @param runClustering
-   *          true if the input points are to be clustered once the iterations
-   *          complete
-   * @param runSequential
-   *          if true run in sequential execution mode
-   */
-  public static void run(Configuration conf, Path input, Path output,
-      DistanceMeasure measure, IKernelProfile kernelProfile, double t1,
-      double t2, double convergenceDelta, int maxIterations,
-      boolean inputIsCanopies, boolean runClustering, boolean runSequential)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    Path clustersIn = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);
-    if (inputIsCanopies) {
-      clustersIn = input;
-    } else {
-      createCanopyFromVectors(conf, input, clustersIn, measure, runSequential);
-    }
-
-    Path clustersOut = buildClusters(conf, clustersIn, output, measure,
-        kernelProfile, t1, t2, convergenceDelta, maxIterations, runSequential,
-        runClustering);
-    if (runClustering) {
-      clusterData(conf, inputIsCanopies ? input : new Path(output, Cluster.INITIAL_CLUSTERS_DIR), clustersOut,
-          new Path(output, Cluster.CLUSTERED_POINTS_DIR), runSequential);
-    }
-  }
-
-  /**
-   * Convert input vectors to MeanShiftCanopies for further processing
-   */
-  public static void createCanopyFromVectors(Configuration conf, Path input,
-      Path output, DistanceMeasure measure, boolean runSequential)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    if (runSequential) {
-      createCanopyFromVectorsSeq(input, output, measure);
-    } else {
-      createCanopyFromVectorsMR(conf, input, output, measure);
-    }
-  }
-
-  /**
-   * Convert vectors to MeanShiftCanopies sequentially
-   * 
-   * @param input
-   *          the Path to the input VectorWritable data
-   * @param output
-   *          the Path to the initial clusters directory
-   * @param measure
-   *          the DistanceMeasure
-   */
-  private static void createCanopyFromVectorsSeq(Path input, Path output,
-      DistanceMeasure measure) throws IOException {
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    FileStatus[] status = fs.listStatus(input, PathFilters.logsCRCFilter());
-    int part = 0;
-    int id = 0;
-    for (FileStatus s : status) {
-      SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(
-          output, "part-m-" + part++), Text.class, ClusterWritable.class);
-      try {
-        for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(s.getPath(), conf)) {
-          MeanShiftCanopy initialCanopy = MeanShiftCanopy.initialCanopy(value.get(), id++, measure);
-          ClusterWritable clusterWritable = new ClusterWritable();
-          clusterWritable.setValue(initialCanopy);
-          writer.append(new Text(), clusterWritable);
-        }
-      } finally {
-        Closeables.close(writer, false);
-      }
-    }
-  }
-
-  /**
-   * Convert vectors to MeanShiftCanopies using Hadoop
-   */
-  private static void createCanopyFromVectorsMR(Configuration conf, Path input,
-      Path output, DistanceMeasure measure) throws IOException,
-      InterruptedException, ClassNotFoundException {
-    conf.set(KMeansConfigKeys.DISTANCE_MEASURE_KEY, measure.getClass()
-        .getName());
-    Job job = new Job(conf);
-    job.setJarByClass(MeanShiftCanopyDriver.class);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(ClusterWritable.class);
-    job.setMapperClass(MeanShiftCanopyCreatorMapper.class);
-    job.setNumReduceTasks(0);
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-
-    FileInputFormat.setInputPaths(job, input);
-    FileOutputFormat.setOutputPath(job, output);
-
-    if (!job.waitForCompletion(true)) {
-      throw new InterruptedException(
-          "Mean Shift createCanopyFromVectorsMR failed on input " + input);
-    }
-  }
-
-  /**
-   * Iterate over the input clusters to produce the next cluster directories for
-   * each iteration
-   * 
-   * @param conf
-   *          the Configuration to use
-   * @param clustersIn
-   *          the input directory Path
-   * @param output
-   *          the output Path
-   * @param measure
-   *          the DistanceMeasure
-   * @param kernelProfile
-   *          the IKernelProfile
-   * @param t1
-   *          the T1 distance threshold
-   * @param t2
-   *          the T2 distance threshold
-   * @param convergenceDelta
-   *          the double convergence criteria
-   * @param maxIterations
-   *          an int number of iterations
-   * @param runSequential
-   *          if true run in sequential execution mode
-   * @param runClustering
-   *          if true accumulate merged clusters for subsequent clustering step
-   */
-  public static Path buildClusters(Configuration conf, Path clustersIn,
-      Path output, DistanceMeasure measure, IKernelProfile kernelProfile,
-      double t1, double t2, double convergenceDelta, int maxIterations,
-      boolean runSequential, boolean runClustering) throws IOException,
-      InterruptedException, ClassNotFoundException {
-    if (runSequential) {
-      return buildClustersSeq(clustersIn, output, measure, kernelProfile, t1,
-          t2, convergenceDelta, maxIterations, runClustering);
-    } else {
-      return buildClustersMR(conf, clustersIn, output, measure, kernelProfile,
-          t1, t2, convergenceDelta, maxIterations, runClustering);
-    }
-  }
-
-  /**
-   * Build new clusters sequentially
-   * 
-   */
-  private static Path buildClustersSeq(Path clustersIn, Path output,
-      DistanceMeasure measure, IKernelProfile aKernelProfile, double t1,
-      double t2, double convergenceDelta, int maxIterations,
-      boolean runClustering) throws IOException {
-    MeanShiftCanopyClusterer clusterer = new MeanShiftCanopyClusterer(measure,
-        aKernelProfile, t1, t2, convergenceDelta, runClustering);
-    List<MeanShiftCanopy> clusters = Lists.newArrayList();
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(clustersIn.toUri(), conf);
-    for (ClusterWritable clusterWritable : new SequenceFileDirValueIterable<ClusterWritable>(
-        clustersIn, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {
-      MeanShiftCanopy canopy = (MeanShiftCanopy)clusterWritable.getValue();
-      clusterer.mergeCanopy(canopy, clusters);
-    }
-    boolean[] converged = { false };
-    int iteration = 1;
-    while (!converged[0] && iteration <= maxIterations) {
-      log.info("Mean Shift Iteration: {}", iteration);
-      clusters = clusterer.iterate(clusters, converged);
-      Path clustersOut = new Path(output, Cluster.CLUSTERS_DIR + iteration);
-      SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(
-          clustersOut, "part-r-00000"), Text.class, ClusterWritable.class);
-      try {
-        for (MeanShiftCanopy cluster : clusters) {
-          if (log.isDebugEnabled()) {
-            log.debug(
-                "Writing Cluster:{} center:{} numPoints:{} radius:{} to: {}",
-                cluster.getId(),
-                AbstractCluster.formatVector(cluster.getCenter(), null),
-                cluster.getNumObservations(),
-                AbstractCluster.formatVector(cluster.getRadius(), null),
-                clustersOut.getName());
-          }
-          ClusterWritable clusterWritable = new ClusterWritable();
-          clusterWritable.setValue(cluster);
-          writer.append(new Text(cluster.getIdentifier()), clusterWritable);
-        }
-      } finally {
-        Closeables.close(writer, false);
-      }
-      clustersIn = clustersOut;
-      iteration++;
-    }
-    Path fromPath = new Path(output, Cluster.CLUSTERS_DIR + (iteration - 1));
-    Path finalClustersIn = new Path(output, Cluster.CLUSTERS_DIR + (iteration - 1) + "-final");
-    FileSystem.get(fromPath.toUri(), conf).rename(fromPath, finalClustersIn);
-    return finalClustersIn;
-  }
-
-  /**
-   * Build new clusters using Hadoop
-   * 
-   */
-  private static Path buildClustersMR(Configuration conf, Path clustersIn,
-      Path output, DistanceMeasure measure, IKernelProfile aKernelProfile,
-      double t1, double t2, double convergenceDelta, int maxIterations,
-      boolean runClustering) throws IOException, InterruptedException,
-      ClassNotFoundException {
-    // iterate until the clusters converge
-    boolean converged = false;
-    int iteration = 1;
-    while (!converged && iteration <= maxIterations) {
-      int numReducers = Integer.valueOf(conf.get(MAPRED_REDUCE_TASKS, "1"));
-      log.info("Mean Shift Iteration: {}, numReducers {}", iteration, numReducers);
-      // point the output to a new directory per iteration
-      Path clustersOut = new Path(output, Cluster.CLUSTERS_DIR + iteration);
-      Path controlOut = new Path(output, CONTROL_CONVERGED);
-      runIterationMR(conf, clustersIn, clustersOut, controlOut, measure
-          .getClass().getName(), aKernelProfile.getClass().getName(), t1, t2,
-          convergenceDelta, runClustering);
-      converged = FileSystem.get(controlOut.toUri(), conf).exists(controlOut);
-      // now point the input to the old output directory
-      clustersIn = clustersOut;
-      iteration++;
-      // decrease the number of reducers if it is > 1 to cross-pollenate
-      // map sets
-      if (numReducers > 1) {
-        numReducers--;
-        conf.set(MAPRED_REDUCE_TASKS, String.valueOf(numReducers));
-      }
-    }
-    Path fromPath = new Path(output, Cluster.CLUSTERS_DIR + (iteration - 1));
-    Path finalClustersIn = new Path(output, Cluster.CLUSTERS_DIR + (iteration - 1) + Cluster.FINAL_ITERATION_SUFFIX);
-    FileSystem.get(fromPath.toUri(), conf).rename(fromPath, finalClustersIn);
-    return finalClustersIn;
-  }
-
-  /**
-   * Run an iteration using Hadoop
-   * 
-   * @param conf
-   *          the Configuration to use
-   * @param input
-   *          the input pathname String
-   * @param output
-   *          the output pathname String
-   * @param control
-   *          the control path
-   * @param measureClassName
-   *          the DistanceMeasure class name
-   * @param kernelProfileClassName
-   *          an IKernel class name
-   * @param t1
-   *          the T1 distance threshold
-   * @param t2
-   *          the T2 distance threshold
-   * @param convergenceDelta
-   *          the double convergence criteria
-   * @param runClustering
-   *          if true accumulate merged clusters for subsequent clustering step
-   */
-  private static void runIterationMR(Configuration conf, Path input,
-      Path output, Path control, String measureClassName,
-      String kernelProfileClassName, double t1, double t2,
-      double convergenceDelta, boolean runClustering) throws IOException,
-      InterruptedException, ClassNotFoundException {
-
-    conf.set(MeanShiftCanopyConfigKeys.DISTANCE_MEASURE_KEY, measureClassName);
-    conf.set(MeanShiftCanopyConfigKeys.KERNEL_PROFILE_KEY,
-        kernelProfileClassName);
-    conf.set(MeanShiftCanopyConfigKeys.CLUSTER_CONVERGENCE_KEY, String
-        .valueOf(convergenceDelta));
-    conf.set(MeanShiftCanopyConfigKeys.T1_KEY, String.valueOf(t1));
-    conf.set(MeanShiftCanopyConfigKeys.T2_KEY, String.valueOf(t2));
-    conf.set(MeanShiftCanopyConfigKeys.CONTROL_PATH_KEY, control.toString());
-    conf.set(MeanShiftCanopyConfigKeys.CLUSTER_POINTS_KEY, String
-        .valueOf(runClustering));
-    Job job = new Job(conf,
-        "Mean Shift Driver running runIteration over input: " + input);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(ClusterWritable.class);
-
-    FileInputFormat.setInputPaths(job, input);
-    FileOutputFormat.setOutputPath(job, output);
-
-    job.setMapperClass(MeanShiftCanopyMapper.class);
-    job.setReducerClass(MeanShiftCanopyReducer.class);
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setJarByClass(MeanShiftCanopyDriver.class);
-    if (!job.waitForCompletion(true)) {
-      throw new InterruptedException("Mean Shift Iteration failed on input "
-          + input);
-    }
-  }
-
-  /**
-   * Run the job using supplied arguments
-   * 
-   * @param conf
-   *          configuration for Hadoop job - set to null if running sequentially
-   * @param input
-   *          the directory pathname for input points
-   * @param clustersIn
-   *          the directory pathname for input clusters
-   * @param output
-   *          the directory pathname for output clustered points
-   * @param runSequential
-   *          if true run in sequential execution mode
-   */
-  public static void clusterData(Configuration conf, Path input, Path clustersIn, Path output,
-      boolean runSequential) throws IOException, InterruptedException,
-      ClassNotFoundException {
-    if (runSequential) {
-      clusterDataSeq(input, clustersIn, output);
-    } else {
-      clusterDataMR(conf, input, clustersIn, output);
-    }
-  }
-
-  /**
-   * Cluster the data sequentially
-   */
-  private static void clusterDataSeq(Path input, Path clustersIn, Path output) throws IOException {
-    Collection<MeanShiftCanopy> clusters = Lists.newArrayList();
-    Configuration conf = new Configuration();
-    for (ClusterWritable clusterWritable : new SequenceFileDirValueIterable<ClusterWritable>(clustersIn, PathType.LIST,
-        PathFilters.logsCRCFilter(), conf)) {
-      MeanShiftCanopy cluster = (MeanShiftCanopy) clusterWritable.getValue();
-      clusters.add(cluster);
-    }
-    // iterate over all points, assigning each to the closest canopy and
-    // outputting that clustering
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    FileStatus[] status = fs.listStatus(input, PathFilters.logsCRCFilter());
-    int part = 0;
-    for (FileStatus s : status) {
-      SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(
-          output, "part-m-" + part++), IntWritable.class,
-          WeightedVectorWritable.class);
-      try {
-        for (Pair<Writable, ClusterWritable> record
-            : new SequenceFileIterable<Writable, ClusterWritable>(s.getPath(), conf)) {
-          ClusterWritable clusterWritable = record.getSecond();
-          MeanShiftCanopy canopy = (MeanShiftCanopy) clusterWritable.getValue();
-          MeanShiftCanopy closest = MeanShiftCanopyClusterer.findCoveringCanopy(canopy, clusters);
-          writer.append(new IntWritable(closest.getId()), new WeightedVectorWritable(1, canopy.getCenter()));
-        }
-      } finally {
-        Closeables.close(writer, false);
-      }
-    }
-  }
-
-  /**
-   * Cluster the data using Hadoop
-   */
-  private static void clusterDataMR(Configuration conf, Path input, Path clustersIn, Path output)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    conf.set(STATE_IN_KEY, clustersIn.toString());
-    Job job = new Job(conf,
-        "Mean Shift Driver running clusterData over input: " + input);
-    job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(WeightedVectorWritable.class);
-    job.setMapperClass(MeanShiftCanopyClusterMapper.class);
-
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setNumReduceTasks(0);
-    job.setJarByClass(MeanShiftCanopyDriver.class);
-
-    FileInputFormat.setInputPaths(job, input);
-    FileOutputFormat.setOutputPath(job, output);
-
-    if (!job.waitForCompletion(true)) {
-      throw new InterruptedException(
-          "Mean Shift Clustering failed on clustersIn " + clustersIn);
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,70 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-
-import java.io.IOException;
-import java.util.Collection;
-
-@Deprecated
-public class MeanShiftCanopyMapper extends Mapper<WritableComparable<?>, ClusterWritable, Text, ClusterWritable> {
-
-  private final Collection<MeanShiftCanopy> canopies = Lists.newArrayList();
-
-  private MeanShiftCanopyClusterer clusterer;
-
-  private Integer numReducers;
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    Configuration conf = context.getConfiguration();
-    clusterer = new MeanShiftCanopyClusterer(conf);
-    numReducers = Integer.valueOf(conf.get(MeanShiftCanopyDriver.MAPRED_REDUCE_TASKS, "1"));
-  }
-
-  @Override
-  protected void map(WritableComparable<?> key, ClusterWritable clusterWritable, Context context)
-    throws IOException, InterruptedException {
-    MeanShiftCanopy canopy = (MeanShiftCanopy) clusterWritable.getValue();
-    clusterer.mergeCanopy(canopy.shallowCopy(), canopies);
-  }
-
-  @Override
-  protected void cleanup(Context context) throws IOException, InterruptedException {
-    int reducer = 0;
-    for (MeanShiftCanopy canopy : canopies) {
-      clusterer.shiftToMean(canopy);
-      ClusterWritable clusterWritable = new ClusterWritable();
-      clusterWritable.setValue(canopy);
-      context.write(new Text(String.valueOf(reducer)), clusterWritable);
-      reducer++;
-      if (reducer >= numReducers) {
-        reducer = 0;
-      }
-    }
-    super.cleanup(context);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/meanshift/MeanShiftCanopyReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-
-import com.google.common.collect.Lists;
-
-@Deprecated
-public class MeanShiftCanopyReducer extends Reducer<Text,ClusterWritable,Text,ClusterWritable> {
-
-  private final Collection<MeanShiftCanopy> canopies = Lists.newArrayList();
-  private MeanShiftCanopyClusterer clusterer;
-  private boolean allConverged = true;
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    clusterer = new MeanShiftCanopyClusterer(context.getConfiguration());
-  }
-
-  @Override
-  protected void reduce(Text key, Iterable<ClusterWritable> values, Context context)
-    throws IOException, InterruptedException {
-    for (ClusterWritable clusterWritable : values) {
-      MeanShiftCanopy canopy = (MeanShiftCanopy)clusterWritable.getValue();
-      clusterer.mergeCanopy(canopy.shallowCopy(), canopies);
-    }
-
-    for (MeanShiftCanopy canopy : canopies) {
-      boolean converged = clusterer.shiftToMean(canopy);
-      if (converged) {
-        context.getCounter("Clustering", "Converged Clusters").increment(1);
-      }
-      allConverged = converged && allConverged;
-      ClusterWritable clusterWritable = new ClusterWritable();
-      clusterWritable.setValue(canopy);
-      context.write(new Text(canopy.getIdentifier()), clusterWritable);
-    }
-
-  }
-
-  @Override
-  protected void cleanup(Context context) throws IOException, InterruptedException {
-    Configuration conf = context.getConfiguration();
-    if (allConverged) {
-      Path path = new Path(conf.get(MeanShiftCanopyConfigKeys.CONTROL_PATH_KEY));
-      FileSystem.get(path.toUri(), conf).createNewFile(path);
-    }
-    super.cleanup(context);
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFactory.java mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFactory.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFactory.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFactory.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.clustering.minhash;
-
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.math.MurmurHash;
-import org.apache.mahout.math.MurmurHash3;
-
-import java.util.Random;
-
-@Deprecated
-public final class HashFactory {
-
-  private HashFactory() {
-  }
-
-  public enum HashType {
-    LINEAR, POLYNOMIAL, MURMUR, MURMUR3
-  }
-
-  public static HashFunction[] createHashFunctions(HashType type, int numFunctions) {
-    HashFunction[] hashFunction = new HashFunction[numFunctions];
-    Random seed = RandomUtils.getRandom(11);
-    switch (type) {
-      case LINEAR:
-        for (int i = 0; i < numFunctions; i++) {
-          hashFunction[i] = new LinearHash(seed.nextInt(), seed.nextInt());
-        }
-        break;
-      case POLYNOMIAL:
-        for (int i = 0; i < numFunctions; i++) {
-          hashFunction[i] = new PolynomialHash(seed.nextInt(), seed.nextInt(), seed.nextInt());
-        }
-        break;
-      case MURMUR:
-        for (int i = 0; i < numFunctions; i++) {
-          hashFunction[i] = new MurmurHashWrapper(seed.nextInt());
-        }
-        break;
-      case MURMUR3:
-        for (int i = 0; i < numFunctions; i++) {
-          hashFunction[i] = new MurmurHash3Wrapper(seed.nextInt());
-        }
-        break;
-      default:
-        throw new IllegalStateException("Unknown type: " + type);
-    }
-    return hashFunction;
-  }
-
-  static class LinearHash implements HashFunction {
-    private final int seedA;
-    private final int seedB;
-
-    LinearHash(int seedA, int seedB) {
-      this.seedA = seedA;
-      this.seedB = seedB;
-    }
-
-    @Override
-    public int hash(byte[] bytes) {
-      long hashValue = 31;
-      for (long byteVal : bytes) {
-        hashValue *= seedA * byteVal;
-        hashValue += seedB;
-      }
-      return Math.abs((int) (hashValue % RandomUtils.MAX_INT_SMALLER_TWIN_PRIME));
-    }
-  }
-
-  static class PolynomialHash implements HashFunction {
-    private final int seedA;
-    private final int seedB;
-    private final int seedC;
-
-    PolynomialHash(int seedA, int seedB, int seedC) {
-      this.seedA = seedA;
-      this.seedB = seedB;
-      this.seedC = seedC;
-    }
-
-    @Override
-    public int hash(byte[] bytes) {
-      long hashValue = 31;
-      for (long byteVal : bytes) {
-        hashValue *= seedA * (byteVal >> 4);
-        hashValue += seedB * byteVal + seedC;
-      }
-      return Math
-          .abs((int) (hashValue % RandomUtils.MAX_INT_SMALLER_TWIN_PRIME));
-    }
-  }
-
-  static class MurmurHashWrapper implements HashFunction {
-    private final int seed;
-
-    MurmurHashWrapper(int seed) {
-      this.seed = seed;
-    }
-
-    @Override
-    public int hash(byte[] bytes) {
-      long hashValue = MurmurHash.hash64A(bytes, seed);
-      return Math.abs((int) (hashValue % RandomUtils.MAX_INT_SMALLER_TWIN_PRIME));
-    }
-  }
-
-  static class MurmurHash3Wrapper implements HashFunction {
-    private final int seed;
-
-    MurmurHash3Wrapper(int seed) {
-      this.seed = seed;
-    }
-
-    @Override
-    public int hash(byte[] bytes) {
-      long hashValue = MurmurHash3.murmurhash3x8632(bytes, 0, bytes.length, seed);
-      return Math.abs((int) (hashValue % RandomUtils.MAX_INT_SMALLER_TWIN_PRIME));
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFunction.java mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFunction.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFunction.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/minhash/HashFunction.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,24 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.clustering.minhash;
-
-@Deprecated
-public interface HashFunction {
-
-  int hash(byte[] bytes);
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashDriver.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,118 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.minhash;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.math.VectorWritable;
-
-@Deprecated
-public final class MinHashDriver extends AbstractJob {
-
-  public static final String NUM_HASH_FUNCTIONS = "numHashFunctions";
-  public static final String KEY_GROUPS = "keyGroups";
-  public static final String HASH_TYPE = "hashType";
-  public static final String MIN_CLUSTER_SIZE = "minClusterSize";
-  public static final String MIN_VECTOR_SIZE = "minVectorSize";
-  public static final String NUM_REDUCERS = "numReducers";
-  public static final String DEBUG_OUTPUT = "debugOutput";
-  public static final String VECTOR_DIMENSION_TO_HASH  = "vectorDimensionToHash";
-
-  static final String HASH_DIMENSION_VALUE = "value";
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new MinHashDriver(), args);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    addInputOption();
-    addOutputOption();
-
-
-    addOption(MIN_CLUSTER_SIZE, "mcs", "Minimum points inside a cluster", String.valueOf(10));
-    addOption(MIN_VECTOR_SIZE, "mvs", "Minimum size of vector to be hashed", String.valueOf(5));
-    addOption(VECTOR_DIMENSION_TO_HASH, "vdh", "Dimension of vector to hash. Available types: (value, index). "
-        + "Defaults to 'value'", HASH_DIMENSION_VALUE);
-    addOption(HASH_TYPE, "ht", "Type of hash function to use. Available types: (linear, polynomial, murmur) ",
-        HashFactory.HashType.MURMUR.toString());
-    addOption(NUM_HASH_FUNCTIONS, "nh", "Number of hash functions to be used", String.valueOf(10));
-    addOption(KEY_GROUPS, "kg", "Number of key groups to be used", String.valueOf(2));
-    addOption(NUM_REDUCERS, "nr", "The number of reduce tasks. Defaults to 2", String.valueOf(2));
-    addFlag(DEBUG_OUTPUT, "debug", "Output the whole vectors for debugging");
-    addOption(DefaultOptionCreator.overwriteOption().create());
-
-    if (parseArguments(args) == null) {
-      return -1;
-    }
-
-    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(getConf(), getOutputPath());
-    }
-
-    int minClusterSize = Integer.valueOf(getOption(MIN_CLUSTER_SIZE));
-    int minVectorSize = Integer.valueOf(getOption(MIN_VECTOR_SIZE));
-    String dimensionToHash = getOption(VECTOR_DIMENSION_TO_HASH);
-    String hashType = getOption(HASH_TYPE);
-    int numHashFunctions = Integer.valueOf(getOption(NUM_HASH_FUNCTIONS));
-    int keyGroups = Integer.valueOf(getOption(KEY_GROUPS));
-    int numReduceTasks = Integer.parseInt(getOption(NUM_REDUCERS));
-    boolean debugOutput = hasOption(DEBUG_OUTPUT);
-
-    try {
-      HashFactory.HashType.valueOf(hashType);
-    } catch (IllegalArgumentException e) {
-      System.err.println("Unknown hashType: " + hashType);
-      return -1;
-    }
-
-    Class<? extends Writable> outputClass = debugOutput ? VectorWritable.class : Text.class;
-    Class<? extends OutputFormat> outputFormatClass =
-        debugOutput ? SequenceFileOutputFormat.class : TextOutputFormat.class;
-
-    Job minHash = prepareJob(getInputPath(), getOutputPath(), SequenceFileInputFormat.class, MinHashMapper.class,
-            Text.class, outputClass, MinHashReducer.class, Text.class, VectorWritable.class, outputFormatClass);
-
-    Configuration minHashConfiguration = minHash.getConfiguration();
-    minHashConfiguration.setInt(MIN_CLUSTER_SIZE, minClusterSize);
-    minHashConfiguration.setInt(MIN_VECTOR_SIZE, minVectorSize);
-    minHashConfiguration.set(VECTOR_DIMENSION_TO_HASH, dimensionToHash);
-    minHashConfiguration.set(HASH_TYPE, hashType);
-    minHashConfiguration.setInt(NUM_HASH_FUNCTIONS, numHashFunctions);
-    minHashConfiguration.setInt(KEY_GROUPS, keyGroups);
-    minHashConfiguration.setBoolean(DEBUG_OUTPUT, debugOutput);
-    minHash.setNumReduceTasks(numReduceTasks);
-
-    boolean succeeded = minHash.waitForCompletion(true);
-    if (!succeeded) {
-      return -1;
-    }
-
-    return 0;
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,114 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.minhash;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.minhash.HashFactory.HashType;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-import java.io.IOException;
-
-@Deprecated
-public class MinHashMapper extends Mapper<Text, VectorWritable, Text, Writable> {
-
-  private HashFunction[] hashFunction;
-  private int numHashFunctions;
-  private int keyGroups;
-  private int minVectorSize;
-  private boolean debugOutput;
-  private int[] minHashValues;
-  private byte[] bytesToHash;
-  private boolean hashValue;
-
-  private final Text cluster = new Text();
-  private final VectorWritable vector = new VectorWritable();
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    Configuration conf = context.getConfiguration();
-    numHashFunctions = conf.getInt(MinHashDriver.NUM_HASH_FUNCTIONS, 10);
-    minHashValues = new int[numHashFunctions];
-    bytesToHash = new byte[4];
-    keyGroups = conf.getInt(MinHashDriver.KEY_GROUPS, 1);
-    minVectorSize = conf.getInt(MinHashDriver.MIN_VECTOR_SIZE, 5);
-    debugOutput = conf.getBoolean(MinHashDriver.DEBUG_OUTPUT, false);
-
-    String dimensionToHash = conf.get(MinHashDriver.VECTOR_DIMENSION_TO_HASH);
-    hashValue = MinHashDriver.HASH_DIMENSION_VALUE.equalsIgnoreCase(dimensionToHash);
-
-    HashType hashType = HashType.valueOf(conf.get(MinHashDriver.HASH_TYPE));
-    hashFunction = HashFactory.createHashFunctions(hashType, numHashFunctions);
-  }
-
-  /**
-   * Hash all items with each function and retain min. value for each iteration. We up with X number of
-   * minhash signatures.
-   * <p/>
-   * Now depending upon the number of key-groups (1 - 4) concatenate that many minhash values to form
-   * cluster-id as 'key' and item-id as 'value'
-   */
-  @Override
-  public void map(Text item, VectorWritable features, Context context) throws IOException, InterruptedException {
-    Vector featureVector = features.get();
-    if (featureVector.size() < minVectorSize) {
-      return;
-    }
-    // Initialize the MinHash values to highest
-    for (int i = 0; i < numHashFunctions; i++) {
-      minHashValues[i] = Integer.MAX_VALUE;
-    }
-
-    for (int i = 0; i < numHashFunctions; i++) {
-      for (Vector.Element ele : featureVector.nonZeroes()) {
-        int value = hashValue ? (int) ele.get() : ele.index();
-        bytesToHash[0] = (byte) (value >> 24);
-        bytesToHash[1] = (byte) (value >> 16);
-        bytesToHash[2] = (byte) (value >> 8);
-        bytesToHash[3] = (byte) value;
-        int hashIndex = hashFunction[i].hash(bytesToHash);
-        //if our new hash value is less than the old one, replace the old one
-        if (minHashValues[i] > hashIndex) {
-          minHashValues[i] = hashIndex;
-        }
-      }
-    }
-    // output the cluster information
-    for (int i = 0; i < numHashFunctions; i++) {
-      StringBuilder clusterIdBuilder = new StringBuilder();
-      for (int j = 0; j < keyGroups; j++) {
-        clusterIdBuilder.append(minHashValues[(i + j) % numHashFunctions]).append('-');
-      }
-      //remove the last dash
-      clusterIdBuilder.deleteCharAt(clusterIdBuilder.length() - 1);
-
-      cluster.set(clusterIdBuilder.toString());
-
-      if (debugOutput) {
-        vector.set(featureVector);
-        context.write(cluster, vector);
-      } else {
-        context.write(cluster, item);
-      }
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/minhash/MinHashReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.minhash;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-import java.io.IOException;
-import java.util.Collection;
-
-@Deprecated
-public class MinHashReducer extends Reducer<Text,Writable,Text,Writable> {
-  
-  private int minClusterSize;
-  private boolean debugOutput;
-  
-  enum Clusters {
-    ACCEPTED,
-    DISCARDED
-  }
-  
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    Configuration conf = context.getConfiguration();
-    minClusterSize = conf.getInt(MinHashDriver.MIN_CLUSTER_SIZE, 5);
-    debugOutput = conf.getBoolean(MinHashDriver.DEBUG_OUTPUT, false);
-  }
-  
-  /**
-   * output the items clustered
-   */
-  @Override
-  protected void reduce(Text cluster, Iterable<Writable> points, Context context)
-    throws IOException, InterruptedException {
-    Collection<Writable> pointList = Lists.newArrayList();
-    for (Writable point : points) {
-      if (debugOutput) {
-        Vector pointVector = ((VectorWritable) point).get().clone();
-        Writable writablePointVector = new VectorWritable(pointVector);
-        pointList.add(writablePointVector);
-      } else {
-        Writable pointText = new Text(point.toString());
-        pointList.add(pointText);
-      }
-    }
-    if (pointList.size() >= minClusterSize) {
-      context.getCounter(Clusters.ACCEPTED).increment(1);
-      for (Writable point : pointList) {
-        context.write(cluster, point);
-      }
-    } else {
-      context.getCounter(Clusters.DISCARDED).increment(1);
-    }
-  }
-  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.hadoop.DistributedRowMatrix;
+
+public final class AffinityMatrixInputJob {
+
+  private AffinityMatrixInputJob() {
+  }
+
+  /**
+   * Initializes and executes the job of reading the documents containing
+   * the data of the affinity matrix in (x_i, x_j, value) format.
+   */
+  public static void runJob(Path input, Path output, int rows, int cols)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    Configuration conf = new Configuration();
+    HadoopUtil.delete(conf, output);
+
+    conf.setInt(Keys.AFFINITY_DIMENSIONS, rows);
+    Job job = new Job(conf, "AffinityMatrixInputJob: " + input + " -> M/R -> " + output);
+
+    job.setMapOutputKeyClass(IntWritable.class);
+    job.setMapOutputValueClass(DistributedRowMatrix.MatrixEntryWritable.class);
+    job.setOutputKeyClass(IntWritable.class);
+    job.setOutputValueClass(VectorWritable.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    job.setMapperClass(AffinityMatrixInputMapper.class);   
+    job.setReducerClass(AffinityMatrixInputReducer.class);
+
+    FileInputFormat.addInputPath(job, input);
+    FileOutputFormat.setOutputPath(job, output);
+
+    job.setJarByClass(AffinityMatrixInputJob.class);
+
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+  }
+
+  /**
+   * A transparent wrapper for the above method which handles the tedious tasks
+   * of setting and retrieving system Paths. Hands back a fully-populated
+   * and initialized DistributedRowMatrix.
+   */
+  public static DistributedRowMatrix runJob(Path input, Path output, int dimensions)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    Path seqFiles = new Path(output, "seqfiles-" + (System.nanoTime() & 0xFF));
+    runJob(input, seqFiles, dimensions, dimensions);
+    DistributedRowMatrix a = new DistributedRowMatrix(seqFiles,
+        new Path(seqFiles, "seqtmp-" + (System.nanoTime() & 0xFF)), 
+        dimensions, dimensions);
+    a.setConf(new Configuration());
+    return a;
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputMapper.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,78 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.math.hadoop.DistributedRowMatrix;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * <p>Handles reading the files representing the affinity matrix. Since the affinity
+ * matrix is representative of a graph, each line in all the files should
+ * take the form:</p>
+ *
+ * {@code i,j,value}
+ *
+ * <p>where {@code i} and {@code j} are the {@code i}th and
+ * {@code j} data points in the entire set, and {@code value}
+ * represents some measurement of their relative absolute magnitudes. This
+ * is, simply, a method for representing a graph textually.
+ */
+public class AffinityMatrixInputMapper
+    extends Mapper<LongWritable, Text, IntWritable, DistributedRowMatrix.MatrixEntryWritable> {
+
+  private static final Logger log = LoggerFactory.getLogger(AffinityMatrixInputMapper.class);
+
+  private static final Pattern COMMA_PATTERN = Pattern.compile(",");
+
+  @Override
+  protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
+
+    String[] elements = COMMA_PATTERN.split(value.toString());
+    log.debug("(DEBUG - MAP) Key[{}], Value[{}]", key.get(), value);
+
+    // enforce well-formed textual representation of the graph
+    if (elements.length != 3) {
+      throw new IOException("Expected input of length 3, received "
+                            + elements.length + ". Please make sure you adhere to "
+                            + "the structure of (i,j,value) for representing a graph in text. "
+                            + "Input line was: '" + value + "'.");
+    }
+    if (elements[0].isEmpty() || elements[1].isEmpty() || elements[2].isEmpty()) {
+      throw new IOException("Found an element of 0 length. Please be sure you adhere to the structure of "
+          + "(i,j,value) for  representing a graph in text.");
+    }
+
+    // parse the line of text into a DistributedRowMatrix entry,
+    // making the row (elements[0]) the key to the Reducer, and
+    // setting the column (elements[1]) in the entry itself
+    DistributedRowMatrix.MatrixEntryWritable toAdd = new DistributedRowMatrix.MatrixEntryWritable();
+    IntWritable row = new IntWritable(Integer.valueOf(elements[0]));
+    toAdd.setRow(-1); // already set as the Reducer's key
+    toAdd.setCol(Integer.valueOf(elements[1]));
+    toAdd.setVal(Double.valueOf(elements[2]));
+    context.write(row, toAdd);
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputReducer.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/AffinityMatrixInputReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.math.RandomAccessSparseVector;
+import org.apache.mahout.math.SequentialAccessSparseVector;
+import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.hadoop.DistributedRowMatrix;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Tasked with taking each DistributedRowMatrix entry and collecting them
+ * into vectors corresponding to rows. The input and output keys are the same,
+ * corresponding to the row in the ensuing matrix. The matrix entries are
+ * entered into a vector according to the column to which they belong, and
+ * the vector is then given the key corresponding to its row.
+ */
+public class AffinityMatrixInputReducer
+    extends Reducer<IntWritable, DistributedRowMatrix.MatrixEntryWritable, IntWritable, VectorWritable> {
+
+  private static final Logger log = LoggerFactory.getLogger(AffinityMatrixInputReducer.class);
+
+  @Override
+  protected void reduce(IntWritable row, Iterable<DistributedRowMatrix.MatrixEntryWritable> values, Context context)
+    throws IOException, InterruptedException {
+    int size = context.getConfiguration().getInt(Keys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE);
+    RandomAccessSparseVector out = new RandomAccessSparseVector(size, 100);
+
+    for (DistributedRowMatrix.MatrixEntryWritable element : values) {
+      out.setQuick(element.getCol(), element.getVal());
+      if (log.isDebugEnabled()) {
+        log.debug("(DEBUG - REDUCE) Row[{}], Column[{}], Value[{}]",
+                  row.get(), element.getCol(), element.getVal());
+      }
+    }
+    SequentialAccessSparseVector output = new SequentialAccessSparseVector(out);
+    context.write(row, new VectorWritable(output));
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/IntDoublePairWritable.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/IntDoublePairWritable.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/IntDoublePairWritable.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/IntDoublePairWritable.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * This class is a Writable implementation of the mahout.common.Pair
+ * generic class. Since the generic types would also themselves have to
+ * implement Writable, it made more sense to create a more specialized
+ * version of the class altogether.
+ * 
+ * In essence, this can be treated as a single Vector Element.
+ */
+public class IntDoublePairWritable implements Writable {
+  
+  private int key;
+  private double value;
+  
+  public IntDoublePairWritable() {
+  }
+  
+  public IntDoublePairWritable(int k, double v) {
+    this.key = k;
+    this.value = v;
+  }
+  
+  public void setKey(int k) {
+    this.key = k;
+  }
+  
+  public void setValue(double v) {
+    this.value = v;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.key = in.readInt();
+    this.value = in.readDouble();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(key);
+    out.writeDouble(value);
+  }
+
+  public int getKey() {
+    return key;
+  }
+
+  public double getValue() {
+    return value;
+  }
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/Keys.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/Keys.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/Keys.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/Keys.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,31 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+public class Keys {
+
+  /**
+   * Sets the SequenceFile index for the diagonal matrix.
+   */
+  public static final int DIAGONAL_CACHE_INDEX = 1;
+
+  public static final String AFFINITY_DIMENSIONS = "org.apache.mahout.clustering.spectral.common.affinitydimensions";
+
+  private Keys() {}
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/MatrixDiagonalizeJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/MatrixDiagonalizeJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/MatrixDiagonalizeJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/MatrixDiagonalizeJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+
+/**
+ * Given a matrix, this job returns a vector whose i_th element is the 
+ * sum of all the elements in the i_th row of the original matrix.
+ */
+public final class MatrixDiagonalizeJob {
+
+  private MatrixDiagonalizeJob() {
+  }
+
+  public static Vector runJob(Path affInput, int dimensions)
+    throws IOException, ClassNotFoundException, InterruptedException {
+    
+    // set up all the job tasks
+    Configuration conf = new Configuration();
+    Path diagOutput = new Path(affInput.getParent(), "diagonal");
+    HadoopUtil.delete(conf, diagOutput);
+    conf.setInt(Keys.AFFINITY_DIMENSIONS, dimensions);
+    Job job = new Job(conf, "MatrixDiagonalizeJob");
+    
+    job.setInputFormatClass(SequenceFileInputFormat.class);
+    job.setMapOutputKeyClass(NullWritable.class);
+    job.setMapOutputValueClass(IntDoublePairWritable.class);
+    job.setOutputKeyClass(NullWritable.class);
+    job.setOutputValueClass(VectorWritable.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    job.setMapperClass(MatrixDiagonalizeMapper.class);
+    job.setReducerClass(MatrixDiagonalizeReducer.class);
+    
+    FileInputFormat.addInputPath(job, affInput);
+    FileOutputFormat.setOutputPath(job, diagOutput);
+    
+    job.setJarByClass(MatrixDiagonalizeJob.class);
+
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+
+    // read the results back from the path
+    return VectorCache.load(conf, new Path(diagOutput, "part-r-00000"));
+  }
+  
+  public static class MatrixDiagonalizeMapper
+    extends Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable> {
+    
+    @Override
+    protected void map(IntWritable key, VectorWritable row, Context context) 
+      throws IOException, InterruptedException {
+      // store the sum
+      IntDoublePairWritable store = new IntDoublePairWritable(key.get(), row.get().zSum());
+      context.write(NullWritable.get(), store);
+    }
+  }
+  
+  public static class MatrixDiagonalizeReducer
+    extends Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable> {
+    
+    @Override
+    protected void reduce(NullWritable key, Iterable<IntDoublePairWritable> values,
+      Context context) throws IOException, InterruptedException {
+      // create the return vector
+      Vector retval = new DenseVector(context.getConfiguration().getInt(Keys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE));
+      // put everything in its correct spot
+      for (IntDoublePairWritable e : values) {
+        retval.setQuick(e.getKey(), e.getValue());
+      }
+      // write it out
+      context.write(key, new VectorWritable(retval));
+    }
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/UnitVectorizerJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/UnitVectorizerJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/UnitVectorizerJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/UnitVectorizerJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,79 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
+import org.apache.mahout.math.VectorWritable;
+
+/**
+ * <p>Given a DistributedRowMatrix, this job normalizes each row to unit
+ * vector length. If the input is a matrix U, and the output is a matrix
+ * W, the job follows:</p>
+ *
+ * <p>{@code v_ij = u_ij / sqrt(sum_j(u_ij * u_ij))}</p>
+ */
+public final class UnitVectorizerJob {
+
+  private UnitVectorizerJob() {
+  }
+
+  public static void runJob(Path input, Path output)
+    throws IOException, InterruptedException, ClassNotFoundException {
+    
+    Configuration conf = new Configuration();
+    Job job = new Job(conf, "UnitVectorizerJob");
+    
+    job.setInputFormatClass(SequenceFileInputFormat.class);
+    job.setOutputKeyClass(IntWritable.class);
+    job.setOutputValueClass(VectorWritable.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    job.setMapperClass(UnitVectorizerMapper.class);
+    job.setNumReduceTasks(0);
+    
+    FileInputFormat.addInputPath(job, input);
+    FileOutputFormat.setOutputPath(job, output);
+
+    job.setJarByClass(UnitVectorizerJob.class);
+
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+  }
+  
+  public static class UnitVectorizerMapper
+    extends Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
+    
+    @Override
+    protected void map(IntWritable row, VectorWritable vector, Context context) 
+      throws IOException, InterruptedException {
+      context.write(row, new VectorWritable(vector.get().normalize(2)));
+    }
+
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorCache.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorCache.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorCache.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorCache.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,123 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+import java.net.URI;
+import java.util.Arrays;
+
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Writable;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ * This class handles reading and writing vectors to the Hadoop
+ * distributed cache. Created as a result of Eigencuts' liberal use
+ * of such functionality, but available to any algorithm requiring it.
+ */
+public final class VectorCache {
+
+  private static final Logger log = LoggerFactory.getLogger(VectorCache.class);
+
+  private VectorCache() {
+  }
+
+  /**
+   * @param key    SequenceFile key
+   * @param vector Vector to save, to be wrapped as VectorWritable
+   */
+  public static void save(Writable key,
+                          Vector vector,
+                          Path output,
+                          Configuration conf,
+                          boolean overwritePath,
+                          boolean deleteOnExit) throws IOException {
+
+    FileSystem fs = FileSystem.get(output.toUri(), conf);
+    output = fs.makeQualified(output);
+    if (overwritePath) {
+      HadoopUtil.delete(conf, output);
+    }
+
+    // set the cache
+    DistributedCache.setCacheFiles(new URI[]{output.toUri()}, conf);
+
+    // set up the writer
+    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
+            IntWritable.class, VectorWritable.class);
+    try {
+      writer.append(key, new VectorWritable(vector));
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    if (deleteOnExit) {
+      fs.deleteOnExit(output);
+    }
+  }
+
+  /**
+   * Calls the save() method, setting the cache to overwrite any previous
+   * Path and to delete the path after exiting
+   */
+  public static void save(Writable key, Vector vector, Path output, Configuration conf) throws IOException {
+    save(key, vector, output, conf, true, true);
+  }
+
+  /**
+   * Loads the vector from {@link DistributedCache}. Returns null if no vector exists.
+   */
+  public static Vector load(Configuration conf) throws IOException {
+    Path[] files = HadoopUtil.getCachedFiles(conf);
+
+    if (files.length != 1) {
+      throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
+    }
+
+    if (log.isInfoEnabled()) {
+      log.info("Files are: {}", Arrays.toString(files));
+    }
+    return load(conf, files[0]);
+  }
+
+  /**
+   * Loads a Vector from the specified path. Returns null if no vector exists.
+   */
+  public static Vector load(Configuration conf, Path input) throws IOException {
+    log.info("Loading vector from: {}", input);
+    SequenceFileValueIterator<VectorWritable> iterator =
+            new SequenceFileValueIterator<VectorWritable>(input, true, conf);
+    try {
+      return iterator.next().get();
+    } finally {
+      Closeables.close(iterator, true);
+    }
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorMatrixMultiplicationJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorMatrixMultiplicationJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorMatrixMultiplicationJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VectorMatrixMultiplicationJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,139 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.function.Functions;
+import org.apache.mahout.math.hadoop.DistributedRowMatrix;
+
+/**
+ * <p>This class handles the three-way multiplication of the digonal matrix
+ * and the Markov transition matrix inherent in the Eigencuts algorithm.
+ * The equation takes the form:</p>
+ *
+ * {@code W = D^(1/2) * M * D^(1/2)}
+ *
+ * <p>Since the diagonal matrix D has only n non-zero elements, it is represented
+ * as a dense vector in this job, rather than a full n-by-n matrix. This job
+ * performs the multiplications and returns the new DRM.
+ */
+public final class VectorMatrixMultiplicationJob {
+
+  private VectorMatrixMultiplicationJob() {
+  }
+
+  /**
+   * Invokes the job.
+   * @param markovPath Path to the markov DRM's sequence files
+   */
+  public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path outputPath)
+    throws IOException, ClassNotFoundException, InterruptedException {
+    
+    return runJob(markovPath, diag, outputPath, new Path(outputPath, "tmp"));
+  }
+
+  public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path outputPath, Path tmpPath)
+    throws IOException, ClassNotFoundException, InterruptedException {
+
+    // set up the serialization of the diagonal vector
+    Configuration conf = new Configuration();
+    FileSystem fs = FileSystem.get(markovPath.toUri(), conf);
+    markovPath = fs.makeQualified(markovPath);
+    outputPath = fs.makeQualified(outputPath);
+    Path vectorOutputPath = new Path(outputPath.getParent(), "vector");
+    VectorCache.save(new IntWritable(Keys.DIAGONAL_CACHE_INDEX), diag, vectorOutputPath, conf);
+
+    // set up the job itself
+    Job job = new Job(conf, "VectorMatrixMultiplication");
+    job.setInputFormatClass(SequenceFileInputFormat.class);
+    job.setOutputKeyClass(IntWritable.class);
+    job.setOutputValueClass(VectorWritable.class);
+    job.setOutputFormatClass(SequenceFileOutputFormat.class);
+    job.setMapperClass(VectorMatrixMultiplicationMapper.class);
+    job.setNumReduceTasks(0);
+
+    FileInputFormat.addInputPath(job, markovPath);
+    FileOutputFormat.setOutputPath(job, outputPath);
+
+    job.setJarByClass(VectorMatrixMultiplicationJob.class);
+
+    boolean succeeded = job.waitForCompletion(true);
+    if (!succeeded) {
+      throw new IllegalStateException("Job failed!");
+    }
+
+    // build the resulting DRM from the results
+    return new DistributedRowMatrix(outputPath, tmpPath,
+        diag.size(), diag.size());
+  }
+  
+  public static class VectorMatrixMultiplicationMapper
+    extends Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
+    
+    private Vector diagonal;
+    
+    @Override
+    protected void setup(Context context) throws IOException, InterruptedException {
+      // read in the diagonal vector from the distributed cache
+      super.setup(context);
+      Configuration config = context.getConfiguration();
+      diagonal = VectorCache.load(config);
+      if (diagonal == null) {
+        throw new IOException("No vector loaded from cache!");
+      }
+      if (!(diagonal instanceof DenseVector)) {
+        diagonal = new DenseVector(diagonal);
+      }
+    }
+    
+    @Override
+    protected void map(IntWritable key, VectorWritable row, Context ctx) 
+      throws IOException, InterruptedException {
+      
+      for (Vector.Element e : row.get().all()) {
+        double dii = Functions.SQRT.apply(diagonal.get(key.get()));
+        double djj = Functions.SQRT.apply(diagonal.get(e.index()));
+        double mij = e.get();
+        e.set(dii * mij * djj);
+      }
+      ctx.write(key, row);
+    }
+    
+    /**
+     * Performs the setup of the Mapper. Used by unit tests.
+     * @param diag
+     */
+    void setup(Vector diag) {
+      this.diagonal = diag;
+    }
+  }
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VertexWritable.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VertexWritable.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VertexWritable.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/VertexWritable.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Represents a vertex within the affinity graph for Eigencuts.
+ */
+public class VertexWritable implements Writable {
+  
+  /** the row */
+  private int i;
+  
+  /** the column */
+  private int j;
+  
+  /** the value at this vertex */
+  private double value;
+  
+  /** an extra type delimeter, can probably be null */
+  private String type;
+  
+  public VertexWritable() {
+  }
+
+  public VertexWritable(int i, int j, double v, String t) {
+    this.i = i;
+    this.j = j;
+    this.value = v;
+    this.type = t;
+  }
+  
+  public int getRow() {
+    return i;
+  }
+  
+  public void setRow(int i) {
+    this.i = i;
+  }
+  
+  public int getCol() {
+    return j;
+  }
+  
+  public void setCol(int j) { 
+    this.j = j;
+  }
+  
+  public double getValue() {
+    return value;
+  }
+  
+  public void setValue(double v) {
+    this.value = v;
+  }
+  
+  public String getType() {
+    return type;
+  }
+  
+  public void setType(String t) {
+    this.type = t;
+  }
+  
+  @Override
+  public void readFields(DataInput arg0) throws IOException {
+    this.i = arg0.readInt();
+    this.j = arg0.readInt();
+    this.value = arg0.readDouble();
+    this.type = arg0.readUTF();
+  }
+
+  @Override
+  public void write(DataOutput arg0) throws IOException {
+    arg0.writeInt(i);
+    arg0.writeInt(j);
+    arg0.writeDouble(value);
+    arg0.writeUTF(type);
+  }
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.hadoop.DistributedRowMatrix;
-
-public final class AffinityMatrixInputJob {
-
-  private AffinityMatrixInputJob() {
-  }
-
-  /**
-   * Initializes and executes the job of reading the documents containing
-   * the data of the affinity matrix in (x_i, x_j, value) format.
-   */
-  public static void runJob(Path input, Path output, int rows, int cols)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    Configuration conf = new Configuration();
-    HadoopUtil.delete(conf, output);
-
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, rows);
-    Job job = new Job(conf, "AffinityMatrixInputJob: " + input + " -> M/R -> " + output);
-
-    job.setMapOutputKeyClass(IntWritable.class);
-    job.setMapOutputValueClass(DistributedRowMatrix.MatrixEntryWritable.class);
-    job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(VectorWritable.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapperClass(AffinityMatrixInputMapper.class);   
-    job.setReducerClass(AffinityMatrixInputReducer.class);
-
-    FileInputFormat.addInputPath(job, input);
-    FileOutputFormat.setOutputPath(job, output);
-
-    job.setJarByClass(AffinityMatrixInputJob.class);
-
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-  }
-
-  /**
-   * A transparent wrapper for the above method which handles the tedious tasks
-   * of setting and retrieving system Paths. Hands back a fully-populated
-   * and initialized DistributedRowMatrix.
-   */
-  public static DistributedRowMatrix runJob(Path input, Path output, int dimensions)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    Path seqFiles = new Path(output, "seqfiles-" + (System.nanoTime() & 0xFF));
-    runJob(input, seqFiles, dimensions, dimensions);
-    DistributedRowMatrix a = new DistributedRowMatrix(seqFiles,
-        new Path(seqFiles, "seqtmp-" + (System.nanoTime() & 0xFF)), 
-        dimensions, dimensions);
-    a.setConf(new Configuration());
-    return a;
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,78 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.math.hadoop.DistributedRowMatrix;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * <p>Handles reading the files representing the affinity matrix. Since the affinity
- * matrix is representative of a graph, each line in all the files should
- * take the form:</p>
- *
- * {@code i,j,value}
- *
- * <p>where {@code i} and {@code j} are the {@code i}th and
- * {@code j} data points in the entire set, and {@code value}
- * represents some measurement of their relative absolute magnitudes. This
- * is, simply, a method for representing a graph textually.
- */
-public class AffinityMatrixInputMapper
-    extends Mapper<LongWritable, Text, IntWritable, DistributedRowMatrix.MatrixEntryWritable> {
-
-  private static final Logger log = LoggerFactory.getLogger(AffinityMatrixInputMapper.class);
-
-  private static final Pattern COMMA_PATTERN = Pattern.compile(",");
-
-  @Override
-  protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
-
-    String[] elements = COMMA_PATTERN.split(value.toString());
-    log.debug("(DEBUG - MAP) Key[{}], Value[{}]", key.get(), value);
-
-    // enforce well-formed textual representation of the graph
-    if (elements.length != 3) {
-      throw new IOException("Expected input of length 3, received "
-                            + elements.length + ". Please make sure you adhere to "
-                            + "the structure of (i,j,value) for representing a graph in text. "
-                            + "Input line was: '" + value + "'.");
-    }
-    if (elements[0].isEmpty() || elements[1].isEmpty() || elements[2].isEmpty()) {
-      throw new IOException("Found an element of 0 length. Please be sure you adhere to the structure of "
-          + "(i,j,value) for  representing a graph in text.");
-    }
-
-    // parse the line of text into a DistributedRowMatrix entry,
-    // making the row (elements[0]) the key to the Reducer, and
-    // setting the column (elements[1]) in the entry itself
-    DistributedRowMatrix.MatrixEntryWritable toAdd = new DistributedRowMatrix.MatrixEntryWritable();
-    IntWritable row = new IntWritable(Integer.valueOf(elements[0]));
-    toAdd.setRow(-1); // already set as the Reducer's key
-    toAdd.setCol(Integer.valueOf(elements[1]));
-    toAdd.setVal(Double.valueOf(elements[2]));
-    context.write(row, toAdd);
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/AffinityMatrixInputReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,60 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.SequentialAccessSparseVector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.hadoop.DistributedRowMatrix;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Tasked with taking each DistributedRowMatrix entry and collecting them
- * into vectors corresponding to rows. The input and output keys are the same,
- * corresponding to the row in the ensuing matrix. The matrix entries are
- * entered into a vector according to the column to which they belong, and
- * the vector is then given the key corresponding to its row.
- */
-public class AffinityMatrixInputReducer
-    extends Reducer<IntWritable, DistributedRowMatrix.MatrixEntryWritable, IntWritable, VectorWritable> {
-
-  private static final Logger log = LoggerFactory.getLogger(AffinityMatrixInputReducer.class);
-
-  @Override
-  protected void reduce(IntWritable row, Iterable<DistributedRowMatrix.MatrixEntryWritable> values, Context context)
-    throws IOException, InterruptedException {
-    int size = context.getConfiguration().getInt(EigencutsKeys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE);
-    RandomAccessSparseVector out = new RandomAccessSparseVector(size, 100);
-
-    for (DistributedRowMatrix.MatrixEntryWritable element : values) {
-      out.setQuick(element.getCol(), element.getVal());
-      if (log.isDebugEnabled()) {
-        log.debug("(DEBUG - REDUCE) Row[{}], Column[{}], Value[{}]",
-                  row.get(), element.getCol(), element.getVal());
-      }
-    }
-    SequentialAccessSparseVector output = new SequentialAccessSparseVector(out);
-    context.write(row, new VectorWritable(output));
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/IntDoublePairWritable.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/IntDoublePairWritable.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/IntDoublePairWritable.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/IntDoublePairWritable.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-
-/**
- * This class is a Writable implementation of the mahout.common.Pair
- * generic class. Since the generic types would also themselves have to
- * implement Writable, it made more sense to create a more specialized
- * version of the class altogether.
- * 
- * In essence, this can be treated as a single Vector Element.
- */
-public class IntDoublePairWritable implements Writable {
-  
-  private int key;
-  private double value;
-  
-  public IntDoublePairWritable() {
-  }
-  
-  public IntDoublePairWritable(int k, double v) {
-    this.key = k;
-    this.value = v;
-  }
-  
-  public void setKey(int k) {
-    this.key = k;
-  }
-  
-  public void setValue(double v) {
-    this.value = v;
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    this.key = in.readInt();
-    this.value = in.readDouble();
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    out.writeInt(key);
-    out.writeDouble(value);
-  }
-
-  public int getKey() {
-    return key;
-  }
-
-  public double getValue() {
-    return value;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/MatrixDiagonalizeJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-/**
- * Given a matrix, this job returns a vector whose i_th element is the 
- * sum of all the elements in the i_th row of the original matrix.
- */
-public final class MatrixDiagonalizeJob {
-
-  private MatrixDiagonalizeJob() {
-  }
-
-  public static Vector runJob(Path affInput, int dimensions)
-    throws IOException, ClassNotFoundException, InterruptedException {
-    
-    // set up all the job tasks
-    Configuration conf = new Configuration();
-    Path diagOutput = new Path(affInput.getParent(), "diagonal");
-    HadoopUtil.delete(conf, diagOutput);
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, dimensions);
-    Job job = new Job(conf, "MatrixDiagonalizeJob");
-    
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setMapOutputKeyClass(NullWritable.class);
-    job.setMapOutputValueClass(IntDoublePairWritable.class);
-    job.setOutputKeyClass(NullWritable.class);
-    job.setOutputValueClass(VectorWritable.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapperClass(MatrixDiagonalizeMapper.class);
-    job.setReducerClass(MatrixDiagonalizeReducer.class);
-    
-    FileInputFormat.addInputPath(job, affInput);
-    FileOutputFormat.setOutputPath(job, diagOutput);
-    
-    job.setJarByClass(MatrixDiagonalizeJob.class);
-
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-    // read the results back from the path
-    return VectorCache.load(conf, new Path(diagOutput, "part-r-00000"));
-  }
-  
-  public static class MatrixDiagonalizeMapper
-    extends Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable> {
-    
-    @Override
-    protected void map(IntWritable key, VectorWritable row, Context context) 
-      throws IOException, InterruptedException {
-      // store the sum
-      IntDoublePairWritable store = new IntDoublePairWritable(key.get(), row.get().zSum());
-      context.write(NullWritable.get(), store);
-    }
-  }
-  
-  public static class MatrixDiagonalizeReducer
-    extends Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable> {
-    
-    @Override
-    protected void reduce(NullWritable key, Iterable<IntDoublePairWritable> values,
-      Context context) throws IOException, InterruptedException {
-      // create the return vector
-      Vector retval = new DenseVector(context.getConfiguration().getInt(
-          EigencutsKeys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE));
-      // put everything in its correct spot
-      for (IntDoublePairWritable e : values) {
-        retval.setQuick(e.getKey(), e.getValue());
-      }
-      // write it out
-      context.write(key, new VectorWritable(retval));
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/UnitVectorizerJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,79 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.math.VectorWritable;
-
-/**
- * <p>Given a DistributedRowMatrix, this job normalizes each row to unit
- * vector length. If the input is a matrix U, and the output is a matrix
- * W, the job follows:</p>
- *
- * <p>{@code v_ij = u_ij / sqrt(sum_j(u_ij * u_ij))}</p>
- */
-public final class UnitVectorizerJob {
-
-  private UnitVectorizerJob() {
-  }
-
-  public static void runJob(Path input, Path output)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    
-    Configuration conf = new Configuration();
-    Job job = new Job(conf, "UnitVectorizerJob");
-    
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(VectorWritable.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapperClass(UnitVectorizerMapper.class);
-    job.setNumReduceTasks(0);
-    
-    FileInputFormat.addInputPath(job, input);
-    FileOutputFormat.setOutputPath(job, output);
-
-    job.setJarByClass(UnitVectorizerJob.class);
-
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-  }
-  
-  public static class UnitVectorizerMapper
-    extends Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
-    
-    @Override
-    protected void map(IntWritable row, VectorWritable vector, Context context) 
-      throws IOException, InterruptedException {
-      context.write(row, new VectorWritable(vector.get().normalize(2)));
-    }
-
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorCache.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorCache.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorCache.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorCache.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,123 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-import java.net.URI;
-import java.util.Arrays;
-
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.filecache.DistributedCache;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Writable;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-
-/**
- * This class handles reading and writing vectors to the Hadoop
- * distributed cache. Created as a result of Eigencuts' liberal use
- * of such functionality, but available to any algorithm requiring it.
- */
-public final class VectorCache {
-
-  private static final Logger log = LoggerFactory.getLogger(VectorCache.class);
-
-  private VectorCache() {
-  }
-
-  /**
-   * @param key    SequenceFile key
-   * @param vector Vector to save, to be wrapped as VectorWritable
-   */
-  public static void save(Writable key,
-                          Vector vector,
-                          Path output,
-                          Configuration conf,
-                          boolean overwritePath,
-                          boolean deleteOnExit) throws IOException {
-
-    FileSystem fs = FileSystem.get(output.toUri(), conf);
-    output = fs.makeQualified(output);
-    if (overwritePath) {
-      HadoopUtil.delete(conf, output);
-    }
-
-    // set the cache
-    DistributedCache.setCacheFiles(new URI[]{output.toUri()}, conf);
-
-    // set up the writer
-    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
-            IntWritable.class, VectorWritable.class);
-    try {
-      writer.append(key, new VectorWritable(vector));
-    } finally {
-      Closeables.close(writer, false);
-    }
-
-    if (deleteOnExit) {
-      fs.deleteOnExit(output);
-    }
-  }
-
-  /**
-   * Calls the save() method, setting the cache to overwrite any previous
-   * Path and to delete the path after exiting
-   */
-  public static void save(Writable key, Vector vector, Path output, Configuration conf) throws IOException {
-    save(key, vector, output, conf, true, true);
-  }
-
-  /**
-   * Loads the vector from {@link DistributedCache}. Returns null if no vector exists.
-   */
-  public static Vector load(Configuration conf) throws IOException {
-    Path[] files = HadoopUtil.getCachedFiles(conf);
-
-    if (files.length != 1) {
-      throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');
-    }
-
-    if (log.isInfoEnabled()) {
-      log.info("Files are: {}", Arrays.toString(files));
-    }
-    return load(conf, files[0]);
-  }
-
-  /**
-   * Loads a Vector from the specified path. Returns null if no vector exists.
-   */
-  public static Vector load(Configuration conf, Path input) throws IOException {
-    log.info("Loading vector from: {}", input);
-    SequenceFileValueIterator<VectorWritable> iterator =
-            new SequenceFileValueIterator<VectorWritable>(input, true, conf);
-    try {
-      return iterator.next().get();
-    } finally {
-      Closeables.close(iterator, true);
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VectorMatrixMultiplicationJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,140 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.function.Functions;
-import org.apache.mahout.math.hadoop.DistributedRowMatrix;
-
-/**
- * <p>This class handles the three-way multiplication of the digonal matrix
- * and the Markov transition matrix inherent in the Eigencuts algorithm.
- * The equation takes the form:</p>
- *
- * {@code W = D^(1/2) * M * D^(1/2)}
- *
- * <p>Since the diagonal matrix D has only n non-zero elements, it is represented
- * as a dense vector in this job, rather than a full n-by-n matrix. This job
- * performs the multiplications and returns the new DRM.
- */
-public final class VectorMatrixMultiplicationJob {
-
-  private VectorMatrixMultiplicationJob() {
-  }
-
-  /**
-   * Invokes the job.
-   * @param markovPath Path to the markov DRM's sequence files
-   */
-  public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path outputPath)
-    throws IOException, ClassNotFoundException, InterruptedException {
-    
-    return runJob(markovPath, diag, outputPath, new Path(outputPath, "tmp"));
-  }
-
-  public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path outputPath, Path tmpPath)
-    throws IOException, ClassNotFoundException, InterruptedException {
-
-    // set up the serialization of the diagonal vector
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(markovPath.toUri(), conf);
-    markovPath = fs.makeQualified(markovPath);
-    outputPath = fs.makeQualified(outputPath);
-    Path vectorOutputPath = new Path(outputPath.getParent(), "vector");
-    VectorCache.save(new IntWritable(EigencutsKeys.DIAGONAL_CACHE_INDEX), diag, vectorOutputPath, conf);
-
-    // set up the job itself
-    Job job = new Job(conf, "VectorMatrixMultiplication");
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(VectorWritable.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapperClass(VectorMatrixMultiplicationMapper.class);
-    job.setNumReduceTasks(0);
-
-    FileInputFormat.addInputPath(job, markovPath);
-    FileOutputFormat.setOutputPath(job, outputPath);
-
-    job.setJarByClass(VectorMatrixMultiplicationJob.class);
-
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-    // build the resulting DRM from the results
-    return new DistributedRowMatrix(outputPath, tmpPath,
-        diag.size(), diag.size());
-  }
-  
-  public static class VectorMatrixMultiplicationMapper
-    extends Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable> {
-    
-    private Vector diagonal;
-    
-    @Override
-    protected void setup(Context context) throws IOException, InterruptedException {
-      // read in the diagonal vector from the distributed cache
-      super.setup(context);
-      Configuration config = context.getConfiguration();
-      diagonal = VectorCache.load(config);
-      if (diagonal == null) {
-        throw new IOException("No vector loaded from cache!");
-      }
-      if (!(diagonal instanceof DenseVector)) {
-        diagonal = new DenseVector(diagonal);
-      }
-    }
-    
-    @Override
-    protected void map(IntWritable key, VectorWritable row, Context ctx) 
-      throws IOException, InterruptedException {
-      
-      for (Vector.Element e : row.get().all()) {
-        double dii = Functions.SQRT.apply(diagonal.get(key.get()));
-        double djj = Functions.SQRT.apply(diagonal.get(e.index()));
-        double mij = e.get();
-        e.set(dii * mij * djj);
-      }
-      ctx.write(key, row);
-    }
-    
-    /**
-     * Performs the setup of the Mapper. Used by unit tests.
-     * @param diag
-     */
-    void setup(Vector diag) {
-      this.diagonal = diag;
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VertexWritable.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VertexWritable.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VertexWritable.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/common/VertexWritable.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-
-/**
- * Represents a vertex within the affinity graph for Eigencuts.
- */
-public class VertexWritable implements Writable {
-  
-  /** the row */
-  private int i;
-  
-  /** the column */
-  private int j;
-  
-  /** the value at this vertex */
-  private double value;
-  
-  /** an extra type delimeter, can probably be null */
-  private String type;
-  
-  public VertexWritable() {
-  }
-
-  public VertexWritable(int i, int j, double v, String t) {
-    this.i = i;
-    this.j = j;
-    this.value = v;
-    this.type = t;
-  }
-  
-  public int getRow() {
-    return i;
-  }
-  
-  public void setRow(int i) {
-    this.i = i;
-  }
-  
-  public int getCol() {
-    return j;
-  }
-  
-  public void setCol(int j) { 
-    this.j = j;
-  }
-  
-  public double getValue() {
-    return value;
-  }
-  
-  public void setValue(double v) {
-    this.value = v;
-  }
-  
-  public String getType() {
-    return type;
-  }
-  
-  public void setType(String t) {
-    this.type = t;
-  }
-  
-  @Override
-  public void readFields(DataInput arg0) throws IOException {
-    this.i = arg0.readInt();
-    this.j = arg0.readInt();
-    this.value = arg0.readDouble();
-    this.type = arg0.readUTF();
-  }
-
-  @Override
-  public void write(DataOutput arg0) throws IOException {
-    arg0.writeInt(i);
-    arg0.writeInt(j);
-    arg0.writeDouble(value);
-    arg0.writeUTF(type);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsAffinityCutsJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,215 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.clustering.spectral.common.VertexWritable;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-@Deprecated
-public final class EigencutsAffinityCutsJob {
-
-  private static final Logger log = LoggerFactory.getLogger(EigencutsAffinityCutsJob.class);
-
-  private EigencutsAffinityCutsJob() {
-  }
-
-  enum CUTSCOUNTER {
-    NUM_CUTS
-  }
-
-  /**
-   * Runs a single iteration of defining cluster boundaries, based on
-   * previous calculations and the formation of the "cut matrix".
-   * 
-   * @param currentAffinity Path to the current affinity matrix.
-   * @param cutMatrix Path to the sensitivity matrix.
-   * @param nextAffinity Output path for the new affinity matrix.
-   */
-  public static long runjob(Path currentAffinity, Path cutMatrix, Path nextAffinity, Configuration conf)
-    throws IOException, ClassNotFoundException, InterruptedException {
-    
-    // these options allow us to differentiate between the two vectors
-    // in the mapper and reducer - we'll know from the working path
-    // which SequenceFile we're accessing
-    conf.set(EigencutsKeys.AFFINITY_PATH, currentAffinity.getName());
-    conf.set(EigencutsKeys.CUTMATRIX_PATH, cutMatrix.getName());
-    
-    Job job = new Job(conf, "EigencutsAffinityCutsJob");
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapOutputKeyClass(Text.class);
-    job.setMapOutputValueClass(VertexWritable.class);
-    job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(VectorWritable.class);
-    job.setMapperClass(EigencutsAffinityCutsMapper.class);
-    job.setCombinerClass(EigencutsAffinityCutsCombiner.class);
-    job.setReducerClass(EigencutsAffinityCutsReducer.class);
-    
-    //FileInputFormat.addInputPath(job, currentAffinity);
-    FileInputFormat.addInputPath(job, cutMatrix);
-    FileOutputFormat.setOutputPath(job, nextAffinity);
-    
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-    return job.getCounters().findCounter(CUTSCOUNTER.NUM_CUTS).getValue();
-  }
-  
-  public static class EigencutsAffinityCutsMapper
-    extends Mapper<IntWritable, VectorWritable, Text, VertexWritable> {
-    
-    @Override
-    protected void map(IntWritable key, VectorWritable row, Context context) 
-      throws IOException, InterruptedException {
-      
-      // all this method does is construct a bunch of vertices, mapping those
-      // together which have the same *combination* of indices; for example,
-      // (1, 3) will have the same key as (3, 1) but a different key from (1, 1)
-      // and (3, 3) (which, incidentally, will also not be grouped together)
-      String type = context.getWorkingDirectory().getName();
-      Vector vector = row.get();
-      for (Vector.Element e : vector.all()) {
-        String newkey = Math.max(key.get(), e.index()) + "_" + Math.min(key.get(), e.index());
-        context.write(new Text(newkey), new VertexWritable(key.get(), e.index(), e.get(), type));
-      }
-    }
-  }
-  
-  public static class EigencutsAffinityCutsCombiner
-    extends Reducer<Text, VertexWritable, Text, VertexWritable> {
-    
-    @Override
-    protected void reduce(Text t, Iterable<VertexWritable> vertices, 
-        Context context) throws IOException, InterruptedException {
-      // there should be exactly 4 items in the iterable; two from the
-      // first Path source, and two from the second with matching (i, j) indices
-      
-      // the idea here is that we want the two vertices of the "cut" matrix,
-      // and if either of them has a non-zero value, we want to:
-      //
-      // 1) zero out the two affinity vertices, and 
-      // 2) add their former values to the (i, i) and (j, j) coordinates
-      //
-      // though obviously we want to perform these steps in reverse order
-      Configuration conf = context.getConfiguration();
-      log.debug("{}", t);
-      boolean zero = false;
-      int i = -1;
-      int j = -1;
-      double k = 0;
-      int count = 0;
-      for (VertexWritable v : vertices) {
-        count++;
-        if (v.getType().equals(conf.get(EigencutsKeys.AFFINITY_PATH))) {
-          i = v.getRow();
-          j = v.getCol();
-          k = v.getValue();
-        } else if (v.getValue() != 0.0) {
-          zero = true;
-        }
-      }
-      // if there are only two vertices, we have a diagonal
-      // we want to preserve whatever is currently in the diagonal,
-      // since this is acting as a running sum of all other values
-      // that have been "cut" so far - simply return this element as is
-      if (count == 2) {
-        VertexWritable vw = new VertexWritable(i, j, k, "unimportant");
-        context.write(new Text(String.valueOf(i)), vw);
-        return;
-      }
-      
-      // do we zero out the values?
-      VertexWritable outI = new VertexWritable();
-      VertexWritable outJ = new VertexWritable();
-      if (zero) {
-        // increment the cut counter
-        context.getCounter(CUTSCOUNTER.NUM_CUTS).increment(1);
-        
-        // we want the values to exist on the diagonal
-        outI.setCol(i);
-        outJ.setCol(j);
-        
-        // also, set the old values to zero
-        VertexWritable zeroI = new VertexWritable();
-        VertexWritable zeroJ = new VertexWritable();
-        zeroI.setCol(j);
-        zeroI.setValue(0);
-        zeroJ.setCol(i);
-        zeroJ.setValue(0);
-        zeroI.setType("unimportant");
-        zeroJ.setType("unimportant");
-        context.write(new Text(String.valueOf(i)), zeroI);
-        context.write(new Text(String.valueOf(j)), zeroJ);
-      } else {
-        outI.setCol(j);
-        outJ.setCol(i);
-      }
-      
-      // set the values and write them
-      outI.setValue(k);
-      outJ.setValue(k);
-      outI.setType("unimportant");
-      outJ.setType("unimportant");
-      context.write(new Text(String.valueOf(i)), outI);
-      context.write(new Text(String.valueOf(j)), outJ);
-    }
-  }
-  
-  public static class EigencutsAffinityCutsReducer 
-    extends Reducer<Text, VertexWritable, IntWritable, VectorWritable> {
-    
-    @Override
-    protected void reduce(Text row, Iterable<VertexWritable> entries, 
-        Context context) throws IOException, InterruptedException {
-      // now to assemble the vectors
-      RandomAccessSparseVector output = new RandomAccessSparseVector(
-          context.getConfiguration().getInt(EigencutsKeys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE), 100);
-      int rownum = Integer.parseInt(row.toString());
-      for (VertexWritable e : entries) {
-        // first, are we setting a diagonal?
-        if (e.getCol() == rownum) {
-          // add to what's already present
-          output.setQuick(e.getCol(), output.getQuick(e.getCol()) + e.getValue());
-        } else {
-          // simply set the value
-          output.setQuick(e.getCol(), e.getValue());
-        }
-      }
-      context.write(new IntWritable(rownum), new VectorWritable(output));
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsDriver.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,226 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.spectral.common.AffinityMatrixInputJob;
-import org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob;
-import org.apache.mahout.clustering.spectral.common.VectorMatrixMultiplicationJob;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.decomposer.lanczos.LanczosState;
-import org.apache.mahout.math.hadoop.DistributedRowMatrix;
-import org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver;
-import org.apache.mahout.math.hadoop.decomposer.EigenVerificationJob;
-import org.apache.mahout.math.stats.OnlineSummarizer;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
-
-@Deprecated
-public class EigencutsDriver extends AbstractJob {
-
-  public static final double EPSILON_DEFAULT = 0.25;
-
-  public static final double TAU_DEFAULT = -0.1;
-
-  public static final double OVERSHOOT_MULTIPLIER = 1.5;
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new EigencutsDriver(), args);
-  }
-
-  @Override
-  public int run(String[] arg0) throws Exception {
-
-    // set up command line arguments
-    addOption("half-life", "b", "Minimal half-life threshold", true);
-    addOption("dimensions", "d", "Square dimensions of affinity matrix", true);
-    addOption("epsilon", "e", "Half-life threshold coefficient", Double.toString(EPSILON_DEFAULT));
-    addOption("tau", "t", "Threshold for cutting affinities", Double.toString(TAU_DEFAULT));
-    addOption("eigenrank", "k", "Number of top eigenvectors to use", true);
-    addOption(DefaultOptionCreator.inputOption().create());
-    addOption(DefaultOptionCreator.outputOption().create());
-    addOption(DefaultOptionCreator.overwriteOption().create());
-    Map<String, List<String>> parsedArgs = parseArguments(arg0);
-    if (parsedArgs == null) {
-      return 0;
-    }
-
-    // read in the command line values
-    Path input = getInputPath();
-    Path output = getOutputPath();
-    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(getConf(), output);
-    }
-    int dimensions = Integer.parseInt(getOption("dimensions"));
-    double halflife = Double.parseDouble(getOption("half-life"));
-    double epsilon = Double.parseDouble(getOption("epsilon"));
-    double tau = Double.parseDouble(getOption("tau"));
-    int eigenrank = Integer.parseInt(getOption("eigenrank"));
-
-    run(getConf(), input, output, eigenrank, dimensions, halflife, epsilon, tau);
-
-    return 0;
-  }
-
-  /**
-   * Run the Eigencuts clustering algorithm using the supplied arguments
-   * 
-   * @param conf the Configuration to use
-   * @param input the Path to the directory containing input affinity tuples
-   * @param output the Path to the output directory
-   * @param eigenrank The number of top eigenvectors/eigenvalues to use
-   * @param dimensions the int number of dimensions of the square affinity matrix
-   * @param halflife the double minimum half-life threshold
-   * @param epsilon the double coefficient for setting minimum half-life threshold
-   * @param tau the double tau threshold for cutting links in the affinity graph
-   */
-  public static void run(Configuration conf,
-                         Path input,
-                         Path output,
-                         int dimensions,
-                         int eigenrank,
-                         double halflife,
-                         double epsilon,
-                         double tau)
-    throws IOException, InterruptedException, ClassNotFoundException {
-    // set the instance variables
-    // create a few new Paths for temp files and transformations
-    Path outputCalc = new Path(output, "calculations");
-    Path outputTmp = new Path(output, "temporary");
-
-    DistributedRowMatrix A = AffinityMatrixInputJob.runJob(input, outputCalc, dimensions);
-    Vector D = MatrixDiagonalizeJob.runJob(A.getRowPath(), dimensions);
-
-    long numCuts;
-    do {
-      // first three steps are the same as spectral k-means:
-      // 1) calculate D from A
-      // 2) calculate L = D^-0.5 * A * D^-0.5
-      // 3) calculate eigenvectors of L
-
-      DistributedRowMatrix L =
-          VectorMatrixMultiplicationJob.runJob(A.getRowPath(), D,
-              new Path(outputCalc, "laplacian-" + (System.nanoTime() & 0xFF)));
-      L.setConf(new Configuration(conf));
-
-      // eigendecomposition (step 3)
-      int overshoot = (int) ((double) eigenrank * OVERSHOOT_MULTIPLIER);
-      LanczosState state = new LanczosState(L, eigenrank,
-          DistributedLanczosSolver.getInitialVector(L));
-
-      DistributedRowMatrix U = performEigenDecomposition(conf, L, state, eigenrank, overshoot, outputCalc);
-      U.setConf(new Configuration(conf));
-      List<Double> eigenValues = Lists.newArrayList();
-      for (int i = 0; i < eigenrank; i++) {
-        eigenValues.set(i, state.getSingularValue(i));
-      }
-
-      // here's where things get interesting: steps 4, 5, and 6 are unique
-      // to this algorithm, and depending on the final output, steps 1-3
-      // may be repeated as well
-
-      // helper method, since apparently List and Vector objects don't play nicely
-      Vector evs = listToVector(eigenValues);
-
-      // calculate sensitivities (step 4 and step 5)
-      Path sensitivities = new Path(outputCalc, "sensitivities-" + (System.nanoTime() & 0xFF));
-      EigencutsSensitivityJob.runJob(evs, D, U.getRowPath(), halflife, tau, median(D), epsilon, sensitivities);
-
-      // perform the cuts (step 6)
-      input = new Path(outputTmp, "nextAff-" + (System.nanoTime() & 0xFF));
-      numCuts = EigencutsAffinityCutsJob.runjob(A.getRowPath(), sensitivities, input, conf);
-
-      // how many cuts were made?
-      if (numCuts > 0) {
-        // recalculate A
-        A = new DistributedRowMatrix(input,
-                                     new Path(outputTmp, Long.toString(System.nanoTime())), dimensions, dimensions);
-        A.setConf(new Configuration());
-      }
-    } while (numCuts > 0);
-
-    // TODO: MAHOUT-517: Eigencuts needs an output format
-  }
-
-  /**
-   * Does most of the heavy lifting in setting up Paths, configuring return
-   * values, and generally performing the tedious administrative tasks involved
-   * in an eigen-decomposition and running the verifier
-   */
-  public static DistributedRowMatrix performEigenDecomposition(Configuration conf,
-                                                               DistributedRowMatrix input,
-                                                               LanczosState state,
-                                                               int numEigenVectors,
-                                                               int overshoot,
-                                                               Path tmp) throws IOException {
-    DistributedLanczosSolver solver = new DistributedLanczosSolver();
-    Path seqFiles = new Path(tmp, "eigendecomp-" + (System.nanoTime() & 0xFF));
-    solver.runJob(conf,
-                  state,
-                  overshoot,
-                  true,
-                  seqFiles.toString());
-
-    // now run the verifier to trim down the number of eigenvectors
-    EigenVerificationJob verifier = new EigenVerificationJob();
-    Path verifiedEigens = new Path(tmp, "verifiedeigens");
-    verifier.runJob(conf, seqFiles, input.getRowPath(), verifiedEigens, false, 1.0, numEigenVectors);
-    Path cleanedEigens = verifier.getCleanedEigensPath();
-    return new DistributedRowMatrix(cleanedEigens, new Path(cleanedEigens, "tmp"), numEigenVectors, input.numRows());
-  }
-
-  /**
-   * A quick and dirty hack to compute the median of a vector...
-   * @param v
-   * @return
-   */
-  private static double median(Vector v) {
-    if (v.size() < 100) {
-      return v.zSum() / v.size();
-    }
-    OnlineSummarizer med = new OnlineSummarizer();
-    for (Vector.Element e : v.all()) {
-      med.add(e.get());
-    }
-    return med.getMedian();
-  }
-
-  /**
-   * Iteratively loops through the list, converting it to a Vector of double
-   * primitives worthy of other Mahout operations
-   */
-  private static Vector listToVector(Collection<Double> list) {
-    Vector retval = new DenseVector(list.size());
-    int index = 0;
-    for (Double d : list) {
-      retval.setQuick(index++, d);
-    }
-    return retval;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsKeys.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsKeys.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsKeys.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsKeys.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,87 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-/**
- * Configuration keys for the Eigencuts algorithm (analogous to KMeansConfigKeys)
- */
-@Deprecated
-public final class EigencutsKeys {
-
-  private EigencutsKeys() {}
-
-  /**
-   * B_0, or the user-specified minimum eigenflow half-life threshold
-   * for an eigenvector/eigenvalue pair to be considered. Increasing
-   * B_0 equates to fewer clusters
-   */
-  public static final String BETA = "org.apache.mahout.clustering.spectral.beta";
-
-  /**
-   * Tau, or the user-specified threshold for making cuts (setting edge
-   * affinities to 0) after performing non-maximal suppression on edge weight
-   * sensitivies. Increasing tau equates to more edge cuts
-   */
-  public static final String TAU = "org.apache.mahout.clustering.spectral.tau";
-
-  /**
-   * The normalization factor for computing the cut threshold
-   */
-  public static final String DELTA = "org.apache.mahout.clustering.spectral.delta";
-
-  /**
-   * Epsilon, or the user-specified coefficient that works in tandem with
-   * MINIMUM_HALF_LIFE to determine which eigenvector/eigenvalue pairs to use.
-   * Increasing epsilon equates to fewer eigenvector/eigenvalue pairs
-   */
-  public static final String EPSILON = "org.apache.mahout.clustering.spectral.epsilon";
-
-  /**
-   * Base path to the location on HDFS where the diagonal matrix (a vector)
-   * and the list of eigenvalues will be stored for one of the map/reduce
-   * jobs in Eigencuts.
-   */
-  public static final String VECTOR_CACHE_BASE = "org.apache.mahout.clustering.spectral.eigencuts.vectorcache";
-
-  /**
-   * Refers to the dimensions of the raw affinity matrix input. Since this
-   * matrix is symmetrical, it is a square matrix, hence all its dimensions
-   * are equal.
-   */
-  public static final String AFFINITY_DIMENSIONS = "org.apache.mahout.clustering.spectral.eigencuts.affinitydimensions";
-
-  /**
-   * Refers to the Path to the SequenceFile representing the affinity matrix
-   */
-  public static final String AFFINITY_PATH = "org.apache.mahout.clustering.spectral.eigencuts.affinitypath";
-
-  /**
-   * Refers to the Path to the SequenceFile representing the cut matrix
-   */
-  public static final String CUTMATRIX_PATH = "org.apache.mahout.clustering.spectral.eigencuts.cutmatrixpath";
-
-  /**
-   * Sets the SequenceFile index for the list of eigenvalues.
-   */
-  public static final int EIGENVALUES_CACHE_INDEX = 0;
-
-  /**
-   * Sets the SequenceFile index for the diagonal matrix.
-   */
-  public static final int DIAGONAL_CACHE_INDEX = 1;
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.clustering.spectral.common.VectorCache;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-/**
- * <p>There are a quite a few operations bundled within this mapper. Gather 'round
- * and listen, all of ye.</p>
- * 
- * <p>The input to this job is eight items:</p>
- * <ol><li>B<sub>0</sub>, which is a command-line parameter fed through the Configuration object</li>
- * <li>diagonal matrix, a constant vector fed through the Hadoop cache</li>
- * <li>list of eigenvalues, a constant vector fed through the Hadoop cache</li>
- * <li>eigenvector, the input value to the mapper</li>
- * <li>epsilon</li>
- * <li>delta</li>
- * <li>tau</li>
- * <li>output, the Path to the output matrix of sensitivities</li></ol>
- * 
- * <p>The first three items are constant and are used in all of the map
- * tasks. The row index indicates which eigenvalue from the list to use, and
- * also serves as the output identifier. The diagonal matrix and the 
- * eigenvector are both of equal length and are iterated through twice
- * within each map task, unfortunately lending each task to a runtime of 
- * n<sup>2</sup>. This is unavoidable.</p>
- * 
- * <p>For each (i, j) combination of elements within the eigenvector, a complex
- * equation is run that explicitly computes the sensitivity to perturbation of 
- * the flow of probability within the specific edge of the graph. Each
- * sensitivity, as it is computed, is simultaneously applied to a non-maximal
- * suppression step: for a given sensitivity S_ij, it must be suppressed if
- * any other S_in or S_mj has a more negative value. Thus, only the most
- * negative S_ij within its row i or its column j is stored in the return
- * array, leading to an output (per eigenvector!) with maximum length n, 
- * minimum length 1.</p>
- * 
- * <p>Overall, this creates an n-by-n (possibly sparse) matrix with a maximum
- * of n^2 non-zero elements, minimum of n non-zero elements.</p>
- */
-@Deprecated
-public final class EigencutsSensitivityJob {
-
-  private EigencutsSensitivityJob() {
-  }
-
-  /**
-   * Initializes the configuration tasks, loads the needed data into
-   * the HDFS cache, and executes the job.
-   * 
-   * @param eigenvalues Vector of eigenvalues
-   * @param diagonal Vector representing the diagonal matrix
-   * @param eigenvectors Path to the DRM of eigenvectors
-   * @param output Path to the output matrix (will have between n and full-rank
-   *                non-zero elements)
-   */
-  public static void runJob(Vector eigenvalues,
-                            Vector diagonal,
-                            Path eigenvectors,
-                            double beta,
-                            double tau,
-                            double delta,
-                            double epsilon,
-                            Path output)
-    throws IOException, ClassNotFoundException, InterruptedException {
-    
-    // save the two vectors to the distributed cache
-    Configuration jobConfig = new Configuration();
-    Path eigenOutputPath = new Path(output.getParent(), "eigenvalues");
-    Path diagOutputPath = new Path(output.getParent(), "diagonal");
-    jobConfig.set(EigencutsKeys.VECTOR_CACHE_BASE, output.getParent().getName());
-    VectorCache.save(new IntWritable(EigencutsKeys.EIGENVALUES_CACHE_INDEX), 
-        eigenvalues, eigenOutputPath, jobConfig);
-    VectorCache.save(new IntWritable(EigencutsKeys.DIAGONAL_CACHE_INDEX), 
-        diagonal, diagOutputPath, jobConfig);
-    
-    // set up the rest of the job
-    jobConfig.set(EigencutsKeys.BETA, Double.toString(beta));
-    jobConfig.set(EigencutsKeys.EPSILON, Double.toString(epsilon));
-    jobConfig.set(EigencutsKeys.DELTA, Double.toString(delta));
-    jobConfig.set(EigencutsKeys.TAU, Double.toString(tau));
-    
-    Job job = new Job(jobConfig, "EigencutsSensitivityJob");
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setMapOutputKeyClass(IntWritable.class);
-    job.setMapOutputValueClass(EigencutsSensitivityNode.class);
-    job.setOutputKeyClass(IntWritable.class);
-    job.setOutputValueClass(VectorWritable.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapperClass(EigencutsSensitivityMapper.class);
-    job.setReducerClass(EigencutsSensitivityReducer.class);
-    
-    FileInputFormat.addInputPath(job, eigenvectors);
-    FileOutputFormat.setOutputPath(job, output);
-    
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-  }  
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.io.IOException;
-import java.util.Map;
-
-import com.google.common.collect.Maps;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.spectral.common.VectorCache;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.SequentialAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.function.Functions;
-
-@Deprecated
-public class EigencutsSensitivityMapper extends
-    Mapper<IntWritable, VectorWritable, IntWritable, EigencutsSensitivityNode> {
-
-  private Vector eigenvalues;
-  private Vector diagonal;
-  private double beta0;
-  private double epsilon;
-  
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    Configuration config = context.getConfiguration();
-    beta0 = Double.parseDouble(config.get(EigencutsKeys.BETA));
-    epsilon = Double.parseDouble(config.get(EigencutsKeys.EPSILON));
-    
-    // read in the two vectors from the cache
-    eigenvalues = VectorCache.load(config);
-    diagonal = VectorCache.load(config);
-    if (!(eigenvalues instanceof SequentialAccessSparseVector || eigenvalues instanceof DenseVector)) {
-      eigenvalues = new SequentialAccessSparseVector(eigenvalues);
-    }
-    if (!(diagonal instanceof SequentialAccessSparseVector || diagonal instanceof DenseVector)) {
-      diagonal = new SequentialAccessSparseVector(diagonal);
-    }
-  }
-  
-  @Override
-  protected void map(IntWritable row, VectorWritable vw, Context context) 
-    throws IOException, InterruptedException {
-    
-    // first, does this particular eigenvector even pass the required threshold?
-    double eigenvalue = Math.abs(eigenvalues.get(row.get()));
-    double betak = -Functions.LOGARITHM.apply(2) / Functions.LOGARITHM.apply(eigenvalue);
-    if (eigenvalue >= 1.0 || betak <= epsilon * beta0) {
-      // doesn't pass the threshold! quit
-      return;
-    }
-    
-    // go through the vector, performing the calculations
-    // sadly, no way to get around n^2 computations      
-    Map<Integer, EigencutsSensitivityNode> columns = Maps.newHashMap();
-    Vector ev = vw.get();
-    for (int i = 0; i < ev.size(); i++) {
-      double minsij = Double.MAX_VALUE;
-      int minInd = -1;
-      for (int j = 0; j < ev.size(); j++) {          
-        double sij = performSensitivityCalculation(eigenvalue, ev.get(i),
-            ev.get(j), diagonal.get(i), diagonal.get(j));
-        
-        // perform non-maximal suppression
-        // is this the smallest value in the row?
-        if (sij < minsij) {
-          minsij = sij;
-          minInd = j;
-        }
-      }
-      
-      // is this the smallest value in the column?
-      Integer column = minInd;
-      EigencutsSensitivityNode value = new EigencutsSensitivityNode(i, minInd, minsij);
-      if (!columns.containsKey(column)) {
-        columns.put(column, value);
-      } else if (columns.get(column).getSensitivity() > minsij) {
-        columns.remove(column);
-        columns.put(column, value);
-      }
-    }
-    
-    // write whatever values made it through
-    
-    for (EigencutsSensitivityNode e : columns.values()) {
-      context.write(new IntWritable(e.getRow()), e);
-    }
-  }
-  
-  /**
-   * Helper method, performs the actual calculation. Looks something like this:
-   *
-   * (log(2) / lambda_k * log(lambda_k) * log(lambda_k^beta0 / 2)) * [
-   * - (((u_i / sqrt(d_i)) - (u_j / sqrt(d_j)))^2 + (1 - lambda) * 
-   *   ((u_i^2 / d_i) + (u_j^2 / d_j))) ]
-   */
-  private double performSensitivityCalculation(double eigenvalue,
-                                               double evi,
-                                               double evj,
-                                               double diagi,
-                                               double diagj) {
-    
-    double firsthalf = Functions.LOGARITHM.apply(2)
-        / (eigenvalue * Functions.LOGARITHM.apply(eigenvalue)
-           * Functions.LOGARITHM.apply(Functions.POW.apply(eigenvalue, beta0) / 2));
-    
-    double secondhalf =
-        -Functions.POW.apply(evi / Functions.SQRT.apply(diagi) - evj / Functions.SQRT.apply(diagj), 2)
-        + (1.0 - eigenvalue) * (Functions.POW.apply(evi, 2) / diagi + Functions.POW.apply(evj, 2) / diagj);
-    
-    return firsthalf * secondhalf;
-  }
-  
-  /**
-   * Utility helper method, used for unit testing.
-   */
-  void setup(double beta0, double epsilon, Vector eigenvalues, Vector diagonal) {
-    this.beta0 = beta0;
-    this.epsilon = epsilon;
-    this.eigenvalues = eigenvalues;
-    this.diagonal = diagonal;
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityNode.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,70 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-
-/**
- * This class allows the storage of computed sensitivities in an
- * unordered fashion, instead having each sensitivity track its
- * own (i, j) coordinate. Thus these objects can be stored as elements
- * in any list or, in particular, Writable array.
- */
-@Deprecated
-public class EigencutsSensitivityNode implements Writable {
-  
-  private int row;
-  private int column;
-  private double sensitivity;
-  
-  public EigencutsSensitivityNode(int i, int j, double s) {
-    row = i;
-    column = j;
-    sensitivity = s;
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    this.row = in.readInt();
-    this.column = in.readInt();
-    this.sensitivity = in.readDouble();
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    out.writeInt(row);
-    out.writeInt(column);
-    out.writeDouble(sensitivity);
-  }
-
-  public int getRow() {
-    return row;
-  }
-
-  public int getColumn() {
-    return column;
-  }
-
-  public double getSensitivity() {
-    return sensitivity;
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/eigencuts/EigencutsSensitivityReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-/**
- * <p>The point of this class is to take all the arrays of sensitivities
- * and convert them to a single matrix. Since there may be many values
- * that, according to their (i, j) coordinates, overlap in the matrix,
- * the "winner" will be determined by whichever value is smaller.</p> 
- */
-@Deprecated
-public class EigencutsSensitivityReducer extends
-    Reducer<IntWritable, EigencutsSensitivityNode, IntWritable, VectorWritable> {
-
-  @Override
-  protected void reduce(IntWritable key, Iterable<EigencutsSensitivityNode> arr, Context context)
-    throws IOException, InterruptedException {
-    Configuration conf = context.getConfiguration();
-    Vector v = new RandomAccessSparseVector(conf.getInt(EigencutsKeys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE), 100);
-    double threshold = Double.parseDouble(conf.get(EigencutsKeys.TAU))
-        / Double.parseDouble(conf.get(EigencutsKeys.DELTA));
-    
-    for (EigencutsSensitivityNode n : arr) {
-      if (n.getSensitivity() < threshold && n.getSensitivity() < v.getQuick(n.getColumn())) {
-        v.setQuick(n.getColumn(), n.getSensitivity());
-      }
-    }
-    context.write(key, new VectorWritable(v));
-  }
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/EigenSeedGenerator.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/EigenSeedGenerator.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/EigenSeedGenerator.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/EigenSeedGenerator.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,124 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral.kmeans;
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.mahout.clustering.iterator.ClusterWritable;
+import org.apache.mahout.clustering.kmeans.Kluster;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.distance.DistanceMeasure;
+import org.apache.mahout.common.iterator.sequencefile.PathFilters;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Maps;
+import com.google.common.io.Closeables;
+
+/**
+ * Given an Input Path containing a {@link org.apache.hadoop.io.SequenceFile}, select k vectors and write them to the
+ * output file as a {@link org.apache.mahout.clustering.kmeans.Kluster} representing the initial centroid to use. The
+ * selection criterion is the rows with max value in that respective column
+ */
+public final class EigenSeedGenerator {
+
+  private static final Logger log = LoggerFactory.getLogger(EigenSeedGenerator.class);
+
+  public static final String K = "k";
+
+  private EigenSeedGenerator() {}
+
+  public static Path buildFromEigens(Configuration conf, Path input, Path output, int k, DistanceMeasure measure)
+      throws IOException {
+    // delete the output directory
+    FileSystem fs = FileSystem.get(output.toUri(), conf);
+    HadoopUtil.delete(conf, output);
+    Path outFile = new Path(output, "part-eigenSeed");
+    boolean newFile = fs.createNewFile(outFile);
+    if (newFile) {
+      Path inputPathPattern;
+
+      if (fs.getFileStatus(input).isDir()) {
+        inputPathPattern = new Path(input, "*");
+      } else {
+        inputPathPattern = input;
+      }
+
+      FileStatus[] inputFiles = fs.globStatus(inputPathPattern, PathFilters.logsCRCFilter());
+      SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, outFile, Text.class, ClusterWritable.class);
+      Map<Integer,Double> maxEigens = Maps.newHashMapWithExpectedSize(k); // store
+                                                                          // max
+                                                                          // value
+                                                                          // of
+                                                                          // each
+                                                                          // column
+      Map<Integer,Text> chosenTexts = Maps.newHashMapWithExpectedSize(k);
+      Map<Integer,ClusterWritable> chosenClusters = Maps.newHashMapWithExpectedSize(k);
+
+      for (FileStatus fileStatus : inputFiles) {
+        if (!fileStatus.isDir()) {
+          for (Pair<Writable,VectorWritable> record : new SequenceFileIterable<Writable,VectorWritable>(
+              fileStatus.getPath(), true, conf)) {
+            Writable key = record.getFirst();
+            VectorWritable value = record.getSecond();
+
+            for (Vector.Element e : value.get().nonZeroes()) {
+              int index = e.index();
+              double v = Math.abs(e.get());
+
+              if (!maxEigens.containsKey(index) || v > maxEigens.get(index)) {
+                maxEigens.put(index, v);
+                Text newText = new Text(key.toString());
+                chosenTexts.put(index, newText);
+                Kluster newCluster = new Kluster(value.get(), index, measure);
+                newCluster.observe(value.get(), 1);
+                ClusterWritable clusterWritable = new ClusterWritable();
+                clusterWritable.setValue(newCluster);
+                chosenClusters.put(index, clusterWritable);
+              }
+            }
+          }
+        }
+      }
+
+      try {
+        for (Integer key : maxEigens.keySet()) {
+          writer.append(chosenTexts.get(key), chosenClusters.get(key));
+        }
+        log.info("EigenSeedGenerator:: Wrote {} Klusters to {}", chosenTexts.size(), outFile);
+      } finally {
+        Closeables.close(writer, false);
+      }
+    }
+
+    return outFile;
+  }
+
+}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/SpectralKMeansDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/SpectralKMeansDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/SpectralKMeansDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/spectral/kmeans/SpectralKMeansDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,12 +30,11 @@
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.clustering.Cluster;
 import org.apache.mahout.clustering.classify.WeightedVectorWritable;
-import org.apache.mahout.clustering.kmeans.EigenSeedGenerator;
 import org.apache.mahout.clustering.kmeans.KMeansDriver;
-import org.apache.mahout.clustering.spectral.common.AffinityMatrixInputJob;
-import org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob;
-import org.apache.mahout.clustering.spectral.common.UnitVectorizerJob;
-import org.apache.mahout.clustering.spectral.common.VectorMatrixMultiplicationJob;
+import org.apache.mahout.clustering.spectral.AffinityMatrixInputJob;
+import org.apache.mahout.clustering.spectral.MatrixDiagonalizeJob;
+import org.apache.mahout.clustering.spectral.UnitVectorizerJob;
+import org.apache.mahout.clustering.spectral.VectorMatrixMultiplicationJob;
 import org.apache.mahout.common.AbstractJob;
 import org.apache.mahout.common.ClassUtils;
 import org.apache.mahout.common.HadoopUtil;
@@ -96,7 +95,8 @@
     Path input = getInputPath();
     Path output = getOutputPath();
     if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(conf, output);
+      HadoopUtil.delete(conf, getTempPath());
+      HadoopUtil.delete(conf, getOutputPath());
     }
     int numDims = Integer.parseInt(getOption("dimensions"));
     int clusters = Integer.parseInt(getOption("clusters"));
@@ -152,6 +152,7 @@
    * @param ssvd
    *          Flag to indicate the eigensolver to use
    * @param numReducers
+   *          Number of reducers
    * @param blockHeight
    * @param oversampling
    * @param poweriters
@@ -161,6 +162,7 @@
       int blockHeight, int oversampling, int poweriters) throws IOException, InterruptedException,
       ClassNotFoundException {
 
+    HadoopUtil.delete(conf, tempDir);
     Path outputCalc = new Path(tempDir, "calculations");
     Path outputTmp = new Path(tempDir, "temporary");
 
@@ -244,7 +246,7 @@
 
     // Run the KMeansDriver
     Path answer = new Path(output, "kmeans_out");
-    KMeansDriver.run(conf, data, initialclusters, answer, measure, convergenceDelta, maxIterations, true, 0.0, false);
+    KMeansDriver.run(conf, data, initialclusters, answer, convergenceDelta, maxIterations, true, 0.0, false);
 
     // Restore name to id mapping and read through the cluster assignments
     Path mappingPath = new Path(new Path(conf.get("hadoop.tmp.dir")), "generic_input_mapping");
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -40,6 +40,7 @@
 import org.apache.mahout.common.AbstractJob;
 import org.apache.mahout.common.HadoopUtil;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
+import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.math.Centroid;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.neighborhood.BruteSearch;
@@ -336,13 +337,14 @@
                                                 String method,
                                                 boolean reduceStreamingKMeans) throws ClassNotFoundException {
     // Checking preconditions for the parameters.
-    Preconditions.checkArgument(numClusters > 0, "Invalid number of clusters requested");
+    Preconditions.checkArgument(numClusters > 0, 
+        "Invalid number of clusters requested: " + numClusters + ". Must be: numClusters > 0!");
 
     // StreamingKMeans
     Preconditions.checkArgument(estimatedNumMapClusters > numClusters, "Invalid number of estimated map "
         + "clusters; There must be more than the final number of clusters (k log n vs k)");
     Preconditions.checkArgument(estimatedDistanceCutoff == INVALID_DISTANCE_CUTOFF || estimatedDistanceCutoff > 0,
-        "estimatedDistanceCutoff cannot be negative");
+        "estimatedDistanceCutoff must be equal to -1 or must be greater then 0!");
 
     // BallKMeans
     Preconditions.checkArgument(maxNumIterations > 0, "Must have at least one BallKMeans iteration");
@@ -405,7 +407,6 @@
    * @param output the directory pathname for output points.
    * @return 0 on success, -1 on failure.
    */
-  @SuppressWarnings("unchecked")
   public static int run(Configuration conf, Path input, Path output)
       throws IOException, InterruptedException, ClassNotFoundException, ExecutionException {
     log.info("Starting StreamingKMeans clustering for vectors in {}; results are output to {}",
@@ -425,7 +426,7 @@
     // Run StreamingKMeans step in parallel by spawning 1 thread per input path to process.
     ExecutorService pool = Executors.newCachedThreadPool();
     List<Future<Iterable<Centroid>>> intermediateCentroidFutures = Lists.newArrayList();
-    for (FileStatus status : HadoopUtil.listStatus(FileSystem.get(conf), input)) {
+    for (FileStatus status : HadoopUtil.listStatus(FileSystem.get(conf), input, PathFilters.logsCRCFilter())) {
       intermediateCentroidFutures.add(pool.submit(new StreamingKMeansThread(status.getPath(), conf)));
     }
     log.info("Finished running Mappers");
@@ -439,7 +440,7 @@
     pool.shutdown();
     pool.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);
     log.info("Finished StreamingKMeans");
-    SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, output, IntWritable.class,
+    SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, new Path(output, "part-r-00000"), IntWritable.class,
         CentroidWritable.class);
     int numCentroids = 0;
     // Run BallKMeans on the intermediate centroids.
@@ -453,7 +454,6 @@
     return 0;
   }
 
-  @SuppressWarnings("unchecked")
   public static int runMapReduce(Configuration conf, Path input, Path output)
     throws IOException, ClassNotFoundException, InterruptedException {
     // Prepare Job for submission.
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansMapper.java mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansMapper.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -80,7 +80,7 @@
         estimatePoints.add(centroid);
       } else if (numPoints == NUM_ESTIMATE_POINTS) {
         clusterEstimatePoints();
-  }
+      }
     } else {
       clusterer.cluster(centroid);
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansReducer.java mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansReducer.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -31,8 +31,13 @@
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
 import org.apache.mahout.math.Centroid;
 import org.apache.mahout.math.Vector;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class StreamingKMeansReducer extends Reducer<IntWritable, CentroidWritable, IntWritable, CentroidWritable> {
+
+  private static final Logger log = LoggerFactory.getLogger(StreamingKMeansReducer.class);
+
   /**
    * Configuration for the MapReduce job.
    */
@@ -57,7 +62,7 @@
             @Override
             public Centroid apply(CentroidWritable input) {
               Preconditions.checkNotNull(input);
-              return input.getCentroid();
+              return input.getCentroid().clone();
             }
           }), conf).call());
     } else {
@@ -66,7 +71,7 @@
 
     int index = 0;
     for (Vector centroid : getBestCentroids(intermediateCentroids, conf)) {
-      context.write(new IntWritable(index), new CentroidWritable((Centroid)centroid));
+      context.write(new IntWritable(index), new CentroidWritable((Centroid) centroid));
       ++index;
     }
   }
@@ -84,6 +89,11 @@
   }
 
   public static Iterable<Vector> getBestCentroids(List<Centroid> centroids, Configuration conf) {
+
+    if (log.isInfoEnabled()) {
+      log.info("Number of Centroids: {}", centroids.size());
+    }
+
     int numClusters = conf.getInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1);
     int maxNumIterations = conf.getInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, 10);
     float trimFraction = conf.getFloat(StreamingKMeansDriver.TRIM_FRACTION, 0.9f);
@@ -92,8 +102,8 @@
     float testProbability = conf.getFloat(StreamingKMeansDriver.TEST_PROBABILITY, 0.1f);
     int numRuns = conf.getInt(StreamingKMeansDriver.NUM_BALLKMEANS_RUNS, 3);
 
-    BallKMeans clusterer = new BallKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(conf),
+    BallKMeans ballKMeansCluster = new BallKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(conf),
         numClusters, maxNumIterations, trimFraction, kMeansPlusPlusInit, correctWeights, testProbability, numRuns);
-    return clusterer.cluster(centroids);
+    return ballKMeansCluster.cluster(centroids);
   }
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java
--- mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansThread.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,20 +30,24 @@
 import org.apache.mahout.math.Centroid;
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.neighborhood.UpdatableSearcher;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class StreamingKMeansThread implements Callable<Iterable<Centroid>> {
+  private static final Logger log = LoggerFactory.getLogger(StreamingKMeansThread.class);
+
   private static final int NUM_ESTIMATE_POINTS = 1000;
 
   private final Configuration conf;
-  private final Iterable<Centroid> datapoints;
+  private final Iterable<Centroid> dataPoints;
 
   public StreamingKMeansThread(Path input, Configuration conf) {
     this(StreamingKMeansUtilsMR.getCentroidsFromVectorWritable(
         new SequenceFileValueIterable<VectorWritable>(input, false, conf)), conf);
   }
 
-  public StreamingKMeansThread(Iterable<Centroid> datapoints, Configuration conf) {
-    this.datapoints = datapoints;
+  public StreamingKMeansThread(Iterable<Centroid> dataPoints, Configuration conf) {
+    this.dataPoints = dataPoints;
     this.conf = conf;
   }
 
@@ -54,22 +58,35 @@
     double estimateDistanceCutoff = conf.getFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF,
         StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF);
 
-    Iterator<Centroid> datapointsIterator = datapoints.iterator();
+    Iterator<Centroid> dataPointsIterator = dataPoints.iterator();
+
     if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
       List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
-      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
-        estimatePoints.add(datapointsIterator.next());
+      while (dataPointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
+        Centroid centroid = dataPointsIterator.next();
+        estimatePoints.add(centroid);
+      }
+
+      if (log.isInfoEnabled()) {
+        log.info("Estimated Points: {}", estimatePoints.size());
       }
       estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
     }
 
-    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
-    while (datapointsIterator.hasNext()) {
-      clusterer.cluster(datapointsIterator.next());
+    StreamingKMeans streamingKMeans = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
+
+    // datapointsIterator could be empty if no estimate distance was initially provided
+    // hence creating the iterator again here for the clustering
+    if (!dataPointsIterator.hasNext()) {
+      dataPointsIterator = dataPoints.iterator();
+    }
+
+    while (dataPointsIterator.hasNext()) {
+      streamingKMeans.cluster(dataPointsIterator.next());
     }
-    clusterer.reindexCentroids();
 
-    return clusterer;
+    streamingKMeans.reindexCentroids();
+    return streamingKMeans;
   }
 
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/AbstractJob.java mahout/core/src/main/java/org/apache/mahout/common/AbstractJob.java
--- mahout/core/src/main/java/org/apache/mahout/common/AbstractJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/AbstractJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -99,11 +99,11 @@
   protected Path inputPath;
   protected File inputFile; //the input represented as a file
 
-  /** output path, populated by {@link #parseArguments(String[]) */
+  /** output path, populated by {@link #parseArguments(String[])} */
   protected Path outputPath;
   protected File outputFile; //the output represented as a file
 
-  /** temp path, populated by {@link #parseArguments(String[]) */
+  /** temp path, populated by {@link #parseArguments(String[])} */
   protected Path tempPath;
 
   protected Map<String, List<String>> argMap;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/HadoopUtil.java mahout/core/src/main/java/org/apache/mahout/common/HadoopUtil.java
--- mahout/core/src/main/java/org/apache/mahout/common/HadoopUtil.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/HadoopUtil.java	2014-03-29 01:03:13.000000000 -0700
@@ -352,18 +352,44 @@
    * @throws IOException - IO Exception
    */
   public static String buildDirList(FileSystem fs, FileStatus fileStatus) throws IOException {
-    boolean bContainsFiles = false;
+    boolean containsFiles = false;
     List<String> directoriesList = Lists.newArrayList();
     for (FileStatus childFileStatus : fs.listStatus(fileStatus.getPath())) {
       if (childFileStatus.isDir()) {
         String subDirectoryList = buildDirList(fs, childFileStatus);
         directoriesList.add(subDirectoryList);
       } else {
-        bContainsFiles = true;
+        containsFiles = true;
       }
     }
 
-    if (bContainsFiles) {
+    if (containsFiles) {
+      directoriesList.add(fileStatus.getPath().toUri().getPath());
+    }
+    return Joiner.on(',').skipNulls().join(directoriesList.iterator());
+  }
+
+  /**
+   * Builds a comma-separated list of input splits
+   * @param fs - File System
+   * @param fileStatus - File Status
+   * @param pathFilter - path filter
+   * @return list of directories as a comma-separated String
+   * @throws IOException - IO Exception
+   */
+  public static String buildDirList(FileSystem fs, FileStatus fileStatus, PathFilter pathFilter) throws IOException {
+    boolean containsFiles = false;
+    List<String> directoriesList = Lists.newArrayList();
+    for (FileStatus childFileStatus : fs.listStatus(fileStatus.getPath(), pathFilter)) {
+      if (childFileStatus.isDir()) {
+        String subDirectoryList = buildDirList(fs, childFileStatus);
+        directoriesList.add(subDirectoryList);
+      } else {
+        containsFiles = true;
+      }
+    }
+
+    if (containsFiles) {
       directoriesList.add(fileStatus.getPath().toUri().getPath());
     }
     return Joiner.on(',').skipNulls().join(directoriesList.iterator());
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/commandline/DefaultOptionCreator.java mahout/core/src/main/java/org/apache/mahout/common/commandline/DefaultOptionCreator.java
--- mahout/core/src/main/java/org/apache/mahout/common/commandline/DefaultOptionCreator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/commandline/DefaultOptionCreator.java	2014-03-29 01:03:13.000000000 -0700
@@ -21,7 +21,6 @@
 import org.apache.commons.cli2.builder.ArgumentBuilder;
 import org.apache.commons.cli2.builder.DefaultOptionBuilder;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver;
 import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
 import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
 
@@ -390,19 +389,6 @@
             "The classname of the IKernelProfile. Default is TriangularKernelProfile");
   }
   
-  public static DefaultOptionBuilder inputIsCanopiesOption() {
-    return new DefaultOptionBuilder()
-        .withLongName(MeanShiftCanopyDriver.INPUT_IS_CANOPIES_OPTION)
-        .withRequired(false)
-        .withShortName("ic")
-        .withArgument(
-            new ArgumentBuilder()
-                .withName(MeanShiftCanopyDriver.INPUT_IS_CANOPIES_OPTION)
-                .withMinimum(1).withMaximum(1).create())
-        .withDescription(
-            "If present, the input directory already contains MeanShiftCanopies");
-  }
-  
   /**
    * Returns a default command line option for specification of OUTLIER THRESHOLD value. Used for
    * Cluster Classification.
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterable.java mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterable.java
--- mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterable.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterable.java	2014-03-29 01:03:13.000000000 -0700
@@ -38,17 +38,17 @@
   private final boolean skipFirstLine;
   private final String origFilename;
   
-  /** Creates a  over a given file, assuming a UTF-8 encoding. */
+  /** Creates a {@link FileLineIterable} over a given file, assuming a UTF-8 encoding. */
   public FileLineIterable(File file) throws IOException {
     this(file, Charsets.UTF_8, false);
   }
 
-  /** Creates a  over a given file, assuming a UTF-8 encoding. */
+  /** Creates a {@link FileLineIterable} over a given file, assuming a UTF-8 encoding. */
   public FileLineIterable(File file, boolean skipFirstLine) throws IOException {
     this(file, Charsets.UTF_8, skipFirstLine);
   }
   
-  /** Creates a  over a given file, using the given encoding. */
+  /** Creates a {@link FileLineIterable} over a given file, using the given encoding. */
   public FileLineIterable(File file, Charset encoding, boolean skipFirstLine) throws IOException {
     this(FileLineIterator.getFileInputStream(file), encoding, skipFirstLine);
   }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterator.java mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterator.java
--- mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/iterator/FileLineIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -49,19 +49,18 @@
   private static final Logger log = LoggerFactory.getLogger(FileLineIterator.class);
 
   /**
-   * Creates a  over a given file, assuming a UTF-8 encoding.
+   * Creates a {@link FileLineIterator} over a given file, assuming a UTF-8 encoding.
    *
    * @throws java.io.FileNotFoundException if the file does not exist
-       * @throws IOException
-       *           if the file cannot be read
-       */
-
+   * @throws IOException
+   *           if the file cannot be read
+   */
   public FileLineIterator(File file) throws IOException {
     this(file, Charsets.UTF_8, false);
   }
 
   /**
-   * Creates a  over a given file, assuming a UTF-8 encoding.
+   * Creates a {@link FileLineIterator} over a given file, assuming a UTF-8 encoding.
    *
    * @throws java.io.FileNotFoundException if the file does not exist
    * @throws IOException                   if the file cannot be read
@@ -71,7 +70,7 @@
   }
 
   /**
-   * Creates a  over a given file, using the given encoding.
+   * Creates a {@link FileLineIterator} over a given file, using the given encoding.
    *
    * @throws java.io.FileNotFoundException if the file does not exist
    * @throws IOException                   if the file cannot be read
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/iterator/SamplingIterator.java mahout/core/src/main/java/org/apache/mahout/common/iterator/SamplingIterator.java
--- mahout/core/src/main/java/org/apache/mahout/common/iterator/SamplingIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/iterator/SamplingIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -41,7 +41,8 @@
 
   public SamplingIterator(RandomWrapper random, Iterator<? extends T> delegate, double samplingRate) {
     Preconditions.checkNotNull(delegate);
-    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0);
+    Preconditions.checkArgument(samplingRate > 0.0 && samplingRate <= 1.0,
+        "Must be: 0.0 < samplingRate <= 1.0. But samplingRate = " + samplingRate);
     // Geometric distribution is special case of negative binomial (aka Pascal) with r=1:
     geometricDistribution = new PascalDistribution(random.getRandomGenerator(), 1, samplingRate);
     this.delegate = delegate;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/lucene/AnalyzerUtils.java mahout/core/src/main/java/org/apache/mahout/common/lucene/AnalyzerUtils.java
--- mahout/core/src/main/java/org/apache/mahout/common/lucene/AnalyzerUtils.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/lucene/AnalyzerUtils.java	2014-03-29 01:03:13.000000000 -0700
@@ -32,7 +32,7 @@
    * @throws ClassNotFoundException - {@link ClassNotFoundException}
    */
   public static Analyzer createAnalyzer(String analyzerClassName) throws ClassNotFoundException {
-    return createAnalyzer(analyzerClassName, Version.LUCENE_43);
+    return createAnalyzer(analyzerClassName, Version.LUCENE_46);
   }
 
   public static Analyzer createAnalyzer(String analyzerClassName, Version version) throws ClassNotFoundException {
@@ -47,7 +47,7 @@
    * @return {@link Analyzer}
    */
   public static Analyzer createAnalyzer(Class<? extends Analyzer> analyzerClass) {
-    return createAnalyzer(analyzerClass, Version.LUCENE_43);
+    return createAnalyzer(analyzerClass, Version.LUCENE_46);
   }
 
   public static Analyzer createAnalyzer(Class<? extends Analyzer> analyzerClass, Version version) {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/lucene/TokenStreamIterator.java mahout/core/src/main/java/org/apache/mahout/common/lucene/TokenStreamIterator.java
--- mahout/core/src/main/java/org/apache/mahout/common/lucene/TokenStreamIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/lucene/TokenStreamIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -45,6 +45,8 @@
       if (tokenStream.incrementToken()) {
         return tokenStream.getAttribute(CharTermAttribute.class).toString();
       } else {
+        tokenStream.end();
+        tokenStream.close();
         return endOfData();
       }
     } catch (IOException e) {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumCombiner.java mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumCombiner.java
--- mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumCombiner.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumCombiner.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.common.mapreduce;
+
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors;
+
+import java.io.IOException;
+
+public class VectorSumCombiner
+      extends Reducer<WritableComparable<?>, VectorWritable, WritableComparable<?>, VectorWritable> {
+
+    private final VectorWritable result = new VectorWritable();
+
+    @Override
+    protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
+      throws IOException, InterruptedException {
+      result.set(Vectors.sum(values.iterator()));
+      ctx.write(key, result);
+    }
+  }
\ No newline at end of file
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumReducer.java mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumReducer.java
--- mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/common/mapreduce/VectorSumReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -19,9 +19,8 @@
 
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.function.Functions;
+import org.apache.mahout.math.hadoop.similarity.cooccurrence.Vectors;
 
 import java.io.IOException;
 
@@ -31,14 +30,6 @@
   @Override
   protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx)
     throws IOException, InterruptedException {
-    Vector vector = null;
-    for (VectorWritable v : values) {
-      if (vector == null) {
-        vector = v.get();
-      } else {
-        vector.assign(v.get(), Functions.PLUS);
-      }
-    }
-    ctx.write(key, new VectorWritable(vector));
+    ctx.write(key, new VectorWritable(Vectors.sum(values.iterator())));
   }
 }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/driver/MahoutDriver.java mahout/core/src/main/java/org/apache/mahout/driver/MahoutDriver.java
--- mahout/core/src/main/java/org/apache/mahout/driver/MahoutDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/driver/MahoutDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -120,6 +120,7 @@
 
     if (args.length < 1 || args[0] == null || "-h".equals(args[0]) || "--help".equals(args[0])) {
       programDriver.driver(args);
+      return;
     }
 
     String progName = args[0];
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/ep/EvolutionaryProcess.java mahout/core/src/main/java/org/apache/mahout/ep/EvolutionaryProcess.java
--- mahout/core/src/main/java/org/apache/mahout/ep/EvolutionaryProcess.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/ep/EvolutionaryProcess.java	2014-03-29 01:03:13.000000000 -0700
@@ -33,6 +33,7 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
 
 /**
  * Allows evolutionary optimization where the state function can't be easily
@@ -191,6 +192,11 @@
   @Override
   public void close() {
     List<Runnable> remainingTasks = pool.shutdownNow();
+    try {
+      pool.awaitTermination(10, TimeUnit.SECONDS);
+    } catch (InterruptedException e) {
+      throw new IllegalStateException("Had to forcefully shut down " + remainingTasks.size() + " tasks");
+    }
     if (!remainingTasks.isEmpty()) {
       throw new IllegalStateException("Had to forcefully shut down " + remainingTasks.size() + " tasks");
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -32,7 +32,6 @@
  * and select the top K frequent patterns
  * 
  */
-@Deprecated
 public class AggregatorMapper extends Mapper<Text,TopKStringPatterns,Text,TopKStringPatterns> {
   
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/AggregatorReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,7 +30,6 @@
  * containing that particular item
  * 
  */
-@Deprecated
 public class AggregatorReducer extends Reducer<Text,TopKStringPatterns,Text,TopKStringPatterns> {
   
   private int maxHeapSize = 50;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/CountDescendingPairComparator.java	2014-03-29 01:03:13.000000000 -0700
@@ -27,7 +27,6 @@
  * high count first (that is, descending), and for those of equal count, orders by the first element in the
  * pair, ascending. It is used in several places in the FPM code.
  */
-@Deprecated
 public final class CountDescendingPairComparator<A extends Comparable<? super A>,B extends Comparable<? super B>>
   implements Comparator<Pair<A,B>>, Serializable {
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthDriver.java	2014-03-29 01:03:13.000000000 -0700
@@ -45,7 +45,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
 public final class FPGrowthDriver extends AbstractJob {
 
   private static final Logger log = LoggerFactory.getLogger(FPGrowthDriver.class);
@@ -177,8 +176,8 @@
                 minSupport,
                 maxHeapSize,
                 features,
-                new StringOutputConverter(new SequenceFileOutputCollector<Text, TopKStringPatterns>(writer)),
-                new ContextStatusUpdater(null));
+                new StringOutputConverter(new SequenceFileOutputCollector<Text, TopKStringPatterns>(writer))
+        );
       } finally {
         Closeables.close(writer, false);
         Closeables.close(inputStream, true);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/MultiTransactionTreeIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -26,7 +26,6 @@
 /**
  * Iterates over multiple transaction trees to produce a single iterator of transactions
  */
-@Deprecated
 public final class MultiTransactionTreeIterator extends AbstractIterator<IntArrayList> {
   
   private final Iterator<Pair<IntArrayList,Long>> pIterator;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowth.java	2014-03-29 01:03:13.000000000 -0700
@@ -55,7 +55,6 @@
  * http://infolab.stanford.edu/~echang/recsys08-69.pdf
  * 
  */
-@Deprecated
 public final class PFPGrowth {
   
   public static final String ENCODING = "encoding";
@@ -65,11 +64,11 @@
   public static final String MAX_PER_GROUP = "maxPerGroup";
   public static final String OUTPUT = "output";
   public static final String MIN_SUPPORT = "minSupport";
-  public static final String MAX_HEAPSIZE = "maxHeapSize";
+  public static final String MAX_HEAP_SIZE = "maxHeapSize";
   public static final String INPUT = "input";
   public static final String PFP_PARAMETERS = "pfp.parameters";
   public static final String FILE_PATTERN = "part-*";
-  public static final String FPGROWTH = "fpgrowth";
+  public static final String FP_GROWTH = "fpgrowth";
   public static final String FREQUENT_PATTERNS = "frequentpatterns";
   public static final String PARALLEL_COUNTING = "parallelcounting";
   public static final String SPLIT_PATTERN = "splitPattern";
@@ -161,9 +160,7 @@
     return itemId / maxPerGroup;
   }
 
-  public static IntArrayList getGroupMembers(int groupId, 
-                                                   int maxPerGroup, 
-                                                   int numFeatures) {
+  public static IntArrayList getGroupMembers(int groupId, int maxPerGroup, int numFeatures) {
     int start = groupId * maxPerGroup;
     int end = start + maxPerGroup;
     if (end > numFeatures) {
@@ -197,14 +194,12 @@
   }
   
   /**
-   * @throws ClassNotFoundException 
- * @throws InterruptedException 
- * @throws IOException 
- * @params
-   *    input, output locations, additional parameters like minSupport(3), maxHeapSize(50), numGroups(1000)
-   * @conf
-   *    initial Hadoop configuration to use.
-   * 
+   * @param params params
+   * @param conf Configuration
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   * @throws IOException
+   *
    * */
   public static void runPFPGrowth(Parameters params, Configuration conf) throws IOException,
                                                                         InterruptedException,
@@ -253,7 +248,7 @@
     conf.set("mapred.compress.map.output", "true");
     conf.set("mapred.output.compression.type", "BLOCK");
     
-    Path input = new Path(params.get(OUTPUT), FPGROWTH);
+    Path input = new Path(params.get(OUTPUT), FP_GROWTH);
     Job job = new Job(conf, "PFP Aggregator Driver running over input: " + input);
     job.setJarByClass(PFPGrowth.class);
     
@@ -332,7 +327,7 @@
     job.setOutputValueClass(TopKStringPatterns.class);
     
     FileInputFormat.addInputPath(job, input);
-    Path outPath = new Path(params.get(OUTPUT), FPGROWTH);
+    Path outPath = new Path(params.get(OUTPUT), FP_GROWTH);
     FileOutputFormat.setOutputPath(job, outPath);
     
     HadoopUtil.delete(conf, outPath);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -34,7 +34,6 @@
  * WordCount example
  * 
  */
-@Deprecated
 public class ParallelCountingMapper extends Mapper<LongWritable,Text,Text,LongWritable> {
   
   private static final LongWritable ONE = new LongWritable(1);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelCountingReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -27,7 +27,6 @@
  *  sums up the item count and output the item and the count This can also be
  * used as a local Combiner. A simple summing reducer
  */
-@Deprecated
 public class ParallelCountingReducer extends Reducer<Text,LongWritable,Text,LongWritable> {
   
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthCombiner.java	2014-03-29 01:03:13.000000000 -0700
@@ -29,7 +29,6 @@
  *  takes each group of dependent transactions and\ compacts it in a
  * TransactionTree structure
  */
-@Deprecated
 public class ParallelFPGrowthCombiner extends Reducer<IntWritable,TransactionTree,IntWritable,TransactionTree> {
   
   @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -36,7 +36,6 @@
  * outputs the group id as key and the transaction as value
  * 
  */
-@Deprecated
 public class ParallelFPGrowthMapper extends Mapper<LongWritable,Text,IntWritable,TransactionTree> {
 
   private final OpenObjectIntHashMap<String> fMap = new OpenObjectIntHashMap<String>();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/ParallelFPGrowthReducer.java	2014-03-29 01:03:13.000000000 -0700
@@ -45,7 +45,6 @@
  * outputs the the Top K frequent Patterns for each group.
  * 
  */
-@Deprecated
 public final class ParallelFPGrowthReducer extends Reducer<IntWritable,TransactionTree,Text,TopKStringPatterns> {
 
   private final List<String> featureReverseMap = Lists.newArrayList();
@@ -105,8 +104,8 @@
           PFPGrowth.getGroupMembers(key.get(), maxPerGroup, numFeatures),
           new IntegerStringOutputConverter(
               new ContextWriteOutputCollector<IntWritable, TransactionTree, Text, TopKStringPatterns>(context),
-              featureReverseMap),
-          new ContextStatusUpdater<IntWritable, TransactionTree, Text, TopKStringPatterns>(context));
+              featureReverseMap)
+      );
     } else {
       FPGrowth<Integer> fpGrowth = new FPGrowth<Integer>();
       fpGrowth.generateTopKFrequentPatterns(
@@ -135,7 +134,7 @@
       freqList.add(e.getSecond());
     }
     
-    maxHeapSize = Integer.valueOf(params.get(PFPGrowth.MAX_HEAPSIZE, "50"));
+    maxHeapSize = Integer.valueOf(params.get(PFPGrowth.MAX_HEAP_SIZE, "50"));
     minSupport = Integer.valueOf(params.get(PFPGrowth.MIN_SUPPORT, "3"));
 
     maxPerGroup = params.getInt(PFPGrowth.MAX_PER_GROUP, 0);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTree.java	2014-03-29 01:03:13.000000000 -0700
@@ -42,7 +42,6 @@
  * Map/Reduce of {@link PFPGrowth} algorithm by reducing data size passed from the Mapper to the reducer where
  * {@link org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth} mining is done
  */
-@Deprecated
 public final class TransactionTree implements Writable, Iterable<Pair<IntArrayList,Long>> {
 
   private static final Logger log = LoggerFactory.getLogger(TransactionTree.class);
@@ -167,9 +166,7 @@
   
   public Map<Integer,MutableLong> generateFList() {
     Map<Integer,MutableLong> frequencyList = Maps.newHashMap();
-    Iterator<Pair<IntArrayList,Long>> it = iterator();
-    while (it.hasNext()) {
-      Pair<IntArrayList,Long> p = it.next();
+    for (Pair<IntArrayList, Long> p : this) {
       IntArrayList items = p.getFirst();
       for (int idx = 0; idx < items.size(); idx++) {
         if (!frequencyList.containsKey(items.get(idx))) {
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -29,7 +29,6 @@
  * Generates a List of transactions view of Transaction Tree by doing Depth First Traversal on the tree
  * structure
  */
-@Deprecated
 final class TransactionTreeIterator extends AbstractIterator<Pair<IntArrayList,Long>> {
 
   private final Stack<int[]> depth = new Stack<int[]>();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextStatusUpdater.java	2014-03-29 01:03:13.000000000 -0700
@@ -28,7 +28,6 @@
  * @param <K>
  * @param <V>
  */
-@Deprecated
 public class ContextStatusUpdater<IK extends Writable,IV extends Writable,K extends Writable,V extends Writable>
     implements StatusUpdater {
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/ContextWriteOutputCollector.java	2014-03-29 01:03:13.000000000 -0700
@@ -34,7 +34,6 @@
  * @param <K>
  * @param <V>
  */
-@Deprecated
 public class ContextWriteOutputCollector<IK extends Writable,IV extends Writable,K extends Writable,V extends Writable>
     implements OutputCollector<K,V> {
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/SequenceFileOutputCollector.java	2014-03-29 01:03:13.000000000 -0700
@@ -29,7 +29,6 @@
  * @param <K>
  * @param <V>
  */
-@Deprecated
 public class SequenceFileOutputCollector<K extends Writable,V extends Writable> implements
     OutputCollector<K,V> {
   private final SequenceFile.Writer writer;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/StatusUpdater.java	2014-03-29 01:03:13.000000000 -0700
@@ -21,7 +21,6 @@
  * An interface of a Status updater
  * 
  */
-@Deprecated
 public interface StatusUpdater {
   
   void update(String status);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TopKPatternsOutputConverter.java	2014-03-29 01:03:13.000000000 -0700
@@ -35,7 +35,6 @@
  * 
  * @param <A>
  */
-@Deprecated
 public final class TopKPatternsOutputConverter<A extends Comparable<? super A>> implements
     OutputCollector<Integer,FrequentPatternMaxHeap> {
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/TransactionIterator.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,7 +30,6 @@
  * Iterates over a Transaction and outputs the transaction integer id mapping and the support of the
  * transaction
  */
-@Deprecated
 public class TransactionIterator<T> extends ForwardingIterator<Pair<int[],Long>> {
 
   private final int[] transactionBuffer;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/integer/IntegerStringOutputConverter.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,7 +30,6 @@
  * Collects the Patterns with Integer id and Long support and converts them to Pattern of Strings based on a
  * reverse feature lookup map.
  */
-@Deprecated
 public final class IntegerStringOutputConverter implements
     OutputCollector<Integer,List<Pair<List<Integer>,Long>>> {
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/StringOutputConverter.java	2014-03-29 01:03:13.000000000 -0700
@@ -28,7 +28,6 @@
  * Collects a string pattern in a MaxHeap and outputs the top K patterns
  * 
  */
-@Deprecated
 public final class StringOutputConverter implements OutputCollector<String,List<Pair<List<String>,Long>>> {
   
   private final OutputCollector<Text,TopKStringPatterns> collector;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/convertors/string/TopKStringPatterns.java	2014-03-29 01:03:13.000000000 -0700
@@ -32,7 +32,6 @@
  * A class which collects Top K string patterns
  *
  */
-@Deprecated
 public final class TopKStringPatterns implements Writable {
   private final List<Pair<List<String>,Long>> frequentPatterns;
   
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPGrowth.java	2014-03-29 01:03:13.000000000 -0700
@@ -50,7 +50,6 @@
  *
  * @param <A> object type used as the cell items in a transaction list
  */
-@Deprecated
 public class FPGrowth<A extends Comparable<? super A>> {
 
   private static final Logger log = LoggerFactory.getLogger(FPGrowth.class);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTree.java	2014-03-29 01:03:13.000000000 -0700
@@ -26,7 +26,6 @@
  * {@link FPGrowth} algorithm
  * 
  */
-@Deprecated
 public class FPTree {
   
   public static final int ROOTNODEID = 0;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FPTreeDepthCache.java	2014-03-29 01:03:13.000000000 -0700
@@ -25,7 +25,6 @@
  * Caches large FPTree {@link Object} for each level of the recursive
  * {@link FPGrowth} algorithm to reduce allocation overhead.
  */
-@Deprecated
 public class FPTreeDepthCache {
 
   private final LeastKCache<Integer,FPTree> firstLevelCache = new LeastKCache<Integer,FPTree>(5);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeap.java	2014-03-29 01:03:13.000000000 -0700
@@ -24,7 +24,6 @@
 import org.apache.mahout.math.map.OpenLongObjectHashMap;
 
 /**  keeps top K Attributes in a TreeSet */
-@Deprecated
 public final class FrequentPatternMaxHeap {
   
   private int count;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/LeastKCache.java	2014-03-29 01:03:13.000000000 -0700
@@ -23,7 +23,6 @@
 import java.util.Map;
 import java.util.PriorityQueue;
 
-@Deprecated
 public class LeastKCache<K extends Comparable<? super K>,V> {
   
   private final int capacity;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/Pattern.java	2014-03-29 01:03:13.000000000 -0700
@@ -26,7 +26,6 @@
  * support(the number of times the pattern is seen in the dataset)
  * 
  */
-@Deprecated
 public class Pattern implements Comparable<Pattern> {
   
   private static final int DEFAULT_INITIAL_SIZE = 2;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthIds.java	2014-03-29 01:03:13.000000000 -0700
@@ -29,7 +29,6 @@
 import org.apache.commons.lang3.mutable.MutableLong;
 import org.apache.hadoop.mapred.OutputCollector;
 import org.apache.mahout.common.Pair;
-import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
 import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
 import org.apache.mahout.math.list.LongArrayList;
 import org.apache.mahout.math.list.IntArrayList;
@@ -42,7 +41,6 @@
 /**
  * Implementation of PFGrowth Algorithm
  */
-@Deprecated
 public final class FPGrowthIds {
 
   private static final Logger log = LoggerFactory.getLogger(FPGrowthIds.class);
@@ -54,30 +52,30 @@
    * Generate Top K Frequent Patterns for every feature in returnableFeatures
    * given a stream of transactions and the minimum support
    *
-   * @param transactionStream
-   *          Iterator of transaction
-   * @param attributeFrequency
-   *          list of frequent features and their support value
-   * @param minSupport
-   *          minimum support of the transactions
-   * @param k
-   *          Number of top frequent patterns to keep
-   * @param returnableFeatures
-   *          set of features for which the frequent patterns are mined. If the
-   *          set is empty or null, then top K patterns for every frequent item (an item
-   *          whose support> minSupport) is generated
-   * @param output
-   *          The output collector to which the the generated patterns are
-   *          written
-   * @throws IOException
+   *
+  * @param transactionStream
+  *          Iterator of transaction
+  * @param attributeFrequency
+  *          list of frequent features and their support value
+  * @param minSupport
+  *          minimum support of the transactions
+  * @param k
+  *          Number of top frequent patterns to keep
+  * @param returnableFeatures
+  *          set of features for which the frequent patterns are mined. If the
+  *          set is empty or null, then top K patterns for every frequent item (an item
+  *          whose support> minSupport) is generated
+  * @param output
+  *          The output collector to which the the generated patterns are
+  *          written
+  * @throws IOException
    */
   public static void generateTopKFrequentPatterns(Iterator<Pair<IntArrayList, Long>> transactionStream,
                                                   LongArrayList attributeFrequency,
                                                   long minSupport,
                                                   int k,
                                                   IntArrayList returnableFeatures,
-                                                  OutputCollector<Integer, List<Pair<List<Integer>, Long>>> output,
-                                                  StatusUpdater updater) throws IOException {
+                                                  OutputCollector<Integer, List<Pair<List<Integer>, Long>>> output) throws IOException {
 
     for (int i = 0; i < attributeFrequency.size(); i++) {
       if (attributeFrequency.get(i) < minSupport) {
@@ -99,7 +97,7 @@
     log.info("Number of unique pruned items {}", attributeFrequency.size());
     generateTopKFrequentPatterns(transactionStream, attributeFrequency,
         minSupport, k, returnableFeatures,
-        new TopKPatternsOutputConverter<Integer>(output, new IdentityMapping()), updater);
+        new TopKPatternsOutputConverter<Integer>(output, new IdentityMapping()));
   }
 
   private static class IdentityMapping extends AbstractMap<Integer, Integer> {
@@ -119,6 +117,7 @@
   /**
    * Top K FpGrowth Algorithm
    *
+   *
    * @param tree
    *          to be mined
    * @param minSupportValue
@@ -136,8 +135,7 @@
                                                               long minSupportValue,
                                                               int k,
                                                               IntArrayList requiredFeatures,
-                                                              TopKPatternsOutputConverter<Integer> outputCollector,
-                                                              StatusUpdater updater) throws IOException {
+                                                              TopKPatternsOutputConverter<Integer> outputCollector) throws IOException {
 
     Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
     requiredFeatures.sort();
@@ -146,7 +144,7 @@
         log.info("Mining FTree Tree for all patterns with {}", attribute);
         MutableLong minSupport = new MutableLong(minSupportValue);
         FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
-                                                         attribute, updater);
+                                                         attribute);
         patterns.put(attribute, frequentPatterns);
         outputCollector.collect(attribute, frequentPatterns);
 
@@ -169,14 +167,13 @@
    * @param attributeFrequency
    *          array representing the Frequency of the corresponding attribute id
    * @param minSupport
-   *          minimum support of the pattern to be mined
+ *          minimum support of the pattern to be mined
    * @param k
-   *          Max value of the Size of the Max-Heap in which Patterns are held
+*          Max value of the Size of the Max-Heap in which Patterns are held
    * @param returnFeatures
-   *          the id's of the features for which Top K patterns have to be mined
+*          the id's of the features for which Top K patterns have to be mined
    * @param topKPatternsOutputCollector
-   *          the outputCollector which transforms the given Pattern in integer
-   *          format to the corresponding A Format
+*          the outputCollector which transforms the given Pattern in integer
    */
   private static void generateTopKFrequentPatterns(
       Iterator<Pair<IntArrayList, Long>> transactions,
@@ -184,8 +181,7 @@
       long minSupport,
       int k,
       IntArrayList returnFeatures,
-      TopKPatternsOutputConverter<Integer> topKPatternsOutputCollector,
-      StatusUpdater updater) throws IOException {
+      TopKPatternsOutputConverter<Integer> topKPatternsOutputCollector) throws IOException {
 
     FPTree tree = new FPTree(attributeFrequency, minSupport);
 
@@ -201,7 +197,7 @@
       }
     }
 
-    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector, updater);
+    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector);
   }
 
   /** 
@@ -210,8 +206,7 @@
   private static FrequentPatternMaxHeap growth(FPTree tree,
                                                MutableLong minSupportMutable,
                                                int k,
-                                               int currentAttribute,
-                                               StatusUpdater updater) {
+                                               int currentAttribute) {
 
     long currentAttributeCount = tree.headerCount(currentAttribute);
 
@@ -238,7 +233,7 @@
 
     for (int attr : q.attrIterableRev())  {
       mergeHeap(suffixPats,
-                growth(q, minSupportMutable, k, attr, updater),
+                growth(q, minSupportMutable, k, attr),
                 currentAttribute,
                 currentAttributeCount, true);
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPGrowthObj.java	2014-03-29 01:03:13.000000000 -0700
@@ -37,7 +37,6 @@
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
 import org.apache.mahout.fpm.pfpgrowth.CountDescendingPairComparator;
-import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
 import org.apache.mahout.fpm.pfpgrowth.convertors.TopKPatternsOutputConverter;
 import org.apache.mahout.fpm.pfpgrowth.convertors.TransactionIterator;
 import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
@@ -52,7 +51,6 @@
  *
  * @param <A> object type used as the cell items in a transaction list
  */
-@Deprecated
 public class FPGrowthObj<A extends Comparable<? super A>> {
 
   private static final Logger log = LoggerFactory.getLogger(FPGrowthObj.class);
@@ -108,30 +106,30 @@
    * Generate Top K Frequent Patterns for every feature in returnableFeatures
    * given a stream of transactions and the minimum support
    *
-   * @param transactionStream
-   *          Iterator of transaction
-   * @param frequencyList
-   *          list of frequent features and their support value
-   * @param minSupport
-   *          minimum support of the transactions
-   * @param k
-   *          Number of top frequent patterns to keep
-   * @param returnableFeatures
-   *          set of features for which the frequent patterns are mined. If the
-   *          set is empty or null, then top K patterns for every frequent item (an item
-   *          whose support> minSupport) is generated
-   * @param output
-   *          The output collector to which the the generated patterns are
-   *          written
-   * @throws IOException
+   *
+  * @param transactionStream
+  *          Iterator of transaction
+  * @param frequencyList
+  *          list of frequent features and their support value
+  * @param minSupport
+  *          minimum support of the transactions
+  * @param k
+  *          Number of top frequent patterns to keep
+  * @param returnableFeatures
+  *          set of features for which the frequent patterns are mined. If the
+  *          set is empty or null, then top K patterns for every frequent item (an item
+  *          whose support> minSupport) is generated
+  * @param output
+  *          The output collector to which the the generated patterns are
+  *          written
+  * @throws IOException
    */
-  public final void generateTopKFrequentPatterns(Iterator<Pair<List<A>,Long>> transactionStream,
+  public final void generateTopKFrequentPatterns(Iterator<Pair<List<A>, Long>> transactionStream,
                                                  Collection<Pair<A, Long>> frequencyList,
                                                  long minSupport,
                                                  int k,
                                                  Collection<A> returnableFeatures,
-                                                 OutputCollector<A,List<Pair<List<A>,Long>>> output,
-                                                 StatusUpdater updater) throws IOException {
+                                                 OutputCollector<A, List<Pair<List<A>, Long>>> output) throws IOException {
 
     Map<Integer,A> reverseMapping = Maps.newHashMap();
     Map<A,Integer> attributeIdMapping = Maps.newHashMap();
@@ -177,12 +175,13 @@
     generateTopKFrequentPatterns(new TransactionIterator<A>(transactionStream,
         attributeIdMapping), attributeFrequency, minSupport, k, 
         returnFeatures, new TopKPatternsOutputConverter<A>(output,
-            reverseMapping), updater);
+            reverseMapping));
   }
 
   /**
    * Top K FpGrowth Algorithm
    *
+   *
    * @param tree
    *          to be mined
    * @param minSupportValue
@@ -200,8 +199,7 @@
                                                        long minSupportValue,
                                                        int k,
                                                        Collection<Integer> requiredFeatures,
-                                                       TopKPatternsOutputConverter<A> outputCollector,
-                                                       StatusUpdater updater) throws IOException {
+                                                       TopKPatternsOutputConverter<A> outputCollector) throws IOException {
 
     Map<Integer,FrequentPatternMaxHeap> patterns = Maps.newHashMap();
     for (int attribute : tree.attrIterableRev()) {
@@ -209,7 +207,7 @@
         log.info("Mining FTree Tree for all patterns with {}", attribute);
         MutableLong minSupport = new MutableLong(minSupportValue);
         FrequentPatternMaxHeap frequentPatterns = growth(tree, minSupport, k,
-                                                         attribute, updater);
+                                                         attribute);
         patterns.put(attribute, frequentPatterns);
         outputCollector.collect(attribute, frequentPatterns);
 
@@ -230,23 +228,20 @@
    * @param attributeFrequency
    *          array representing the Frequency of the corresponding attribute id
    * @param minSupport
-   *          minimum support of the pattern to be mined
+ *          minimum support of the pattern to be mined
    * @param k
-   *          Max value of the Size of the Max-Heap in which Patterns are held
+*          Max value of the Size of the Max-Heap in which Patterns are held
    * @param returnFeatures
-   *          the id's of the features for which Top K patterns have to be mined
+*          the id's of the features for which Top K patterns have to be mined
    * @param topKPatternsOutputCollector
-   *          the outputCollector which transforms the given Pattern in integer
-   *          format to the corresponding A Format
-   * @return Top K frequent patterns for each attribute
+*          the outputCollector which transforms the given Pattern in integer
    */
   private void generateTopKFrequentPatterns(
-    Iterator<Pair<int[],Long>> transactions,
-    long[] attributeFrequency,
-    long minSupport,
-    int k,
-    Collection<Integer> returnFeatures, TopKPatternsOutputConverter<A> topKPatternsOutputCollector,
-    StatusUpdater updater) throws IOException {
+      Iterator<Pair<int[], Long>> transactions,
+      long[] attributeFrequency,
+      long minSupport,
+      int k,
+      Collection<Integer> returnFeatures, TopKPatternsOutputConverter<A> topKPatternsOutputCollector) throws IOException {
 
     FPTree tree = new FPTree(attributeFrequency, minSupport);
 
@@ -266,7 +261,7 @@
       }
     }
 
-    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector, updater);
+    fpGrowth(tree, minSupport, k, returnFeatures, topKPatternsOutputCollector);
   }
 
   /** 
@@ -275,8 +270,7 @@
   private static FrequentPatternMaxHeap growth(FPTree tree,
                                                MutableLong minSupportMutable,
                                                int k,
-                                               int currentAttribute,
-                                               StatusUpdater updater) {
+                                               int currentAttribute) {
 
     long currentAttributeCount = tree.headerCount(currentAttribute);
 
@@ -303,7 +297,7 @@
 
     for (int attr : q.attrIterableRev())  {
       mergeHeap(suffixPats,
-                growth(q, minSupportMutable, k, attr, updater),
+                growth(q, minSupportMutable, k, attr),
                 currentAttribute,
                 currentAttributeCount, true);
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java
--- mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth2/FPTree.java	2014-03-29 01:03:13.000000000 -0700
@@ -30,7 +30,6 @@
 /**
  * A straightforward implementation of FPTrees as described in Han et. al.
  */
-@Deprecated
 public final class FPTree {
 
   private final AttrComparator attrComparator = new AttrComparator();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/Varint.java mahout/core/src/main/java/org/apache/mahout/math/Varint.java
--- mahout/core/src/main/java/org/apache/mahout/math/Varint.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/Varint.java	2014-03-29 01:03:13.000000000 -0700
@@ -125,7 +125,7 @@
     while (((b = in.readByte()) & 0x80L) != 0) {
       value |= (b & 0x7F) << i;
       i += 7;
-      Preconditions.checkArgument(i <= 63, "Variable length quantity is too long");
+      Preconditions.checkArgument(i <= 63, "Variable length quantity is too long (must be <= 63)");
     }
     return value | (b << i);
   }
@@ -159,7 +159,7 @@
     while (((b = in.readByte()) & 0x80) != 0) {
       value |= (b & 0x7F) << i;
       i += 7;
-      Preconditions.checkArgument(i <= 35, "Variable length quantity is too long");
+      Preconditions.checkArgument(i <= 35, "Variable length quantity is too long (must be <= 35)");
     }
     return value | (b << i);
   }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/DistributedLanczosSolver.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/DistributedLanczosSolver.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/DistributedLanczosSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/DistributedLanczosSolver.java	2014-03-29 01:03:13.000000000 -0700
@@ -46,10 +46,9 @@
 /**
  * See the SSVD code for a better option than using this:
  *
- * https://cwiki.apache.org/confluence/display/MAHOUT/Stochastic+Singular+Value+Decomposition
+ * http://mahout.apache.org/users/dim-reduction/ssvd.html
  * @see org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver
  */
-@Deprecated
 public class DistributedLanczosSolver extends LanczosSolver implements Tool {
 
   public static final String RAW_EIGENVECTORS = "rawEigenvectors";
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/EigenVerificationJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/EigenVerificationJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/EigenVerificationJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/EigenVerificationJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -71,7 +71,6 @@
  * If all the eigenvectors can fit in memory, --inMemory allows for a speedier completion of this task by doing so.
  * </p>
  */
-@Deprecated
 public class EigenVerificationJob extends AbstractJob {
 
   public static final String CLEAN_EIGENVECTORS = "cleanEigenvectors";
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/HdfsBackedLanczosState.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/HdfsBackedLanczosState.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/HdfsBackedLanczosState.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/decomposer/HdfsBackedLanczosState.java	2014-03-29 01:03:13.000000000 -0700
@@ -37,14 +37,13 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
 public class HdfsBackedLanczosState extends LanczosState implements Configurable {
 
   private static final Logger log = LoggerFactory.getLogger(HdfsBackedLanczosState.class);
 
   public static final String BASIS_PREFIX = "basis";
   public static final String SINGULAR_PREFIX = "singular";
-  public static final String METADATA_FILE = "metadata";
+ //public static final String METADATA_FILE = "metadata";
 
   private Configuration conf;
   private final Path baseDir;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/MutableElement.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/MutableElement.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/MutableElement.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/MutableElement.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.hadoop.similarity.cooccurrence;
 
 import org.apache.mahout.math.Vector;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -22,6 +22,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.Reducer;
@@ -29,7 +30,9 @@
 import org.apache.mahout.common.AbstractJob;
 import org.apache.mahout.common.ClassUtils;
 import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
+import org.apache.mahout.common.mapreduce.VectorSumCombiner;
 import org.apache.mahout.common.mapreduce.VectorSumReducer;
 import org.apache.mahout.math.RandomAccessSparseVector;
 import org.apache.mahout.math.Vector;
@@ -45,11 +48,13 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Random;
 import java.util.concurrent.atomic.AtomicInteger;
 
 public class RowSimilarityJob extends AbstractJob {
 
   public static final double NO_THRESHOLD = Double.MIN_VALUE;
+  public static final long NO_FIXED_RANDOM_SEED = Long.MIN_VALUE;
 
   private static final String SIMILARITY_CLASSNAME = RowSimilarityJob.class + ".distributedSimilarityClassname";
   private static final String NUMBER_OF_COLUMNS = RowSimilarityJob.class + ".numberOfColumns";
@@ -63,17 +68,25 @@
   private static final String NUM_NON_ZERO_ENTRIES_PATH = RowSimilarityJob.class + ".nonZeroEntriesPath";
   private static final int DEFAULT_MAX_SIMILARITIES_PER_ROW = 100;
 
+  private static final String OBSERVATIONS_PER_COLUMN_PATH = RowSimilarityJob.class + ".observationsPerColumnPath";
+
+  private static final String MAX_OBSERVATIONS_PER_ROW = RowSimilarityJob.class + ".maxObservationsPerRow";
+  private static final String MAX_OBSERVATIONS_PER_COLUMN = RowSimilarityJob.class + ".maxObservationsPerColumn";
+  private static final String RANDOM_SEED = RowSimilarityJob.class + ".randomSeed";
+
+  private static final int DEFAULT_MAX_OBSERVATIONS_PER_ROW = 500;
+  private static final int DEFAULT_MAX_OBSERVATIONS_PER_COLUMN = 500;
+
   private static final int NORM_VECTOR_MARKER = Integer.MIN_VALUE;
   private static final int MAXVALUE_VECTOR_MARKER = Integer.MIN_VALUE + 1;
   private static final int NUM_NON_ZERO_ENTRIES_VECTOR_MARKER = Integer.MIN_VALUE + 2;
 
-  enum Counters { ROWS, COOCCURRENCES, PRUNED_COOCCURRENCES }
+  enum Counters { ROWS, USED_OBSERVATIONS, NEGLECTED_OBSERVATIONS, COOCCURRENCES, PRUNED_COOCCURRENCES }
 
   public static void main(String[] args) throws Exception {
     ToolRunner.run(new RowSimilarityJob(), args);
   }
 
-
   @Override
   public int run(String[] args) throws Exception {
 
@@ -86,6 +99,11 @@
         + DEFAULT_MAX_SIMILARITIES_PER_ROW + ')', String.valueOf(DEFAULT_MAX_SIMILARITIES_PER_ROW));
     addOption("excludeSelfSimilarity", "ess", "compute similarity of rows to themselves?", String.valueOf(false));
     addOption("threshold", "tr", "discard row pairs with a similarity value below this", false);
+    addOption("maxObservationsPerRow", null, "sample rows down to this number of entries",
+        String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_ROW));
+    addOption("maxObservationsPerColumn", null, "sample columns down to this number of entries",
+        String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_COLUMN));
+    addOption("randomSeed", null, "use this seed for sampling", false);
     addOption(DefaultOptionCreator.overwriteOption().create());
 
     Map<String,List<String>> parsedArgs = parseArguments(args);
@@ -123,6 +141,11 @@
     boolean excludeSelfSimilarity = Boolean.parseBoolean(getOption("excludeSelfSimilarity"));
     double threshold = hasOption("threshold")
         ? Double.parseDouble(getOption("threshold")) : NO_THRESHOLD;
+    long randomSeed = hasOption("randomSeed")
+        ? Long.parseLong(getOption("randomSeed")) : NO_FIXED_RANDOM_SEED;
+
+    int maxObservationsPerRow = Integer.parseInt(getOption("maxObservationsPerRow"));
+    int maxObservationsPerColumn = Integer.parseInt(getOption("maxObservationsPerColumn"));
 
     Path weightsPath = getTempPath("weights");
     Path normsPath = getTempPath("norms.bin");
@@ -130,8 +153,18 @@
     Path maxValuesPath = getTempPath("maxValues.bin");
     Path pairwiseSimilarityPath = getTempPath("pairwiseSimilarity");
 
+    Path observationsPerColumnPath = getTempPath("observationsPerColumn.bin");
+
     AtomicInteger currentPhase = new AtomicInteger();
 
+    Job countObservations = prepareJob(getInputPath(), getTempPath("notUsed"), CountObservationsMapper.class,
+        NullWritable.class, VectorWritable.class, SumObservationsReducer.class, NullWritable.class,
+        VectorWritable.class);
+    countObservations.setCombinerClass(VectorSumCombiner.class);
+    countObservations.getConfiguration().set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());
+    countObservations.setNumReduceTasks(1);
+    countObservations.waitForCompletion(true);
+
     if (shouldRunNextPhase(parsedArgs, currentPhase)) {
       Job normsAndTranspose = prepareJob(getInputPath(), weightsPath, VectorNormMapper.class, IntWritable.class,
           VectorWritable.class, MergeVectorsReducer.class, IntWritable.class, VectorWritable.class);
@@ -142,6 +175,11 @@
       normsAndTransposeConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());
       normsAndTransposeConf.set(MAXVALUES_PATH, maxValuesPath.toString());
       normsAndTransposeConf.set(SIMILARITY_CLASSNAME, similarityClassname);
+      normsAndTransposeConf.set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());
+      normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_ROW, String.valueOf(maxObservationsPerRow));
+      normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_COLUMN, String.valueOf(maxObservationsPerColumn));
+      normsAndTransposeConf.set(RANDOM_SEED, String.valueOf(randomSeed));
+
       boolean succeeded = normsAndTranspose.waitForCompletion(true);
       if (!succeeded) {
         return -1;
@@ -181,6 +219,35 @@
     return 0;
   }
 
+  public static class CountObservationsMapper extends Mapper<IntWritable,VectorWritable,NullWritable,VectorWritable> {
+
+    private Vector columnCounts = new RandomAccessSparseVector(Integer.MAX_VALUE);
+
+    @Override
+    protected void map(IntWritable rowIndex, VectorWritable rowVectorWritable, Context ctx)
+      throws IOException, InterruptedException {
+
+      Vector row = rowVectorWritable.get();
+      for (Vector.Element elem : row.nonZeroes()) {
+        columnCounts.setQuick(elem.index(), columnCounts.getQuick(elem.index()) + 1);
+      }
+    }
+
+    @Override
+    protected void cleanup(Context ctx) throws IOException, InterruptedException {
+      ctx.write(NullWritable.get(), new VectorWritable(columnCounts));
+    }
+  }
+
+  public static class SumObservationsReducer extends Reducer<NullWritable,VectorWritable,NullWritable,VectorWritable> {
+    @Override
+    protected void reduce(NullWritable nullWritable, Iterable<VectorWritable> partialVectors, Context ctx)
+    throws IOException, InterruptedException {
+      Vector counts = Vectors.sum(partialVectors.iterator());
+      Vectors.write(counts, new Path(ctx.getConfiguration().get(OBSERVATIONS_PER_COLUMN_PATH)), ctx.getConfiguration());
+    }
+  }
+
   public static class VectorNormMapper extends Mapper<IntWritable,VectorWritable,IntWritable,VectorWritable> {
 
     private VectorSimilarityMeasure similarity;
@@ -189,21 +256,71 @@
     private Vector maxValues;
     private double threshold;
 
+    private OpenIntIntHashMap observationsPerColumn;
+    private int maxObservationsPerRow;
+    private int maxObservationsPerColumn;
+
+    private Random random;
+
     @Override
     protected void setup(Context ctx) throws IOException, InterruptedException {
-      similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
-          VectorSimilarityMeasure.class);
+
+      Configuration conf = ctx.getConfiguration();
+
+      similarity = ClassUtils.instantiateAs(conf.get(SIMILARITY_CLASSNAME), VectorSimilarityMeasure.class);
       norms = new RandomAccessSparseVector(Integer.MAX_VALUE);
       nonZeroEntries = new RandomAccessSparseVector(Integer.MAX_VALUE);
       maxValues = new RandomAccessSparseVector(Integer.MAX_VALUE);
-      threshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
+      threshold = Double.parseDouble(conf.get(THRESHOLD));
+
+      observationsPerColumn = Vectors.readAsIntMap(new Path(conf.get(OBSERVATIONS_PER_COLUMN_PATH)), conf);
+      maxObservationsPerRow = conf.getInt(MAX_OBSERVATIONS_PER_ROW, DEFAULT_MAX_OBSERVATIONS_PER_ROW);
+      maxObservationsPerColumn = conf.getInt(MAX_OBSERVATIONS_PER_COLUMN, DEFAULT_MAX_OBSERVATIONS_PER_COLUMN);
+
+      long seed = Long.parseLong(conf.get(RANDOM_SEED));
+      if (seed == NO_FIXED_RANDOM_SEED) {
+        random = RandomUtils.getRandom();
+      } else {
+        random = RandomUtils.getRandom(seed);
+      }
+    }
+
+    private Vector sampleDown(Vector rowVector, Context ctx) {
+
+      int observationsPerRow = rowVector.getNumNondefaultElements();
+      double rowSampleRate = (double) Math.min(maxObservationsPerRow, observationsPerRow) / (double) observationsPerRow;
+
+      Vector downsampledRow = rowVector.like();
+      long usedObservations = 0;
+      long neglectedObservations = 0;
+
+      for (Vector.Element elem : rowVector.nonZeroes()) {
+
+        int columnCount = observationsPerColumn.get(elem.index());
+        double columnSampleRate = (double) Math.min(maxObservationsPerColumn, columnCount) / (double) columnCount;
+
+        if (random.nextDouble() <= Math.min(rowSampleRate, columnSampleRate)) {
+          downsampledRow.setQuick(elem.index(), elem.get());
+          usedObservations++;
+        } else {
+          neglectedObservations++;
+        }
+
+      }
+
+      ctx.getCounter(Counters.USED_OBSERVATIONS).increment(usedObservations);
+      ctx.getCounter(Counters.NEGLECTED_OBSERVATIONS).increment(neglectedObservations);
+
+      return downsampledRow;
     }
 
     @Override
     protected void map(IntWritable row, VectorWritable vectorWritable, Context ctx)
       throws IOException, InterruptedException {
 
-      Vector rowVector = similarity.normalize(vectorWritable.get());
+      Vector sampledRowVector = sampleDown(vectorWritable.get(), ctx);
+
+      Vector rowVector = similarity.normalize(sampledRowVector);
 
       int numNonZeroEntries = 0;
       double maxValue = Double.MIN_VALUE;
@@ -230,8 +347,6 @@
 
     @Override
     protected void cleanup(Context ctx) throws IOException, InterruptedException {
-      super.cleanup(ctx);
-      // dirty trick
       ctx.write(new IntWritable(NORM_VECTOR_MARKER), new VectorWritable(norms));
       ctx.write(new IntWritable(NUM_NON_ZERO_ENTRIES_VECTOR_MARKER), new VectorWritable(nonZeroEntries));
       ctx.write(new IntWritable(MAXVALUE_VECTOR_MARKER), new VectorWritable(maxValues));
@@ -353,7 +468,7 @@
       similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME),
           VectorSimilarityMeasure.class);
       numberOfColumns = ctx.getConfiguration().getInt(NUMBER_OF_COLUMNS, -1);
-      Preconditions.checkArgument(numberOfColumns > 0, "Incorrect number of columns!");
+      Preconditions.checkArgument(numberOfColumns > 0, "Number of columns must be greater then 0! But numberOfColumns = " + numberOfColumns);
       excludeSelfSimilarity = ctx.getConfiguration().getBoolean(EXCLUDE_SELF_SIMILARITY, false);
       norms = Vectors.read(new Path(ctx.getConfiguration().get(NORMS_PATH)), ctx.getConfiguration());
       treshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));
@@ -393,7 +508,7 @@
     @Override
     protected void setup(Mapper.Context ctx) throws IOException, InterruptedException {
       maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);
-      Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Incorrect maximum number of similarities per row!");
+      Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");
     }
 
     @Override
@@ -432,7 +547,7 @@
     @Override
     protected void setup(Context ctx) throws IOException, InterruptedException {
       maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);
-      Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Incorrect maximum number of similarities per row!");
+      Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");
     }
 
     @Override
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/Vectors.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/Vectors.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/Vectors.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/Vectors.java	2014-03-29 01:03:13.000000000 -0700
@@ -34,6 +34,7 @@
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.Vector.Element;
 import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.function.Functions;
 import org.apache.mahout.math.map.OpenIntIntHashMap;
 
 public final class Vectors {
@@ -91,6 +92,14 @@
     return accumulator;
   }
 
+  public static Vector sum(Iterator<VectorWritable> vectors) {
+    Vector sum = vectors.next().get();
+    while (vectors.hasNext()) {
+      sum.assign(vectors.next().get(), Functions.PLUS);
+    }
+    return sum;
+  }
+
   static class TemporaryElement implements Vector.Element {
 
     private final int index;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,11 +17,6 @@
 
 package org.apache.mahout.math.hadoop.stochasticsvd;
 
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayDeque;
-import java.util.Deque;
-
 import org.apache.commons.lang3.Validate;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.filecache.DistributedCache;
@@ -50,32 +45,37 @@
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.NamedVector;
+import org.apache.mahout.math.UpperTriangular;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.function.Functions;
 import org.apache.mahout.math.function.PlusMult;
 import org.apache.mahout.math.hadoop.stochasticsvd.qr.QRLastStep;
 
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayDeque;
+import java.util.Deque;
+
 /**
  * Bt job. For details, see working notes in MAHOUT-376.
- * <P>
- * 
+ * <p/>
+ * <p/>
  * Uses hadoop deprecated API wherever new api has not been updated
  * (MAHOUT-593), hence @SuppressWarning("deprecation").
- * <P>
- * 
+ * <p/>
+ * <p/>
  * This job outputs either Bt in its standard output, or upper triangular
  * matrices representing BBt partial sums if that's requested . If the latter
  * mode is enabled, then we accumulate BBt outer product sums in upper
  * triangular accumulator and output it at the end of the job, thus saving space
  * and BBt job.
- * <P>
- * 
+ * <p/>
+ * <p/>
  * This job also outputs Q and Bt and optionally BBt. Bt is output to standard
  * job output (part-*) and Q and BBt use named multiple outputs.
- * 
- * <P>
- * 
+ * <p/>
+ * <p/>
  */
 @SuppressWarnings("deprecation")
 public final class BtJob {
@@ -101,7 +101,7 @@
   }
 
   public static class BtMapper extends
-      Mapper<Writable, VectorWritable, LongWritable, SparseRowBlockWritable> {
+    Mapper<Writable, VectorWritable, LongWritable, SparseRowBlockWritable> {
 
     private QRLastStep qr;
     private final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
@@ -308,8 +308,8 @@
   }
 
   public static class OuterProductCombiner
-      extends
-      Reducer<Writable, SparseRowBlockWritable, Writable, SparseRowBlockWritable> {
+    extends
+    Reducer<Writable, SparseRowBlockWritable, Writable, SparseRowBlockWritable> {
 
     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
     protected final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
@@ -343,8 +343,8 @@
   }
 
   public static class OuterProductReducer
-      extends
-      Reducer<LongWritable, SparseRowBlockWritable, IntWritable, VectorWritable> {
+    extends
+    Reducer<LongWritable, SparseRowBlockWritable, IntWritable, VectorWritable> {
 
     protected final SparseRowBlockWritable accum = new SparseRowBlockWritable();
     protected final Deque<Closeable> closeables = new ArrayDeque<Closeable>();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/QJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -100,6 +100,8 @@
       String sbPathStr = conf.get(PROP_SB_PATH);
       if (sbPathStr != null) {
         sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);
+        if (sb == null)
+          throw new IOException(String.format("Unable to load s_omega from path %s.", sbPathStr));
       }
 
       outputs = new MultipleOutputs(new JobConf(conf));
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDHelper.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDHelper.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDHelper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDHelper.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,13 +17,15 @@
 
 package org.apache.mahout.math.hadoop.stochasticsvd;
 
+import java.io.Closeable;
 import java.io.IOException;
+import java.util.*;
 import java.util.Arrays;
-import java.util.Comparator;
-import java.util.List;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import com.google.common.base.Function;
+import com.google.common.collect.Iterators;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -31,14 +33,11 @@
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Writable;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Matrix;
+import org.apache.mahout.common.IOUtils;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.sequencefile.*;
+import org.apache.mahout.math.*;
 import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.function.Functions;
 
 import com.google.common.collect.Lists;
@@ -46,7 +45,6 @@
 
 /**
  * set of small file manipulation helpers.
- *
  */
 public final class SSVDHelper {
 
@@ -152,13 +150,13 @@
         matcher.reset(o1.getPath().getName());
         if (!matcher.matches()) {
           throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:"
-              + o1.getPath());
+                                               + o1.getPath());
         }
         int p1 = Integer.parseInt(matcher.group(3));
         matcher.reset(o2.getPath().getName());
         if (!matcher.matches()) {
           throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:"
-              + o2.getPath());
+                                               + o2.getPath());
         }
 
         int p2 = Integer.parseInt(matcher.group(3));
@@ -167,49 +165,63 @@
 
     };
 
+  public static Iterator<Pair<Writable, Vector>> drmIterator(FileSystem fs, Path glob, Configuration conf,
+                                                             Deque<Closeable> closeables)
+    throws IOException {
+    SequenceFileDirIterator<Writable, VectorWritable> ret =
+      new SequenceFileDirIterator<Writable, VectorWritable>(glob,
+                                                            PathType.GLOB,
+                                                            PathFilters.logsCRCFilter(),
+                                                            PARTITION_COMPARATOR,
+                                                            true,
+                                                            conf);
+    closeables.addFirst(ret);
+    return Iterators.transform(ret, new Function<Pair<Writable, VectorWritable>, Pair<Writable, Vector>>() {
+      @Override
+      public Pair<Writable, Vector> apply(Pair<Writable, VectorWritable> p) {
+        return new Pair(p.getFirst(), p.getSecond().get());
+      }
+    });
+  }
+
   /**
    * helper capabiltiy to load distributed row matrices into dense matrix (to
    * support tests mainly).
    *
-   * @param fs
-   *          filesystem
-   * @param glob
-   *          FS glob
-   * @param conf
-   *          configuration
+   * @param fs   filesystem
+   * @param glob FS glob
+   * @param conf configuration
    * @return Dense matrix array
    */
-  public static double[][] loadDistributedRowMatrix(FileSystem fs, Path glob, Configuration conf) throws IOException {
+  public static DenseMatrix drmLoadAsDense(FileSystem fs, Path glob, Configuration conf) throws IOException {
 
-    FileStatus[] files = fs.globStatus(glob);
-    if (files == null) {
-      return null;
-    }
-
-    List<double[]> denseData = Lists.newArrayList();
-
-    /*
-     * assume it is partitioned output, so we need to read them up in order of
-     * partitions.
-     */
-    Arrays.sort(files, PARTITION_COMPARATOR);
-
-    for (FileStatus fstat : files) {
-      for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(fstat.getPath(),
-                                                                                true,
-                                                                                conf)) {
-        Vector v = value.get();
-        int size = v.size();
-        double[] row = new double[size];
-        for (int i = 0; i < size; i++) {
-          row[i] = v.get(i);
+    Deque<Closeable> closeables = new ArrayDeque<Closeable>();
+    try {
+      List<double[]> denseData = new ArrayList<double[]>();
+      for (Iterator<Pair<Writable, Vector>> iter = drmIterator(fs, glob, conf, closeables);
+           iter.hasNext(); ) {
+        Pair<Writable, Vector> p = iter.next();
+        Vector v = p.getSecond();
+        double[] dd = new double[v.size()];
+        if (v.isDense()) {
+          for (int i = 0; i < v.size(); i++) {
+            dd[i] = v.getQuick(i);
+          }
+        } else {
+          for (Vector.Element el : v.nonZeroes()) {
+            dd[el.index()] = el.get();
+          }
         }
-        // ignore row label.
-        denseData.add(row);
+        denseData.add(dd);
       }
+      if (denseData.size() == 0) {
+        return null;
+      } else {
+        return new DenseMatrix(denseData.toArray(new double[denseData.size()][]));
+      }
+    } finally {
+      IOUtils.close(closeables);
     }
-
-    return denseData.toArray(new double[denseData.size()][]);
   }
 
   /**
@@ -217,9 +229,9 @@
    *
    * @return the sum of upper triangular inputs.
    */
-  public static UpperTriangular loadAndSumUpperTriangularMatrices(Path glob, Configuration conf) throws IOException {
+  public static DenseSymmetricMatrix loadAndSumUpperTriangularMatricesAsSymmetric(Path glob, Configuration conf) throws IOException {
     Vector v = loadAndSumUpVectors(glob, conf);
-    return v == null ? null : new UpperTriangular(v);
+    return v == null ? null : new DenseSymmetricMatrix(v);
   }
 
   /**
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDSolver.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDSolver.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDSolver.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,35 +17,30 @@
 
 package org.apache.mahout.math.hadoop.stochasticsvd;
 
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Deque;
-import java.util.Random;
-
+import com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Writable;
 import org.apache.mahout.common.IOUtils;
 import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.DistributedRowMatrixWriter;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.*;
 import org.apache.mahout.math.function.Functions;
-import org.apache.mahout.math.ssvd.EigenSolverWrapper;
+import org.apache.mahout.math.solver.EigenDecomposition;
 
-import com.google.common.collect.Lists;
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Deque;
+import java.util.Random;
 
 /**
  * Stochastic SVD solver (API class).
- * <P>
- * 
+ * <p/>
+ * <p/>
  * Implementation details are in my working notes in MAHOUT-376
  * (https://issues.apache.org/jira/browse/MAHOUT-376).
- * <P>
- * 
+ * <p/>
+ * <p/>
  * As of the time of this writing, I don't have benchmarks for this method in
  * comparison to other methods. However, non-hadoop differentiating
  * characteristics of this method are thought to be :
@@ -56,9 +51,9 @@
  * Lanczos unless full rank SVD decomposition is sought.
  * <LI>"more scale" -- can presumably take on larger problems than Lanczos one
  * (not confirmed by benchmark at this time)
- * <P>
- * <P>
- * 
+ * <p/>
+ * <p/>
+ * <p/>
  * Specifically in regards to this implementation, <i>I think</i> couple of
  * other differentiating points are:
  * <LI>no need to specify input matrix height or width in command line, it is
@@ -68,12 +63,12 @@
  * <LI>can request U or V or U<sub>&sigma;</sub>=U* &Sigma;<sup>0.5</sup> or
  * V<sub>&sigma;</sub>=V* &Sigma;<sup>0.5</sup> none of which would require pass
  * over input A and these jobs are parallel map-only jobs.
- * <P>
- * <P>
- * 
+ * <p/>
+ * <p/>
+ * <p/>
  * This class is central public API for SSVD solver. The use pattern is as
  * follows:
- * 
+ * <p/>
  * <UL>
  * <LI>create the solver using constructor and supplying computation parameters.
  * <LI>set optional parameters thru setter methods.
@@ -82,7 +77,7 @@
  * containing m x k U matrix file(s).
  * <LI> {@link #getVPath()} (if computed) returns the path to the directory
  * containing n x k V matrix file(s).
- * 
+ * <p/>
  * </UL>
  */
 public final class SSVDSolver {
@@ -117,29 +112,24 @@
   private boolean broadcast = true;
   private Path pcaMeanPath;
 
+  // for debugging
+  private long omegaSeed;
+
   /**
    * create new SSVD solver. Required parameters are passed to constructor to
    * ensure they are set. Optional parameters can be set using setters .
-   * <P>
-   * 
-   * @param conf
-   *          hadoop configuration
-   * @param inputPath
-   *          Input path (should be compatible with DistributedRowMatrix as of
-   *          the time of this writing).
-   * @param outputPath
-   *          Output path containing U, V and singular values vector files.
-   * @param ablockRows
-   *          The vertical hight of a q-block (bigger value require more memory
-   *          in mappers+ perhaps larger {@code minSplitSize} values
-   * @param k
-   *          desired rank
-   * @param p
-   *          SSVD oversampling parameter
-   * @param reduceTasks
-   *          Number of reduce tasks (where applicable)
-   * @throws IOException
-   *           when IO condition occurs.
+   * <p/>
+   *
+   * @param conf        hadoop configuration
+   * @param inputPath   Input path (should be compatible with DistributedRowMatrix as of
+   *                    the time of this writing).
+   * @param outputPath  Output path containing U, V and singular values vector files.
+   * @param ablockRows  The vertical hight of a q-block (bigger value require more memory
+   *                    in mappers+ perhaps larger {@code minSplitSize} values
+   * @param k           desired rank
+   * @param p           SSVD oversampling parameter
+   * @param reduceTasks Number of reduce tasks (where applicable)
+   * @throws IOException when IO condition occurs.
    */
   public SSVDSolver(Configuration conf,
                     Path[] inputPath,
@@ -164,7 +154,7 @@
   /**
    * sets q, amount of additional power iterations to increase precision
    * (0..2!). Defaults to 0.
-   * 
+   *
    * @param q
    */
   public void setQ(int q) {
@@ -174,7 +164,6 @@
   /**
    * The setting controlling whether to compute U matrix of low rank SSVD.
    * Default true.
-   * 
    */
   public void setComputeU(boolean val) {
     computeU = val;
@@ -182,16 +171,14 @@
 
   /**
    * Setting controlling whether to compute V matrix of low-rank SSVD.
-   * 
-   * @param val
-   *          true if we want to output V matrix. Default is true.
+   *
+   * @param val true if we want to output V matrix. Default is true.
    */
   public void setComputeV(boolean val) {
     computeV = val;
   }
 
   /**
-   * 
    * @param cUHat whether produce U*Sigma^0.5 as well (default false)
    */
   public void setcUHalfSigma(boolean cUHat) {
@@ -199,7 +186,6 @@
   }
 
   /**
-   * 
    * @param cVHat whether produce V*Sigma^0.5 as well (default false)
    */
   public void setcVHalfSigma(boolean cVHat) {
@@ -207,7 +193,6 @@
   }
 
   /**
-   * 
    * @param cUSigma whether produce U*Sigma output as well (default false)
    */
   public void setcUSigma(boolean cUSigma) {
@@ -225,9 +210,8 @@
    * Sometimes, if requested A blocks become larger than a split, we may need to
    * use that to ensure at least k+p rows of A get into a split. This is
    * requirement necessary to obtain orthonormalized Q blocks of SSVD.
-   * 
-   * @param size
-   *          the minimum split size to use
+   *
+   * @param size the minimum split size to use
    */
   public void setMinSplitSize(int size) {
     minSplitSize = size;
@@ -235,7 +219,7 @@
 
   /**
    * This contains k+p singular values resulted from the solver run.
-   * 
+   *
    * @return singlular values (largest to smallest)
    */
   public Vector getSingularValues() {
@@ -244,7 +228,7 @@
 
   /**
    * returns U path (if computation were requested and successful).
-   * 
+   *
    * @return U output hdfs path, or null if computation was not completed for
    *         whatever reason.
    */
@@ -254,7 +238,7 @@
 
   /**
    * return V path ( if computation was requested and successful ) .
-   * 
+   *
    * @return V output hdfs path, or null if computation was not completed for
    *         whatever reason.
    */
@@ -284,7 +268,7 @@
 
   /**
    * if true, driver to clean output folder first if exists.
-   * 
+   *
    * @param overwrite
    */
   public void setOverwrite(boolean overwrite) {
@@ -300,7 +284,7 @@
    * to produce less keys for combining and shuffle and sort therefore somewhat
    * improving running time; but require larger blocks to be formed in RAM (so
    * setting this too high can lead to OOM).
-   * 
+   *
    * @param outerBlockHeight
    */
   public void setOuterBlockHeight(int outerBlockHeight) {
@@ -316,7 +300,7 @@
    * to set it higher than default 200,000 for extremely sparse inputs and when
    * more ram is available. y_i block height and ABt job would occupy approx.
    * abtBlockHeight x (k+p) x sizeof (double) (as dense).
-   * 
+   *
    * @param abtBlockHeight
    */
   public void setAbtBlockHeight(int abtBlockHeight) {
@@ -330,7 +314,7 @@
   /**
    * If this property is true, use DestributedCache mechanism to broadcast some
    * stuff around. May improve efficiency. Default is false.
-   * 
+   *
    * @param broadcast
    */
   public void setBroadcast(boolean broadcast) {
@@ -340,18 +324,17 @@
   /**
    * Optional. Single-vector file path for a vector (aka xi in MAHOUT-817
    * working notes) to be subtracted from each row of input.
-   * <P>
-   * 
+   * <p/>
+   * <p/>
    * Brute force approach would force would turn input into a dense input, which
    * is often not very desirable. By supplying this offset to SSVD solver, we
    * can avoid most of that overhead due to increased input density.
-   * <P>
-   * 
+   * <p/>
+   * <p/>
    * The vector size for this offest is n (width of A input). In PCA and R this
    * is known as "column means", but in this case it can be any offset of row
    * vectors of course to propagate into SSVD solution.
-   * <P>
-   * 
+   * <p/>
    */
   public Path getPcaMeanPath() {
     return pcaMeanPath;
@@ -361,11 +344,14 @@
     this.pcaMeanPath = pcaMeanPath;
   }
 
+  long getOmegaSeed() {
+    return omegaSeed;
+  }
+
   /**
    * run all SSVD jobs.
-   * 
-   * @throws IOException
-   *           if I/O condition occurs.
+   *
+   * @throws IOException if I/O condition occurs.
    */
   public void run() throws IOException {
 
@@ -388,11 +374,15 @@
 
       Path pcaBasePath = new Path(outputPath, "pca");
 
+      if (overwrite) {
+        fs.delete(outputPath, true);
+      }
+
       if (pcaMeanPath != null) {
         fs.mkdirs(pcaBasePath);
       }
       Random rnd = RandomUtils.getRandom();
-      long seed = rnd.nextLong();
+      omegaSeed = rnd.nextLong();
 
       Path sbPath = null;
       double xisquaredlen = 0.0;
@@ -412,15 +402,10 @@
         }
 
         xisquaredlen = xi.dot(xi);
-        Omega omega = new Omega(seed, k + p);
+        Omega omega = new Omega(omegaSeed, k + p);
         Vector s_b0 = omega.mutlithreadedTRightMultiply(xi);
 
-        SSVDHelper.saveVector(s_b0, sbPath =
-          new Path(pcaBasePath, "somega.seq"), conf);
-      }
-
-      if (overwrite) {
-        fs.delete(outputPath, true);
+        SSVDHelper.saveVector(s_b0, sbPath = new Path(pcaBasePath, "somega.seq"), conf);
       }
 
       /*
@@ -436,7 +421,7 @@
                minSplitSize,
                k,
                p,
-               seed,
+               omegaSeed,
                reduceTasks);
 
       /*
@@ -502,28 +487,21 @@
         sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");
       }
 
-      UpperTriangular bbtTriangular =
-        SSVDHelper.loadAndSumUpperTriangularMatrices(new Path(btPath,
-                                                              BtJob.OUTPUT_BBT
-                                                                  + "-*"), conf);
+      DenseSymmetricMatrix bbt =
+        SSVDHelper.loadAndSumUpperTriangularMatricesAsSymmetric(new Path(btPath,
+                                                                         BtJob.OUTPUT_BBT
+                                                                           + "-*"), conf);
 
       // convert bbt to something our eigensolver could understand
-      assert bbtTriangular.columnSize() == k + p;
+      assert bbt.columnSize() == k + p;
 
       /*
        * we currently use a 3rd party in-core eigensolver. So we need just a
        * dense array representation for it.
        */
-      DenseMatrix bbtSquare = new DenseMatrix(k + p, k + p);
+      Matrix bbtSquare = new DenseMatrix(k + p, k + p);
+      bbtSquare.assign(bbt);
 
-      for (int i = 0; i < k + p; i++) {
-        for (int j = i; j < k + p; j++) {
-          double val = bbtTriangular.getQuick(i, j);
-          bbtSquare.setQuick(i, j, val);
-          bbtSquare.setQuick(j, i, val);
-        }
-      }
-      
       // MAHOUT-817
       if (pcaMeanPath != null) {
         Vector sq = SSVDHelper.loadAndSumUpVectors(sqPath, conf);
@@ -539,16 +517,17 @@
 
       }
 
-      EigenSolverWrapper eigenWrapper = new EigenSolverWrapper(SSVDHelper.extractRawData(bbtSquare));
-      Matrix uHat = new DenseMatrix(eigenWrapper.getUHat());
-      svalues = new DenseVector(eigenWrapper.getEigenValues());
+      EigenDecomposition eigen = new EigenDecomposition(bbtSquare);
+
+      Matrix uHat = eigen.getV();
+      svalues = eigen.getRealEigenvalues().clone();
 
       svalues.assign(Functions.SQRT);
 
       // save/redistribute UHat
       fs.mkdirs(uHatPath);
       DistributedRowMatrixWriter.write(uHatPath =
-        new Path(uHatPath, "uhat.seq"), conf, uHat);
+                                         new Path(uHatPath, "uhat.seq"), conf, uHat);
 
       // save sigma.
       SSVDHelper.saveVector(svalues,
@@ -574,28 +553,28 @@
       if (cUHalfSigma) {
         uhsjob = new UJob();
         uhsjob.run(conf,
-                 new Path(btPath, BtJob.OUTPUT_Q + "-*"),
-                 uHatPath,
-                 svPath,
-                 uHalfSigmaPath,
-                 k,
-                 reduceTasks,
-                 labelType,
-                 OutputScalingEnum.HALFSIGMA);
+                   new Path(btPath, BtJob.OUTPUT_Q + "-*"),
+                   uHatPath,
+                   svPath,
+                   uHalfSigmaPath,
+                   k,
+                   reduceTasks,
+                   labelType,
+                   OutputScalingEnum.HALFSIGMA);
       }
 
       UJob usjob = null;
       if (cUSigma) {
         usjob = new UJob();
         usjob.run(conf,
-                 new Path(btPath, BtJob.OUTPUT_Q + "-*"),
-                 uHatPath,
-                 svPath,
-                 uSigmaPath,
-                 k,
-                 reduceTasks,
-                 labelType,
-                 OutputScalingEnum.SIGMA);
+                  new Path(btPath, BtJob.OUTPUT_Q + "-*"),
+                  uHatPath,
+                  svPath,
+                  uSigmaPath,
+                  k,
+                  reduceTasks,
+                  labelType,
+                  OutputScalingEnum.SIGMA);
       }
 
       VJob vjob = null;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -144,8 +144,7 @@
       Path sigmaPath = new Path(context.getConfiguration().get(PROP_SIGMA_PATH));
       FileSystem fs = FileSystem.get(uHatPath.toUri(), context.getConfiguration());
 
-      uHat = new DenseMatrix(SSVDHelper.loadDistributedRowMatrix(fs,
-          uHatPath, context.getConfiguration()));
+      uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, context.getConfiguration());
       // since uHat is (k+p) x (k+p)
       kp = uHat.columnSize();
       k = context.getConfiguration().getInt(PROP_K, kp);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangular.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangular.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangular.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangular.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,160 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.hadoop.stochasticsvd;
-
-import org.apache.mahout.math.AbstractMatrix;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.IndexException;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.MatrixView;
-import org.apache.mahout.math.Vector;
-
-/**
- * 
- * Quick and dirty implementation of some {@link Matrix} methods 
- * over packed upper triangular matrix.
- *
- */
-public class UpperTriangular extends AbstractMatrix {
-
-  private static final double EPSILON = 1.0e-12; // assume anything less than
-                                                 // that to be 0 during
-                                                 // non-upper assignments
-
-  private double[] values;
-
-  /**
-   * represents n x n upper triangular matrix
-   * 
-   * @param n
-   */
-
-  public UpperTriangular(int n) {
-    super(n, n);
-    values = new double[n * (n + 1) / 2];
-  }
-
-  public UpperTriangular(double[] data, boolean shallow) {
-    this(elementsToMatrixSize(data != null ? data.length : 0));
-    if (data == null) {
-      throw new IllegalArgumentException("data");
-    }
-    values = shallow ? data : data.clone();
-  }
-
-  public UpperTriangular(Vector data) {
-    this(elementsToMatrixSize(data.size()));
-
-    values = new double[rows * (rows + 1) / 2];
-    rows = data.size();
-    for (int i = 0; i < rows; i++) {
-      values[i] = data.getQuick(i);
-    }
-  }
-
-  private static int elementsToMatrixSize(int size) {
-    return (int) Math.round((-1 + Math.sqrt(1 + 8 * size)) / 2);
-  }
-
-  // copy-constructor
-  public UpperTriangular(UpperTriangular mx) {
-    this(mx.values, false);
-  }
-
-  @Override
-  public Matrix assignColumn(int column, Vector other) {
-    if (columnSize() != other.size()) {
-      throw new IndexException(columnSize(), other.size());
-    }
-    if (other.viewPart(column + 1, other.size() - column - 1).norm(1) > 1.0e-14) {
-      throw new IllegalArgumentException("Cannot set lower portion of triangular matrix to non-zero");
-    }
-    for (Vector.Element element : other.viewPart(0, column).all()) {
-      setQuick(element.index(), column, element.get());
-    }
-    return this;
-  }
-
-  @Override
-  public Matrix assignRow(int row, Vector other) {
-    if (columnSize() != other.size()) {
-      throw new IndexException(numCols(), other.size());
-    }
-    for (int i = 0; i < row; i++) {
-      if (Math.abs(other.getQuick(i)) > EPSILON) {
-        throw new IllegalArgumentException("non-triangular source");
-      }
-    }
-    for (int i = row; i < rows; i++) {
-      setQuick(row, i, other.get(i));
-    }
-    return this;
-  }
-
-  public Matrix assignNonZeroElementsInRow(int row, double[] other) {
-    System.arraycopy(other, row, values, getL(row, row), rows - row);
-    return this;
-  }
-
-  @Override
-  public double getQuick(int row, int column) {
-    if (row > column) {
-      return 0;
-    }
-    int i = getL(row, column);
-    return values[i];
-  }
-
-  private int getL(int row, int col) {
-    /*
-     * each row starts with some zero elements that we don't store. this
-     * accumulates an offset of (row+1)*row/2
-     */
-    return col + row * numCols() - (row + 1) * row / 2;
-  }
-
-  @Override
-  public Matrix like() {
-    return like(rowSize(), columnSize());
-  }
-
-  @Override
-  public Matrix like(int rows, int columns) {
-    return new DenseMatrix(rows, columns);
-  }
-
-  @Override
-  public void setQuick(int row, int column, double value) {
-    values[getL(row, column)] = value;
-  }
-
-  @Override
-  public int[] getNumNondefaultElements() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public Matrix viewPart(int[] offset, int[] size) {
-    return new MatrixView(this, offset, size);
-  }
-
-  public double[] getData() {
-    return values;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/VJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/VJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/VJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/VJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -101,8 +101,7 @@
 
       Path sigmaPath = new Path(conf.get(PROP_SIGMA_PATH));
 
-      uHat =
-        new DenseMatrix(SSVDHelper.loadDistributedRowMatrix(fs, uHatPath, conf));
+      uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, conf);
       // since uHat is (k+p) x (k+p)
       kp = uHat.columnSize();
       k = context.getConfiguration().getInt(PROP_K, kp);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/YtYJob.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/YtYJob.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/YtYJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/YtYJob.java	2014-03-29 01:03:13.000000000 -0700
@@ -16,8 +16,6 @@
  */
 package org.apache.mahout.math.hadoop.stochasticsvd;
 
-import java.io.IOException;
-
 import org.apache.commons.lang3.Validate;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -32,12 +30,14 @@
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
 import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.UpperTriangular;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
 
+import java.io.IOException;
+
 /**
  * Job that accumulates Y'Y output
- * 
  */
 public final class YtYJob {
 
@@ -52,7 +52,7 @@
   }
 
   public static class YtYMapper extends
-      Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
+    Mapper<Writable, VectorWritable, IntWritable, VectorWritable> {
 
     private int kp;
     private Omega omega;
@@ -131,13 +131,13 @@
     protected void cleanup(Context context) throws IOException,
       InterruptedException {
       context.write(new IntWritable(context.getTaskAttemptID().getTaskID()
-                                           .getId()),
+                                      .getId()),
                     new VectorWritable(new DenseVector(mYtY.getData())));
     }
   }
 
   public static class YtYReducer extends
-      Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
+    Reducer<IntWritable, VectorWritable, IntWritable, VectorWritable> {
     private final VectorWritable accum = new VectorWritable();
     private DenseVector acc;
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/GivensThinSolver.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/GivensThinSolver.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/GivensThinSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/GivensThinSolver.java	2014-03-29 01:03:13.000000000 -0700
@@ -27,7 +27,7 @@
 import org.apache.mahout.math.Matrix;
 import org.apache.mahout.math.OrderedIntDoubleMapping;
 import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.hadoop.stochasticsvd.UpperTriangular;
+import org.apache.mahout.math.UpperTriangular;
 
 /**
  * Givens Thin solver. Standard Givens operations are reordered in a way that
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRFirstStep.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRFirstStep.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRFirstStep.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRFirstStep.java	2014-03-29 01:03:13.000000000 -0700
@@ -41,7 +41,7 @@
 import org.apache.mahout.math.Vector.Element;
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.hadoop.stochasticsvd.DenseBlockWritable;
-import org.apache.mahout.math.hadoop.stochasticsvd.UpperTriangular;
+import org.apache.mahout.math.UpperTriangular;
 
 import com.google.common.collect.Lists;
 import com.google.common.io.Closeables;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRLastStep.java mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRLastStep.java
--- mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRLastStep.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/qr/QRLastStep.java	2014-03-29 01:03:13.000000000 -0700
@@ -29,7 +29,7 @@
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.hadoop.stochasticsvd.DenseBlockWritable;
-import org.apache.mahout.math.hadoop.stochasticsvd.UpperTriangular;
+import org.apache.mahout.math.UpperTriangular;
 
 import com.google.common.collect.Lists;
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/neighborhood/BruteSearch.java mahout/core/src/main/java/org/apache/mahout/math/neighborhood/BruteSearch.java
--- mahout/core/src/main/java/org/apache/mahout/math/neighborhood/BruteSearch.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/neighborhood/BruteSearch.java	2014-03-29 01:03:13.000000000 -0700
@@ -68,7 +68,7 @@
    */
   @Override
   public List<WeightedThing<Vector>> search(Vector query, int limit) {
-    Preconditions.checkArgument(limit > 0);
+    Preconditions.checkArgument(limit > 0, "limit must be greater then 0!");
     limit = Math.min(limit, referenceVectors.size());
     // A priority queue of the best @limit elements, ordered from worst to best so that the worst
     // element is always on top and can easily be removed.
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/neighborhood/FastProjectionSearch.java mahout/core/src/main/java/org/apache/mahout/math/neighborhood/FastProjectionSearch.java
--- mahout/core/src/main/java/org/apache/mahout/math/neighborhood/FastProjectionSearch.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/neighborhood/FastProjectionSearch.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.neighborhood;
 
 import java.util.Collections;
@@ -63,7 +80,7 @@
   public FastProjectionSearch(DistanceMeasure distanceMeasure, int numProjections, int searchSize) {
     super(distanceMeasure);
     Preconditions.checkArgument(numProjections > 0 && numProjections < 100,
-        "Unreasonable value for number of projections");
+        "Unreasonable value for number of projections. Must be: 0 < numProjections < 100");
     this.numProjections = numProjections;
     this.searchSize = searchSize;
     scalarProjections = Lists.newArrayListWithCapacity(numProjections);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/neighborhood/LocalitySensitiveHashSearch.java mahout/core/src/main/java/org/apache/mahout/math/neighborhood/LocalitySensitiveHashSearch.java
--- mahout/core/src/main/java/org/apache/mahout/math/neighborhood/LocalitySensitiveHashSearch.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/neighborhood/LocalitySensitiveHashSearch.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.neighborhood;
 
 import java.util.Collections;
@@ -171,12 +188,15 @@
   @Override
   public List<WeightedThing<Vector>> search(Vector query, int limit) {
     PriorityQueue<WeightedThing<Vector>> top = searchInternal(query);
-    List<WeightedThing<Vector>> results = Lists.newArrayListWithExpectedSize(limit);
-    while (limit > 0 && top.size() != 0) {
+    List<WeightedThing<Vector>> results = Lists.newArrayListWithExpectedSize(top.size());
+    while (top.size() != 0) {
       WeightedThing<Vector> wv = top.pop();
       results.add(new WeightedThing<Vector>(((HashedVector) wv.getValue()).getVector(), wv.getWeight()));
     }
     Collections.reverse(results);
+    if (limit < results.size()) {
+      results = results.subList(0, limit);
+    }
     return results;
   }
 
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/neighborhood/ProjectionSearch.java mahout/core/src/main/java/org/apache/mahout/math/neighborhood/ProjectionSearch.java
--- mahout/core/src/main/java/org/apache/mahout/math/neighborhood/ProjectionSearch.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/neighborhood/ProjectionSearch.java	2014-03-29 01:03:13.000000000 -0700
@@ -79,7 +79,7 @@
   public ProjectionSearch(DistanceMeasure distanceMeasure, int numProjections,  int searchSize) {
     super(distanceMeasure);
     Preconditions.checkArgument(numProjections > 0 && numProjections < 100,
-        "Unreasonable value for number of projections");
+        "Unreasonable value for number of projections. Must be: 0 < numProjections < 100");
 
     this.searchSize = searchSize;
     this.numProjections = numProjections;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/random/RandomProjector.java mahout/core/src/main/java/org/apache/mahout/math/random/RandomProjector.java
--- mahout/core/src/main/java/org/apache/mahout/math/random/RandomProjector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/random/RandomProjector.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.random;
 
 import java.util.List;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/ssvd/SequentialOutOfCoreSvd.java mahout/core/src/main/java/org/apache/mahout/math/ssvd/SequentialOutOfCoreSvd.java
--- mahout/core/src/main/java/org/apache/mahout/math/ssvd/SequentialOutOfCoreSvd.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/ssvd/SequentialOutOfCoreSvd.java	2014-03-29 01:03:13.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.ssvd;
 
 import org.apache.mahout.math.CholeskyDecomposition;
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyMapper.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyMapper.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * Calculates the entropy for the value with H(x) = x * log(x)
- */
-@Deprecated
-public final class CalculateEntropyMapper extends Mapper<Text, VarIntWritable, NullWritable, DoubleWritable> {
-
-  private final DoubleWritable result = new DoubleWritable();
-
-  @Override
-  protected void map(Text key, VarIntWritable value, Context context) throws IOException, InterruptedException {
-    result.set(value.get() * Math.log(value.get()));
-    context.write(NullWritable.get(), result);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyReducer.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyReducer.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateEntropyReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapreduce.Reducer;
-
-import java.io.IOException;
-
-/**
- * Subtracts the partial entropy.
- */
-@Deprecated
-public final class CalculateEntropyReducer
-    extends Reducer<NullWritable, DoubleWritable, NullWritable, DoubleWritable> {
-
-  private static final double LOG_2 = Math.log(2.0);
-
-  private final DoubleWritable result = new DoubleWritable();
-  private long numberItems;
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    numberItems = Long.parseLong(context.getConfiguration().get(Entropy.NUMBER_ITEMS_PARAM));
-  }
-
-  @Override
-  protected void reduce(NullWritable key, Iterable<DoubleWritable> values, Context context)
-    throws IOException, InterruptedException {
-    double entropy = 0.0;
-    for (DoubleWritable value : values) {
-      entropy += value.get();
-    }
-    result.set((Math.log(numberItems) - entropy / numberItems) / LOG_2);
-    context.write(key, result);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateSpecificConditionalEntropyMapper.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateSpecificConditionalEntropyMapper.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateSpecificConditionalEntropyMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/CalculateSpecificConditionalEntropyMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-
-import java.io.IOException;
-
-/**
- * Drops the key.
- */
-@Deprecated
-public final class CalculateSpecificConditionalEntropyMapper
-    extends Mapper<Text, DoubleWritable, NullWritable, DoubleWritable> {
-
-  @Override
-  protected void map(Text key, DoubleWritable value, Context context) throws IOException, InterruptedException {
-    context.write(NullWritable.get(), value);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ConditionalEntropy.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,133 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.StringTuple;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * A Hadoop job to compute the conditional entropy H(Value|Key) for a sequence file.
- * <ul>
- * <li>-i The input sequence file</li>
- * <li>-o The output sequence file</li>
- * </ul>
- */
-@Deprecated
-public final class ConditionalEntropy extends AbstractJob {
-
-  private long numberItems;
-
-  private Path keyValueCountPath;
-  private Path specificConditionalEntropyPath;
-
-  private static final String KEY_VALUE_COUNT_FILE = "key_value_count";
-  private static final String SPECIFIC_CONDITIONAL_ENTROPY_FILE = "specific_conditional_entropy";
-  static final String NUMBER_ITEMS_PARAM = "items.number";
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new Entropy(), args);
-  }
-
-  @Override
-  public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
-    prepareArguments(args);
-    groupAndCountByKeyAndValue();
-    calculateSpecificConditionalEntropy();
-    calculateConditionalEntropy();
-    return 0;
-  }
-
-  /**
-   * Prepares and sets the arguments.
-   */
-  private void prepareArguments(String[] args) throws IOException {
-    addInputOption();
-    addOutputOption();
-    parseArguments(args);
-    keyValueCountPath = new Path(getTempPath(), KEY_VALUE_COUNT_FILE + '-' + System.currentTimeMillis());
-    specificConditionalEntropyPath =
-        new Path(getTempPath(), SPECIFIC_CONDITIONAL_ENTROPY_FILE + '_' + System.currentTimeMillis());
-  }
-
-  /**
-   * Groups and counts by key and value.
-   * SQL-like: SELECT key, value, COUNT(*) FROM x GROUP BY key, value
-   */
-  private void groupAndCountByKeyAndValue() throws IOException, ClassNotFoundException, InterruptedException {
-
-    Job job = prepareJob(getInputPath(), keyValueCountPath, SequenceFileInputFormat.class,
-        GroupAndCountByKeyAndValueMapper.class, StringTuple.class, VarIntWritable.class, VarIntSumReducer.class,
-        StringTuple.class, VarIntWritable.class, SequenceFileOutputFormat.class);
-    job.setCombinerClass(VarIntSumReducer.class);
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-    numberItems =
-        job.getCounters().findCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS").getValue();
-
-  }
-
-  /**
-   * Calculates the specific conditional entropy which is H(Y|X).
-   * Needs the number of all items for normalizing.
-   */
-  private void calculateSpecificConditionalEntropy() throws IOException, ClassNotFoundException, InterruptedException {
-
-    Job job = prepareJob(keyValueCountPath, specificConditionalEntropyPath, SequenceFileInputFormat.class,
-        SpecificConditionalEntropyMapper.class, Text.class, VarIntWritable.class,
-        SpecificConditionalEntropyReducer.class, Text.class, DoubleWritable.class,
-        SequenceFileOutputFormat.class);
-    job.getConfiguration().set(NUMBER_ITEMS_PARAM, String.valueOf(numberItems));
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-  }
-
-  /**
-   * Sums the calculated specific conditional entropy. Output is in the value.
-   */
-  private void calculateConditionalEntropy() throws IOException, ClassNotFoundException, InterruptedException {
-
-    Job job = prepareJob(specificConditionalEntropyPath, getOutputPath(), SequenceFileInputFormat.class,
-        CalculateSpecificConditionalEntropyMapper.class, NullWritable.class, DoubleWritable.class,
-        DoubleSumReducer.class, NullWritable.class, DoubleWritable.class,
-        SequenceFileOutputFormat.class);
-    job.setCombinerClass(DoubleSumReducer.class);
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/DoubleSumReducer.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/DoubleSumReducer.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/DoubleSumReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/DoubleSumReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Reducer;
-
-import java.io.IOException;
-
-/**
- * Analog of {@link org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer} which sums the double values.
- */
-@Deprecated
-public final class DoubleSumReducer extends Reducer<Writable, DoubleWritable, Writable, DoubleWritable> {
-
-  private final DoubleWritable result = new DoubleWritable();
-
-  @Override
-  protected void reduce(Writable key, Iterable<DoubleWritable> values, Context context)
-    throws IOException, InterruptedException {
-    double sum = 0.0;
-    for (DoubleWritable value : values) {
-      sum += value.get();
-    }
-    result.set(sum);
-    context.write(key, result);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/Entropy.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-/**
- * A Hadoop job to compute the entropy of keys or values in a {@link org.apache.hadoop.io.SequenceFile}.
- * Format has to be {@link Text} for key or value.
- * <p/>
- * <ul>
- * <li>-i The input sequence file</li>
- * <li>-o The output sequence file</li>
- * <li>-s The source. Can be \<key\> or \<value\>. Default is \<key\></li>
- * </ul>
- */
-@Deprecated
-public final class Entropy extends AbstractJob {
-
-  private Path tempPath;
-  private long numberItems;
-  private String source;
-
-  private static final String TEMP_FILE = "temp";
-  static final String NUMBER_ITEMS_PARAM = "number.items";
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new Entropy(), args);
-  }
-
-  /**
-   * Returns the number of elements in the file. Only works after run.
-   *
-   * @return The number of processed items
-   */
-  public long getNumberItems() {
-    return numberItems;
-  }
-
-  @Override
-  public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
-
-    prepareArguments(args);
-    groupAndCount();
-    calculateEntropy();
-
-    return 1;
-  }
-
-  /**
-   * Prepares and sets the arguments.
-   *
-   * @param args
-   * @throws IOException
-   */
-  private void prepareArguments(String[] args) throws IOException {
-
-    addInputOption();
-    addOutputOption();
-    addOption("source", "s", "Sets, if the entropy is calculated for the keys or the values. Can be <key> or <value>"
-        , "key");
-
-    Map<String, List<String>> arguments = parseArguments(args);
-    if (arguments == null) {
-      return;
-    }
-    source = getOption("source");
-    tempPath = new Path(getTempPath(), TEMP_FILE + '-' + System.currentTimeMillis());
-
-  }
-
-
-  /**
-   * Groups the items and counts the occur for each of them.
-   * SQL-like: SELECT item, COUNT(*) FROM x GROUP BY item
-   *
-   * @throws IOException
-   * @throws ClassNotFoundException
-   * @throws InterruptedException
-   */
-  private void groupAndCount() throws IOException, ClassNotFoundException, InterruptedException {
-
-    Class<? extends Mapper> mapper = "key".equals(source) ? KeyCounterMapper.class : ValueCounterMapper.class;
-
-    Job job = prepareJob(getInputPath(), tempPath, SequenceFileInputFormat.class, mapper, Text.class,
-        VarIntWritable.class, VarIntSumReducer.class, Text.class, VarIntWritable.class,
-        SequenceFileOutputFormat.class);
-    job.setCombinerClass(VarIntSumReducer.class);
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-    numberItems =
-        job.getCounters().findCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS").getValue();
-
-  }
-
-  /**
-   * Calculates the entropy with
-   * <p/>
-   * H(X) = -sum_i(x_i/n * log_2(x_i/n))  WITH n = sum_i(x_i)
-   * = -sum_i(x_i/n * (log_2(x_i) - log_2(n)))
-   * = -sum_i(x_i/n * log_2(x_i)) + sum_i(x_i/n * log_2(n))
-   * = (n * log_2(n) - sum_i(x_i * log_2(x_i)) / n
-   * = log_2(n) - sum_i(x_i * log_2(x_i)) / n
-   * = (log(n) - sum_i(x_i * log(x_i)) / n) / log(2)
-   */
-  private void calculateEntropy() throws IOException, ClassNotFoundException, InterruptedException {
-
-    Job job = prepareJob(tempPath, getOutputPath(), SequenceFileInputFormat.class, CalculateEntropyMapper.class,
-        NullWritable.class, DoubleWritable.class, CalculateEntropyReducer.class, NullWritable.class,
-        DoubleWritable.class, SequenceFileOutputFormat.class);
-    job.getConfiguration().set(NUMBER_ITEMS_PARAM, String.valueOf(numberItems));
-    job.setCombinerClass(DoubleSumReducer.class);
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/GroupAndCountByKeyAndValueMapper.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/GroupAndCountByKeyAndValueMapper.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/GroupAndCountByKeyAndValueMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/GroupAndCountByKeyAndValueMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.common.StringTuple;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * Groups the input by key and value. Therefore it merges both to one key of type {@link StringTuple} and emits
- * {@link VarIntWritable}(1) as value.
- */
-@Deprecated
-public final class GroupAndCountByKeyAndValueMapper extends Mapper<Text, Text, StringTuple, VarIntWritable> {
-
-  private static final VarIntWritable ONE = new VarIntWritable(1);
-
-  @Override
-  protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
-    StringTuple tuple = new StringTuple(key.toString());
-    tuple.add(value.toString());
-    context.write(tuple, ONE);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGain.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGain.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGain.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGain.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-/**
- * Calculates the information gain for a {@link org.apache.hadoop.io.SequenceFile}.
- * Computes, how 'useful' are the keys when predicting the values.
- * <ul>
- * <li>-i The input sequence file</li>
- * </ul>
- */
-@Deprecated
-public final class InformationGain extends AbstractJob {
-
-  private static final String ENTROPY_FILE = "entropy";
-  private static final String CONDITIONAL_ENTROPY_FILE = "conditional_entropy";
-
-  private Path entropyPath;
-  private Path conditionalEntropyPath;
-  private double entropy;
-  private double conditionalEntropy;
-  private double informationGain;
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new Entropy(), args);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    prepareArguments(args);
-    calculateEntropy();
-    calculateConditionalEntropy();
-    calculateInformationGain();
-    return 0;
-  }
-
-  public double getEntropy() {
-    return entropy;
-  }
-
-  public double getConditionalEntropy() {
-    return conditionalEntropy;
-  }
-
-  public double getInformationGain() {
-    return informationGain;
-  }
-
-  /**
-   * Prepares and sets the arguments.
-   */
-  private void prepareArguments(String[] args) throws IOException {
-    addInputOption();
-    parseArguments(args);
-    entropyPath = new Path(getTempPath(), ENTROPY_FILE + '-' + System.currentTimeMillis());
-    conditionalEntropyPath = new Path(getTempPath(), CONDITIONAL_ENTROPY_FILE + '-' + System.currentTimeMillis());
-  }
-
-  private void calculateEntropy() throws Exception {
-    String[] args = {
-      "-i", getInputPath().toString(),
-      "-o", entropyPath.toString(),
-      "-s", "value",
-      "--tempDir", getTempPath().toString(),
-    };
-    ToolRunner.run(getConf(), new Entropy(), args);
-    entropy = readDoubleFromPath(entropyPath);
-  }
-
-  private void calculateConditionalEntropy() throws Exception {
-    String[] args = {
-      "-i", getInputPath().toString(),
-      "-o", conditionalEntropyPath.toString(),
-      "--tempDir", getTempPath().toString(),
-    };
-    ToolRunner.run(getConf(), new ConditionalEntropy(), args);
-    conditionalEntropy = readDoubleFromPath(conditionalEntropyPath);
-  }
-
-  private void calculateInformationGain() {
-    informationGain = entropy - conditionalEntropy;
-  }
-
-  private static double readDoubleFromPath(Path path) throws IOException {
-    Iterator<DoubleWritable> iteratorNodes =
-        new SequenceFileDirValueIterator<DoubleWritable>(path,
-                                                         PathType.LIST,
-                                                         PathFilters.logsCRCFilter(),
-                                                         null,
-                                                         false,
-                                                         new Configuration());
-    if (!iteratorNodes.hasNext()) {
-      throw new IllegalArgumentException("Can't read double value from " + path.toString());
-    }
-    return iteratorNodes.next().get();
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGainRatio.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGainRatio.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGainRatio.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/InformationGainRatio.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,62 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.AbstractJob;
-
-/**
- * A job to calculate the normalized information gain.
- * <ul>
- * <li>-i The input sequence file</li>
- * </ul>
- */
-@Deprecated
-public final class InformationGainRatio extends AbstractJob {
-
-  private double entropy;
-  private double informationGain;
-  private double informationGainRatio;
-
-  public static void main(String[] args) throws Exception {
-    ToolRunner.run(new InformationGainRatio(), args);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    InformationGain job = new InformationGain();
-    ToolRunner.run(getConf(), job, args);
-    informationGain = job.getInformationGain();
-    entropy = job.getEntropy();
-    informationGainRatio = informationGain / entropy;
-    return 0;
-  }
-
-  public double getEntropy() {
-    return entropy;
-  }
-
-  public double getInformationGain() {
-    return informationGain;
-  }
-
-  public double getInformationGainRatio() {
-    return informationGainRatio;
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/KeyCounterMapper.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/KeyCounterMapper.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/KeyCounterMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/KeyCounterMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * Emits the key and the count of 1 as {@link VarIntWritable}.
- */
-@Deprecated
-public final class KeyCounterMapper extends Mapper<Writable, Object, Writable, VarIntWritable> {
-
-  private static final VarIntWritable ONE = new VarIntWritable(1);
-
-  @Override
-  protected void map(Writable key, Object value, Context context) throws IOException, InterruptedException {
-    context.write(key, ONE);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyMapper.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyMapper.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.common.StringTuple;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * Converts the key from {@link StringTuple} with values [key, value] to {@link Text} with value key.
- */
-@Deprecated
-public class SpecificConditionalEntropyMapper extends Mapper<StringTuple, VarIntWritable, Text, VarIntWritable> {
-
-  private final Text resultKey = new Text();
-
-  @Override
-  protected void map(StringTuple key, VarIntWritable value, Context context)
-    throws IOException, InterruptedException {
-    resultKey.set(key.stringAt(0));
-    context.write(resultKey, value);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyReducer.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyReducer.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/SpecificConditionalEntropyReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * Does the weighted conditional entropy calculation with
- * <p/>
- * H(values|key) = p(key) * sum_i(p(values_i|key) * log_2(p(values_i|key)))
- * = p(key) * (log(|key|) - sum_i(values_i * log_2(values_i)) / |key|)
- * = (sum * log_2(sum) - sum_i(values_i * log_2(values_i))/n WITH sum = sum_i(values_i)
- * = (sum * log(sum) - sum_i(values_i * log(values_i)) / (n * log(2))
- */
-@Deprecated
-public final class SpecificConditionalEntropyReducer extends Reducer<Text, VarIntWritable, Text, DoubleWritable> {
-  
-  private static final double LOG2 = Math.log(2.0);
-
-  private final DoubleWritable result = new DoubleWritable();
-  private double numberItemsLog2;
-
-  @Override
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-    numberItemsLog2 = LOG2 * Integer.parseInt(context.getConfiguration().get(ConditionalEntropy.NUMBER_ITEMS_PARAM));
-  }
-
-  @Override
-  protected void reduce(Text key, Iterable<VarIntWritable> values, Context context)
-    throws IOException, InterruptedException {
-    double sum = 0.0;
-    double entropy = 0.0;
-    for (VarIntWritable value : values) {
-      int valueInt = value.get();
-      sum += valueInt;
-      entropy += valueInt * Math.log(valueInt);
-    }
-    result.set((sum * Math.log(sum) - entropy) / numberItemsLog2);
-    context.write(key, result);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ValueCounterMapper.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ValueCounterMapper.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ValueCounterMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/ValueCounterMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * Emits the value and the count of 1 as {@link VarIntWritable}.
- */
-@Deprecated
-public final class ValueCounterMapper extends Mapper<Object, Writable, Writable, VarIntWritable> {
-
-  private static final VarIntWritable ONE = new VarIntWritable(1);
-
-  @Override
-  public void map(Object key, Writable value, Context context) throws IOException, InterruptedException {
-    context.write(value, ONE);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/VarIntSumReducer.java mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/VarIntSumReducer.java
--- mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/VarIntSumReducer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/math/stats/entropy/VarIntSumReducer.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.math.VarIntWritable;
-
-import java.io.IOException;
-
-/**
- * The analog of {@link org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer} which uses {@link VarIntWritable}.
- */
-@Deprecated
-public final class VarIntSumReducer extends Reducer<Writable, VarIntWritable, Writable, VarIntWritable> {
-
-  private final VarIntWritable result = new VarIntWritable();
-
-  @Override
-  protected void reduce(Writable key, Iterable<VarIntWritable> values, Context context)
-    throws IOException, InterruptedException {
-    int sum = 0;
-    for (VarIntWritable value : values) {
-      sum += value.get();
-    }
-    result.set(sum);
-    context.write(key, result);
-  }
-
-}
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java mahout/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java
--- mahout/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java	2014-03-29 01:03:13.000000000 -0700
@@ -76,11 +76,11 @@
 
     Option chunkSizeOpt = obuilder.withLongName("chunkSize").withArgument(
             abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription(
-            "The chunkSize in MegaBytes. 100-10000 MB").withShortName("chunk").create();
+            "The chunkSize in MegaBytes. Default Value: 100MB").withShortName("chunk").create();
 
     Option weightOpt = obuilder.withLongName("weight").withRequired(false).withArgument(
             abuilder.withName("weight").withMinimum(1).withMaximum(1).create()).withDescription(
-            "The kind of weight to use. Currently TF or TFIDF").withShortName("wt").create();
+            "The kind of weight to use. Currently TF or TFIDF. Default: TFIDF").withShortName("wt").create();
 
     Option minDFOpt = obuilder.withLongName("minDF").withRequired(false).withArgument(
             abuilder.withName("minDF").withMinimum(1).withMaximum(1).create()).withDescription(
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/vectorizer/document/SequenceFileTokenizerMapper.java mahout/core/src/main/java/org/apache/mahout/vectorizer/document/SequenceFileTokenizerMapper.java
--- mahout/core/src/main/java/org/apache/mahout/vectorizer/document/SequenceFileTokenizerMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/vectorizer/document/SequenceFileTokenizerMapper.java	2014-03-29 01:03:13.000000000 -0700
@@ -42,7 +42,6 @@
   @Override
   protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
     TokenStream stream = analyzer.tokenStream(key.toString(), new StringReader(value.toString()));
-    stream.reset();
     CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
     stream.reset();
     StringTuple document = new StringTuple();
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/ContinuousValueEncoder.java mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/ContinuousValueEncoder.java
--- mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/ContinuousValueEncoder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/ContinuousValueEncoder.java	2014-03-29 01:03:13.000000000 -0700
@@ -42,7 +42,7 @@
     for (int i = 0; i < probes; i++) {
       int n = hashForProbe(originalForm, data.size(), name, i);
       if (isTraceEnabled()) {
-        trace((String) null, n);        
+        trace((String) null, n);
       }
       data.set(n, data.get(n) + getWeight(originalForm,weight));
     }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/FeatureVectorEncoder.java mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/FeatureVectorEncoder.java
--- mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/FeatureVectorEncoder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/FeatureVectorEncoder.java	2014-03-29 01:03:13.000000000 -0700
@@ -83,7 +83,7 @@
    * @param data         The vector to which the value should be added.
    */
   public void addToVector(String originalForm, double weight, Vector data) {
-    addToVector(bytesForString(originalForm), weight, data);        
+    addToVector(bytesForString(originalForm), weight, data);
   }
 
   public abstract void addToVector(byte[] originalForm, double weight, Vector data);
@@ -95,9 +95,9 @@
    * should not be used.
    *
    * @param originalForm  The original byte array value
-   * @param dataSize      The length of hte vector being encoded
+   * @param dataSize      The length of the vector being encoded
    * @param name          The name of the variable being encoded
-   * @param probe             The probe number
+   * @param probe         The probe number
    * @return              The hash of the current probe
    */
   protected abstract int hashForProbe(byte[] originalForm, int dataSize, String name, int probe);
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoder.java mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoder.java
--- mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoder.java	2014-03-29 01:03:13.000000000 -0700
@@ -17,10 +17,11 @@
 
 package org.apache.mahout.vectorizer.encoders;
 
+import java.util.Locale;
+
 import org.apache.mahout.math.Vector;
 
-import java.util.Arrays;
-import java.util.Locale;
+import com.google.common.base.Charsets;
 
 public class InteractionValueEncoder extends FeatureVectorEncoder {
   private final FeatureVectorEncoder firstEncoder;
@@ -86,7 +87,8 @@
         for (Integer j : jValues) {
           int n = (k + j) % data.size();
           if (isTraceEnabled()) {
-            trace(String.format("%s:%s", Arrays.toString(originalForm1), Arrays.toString(originalForm2)), n);
+            trace(String.format("%s:%s", new String(originalForm1, Charsets.UTF_8), new String(originalForm2,
+		Charsets.UTF_8)), n);
           }
           data.set(n, data.get(n) + w);
         }
diff -uNar -x .git mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/LuceneTextValueEncoder.java mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/LuceneTextValueEncoder.java
--- mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/LuceneTextValueEncoder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/main/java/org/apache/mahout/vectorizer/encoders/LuceneTextValueEncoder.java	2014-03-29 01:03:13.000000000 -0700
@@ -99,7 +99,7 @@
       // do nothing
     }
   }
-  //GSI: TODO: we really need a way to make sure we call the TokenStream workflow here (i.e. end and close when done)
+
   private static final class LuceneTokenIterable implements Iterable<String> {
     private boolean firstTime = true;
     private final TokenStream tokenStream;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtilsTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtilsTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtilsTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/TasteHadoopUtilsTest.java	2014-03-29 01:03:13.000000000 -0700
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.cf.taste.hadoop;
+
+import org.apache.mahout.cf.taste.impl.TasteTestCase;
+import org.junit.Test;
+
+/** <p>Tests {@link TasteHadoopUtils}.</p> */
+public class TasteHadoopUtilsTest extends TasteTestCase {
+	
+  @Test
+  public void testWithinRange() {
+    assertTrue(TasteHadoopUtils.idToIndex(0) >= 0);
+    assertTrue(TasteHadoopUtils.idToIndex(0) < Integer.MAX_VALUE);
+
+    assertTrue(TasteHadoopUtils.idToIndex(1) >= 0);
+    assertTrue(TasteHadoopUtils.idToIndex(1) < Integer.MAX_VALUE);
+		
+    assertTrue(TasteHadoopUtils.idToIndex(Long.MAX_VALUE) >= 0);
+    assertTrue(TasteHadoopUtils.idToIndex(Long.MAX_VALUE) < Integer.MAX_VALUE);
+		
+    assertTrue(TasteHadoopUtils.idToIndex(Integer.MAX_VALUE) >= 0);
+    assertTrue(TasteHadoopUtils.idToIndex(Integer.MAX_VALUE) < Integer.MAX_VALUE);
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJobTest.java	2014-03-29 01:03:13.000000000 -0700
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils;
 import org.apache.mahout.cf.taste.impl.TasteTestCase;
 import org.apache.mahout.cf.taste.impl.common.FullRunningAverage;
@@ -60,7 +61,7 @@
     outputDir.delete();
     tmpDir = getTestTempDir("tmp");
 
-    conf = new Configuration();
+    conf = getConfiguration();
     // reset as we run all tests in the same JVM
     SharingMapper.reset();
   }
@@ -339,7 +340,10 @@
     int numIterations = 5;
     double lambda = 0.065;
 
-    int success = alsFactorization.run(new String[] {
+    Configuration conf = getConfiguration();
+
+    int success = ToolRunner.run(alsFactorization, new String[] {
+        "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
         "--input", inputFile.getAbsolutePath(),
         "--output", intermediateDir.getAbsolutePath(),
         "--tempDir", tmpDir.getAbsolutePath(),
@@ -356,7 +360,8 @@
 
     RecommenderJob recommender = new RecommenderJob();
 
-    success = recommender.run(new String[] {
+    success = ToolRunner.run(recommender, new String[] {
+        "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
         "--input", intermediateDir.getAbsolutePath() + "/userRatings/",
         "--userFeatures", intermediateDir.getAbsolutePath() + "/U/",
         "--itemFeatures", intermediateDir.getAbsolutePath() + "/M/",
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJobTest.java	2014-03-29 01:03:13.000000000 -0700
@@ -21,7 +21,6 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/AverageAbsoluteDifferenceRecommenderEvaluatorTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/AverageAbsoluteDifferenceRecommenderEvaluatorTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/AverageAbsoluteDifferenceRecommenderEvaluatorTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/AverageAbsoluteDifferenceRecommenderEvaluatorTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.eval;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.eval.RecommenderBuilder;
-import org.apache.mahout.cf.taste.eval.RecommenderEvaluator;
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.junit.Test;
-
-public final class AverageAbsoluteDifferenceRecommenderEvaluatorTest extends TasteTestCase {
-
-  @Test
-  public void testEvaluate() throws Exception {
-    DataModel model = getDataModel();
-    RecommenderBuilder builder = new RecommenderBuilder() {
-      @Override
-      public Recommender buildRecommender(DataModel dataModel) throws TasteException {
-        return new SlopeOneRecommender(dataModel);
-      }
-    };
-    RecommenderEvaluator evaluator =
-        new AverageAbsoluteDifferenceRecommenderEvaluator();
-    double eval = evaluator.evaluate(builder, null, model, 0.85, 1.0);
-    assertEquals(0.29906444251537323, eval, EPSILON);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluatorImplTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluatorImplTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluatorImplTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/GenericRecommenderIRStatsEvaluatorImplTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,6 @@
 
 package org.apache.mahout.cf.taste.impl.eval;
 
-import org.apache.mahout.cf.taste.common.TasteException;
 import org.apache.mahout.cf.taste.eval.DataModelBuilder;
 import org.apache.mahout.cf.taste.eval.IRStatistics;
 import org.apache.mahout.cf.taste.eval.RecommenderBuilder;
@@ -26,7 +25,6 @@
 import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
 import org.apache.mahout.cf.taste.impl.model.GenericBooleanPrefDataModel;
 import org.apache.mahout.cf.taste.impl.recommender.GenericBooleanPrefItemBasedRecommender;
-import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
 import org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity;
 import org.apache.mahout.cf.taste.model.DataModel;
 import org.apache.mahout.cf.taste.model.PreferenceArray;
@@ -36,25 +34,6 @@
 public final class GenericRecommenderIRStatsEvaluatorImplTest extends TasteTestCase {
 
   @Test
-  public void testEvaluate() throws Exception {
-    DataModel model = getDataModel();
-    RecommenderBuilder builder = new RecommenderBuilder() {
-      @Override
-      public Recommender buildRecommender(DataModel dataModel) throws TasteException {
-        return new SlopeOneRecommender(dataModel);
-      }
-    };
-    RecommenderIRStatsEvaluator evaluator = new GenericRecommenderIRStatsEvaluator();
-    IRStatistics stats = evaluator.evaluate(builder, null, model, null, 1, 0.2, 1.0);
-    assertNotNull(stats);
-    assertEquals(0.75, stats.getPrecision(), EPSILON);
-    assertEquals(0.75, stats.getRecall(), EPSILON);
-    assertEquals(0.75, stats.getF1Measure(), EPSILON);
-    assertEquals(0.75, stats.getFNMeasure(2.0), EPSILON);
-    assertEquals(0.75, stats.getNormalizedDiscountedCumulativeGain(), EPSILON);
-  }
-
-  @Test
   public void testBoolean() throws Exception {
     DataModel model = getBooleanDataModel();
     RecommenderBuilder builder = new RecommenderBuilder() {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/RMSRecommenderEvaluatorTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/RMSRecommenderEvaluatorTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/RMSRecommenderEvaluatorTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/eval/RMSRecommenderEvaluatorTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.eval;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.eval.RecommenderBuilder;
-import org.apache.mahout.cf.taste.eval.RecommenderEvaluator;
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.junit.Test;
-
-public final class RMSRecommenderEvaluatorTest extends TasteTestCase {
-
-  @Test
-  public void testEvaluate() throws Exception {
-    DataModel model = getDataModel();
-    RecommenderBuilder builder = new RecommenderBuilder() {
-      @Override
-      public Recommender buildRecommender(DataModel dataModel) throws TasteException {
-        return new SlopeOneRecommender(dataModel);
-      }
-    };
-    RecommenderEvaluator evaluator = new RMSRecommenderEvaluator();
-    double eval = evaluator.evaluate(builder, null, model, 0.85, 1.0);
-    assertEquals(0.3481984752619784, eval, EPSILON);
-  }
-
-}
\ No newline at end of file
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModelTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModelTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModelTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/model/file/FileDataModelTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,9 @@
 
 package org.apache.mahout.cf.taste.impl.model.file;
 
+import java.io.File;
+import java.util.NoSuchElementException;
+
 import org.apache.commons.lang3.mutable.MutableBoolean;
 import org.apache.mahout.cf.taste.common.TasteException;
 import org.apache.mahout.cf.taste.impl.TasteTestCase;
@@ -33,9 +36,6 @@
 import org.junit.Before;
 import org.junit.Test;
 
-import java.io.File;
-import java.util.NoSuchElementException;
-
 /** <p>Tests {@link FileDataModel}.</p> */
 public final class FileDataModelTest extends TasteTestCase {
 
@@ -55,6 +55,23 @@
       "456,789,0.5",
       "456,654,0.0",
       "456,999,0.2",};
+  
+  private static final String[] DATA_SPLITTED_WITH_TWO_SPACES = {
+      "123  456  0.1",
+      "123  789  0.6",
+      "123  654  0.7",
+      "234  123  0.5",
+      "234  234  1.0",
+      "234  999  0.9",
+      "345  789  0.6",
+      "345  654  0.7",
+      "345  123  1.0",
+      "345  234  0.5",
+      "345  999  0.5",
+      "456  456  0.1",
+      "456  789  0.5",
+      "456  654  0.0",
+      "456  999  0.2",};
 
   private DataModel model;
   private File testFile;
@@ -67,6 +84,15 @@
     writeLines(testFile, DATA);
     model = new FileDataModel(testFile);
   }
+  
+  @Test
+  public void testReadRegexSplittedFile() throws Exception {
+    File testFile = getTestTempFile("testRegex.txt");
+    writeLines(testFile, DATA_SPLITTED_WITH_TWO_SPACES);
+    FileDataModel model = new FileDataModel(testFile,"\\s+");
+    assertEquals(model.getItemIDsFromUser(123).size(), 3);
+    assertEquals(model.getItemIDsFromUser(456).size(), 4);
+  }
 
   @Test
   public void testFile() throws Exception {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommenderTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommenderTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommenderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/BiasedItemBasedRecommenderTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,50 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import org.apache.mahout.math.Sorting;
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-
-public class BiasedItemBasedRecommenderTest {
-
-  @Test
-  public void sorting() {
-
-    double[] similarities = { 0.1, 1.0, 0.5 };
-    float[] ratings = { 3, 1, 2 };
-    long[] itemIDs = { 3, 1, 2 };
-
-    Sorting.quickSort(0, similarities.length, new BiasedItemBasedRecommender.SimilaritiesComparator(similarities),
-        new BiasedItemBasedRecommender.SimilaritiesRatingsItemIDsSwapper(similarities, ratings, itemIDs));
-
-    assertEquals(1.0d, similarities[0], 0.0d);
-    assertEquals(0.5d, similarities[1], 0.0d);
-    assertEquals(0.1d, similarities[2], 0.0d);
-
-    assertEquals(1.0f, ratings[0], 0.0f);
-    assertEquals(2.0f, ratings[1], 0.0f);
-    assertEquals(3.0f, ratings[2], 0.0f);
-
-    assertEquals(1L, itemIDs[0]);
-    assertEquals(2L, itemIDs[1]);
-    assertEquals(3L, itemIDs[2]);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2Test.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2Test.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2Test.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommender2Test.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import java.util.List;
-
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.apache.mahout.cf.taste.similarity.UserSimilarity;
-import org.junit.Test;
-
-/** <p>Tests {@link TreeClusteringRecommender2}.</p> */
-@Deprecated
-public final class TreeClusteringRecommender2Test extends TasteTestCase {
-
-  @Test
-  public void testHowMany() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4, 5},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-                    {0.1, 0.4, 0.5, 0.8, 0.9, 1.0},
-                    {0.2, 0.3, 0.6, 0.7, 0.1, 0.2},
-            });
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender2(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> fewRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> moreRecommended = recommender.recommend(1, 4);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-    recommender.refresh(null);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-  }
-
-  @Test
-  public void testRescorer() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-            });
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender2(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> originalRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> rescoredRecommended =
-        recommender.recommend(1, 2, new ReversingRescorer<Long>());
-    assertNotNull(originalRecommended);
-    assertNotNull(rescoredRecommended);
-    assertEquals(2, originalRecommended.size());
-    assertEquals(2, rescoredRecommended.size());
-    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(1).getItemID());
-    assertEquals(originalRecommended.get(1).getItemID(), rescoredRecommended.get(0).getItemID());
-  }
-
-  @Test
-  public void testEstimatePref() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4},
-            new Double[][] {
-                    {0.1, 0.3},
-                    {0.2, 0.3, 0.3},
-                    {0.4, 0.3, 0.5},
-                    {0.7, 0.3, 0.8, 0.9},
-            });
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender2(dataModel, clusterSimilarity, 2);
-    assertEquals(0.9f, recommender.estimatePreference(3, 3), EPSILON);
-  }
-
-  @Test
-  public void testBestRating() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4},
-            new Double[][] {
-                    {0.1, 0.3},
-                    {0.2, 0.3, 0.3},
-                    {0.4, 0.3, 0.5},
-                    {0.7, 0.3, 0.8},
-            });
-
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender2(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(1, recommended.size());
-    RecommendedItem firstRecommended = recommended.get(0);
-    // item one should be recommended because it has a greater rating/score
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.3, firstRecommended.getValue(), EPSILON);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommenderTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommenderTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommenderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/TreeClusteringRecommenderTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender;
-
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.apache.mahout.cf.taste.similarity.UserSimilarity;
-import org.junit.Test;
-
-import java.util.List;
-
-/** <p>Tests {@link TreeClusteringRecommender}.</p> */
-@Deprecated
-public final class TreeClusteringRecommenderTest extends TasteTestCase {
-
-  @Test
-  public void testNoRecommendations() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.1},
-                    {0.2, 0.6},
-                    {0.4, 0.9},
-            });
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(0, recommended.size());
-    recommender.refresh(null);
-    assertNotNull(recommended);
-    assertEquals(0, recommended.size());
-  }
-
-  @Test
-  public void testHowMany() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4, 5},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-                    {0.1, 0.4, 0.5, 0.8, 0.9, 1.0},
-                    {0.2, 0.3, 0.6, 0.7, 0.1, 0.2},
-            });
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> fewRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> moreRecommended = recommender.recommend(1, 4);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-    recommender.refresh(null);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-  }
-
-  @Test
-  public void testRescorer() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-            });
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> originalRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> rescoredRecommended =
-        recommender.recommend(1, 2, new ReversingRescorer<Long>());
-    assertNotNull(originalRecommended);
-    assertNotNull(rescoredRecommended);
-    assertEquals(2, originalRecommended.size());
-    assertEquals(2, rescoredRecommended.size());
-    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(1).getItemID());
-    assertEquals(originalRecommended.get(1).getItemID(), rescoredRecommended.get(0).getItemID());
-  }
-
-  @Test
-  public void testEstimatePref() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4},
-            new Double[][] {
-                    {0.1, 0.3},
-                    {0.2, 0.3, 0.3},
-                    {0.4, 0.3, 0.5},
-                    {0.7, 0.3, 0.8, 0.9},
-            });
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender(dataModel, clusterSimilarity, 2);
-    assertEquals(0.9f, recommender.estimatePreference(3, 3), EPSILON);
-  }
-
-  @Test
-  public void testBestRating() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4},
-            new Double[][] {
-                    {0.1, 0.3},
-                    {0.2, 0.3, 0.3},
-                    {0.4, 0.3, 0.5},
-                    {0.7, 0.3, 0.8},
-            });
-
-
-    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    ClusterSimilarity clusterSimilarity = new FarthestNeighborClusterSimilarity(similarity);
-    Recommender recommender = new TreeClusteringRecommender(dataModel, clusterSimilarity, 2);
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(1, recommended.size());
-    RecommendedItem firstRecommended = recommended.get(0);
-    // item one should be recommended because it has a greater rating/score
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.3, firstRecommended.getValue(), EPSILON);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommenderTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommenderTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommenderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/knn/KnnItemBasedRecommenderTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,123 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.knn;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.recommender.ReversingRescorer;
-import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.apache.mahout.cf.taste.similarity.ItemSimilarity;
-import org.junit.Test;
-
-import java.util.List;
-
-@Deprecated
-public final class KnnItemBasedRecommenderTest extends TasteTestCase {
-
-  @Test
-  public void testRecommender() throws Exception {
-    Recommender recommender = buildRecommender();
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(1, recommended.size());
-    RecommendedItem firstRecommended = recommended.get(0);
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);
-    recommender.refresh(null);
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);
-  }
-
-  @Test
-  public void testHowMany() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4, 5},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-                    {0.1, 0.4, 0.5, 0.8, 0.9, 1.0},
-                    {0.2, 0.3, 0.6, 0.7, 0.1, 0.2},
-            });
-    ItemSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    Optimizer optimizer = new ConjugateGradientOptimizer();
-    Recommender recommender = new KnnItemBasedRecommender(dataModel, similarity, optimizer, 5);
-    List<RecommendedItem> fewRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> moreRecommended = recommender.recommend(1, 4);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-    recommender.refresh(null);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-  }
-
-  @Test
-  public void testRescorer() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.5, 0.5, 0.9},
-            });
-    ItemSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    Optimizer optimizer = new ConjugateGradientOptimizer();
-    Recommender recommender = new KnnItemBasedRecommender(dataModel, similarity, optimizer, 5);
-    List<RecommendedItem> originalRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> rescoredRecommended =
-        recommender.recommend(1, 2, new ReversingRescorer<Long>());
-    assertNotNull(originalRecommended);
-    assertNotNull(rescoredRecommended);
-    assertEquals(2, originalRecommended.size());
-    assertEquals(2, rescoredRecommended.size());
-    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(1).getItemID());
-    assertEquals(originalRecommended.get(1).getItemID(), rescoredRecommended.get(0).getItemID());
-  }
-
-  @Test
-  public void testEstimatePref() throws Exception {
-    Recommender recommender = buildRecommender();
-    assertEquals(0.1f, recommender.estimatePreference(1, 2), EPSILON);
-  }
-
-  @Test
-  public void testBestRating() throws Exception {
-    Recommender recommender = buildRecommender();
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(1, recommended.size());
-    RecommendedItem firstRecommended = recommended.get(0);
-    // item one should be recommended because it has a greater rating/score
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);
-  }
-
-  private static Recommender buildRecommender() throws TasteException {
-    DataModel dataModel = getDataModel();
-    ItemSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);
-    Optimizer optimizer = new ConjugateGradientOptimizer();
-    return new KnnItemBasedRecommender(dataModel, similarity, optimizer, 5);
-  }
-
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorageTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorageTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorageTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/MemoryDiffStorageTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,216 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone;
-
-import org.apache.mahout.cf.taste.common.NoSuchUserException;
-import org.apache.mahout.cf.taste.common.Weighting;
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.junit.Test;
-
-/** Tests {@link MemoryDiffStorage}. */
-@Deprecated
-public final class MemoryDiffStorageTest extends TasteTestCase {
-
-  @Test
-  public void testRecommendableIDsVariedWeighted() throws Exception {
-    DataModel model = getDataModelVaried();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.WEIGHTED, Long.MAX_VALUE);
-    FastIDSet recommendableItemIDs = storage.getRecommendableItemIDs(1);
-    assertEquals(3, recommendableItemIDs.size());
-    assertTrue(recommendableItemIDs.contains(1));
-    recommendableItemIDs = storage.getRecommendableItemIDs(2);
-    assertEquals(2, recommendableItemIDs.size());
-    assertTrue(recommendableItemIDs.contains(2));
-    assertTrue(recommendableItemIDs.contains(3));
-    
-    recommendableItemIDs = storage.getRecommendableItemIDs(3);
-    assertEquals(1, recommendableItemIDs.size());
-    assertTrue(recommendableItemIDs.contains(3));
-    
-    recommendableItemIDs = storage.getRecommendableItemIDs(4);
-    assertEquals(0, recommendableItemIDs.size());
-    // the last item has only one recommendation, and so only 4 items are usable
-    recommendableItemIDs = storage.getRecommendableItemIDs(5);
-    assertEquals(0, recommendableItemIDs.size());
-  }
-  
-  @Test
-  public void testRecommendableIDsPockedUnweighted() throws Exception {
-    DataModel model = getDataModelPocked();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.UNWEIGHTED, Long.MAX_VALUE);
-    FastIDSet recommendableItemIDs = storage.getRecommendableItemIDs(1);
-    assertEquals(0, recommendableItemIDs.size());
-    recommendableItemIDs = storage.getRecommendableItemIDs(2);
-    assertEquals(1, recommendableItemIDs.size());
-    recommendableItemIDs = storage.getRecommendableItemIDs(3);
-    assertEquals(0, recommendableItemIDs.size());
-    
-    recommendableItemIDs = storage.getRecommendableItemIDs(4);
-    assertEquals(0, recommendableItemIDs.size());
-    
-  }
-  
-  @Test (expected=NoSuchUserException.class)
-  public void testUnRecommendableID() throws Exception {
-    DataModel model = getDataModel();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.WEIGHTED, Long.MAX_VALUE);
-    storage.getRecommendableItemIDs(0);
-  }
-  
-  @Test
-  public void testGetDiff() throws Exception {
-    DataModel model = getDataModel();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.UNWEIGHTED, Long.MAX_VALUE);
-    RunningAverage average = storage.getDiff(1, 2);
-    assertEquals(0.23333333333333334, average.getAverage(), EPSILON);
-    assertEquals(3, average.getCount());
-  }
-  
-  @Test
-  public void testAdd() throws Exception {
-    DataModel model = getDataModel();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.UNWEIGHTED, Long.MAX_VALUE);
-    
-    RunningAverage average1 = storage.getDiff(0, 2);
-    assertEquals(0.1, average1.getAverage(), EPSILON);
-    assertEquals(3, average1.getCount());
-    
-    RunningAverage average2 = storage.getDiff(1, 2);
-    assertEquals(0.23333332935969034, average2.getAverage(), EPSILON);
-    assertEquals(3, average2.getCount());
-    
-    storage.addItemPref(1, 2, 0.8f);
-    
-    average1 = storage.getDiff(0, 2);
-    assertEquals(0.25, average1.getAverage(), EPSILON);
-    assertEquals(4, average1.getCount());
-    
-    average2 = storage.getDiff(1, 2);
-    assertEquals(0.3, average2.getAverage(), EPSILON);
-    assertEquals(4, average2.getCount());
-  }
-  
-  @Test
-  public void testUpdate() throws Exception {
-    DataModel model = getDataModel();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.UNWEIGHTED, Long.MAX_VALUE);
-    
-    RunningAverage average = storage.getDiff(1, 2);
-    assertEquals(0.23333332935969034, average.getAverage(), EPSILON);
-    assertEquals(3, average.getCount());
-    
-    storage.updateItemPref(1, 0.5f);
-    
-    average = storage.getDiff(1, 2);
-    assertEquals(0.06666666666666668, average.getAverage(), EPSILON);
-    assertEquals(3, average.getCount());
-  }
-  
-  @Test
-  public void testRemove() throws Exception {
-    DataModel model = getDataModel();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.UNWEIGHTED, Long.MAX_VALUE);
-    
-    RunningAverage average1 = storage.getDiff(0, 2);
-    assertEquals(0.1, average1.getAverage(), EPSILON);
-    assertEquals(3, average1.getCount());
-    
-    RunningAverage average2 = storage.getDiff(1, 2);
-    assertEquals(0.23333332935969034, average2.getAverage(), EPSILON);
-    assertEquals(3, average2.getCount());
-    
-    storage.removeItemPref(4, 2, 0.8f);
-    
-    average1 = storage.getDiff(0, 2);
-    assertEquals(0.1, average1.getAverage(), EPSILON);
-    assertEquals(2, average1.getCount());
-    
-    average2 = storage.getDiff(1, 2);
-    assertEquals(0.1, average2.getAverage(), EPSILON);
-    assertEquals(2, average2.getCount());
-  }
-  
-  @Test (expected=UnsupportedOperationException.class)
-  public void testUpdateWeighted() throws Exception {
-    DataModel model = getDataModelVaried();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.WEIGHTED, Long.MAX_VALUE);
-    
-    storage.updateItemPref(2, 0.8f);
-  }
-  
-  @Test
-  public void testRemovePref() throws Exception {
-    double eps = 0.0001;
-    DataModel model = getDataModelPocked();
-    MemoryDiffStorage storage = new MemoryDiffStorage(model, Weighting.WEIGHTED, Long.MAX_VALUE);
-    
-    RunningAverageAndStdDev average = (RunningAverageAndStdDev) storage.getDiff(0, 1);
-    assertEquals(-0.033333, average.getAverage(), eps);
-    assertEquals(0.32145, average.getStandardDeviation(), eps);
-    assertEquals(3, average.getCount());
-
-    storage.removeItemPref(2, 1, 0.1f);
-    average = (RunningAverageAndStdDev) storage.getDiff(0, 1);
-    assertEquals(0.00000001, average.getAverage(), eps);
-    assertEquals(0.44721, average.getStandardDeviation(), eps);
-    assertEquals(2, average.getCount());
-  }
-  
-  static DataModel getDataModelVaried() {
-    return getDataModel(
-        new long[] {1, 2, 3, 4, 5},
-        new Double[][] {
-            {0.2},
-            {0.4, 0.5},
-            {0.7, 0.1, 0.5},
-            {0.7, 0.3, 0.8, 0.1},
-            {0.2, 0.3, 0.6, 0.1, 0.3},
-        });
-  }
-  
-  static DataModel getDataModelPocked() {
-    return getDataModel(
-        new long[] {1, 2, 3, 4},
-        new Double[][] {
-            {0.1, 0.3},
-            {0.2},
-            {0.4, 0.5},
-            {0.7, 0.3, 0.8},
-        });
-  }
-  
-  static DataModel getDataModelLarge() {
-    return getDataModel(
-        new long[] {1, 2, 3, 4, 5, 6, 7},
-        new Double[][] {
-            {0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2},
-            {0.4, 0.5, 0.3, 0.3, 0.3, 0.3, 0.3},
-            {0.7, 0.1, 0.5, 0.2, 0.7, 0.8, 0.9},
-            {0.7, 0.3, 0.8, 0.1, 0.6, 0.6, 0.6},
-            {0.2, 0.3, 0.6, 0.1, 0.3, 0.4, 0.4},
-            {0.2, 0.3, 0.6, 0.1, 0.3, 0.4, 0.4},
-            {0.2, 0.3, 0.6, 0.1, 0.3, 0.5, 0.5},
-        });
-  }
-  
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommenderTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommenderTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommenderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/SlopeOneRecommenderTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,139 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.impl.recommender.ReversingRescorer;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-import org.junit.Test;
-
-import java.util.List;
-
-/** <p>Tests {@link SlopeOneRecommender}.</p> */
-@Deprecated
-public final class SlopeOneRecommenderTest extends TasteTestCase {
-
-  @Test
-  public void testRecommender() throws Exception {
-    Recommender recommender = buildRecommender();
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(1, recommended.size());
-    RecommendedItem firstRecommended = recommended.get(0);
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.34803885284992736, firstRecommended.getValue(), EPSILON);
-    recommender.refresh(null);
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.34803885284992736, firstRecommended.getValue(), EPSILON);
-  }
-
-  @Test
-  public void testHowMany() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4, 5},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-                    {0.1, 0.4, 0.5, 0.8, 0.9, 1.0},
-                    {0.2, 0.3, 0.6, 0.7, 0.1, 0.2},
-            });
-
-    Recommender recommender = new SlopeOneRecommender(dataModel);
-    List<RecommendedItem> fewRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> moreRecommended = recommender.recommend(1, 4);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-    recommender.refresh(null);
-    for (int i = 0; i < fewRecommended.size(); i++) {
-      assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());
-    }
-  }
-
-  @Test
-  public void testRescorer() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.3, 0.6},
-                    {0.4, 0.4, 0.5, 0.9},
-            });
-
-    Recommender recommender = new SlopeOneRecommender(dataModel);
-    List<RecommendedItem> originalRecommended = recommender.recommend(1, 2);
-    List<RecommendedItem> rescoredRecommended =
-        recommender.recommend(1, 2, new ReversingRescorer<Long>());
-    assertNotNull(originalRecommended);
-    assertNotNull(rescoredRecommended);
-    assertEquals(2, originalRecommended.size());
-    assertEquals(2, rescoredRecommended.size());
-    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(1).getItemID());
-    assertEquals(originalRecommended.get(1).getItemID(), rescoredRecommended.get(0).getItemID());
-  }
-
-  @Test
-  public void testEstimatePref() throws Exception {
-    Recommender recommender = buildRecommender();
-    assertEquals(0.34803885284992736, recommender.estimatePreference(1, 2), EPSILON);
-  }
-
-  @Test
-  public void testBestRating() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.0, 0.3},
-                    {0.2, 0.3, 0.3},
-                    {0.4, 0.3, 0.5},
-            });
-
-    Recommender recommender = new SlopeOneRecommender(dataModel);
-    List<RecommendedItem> recommended = recommender.recommend(1, 1);
-    assertNotNull(recommended);
-    assertEquals(1, recommended.size());
-    RecommendedItem firstRecommended = recommended.get(0);
-    // item one should be recommended because it has a greater rating/score
-    assertEquals(2, firstRecommended.getItemID());
-    assertEquals(0.2400938676203033, firstRecommended.getValue(), EPSILON);
-  }
-
-  @Test  
-  public void testDiffStdevBehavior() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3},
-            new Double[][] {
-                    {0.1, 0.2},
-                    {0.2, 0.3, 0.6},
-                    {0.3, 0.3, 0.3},
-            });
-
-    Recommender recommender = new SlopeOneRecommender(dataModel);
-    assertEquals(0.3257085f, recommender.estimatePreference(1, 2), EPSILON);
-  }
-
-  private static Recommender buildRecommender() throws TasteException {
-    DataModel dataModel = getDataModel();
-    return new SlopeOneRecommender(dataModel);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizerTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizerTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ALSWRFactorizerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -38,6 +38,8 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
+
 import java.util.Arrays;
 import java.util.Iterator;
 
@@ -132,6 +134,7 @@
     }
   }
 
+  @ThreadLeakLingering(linger = 10)
   @Test
   public void toyExample() throws Exception {
 
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ParallelSGDFactorizerTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ParallelSGDFactorizerTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ParallelSGDFactorizerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/recommender/svd/ParallelSGDFactorizerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,6 +20,7 @@
 import java.util.Arrays;
 import java.util.List;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
 import com.google.common.collect.Lists;
 import org.apache.mahout.cf.taste.impl.TasteTestCase;
 import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
@@ -184,6 +185,7 @@
     assertEquals(index, shuffler.size());
   }
 
+  @ThreadLeakLingering(linger = 1000)
   @Test
   public void testFactorizerWithToyData() throws Exception {
 
@@ -240,6 +242,7 @@
     assertTrue(rmse < 0.2);
   }
 
+  @ThreadLeakLingering(linger = 1000)
   @Test
   public void testRecommenderWithToyData() throws Exception {
 
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplificationTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplificationTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplificationTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/CaseAmplificationTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import org.junit.Test;
-
-/** <p>Tests {@link CaseAmplification}.</p> */
-public final class CaseAmplificationTest extends TransformTestCase {
-
-  @Test
-  public void testOneValue() {
-    assertEquals(2.0, new CaseAmplification(0.5).transformSimilarity(0, 0, 4.0), EPSILON);
-    assertEquals(-2.0, new CaseAmplification(0.5).transformSimilarity(0, 0, -4.0), EPSILON);
-  }
-
-  @Test
-  public void testRefresh() {
-    // Make sure this doesn't throw an exception
-    new CaseAmplification(1.0).refresh(null);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequencyTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequencyTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequencyTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/InverseUserFrequencyTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.junit.Test;
-
-/** <p>Tests {@link InverseUserFrequency}.</p> */
-public final class InverseUserFrequencyTest extends TransformTestCase {
-
-  @Test
-  public void testIUF() throws Exception {
-    DataModel dataModel = getDataModel(
-            new long[] {1, 2, 3, 4, 5},
-            new Double[][] {
-                    {0.1},
-                    {0.2, 0.3},
-                    {0.4, 0.5, 0.6},
-                    {0.7, 0.8, 0.9, 1.0},
-                    {1.0, 1.0, 1.0, 1.0, 1.0},
-            });
-
-    InverseUserFrequency iuf = new InverseUserFrequency(dataModel, 10.0);
-
-    PreferenceArray user5Prefs = dataModel.getPreferencesFromUser(5);
-
-    for (int i = 0; i < 5; i++) {
-      Preference pref = user5Prefs.get(i);
-      assertNotNull(pref);
-      assertEquals(Math.log(5.0 / (5.0 - i)) / Math.log(iuf.getLogBase()),
-          iuf.getTransformedValue(pref),
-          EPSILON);
-    }
-
-    // Make sure this doesn't throw an exception
-    iuf.refresh(null);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/TransformTestCase.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/TransformTestCase.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/TransformTestCase.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/TransformTestCase.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.TasteTestCase;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-
-abstract class TransformTestCase extends TasteTestCase {
-
-  static void assertPrefsEquals(DataModel dataModel, long userID, double... expected) throws TasteException {
-    int i = 0;
-    for (Preference pref : dataModel.getPreferencesFromUser(userID)) {
-      assertEquals(expected[i], pref.getValue(), EPSILON);
-      i++;
-    }
-    assertEquals(expected.length, i);
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/ZScoreTest.java mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/ZScoreTest.java
--- mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/ZScoreTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/cf/taste/impl/transforms/ZScoreTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.transforms;
-
-import org.apache.mahout.cf.taste.impl.model.GenericPreference;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.transforms.PreferenceTransform;
-import org.junit.Test;
-
-/** <p>Tests {@link ZScore}.</p> */
-public final class ZScoreTest extends TransformTestCase {
-
-  @Test
-  public void testOnePref() throws Exception {
-    DataModel dataModel = getDataModel(new long[] {1}, new Double[][] {{1.0}});
-    PreferenceTransform zScore = new ZScore(dataModel);
-    assertEquals(0.0, zScore.getTransformedValue(new GenericPreference(1, 0, 1.0f)), EPSILON);
-  }
-
-  @Test
-  public void testAllSame() throws Exception {
-    DataModel dataModel = getDataModel(new long[] {1}, new Double[][] {{1.0,1.0,1.0}});
-    PreferenceTransform zScore = new ZScore(dataModel);
-    assertEquals(0.0, zScore.getTransformedValue(new GenericPreference(1, 0, 1.0f)), EPSILON);
-    assertEquals(0.0, zScore.getTransformedValue(new GenericPreference(1, 1, 1.0f)), EPSILON);
-    assertEquals(0.0, zScore.getTransformedValue(new GenericPreference(1, 2, 1.0f)), EPSILON);
-  }
-
-  @Test
-  public void testStdev() throws Exception {
-    DataModel dataModel = getDataModel(new long[] {1}, new Double[][] {{-1.0,-2.0}});
-    PreferenceTransform zScore = new ZScore(dataModel);
-    assertEquals(Math.sqrt(2.0)/2.0, zScore.getTransformedValue(new GenericPreference(1, 0, -1.0f)), EPSILON);
-    assertEquals(-Math.sqrt(2.0)/2.0, zScore.getTransformedValue(new GenericPreference(1, 1, -2.0f)), EPSILON);
-  }
-
-  @Test
-  public void testExample() throws Exception {
-    DataModel dataModel = getDataModel(new long[] {1}, new Double[][] {{5.0, 7.0, 9.0}});
-    PreferenceTransform zScore = new ZScore(dataModel);
-    assertEquals(-1.0, zScore.getTransformedValue(new GenericPreference(1, 0, 5.0f)), EPSILON);
-    assertEquals(0.0, zScore.getTransformedValue(new GenericPreference(1, 1, 7.0f)), EPSILON);
-    assertEquals(1.0, zScore.getTransformedValue(new GenericPreference(1, 2, 9.0f)), EPSILON);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DataLoaderTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DataLoaderTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DataLoaderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DataLoaderTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -257,7 +257,7 @@
     Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);
 
     Path dataPath = Utils.writeDataToTestFile(sData);
-    FileSystem fs = dataPath.getFileSystem(new Configuration());
+    FileSystem fs = dataPath.getFileSystem(getConfiguration());
     Data loaded = DataLoader.loadData(dataset, fs, dataPath);
 
     testLoadedData(source, attrs, missings, loaded);
@@ -269,7 +269,7 @@
     dataset = DataLoader.generateDataset(descriptor, true, sData);
 
     dataPath = Utils.writeDataToTestFile(sData);
-    fs = dataPath.getFileSystem(new Configuration());
+    fs = dataPath.getFileSystem(getConfiguration());
     loaded = DataLoader.loadData(dataset, fs, dataPath);
 
     testLoadedData(source, attrs, missings, loaded);
@@ -295,7 +295,7 @@
     Dataset expected = DataLoader.generateDataset(descriptor, false, sData);
 
     Path path = Utils.writeDataToTestFile(sData);
-    FileSystem fs = path.getFileSystem(new Configuration());
+    FileSystem fs = path.getFileSystem(getConfiguration());
     
     Dataset dataset = DataLoader.generateDataset(descriptor, false, fs, path);
     
@@ -308,7 +308,7 @@
     expected = DataLoader.generateDataset(descriptor, false, sData);
 
     path = Utils.writeDataToTestFile(sData);
-    fs = path.getFileSystem(new Configuration());
+    fs = path.getFileSystem(getConfiguration());
     
     dataset = DataLoader.generateDataset(descriptor, false, fs, path);
     
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DatasetTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DatasetTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DatasetTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/data/DatasetTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -16,7 +16,6 @@
  */
 package org.apache.mahout.classifier.df.data;
 
-
 import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
@@ -24,7 +23,6 @@
 
   @Test
   public void jsonEncoding() throws DescriptorException {
-
     String json = "["
             + "{\"values\":null,\"label\":false,\"type\":\"numerical\"},"
             + "{\"values\":[\"foo\",\"bar\"],\"label\":false,\"type\":\"categorical\"},"
@@ -42,7 +40,7 @@
     assertTrue(to.isNumerical(0));
 
     // from JSON
-    Dataset fromJson = new Dataset().fromJSON(json);
+    Dataset fromJson = Dataset.fromJSON(json);
     assertEquals(3, fromJson.nbAttributes());
     assertEquals(1, fromJson.getIgnored().length);
     assertEquals(2, fromJson.getIgnored()[0]);
@@ -50,6 +48,37 @@
     
     // read values for a nominal
     assertEquals(0, fromJson.valueOf(1, "foo"));
+  }
+
+  @Test
+  public void jsonEncodingIgnoreFeatures() throws DescriptorException {
+    String json = "["
+        + "{\"values\":null,\"label\":false,\"type\":\"numerical\"},"
+        + "{\"values\":[\"foo\",\"bar\"],\"label\":false,\"type\":\"categorical\"},"
+        + "{\"values\":null,\"label\":false,\"type\":\"ignored\"},"
+        + "{\"values\":[\"Blue\",\"Red\"],\"label\":true,\"type\":\"categorical\"}"
+        + "]";
+    Dataset to = DataLoader.generateDataset("N C I L", false, new String[]{"1 foo 2 Red", "4 bar 5 Blue"});
 
+    // to JSON
+    assertEquals(json, to.toJSON());
+    assertEquals(3, to.nbAttributes());
+    assertEquals(1, to.getIgnored().length);
+    assertEquals(2, to.getIgnored()[0]);
+    assertEquals(2, to.getLabelId());
+    assertTrue(to.isNumerical(0));
+    assertEquals(0, to.valueOf(1, "foo"));
+    assertEquals(0, to.valueOf(2, "Blue"));
+
+    // from JSON
+    Dataset fromJson = Dataset.fromJSON(json);
+    assertEquals(3, fromJson.nbAttributes());
+    assertEquals(1, fromJson.getIgnored().length);
+    assertEquals(2, fromJson.getIgnored()[0]);
+    assertTrue(fromJson.isNumerical(0));
+
+    // read values for a nominal, one before and one after the ignore feature
+    assertEquals(0, fromJson.valueOf(1, "foo"));
+    assertEquals(0, fromJson.valueOf(2, "Blue"));
   }
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/data/Utils.java mahout/core/src/test/java/org/apache/mahout/classifier/df/data/Utils.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/data/Utils.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/data/Utils.java	2014-03-29 01:03:14.000000000 -0700
@@ -27,10 +27,10 @@
 import com.google.common.io.Closeables;
 import com.google.common.io.Files;
 import org.apache.commons.lang3.ArrayUtils;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.mahout.classifier.df.data.Dataset.Attribute;
+import org.apache.mahout.common.MahoutTestCase;
 
 /**
  * Helper methods used by the tests
@@ -249,7 +249,8 @@
 
   public static Path writeDataToTestFile(String[] sData) throws IOException {
     Path testData = new Path("testdata/Data");
-    FileSystem fs = testData.getFileSystem(new Configuration());
+    MahoutTestCase ca = new MahoutTestCase();
+    FileSystem fs = testData.getFileSystem(ca.getConfiguration());
     if (!fs.exists(testData)) {
       fs.mkdirs(testData);
     }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/inmem/InMemInputFormatTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/inmem/InMemInputFormatTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/inmem/InMemInputFormatTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/inmem/InMemInputFormatTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -43,7 +43,7 @@
       int numSplits = rng.nextInt(maxNumSplits) + 1;
       int nbTrees = rng.nextInt(maxNbTrees) + 1;
 
-      Configuration conf = new Configuration();
+      Configuration conf = getConfiguration();
       Builder.setNbTrees(conf, nbTrees);
 
       InMemInputFormat inputFormat = new InMemInputFormat();
@@ -86,7 +86,7 @@
       int numSplits = rng.nextInt(maxNumSplits) + 1;
       int nbTrees = rng.nextInt(maxNbTrees) + 1;
 
-      Configuration conf = new Configuration();
+      Configuration conf = getConfiguration();
       Builder.setNbTrees(conf, nbTrees);
 
       InMemInputFormat inputFormat = new InMemInputFormat();
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilderTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilderTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/PartialBuilderTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -50,7 +50,7 @@
 
   @Test
   public void testProcessOutput() throws Exception {
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.setInt("mapred.map.tasks", NUM_MAPS);
 
     Random rng = RandomUtils.getRandom();
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1MapperTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1MapperTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1MapperTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/mapreduce/partial/Step1MapperTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -100,7 +100,6 @@
   @SuppressWarnings({ "rawtypes", "unchecked" })
   @Test
   public void testMapper() throws Exception {
-    Long seed = null;
     Random rng = RandomUtils.getRandom();
 
     // prepare the data
@@ -124,14 +123,14 @@
       // expected number of trees that this mapper will build
       int mapNbTrees = Step1Mapper.nbTrees(NUM_MAPPERS, NUM_TREES, partition);
 
-      Mapper.Context context = EasyMock.createMock(Mapper.Context.class);
+      Mapper.Context context = EasyMock.createNiceMock(Mapper.Context.class);
       Capture<TreeID> capturedKeys = new TreeIDCapture();
       context.write(EasyMock.capture(capturedKeys), EasyMock.anyObject());
       EasyMock.expectLastCall().anyTimes();
 
       EasyMock.replay(context);
 
-      MockStep1Mapper mapper = new MockStep1Mapper(treeBuilder, dataset, seed,
+      MockStep1Mapper mapper = new MockStep1Mapper(treeBuilder, dataset, null,
           partition, NUM_MAPPERS, NUM_TREES);
 
       // make sure the mapper computed firstTreeId correctly
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/split/OptIgSplitTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/split/OptIgSplitTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/split/OptIgSplitTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/split/OptIgSplitTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.classifier.df.split;
-
-import java.util.Random;
-
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.classifier.df.data.Data;
-import org.apache.mahout.classifier.df.data.Utils;
-import org.junit.Test;
-
-public final class OptIgSplitTest extends MahoutTestCase {
-
-  private static final int NUM_ATTRIBUTES = 20;
-
-  private static final int NUM_INSTANCES = 100;
-
-  @Test
-  public void testComputeSplit() throws Exception {
-    IgSplit ref = new DefaultIgSplit();
-    IgSplit opt = new OptIgSplit();
-
-    Random rng = RandomUtils.getRandom();
-    Data data = Utils.randomData(rng, NUM_ATTRIBUTES, false, NUM_INSTANCES);
-
-    for (int nloop = 0; nloop < 100; nloop++) {
-      int attr = rng.nextInt(data.getDataset().nbAttributes());
-      // System.out.println("IsNumerical: " + data.dataset.isNumerical(attr));
-
-      Split expected = ref.computeSplit(data, attr);
-      Split actual = opt.computeSplit(data, attr);
-
-      assertEquals(expected.getIg(), actual.getIg(), EPSILON);
-      assertEquals(expected.getSplit(), actual.getSplit(), EPSILON);
-    }
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/df/tools/VisualizerTest.java mahout/core/src/test/java/org/apache/mahout/classifier/df/tools/VisualizerTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/df/tools/VisualizerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/df/tools/VisualizerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -86,7 +86,7 @@
     Node tree = builder.build(rng, data);
     
     assertEquals("\noutlook = rainy\n|   windy = FALSE : yes\n|   windy = TRUE : no\n"
-            + "outlook = sunny\n|   humidity < 85 : yes\n|   humidity >= 85 : no\n"
+            + "outlook = sunny\n|   humidity < 77.5 : yes\n|   humidity >= 77.5 : no\n"
             + "outlook = overcast : yes", TreeVisualizer.toString(tree, data.getDataset(), ATTR_NAMES));
   }
   
@@ -101,7 +101,7 @@
         ATTR_NAMES);
     Assert.assertArrayEquals(new String[] {
         "outlook = rainy -> windy = TRUE -> no", "outlook = overcast -> yes",
-        "outlook = sunny -> (humidity = 90) >= 85 -> no"}, prediction);
+        "outlook = sunny -> (humidity = 90) >= 77.5 -> no"}, prediction);
   }
   
   @Test
@@ -142,7 +142,7 @@
     builder.setComplemented(false);
     Node tree = builder.build(rng, lessData);
 
-    assertEquals("\noutlook = sunny\n|   humidity < 85 : yes\n|   humidity >= 85 : no\noutlook = overcast : yes", TreeVisualizer.toString(tree, data.getDataset(), ATTR_NAMES));
+    assertEquals("\noutlook = sunny\n|   humidity < 77.5 : yes\n|   humidity >= 77.5 : no\noutlook = overcast : yes", TreeVisualizer.toString(tree, data.getDataset(), ATTR_NAMES));
   }
   
   @Test
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/LinearModelTest.java mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/LinearModelTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/LinearModelTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/LinearModelTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.junit.Before;
-import org.junit.Test;
-
-@Deprecated
-public final class LinearModelTest extends MahoutTestCase {
-
-  private LinearModel model;
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    double[] values = {0.0, 1.0, 0.0, 1.0, 0.0};
-    Vector hyperplane = new DenseVector(values);
-    this.model = new LinearModel(hyperplane, 0.1, 0.5);
-  }
-
-  @Test
-  public void testClassify() {
-    double[] valuesFalse = {1.0, 0.0, 1.0, 0.0, 1.0};
-    Vector dataPointFalse = new DenseVector(valuesFalse);
-    assertFalse(this.model.classify(dataPointFalse));
-
-    double[] valuesTrue = {0.0, 1.0, 0.0, 1.0, 0.0};
-    Vector dataPointTrue = new DenseVector(valuesTrue);
-    assertTrue(this.model.classify(dataPointTrue));
-  }
-
-  @Test
-  public void testAddDelta() {
-    double[] values = {1.0, -1.0, 1.0, -1.0, 1.0};
-    this.model.addDelta(new DenseVector(values));
-
-    double[] valuesFalse = {1.0, 0.0, 1.0, 0.0, 1.0};
-    Vector dataPointFalse = new DenseVector(valuesFalse);
-    assertTrue(this.model.classify(dataPointFalse));
-
-    double[] valuesTrue = {0.0, 1.0, 0.0, 1.0, 0.0};
-    Vector dataPointTrue = new DenseVector(valuesTrue);
-    assertFalse(this.model.classify(dataPointTrue));
-  }
-
-  @Test
-  public void testTimesDelta() {
-    double[] values = {-1.0, -1.0, -1.0, -1.0, -1.0};
-    this.model.addDelta(new DenseVector(values));
-    double[] dotval = {-1.0, -1.0, -1.0, -1.0, -1.0};
-    
-    for (int i = 0; i < dotval.length; i++) {
-      this.model.timesDelta(i, dotval[i]);
-    }
-
-    double[] valuesFalse = {1.0, 0.0, 1.0, 0.0, 1.0};
-    Vector dataPointFalse = new DenseVector(valuesFalse);
-    assertTrue(this.model.classify(dataPointFalse));
-
-    double[] valuesTrue = {0.0, 1.0, 0.0, 1.0, 0.0};
-    Vector dataPointTrue = new DenseVector(valuesTrue);
-    assertFalse(this.model.classify(dataPointTrue));
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/PerceptronTrainerTest.java mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/PerceptronTrainerTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/PerceptronTrainerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/PerceptronTrainerTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,62 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.Vector;
-import org.junit.Before;
-import org.junit.Test;
-
-@Deprecated
-public final class PerceptronTrainerTest extends MahoutTestCase {
-
-  private PerceptronTrainer trainer;
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    trainer = new PerceptronTrainer(3, 0.5, 0.1, 1.0, 1.0);
-  }
-
-  @Test
-  public void testUpdate() throws Exception {
-    double[] labels = { 1.0, 1.0, 1.0, 0.0 };
-    Vector labelset = new DenseVector(labels);
-    double[][] values = new double[3][4];
-    for (int i = 0; i < 3; i++) {
-      values[i][0] = 1.0;
-      values[i][1] = 1.0;
-      values[i][2] = 1.0;
-      values[i][3] = 1.0;
-    }
-    values[1][0] = 0.0;
-    values[2][0] = 0.0;
-    values[1][1] = 0.0;
-    values[2][2] = 0.0;
-
-    Matrix dataset = new DenseMatrix(values);
-    this.trainer.train(labelset, dataset);
-    assertFalse(this.trainer.getModel().classify(dataset.viewColumn(3)));
-    assertTrue(this.trainer.getModel().classify(dataset.viewColumn(0)));
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/WinnowTrainerTest.java mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/WinnowTrainerTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/WinnowTrainerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/discriminative/WinnowTrainerTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,62 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.classifier.discriminative;
-
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.Vector;
-import org.junit.Before;
-import org.junit.Test;
-
-@Deprecated
-public final class WinnowTrainerTest extends MahoutTestCase {
-
-  private WinnowTrainer trainer;
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    trainer = new WinnowTrainer(3);
-  }
-
-  @Test
-  public void testUpdate() throws Exception {
-    double[] labels = { 0.0, 0.0, 0.0, 1.0 };
-    Vector labelset = new DenseVector(labels);
-    double[][] values = new double[3][4];
-    for (int i = 0; i < 3; i++) {
-      values[i][0] = 1.0;
-      values[i][1] = 1.0;
-      values[i][2] = 1.0;
-      values[i][3] = 1.0;
-    }
-    values[1][0] = 0.0;
-    values[2][0] = 0.0;
-    values[1][1] = 0.0;
-    values[2][2] = 0.0;
-
-    Matrix dataset = new DenseMatrix(values);
-    trainer.train(labelset, dataset);
-    assertTrue(trainer.getModel().classify(dataset.viewColumn(3)));
-    assertFalse(trainer.getModel().classify(dataset.viewColumn(0)));
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestMultilayerPerceptron.java mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestMultilayerPerceptron.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestMultilayerPerceptron.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestMultilayerPerceptron.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,88 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.classifier.mlp;
+
+import java.io.File;
+import java.io.IOException;
+
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.math.Arrays;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Vector;
+import org.junit.Test;
+
+/**
+ * Test the functionality of {@link MultilayerPerceptron}
+ */
+public class TestMultilayerPerceptron extends MahoutTestCase {
+
+  @Test
+  public void testMLP() throws IOException {
+    testMLP("testMLPXORLocal", false, false, 8000);
+    testMLP("testMLPXORLocalWithMomentum", true, false, 4000);
+    testMLP("testMLPXORLocalWithRegularization", true, true, 2000);
+  }
+
+  private void testMLP(String modelFilename, boolean useMomentum,
+      boolean useRegularization, int iterations) throws IOException {
+    MultilayerPerceptron mlp = new MultilayerPerceptron();
+    mlp.addLayer(2, false, "Sigmoid");
+    mlp.addLayer(3, false, "Sigmoid");
+    mlp.addLayer(1, true, "Sigmoid");
+    mlp.setCostFunction("Minus_Squared").setLearningRate(0.2);
+    if (useMomentum) {
+      mlp.setMomentumWeight(0.6);
+    }
+
+    if (useRegularization) {
+      mlp.setRegularizationWeight(0.01);
+    }
+
+    double[][] instances = { { 0, 1, 1 }, { 0, 0, 0 }, { 1, 0, 1 }, { 1, 1, 0 } };
+    for (int i = 0; i < iterations; ++i) {
+      for (double[] instance : instances) {
+        Vector features = new DenseVector(Arrays.copyOf(instance, instance.length - 1));
+        mlp.train((int) instance[2], features);
+      }
+    }
+
+    for (double[] instance : instances) {
+      Vector input = new DenseVector(instance).viewPart(0, instance.length - 1);
+      // the expected output is the last element in array
+      double actual = instance[2];
+      double expected = mlp.getOutput(input).get(0);
+      assertTrue(actual < 0.5 && expected < 0.5 || actual >= 0.5 && expected >= 0.5);
+    }
+
+    // write model into file and read out
+    File modelFile = this.getTestTempFile(modelFilename);
+    mlp.setModelPath(modelFile.getAbsolutePath());
+    mlp.writeModelToFile();
+    mlp.close();
+
+    MultilayerPerceptron mlpCopy = new MultilayerPerceptron(modelFile.getAbsolutePath());
+    // test on instances
+    for (double[] instance : instances) {
+      Vector input = new DenseVector(instance).viewPart(0, instance.length - 1);
+      // the expected output is the last element in array
+      double actual = instance[2];
+      double expected = mlpCopy.getOutput(input).get(0);
+      assertTrue(actual < 0.5 && expected < 0.5 || actual >= 0.5 && expected >= 0.5);
+    }
+    mlpCopy.close();
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestNeuralNetwork.java mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestNeuralNetwork.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestNeuralNetwork.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/classifier/mlp/TestNeuralNetwork.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,346 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.classifier.mlp;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.commons.csv.CSVUtils;
+import org.apache.mahout.classifier.mlp.NeuralNetwork.TrainingMethod;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.math.DenseMatrix;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Matrix;
+import org.apache.mahout.math.Vector;
+import org.junit.Test;
+
+import com.google.common.base.Charsets;
+import com.google.common.collect.Lists;
+import com.google.common.io.Files;
+
+/**
+ * Test the functionality of {@link NeuralNetwork}.
+ */
+public class TestNeuralNetwork extends MahoutTestCase {
+
+  @Test
+  public void testReadWrite() throws IOException {
+    NeuralNetwork ann = new MultilayerPerceptron();
+    ann.addLayer(2, false, "Identity");
+    ann.addLayer(5, false, "Identity");
+    ann.addLayer(1, true, "Identity");
+    ann.setCostFunction("Minus_Squared");
+    double learningRate = 0.2;
+    double momentumWeight = 0.5;
+    double regularizationWeight = 0.05;
+    ann.setLearningRate(learningRate).setMomentumWeight(momentumWeight).setRegularizationWeight(regularizationWeight);
+
+    // manually set weights
+    Matrix[] matrices = new DenseMatrix[2];
+    matrices[0] = new DenseMatrix(5, 3);
+    matrices[0].assign(0.2);
+    matrices[1] = new DenseMatrix(1, 6);
+    matrices[1].assign(0.8);
+    ann.setWeightMatrices(matrices);
+
+    // write to file
+    String modelFilename = "testNeuralNetworkReadWrite";
+    File tmpModelFile = this.getTestTempFile(modelFilename);
+    ann.setModelPath(tmpModelFile.getAbsolutePath());
+    ann.writeModelToFile();
+
+    // read from file
+    NeuralNetwork annCopy = new MultilayerPerceptron(tmpModelFile.getAbsolutePath());
+    assertEquals(annCopy.getClass().getSimpleName(), annCopy.getModelType());
+    assertEquals(tmpModelFile.getAbsolutePath(), annCopy.getModelPath());
+    assertEquals(learningRate, annCopy.getLearningRate(), 0.000001);
+    assertEquals(momentumWeight, annCopy.getMomentumWeight(), 0.000001);
+    assertEquals(regularizationWeight, annCopy.getRegularizationWeight(), 0.000001);
+    assertEquals(TrainingMethod.GRADIENT_DESCENT, annCopy.getTrainingMethod());
+
+    // compare weights
+    Matrix[] weightsMatrices = annCopy.getWeightMatrices();
+    for (int i = 0; i < weightsMatrices.length; ++i) {
+      Matrix expectMat = matrices[i];
+      Matrix actualMat = weightsMatrices[i];
+      for (int j = 0; j < expectMat.rowSize(); ++j) {
+        for (int k = 0; k < expectMat.columnSize(); ++k) {
+          assertEquals(expectMat.get(j, k), actualMat.get(j, k), 0.000001);
+        }
+      }
+    }
+  }
+
+  /**
+   * Test the forward functionality.
+   */
+  @Test
+  public void testOutput() {
+    // first network
+    NeuralNetwork ann = new MultilayerPerceptron();
+    ann.addLayer(2, false, "Identity");
+    ann.addLayer(5, false, "Identity");
+    ann.addLayer(1, true, "Identity");
+    ann.setCostFunction("Minus_Squared").setLearningRate(0.1);
+
+    // intentionally initialize all weights to 0.5
+    Matrix[] matrices = new Matrix[2];
+    matrices[0] = new DenseMatrix(5, 3);
+    matrices[0].assign(0.5);
+    matrices[1] = new DenseMatrix(1, 6);
+    matrices[1].assign(0.5);
+    ann.setWeightMatrices(matrices);
+
+    double[] arr = new double[]{0, 1};
+    Vector training = new DenseVector(arr);
+    Vector result = ann.getOutput(training);
+    assertEquals(1, result.size());
+
+    // second network
+    NeuralNetwork ann2 = new MultilayerPerceptron();
+    ann2.addLayer(2, false, "Sigmoid");
+    ann2.addLayer(3, false, "Sigmoid");
+    ann2.addLayer(1, true, "Sigmoid");
+    ann2.setCostFunction("Minus_Squared");
+    ann2.setLearningRate(0.3);
+
+    // intentionally initialize all weights to 0.5
+    Matrix[] matrices2 = new Matrix[2];
+    matrices2[0] = new DenseMatrix(3, 3);
+    matrices2[0].assign(0.5);
+    matrices2[1] = new DenseMatrix(1, 4);
+    matrices2[1].assign(0.5);
+    ann2.setWeightMatrices(matrices2);
+
+    double[] test = {0, 0};
+    double[] result2 = {0.807476};
+
+    Vector vec = ann2.getOutput(new DenseVector(test));
+    double[] arrVec = new double[vec.size()];
+    for (int i = 0; i < arrVec.length; ++i) {
+      arrVec[i] = vec.getQuick(i);
+    }
+    assertArrayEquals(result2, arrVec, 0.000001);
+
+    NeuralNetwork ann3 = new MultilayerPerceptron();
+    ann3.addLayer(2, false, "Sigmoid");
+    ann3.addLayer(3, false, "Sigmoid");
+    ann3.addLayer(1, true, "Sigmoid");
+    ann3.setCostFunction("Minus_Squared").setLearningRate(0.3);
+
+    // intentionally initialize all weights to 0.5
+    Matrix[] initMatrices = new Matrix[2];
+    initMatrices[0] = new DenseMatrix(3, 3);
+    initMatrices[0].assign(0.5);
+    initMatrices[1] = new DenseMatrix(1, 4);
+    initMatrices[1].assign(0.5);
+    ann3.setWeightMatrices(initMatrices);
+
+    double[] instance = {0, 1};
+    Vector output = ann3.getOutput(new DenseVector(instance));
+    assertEquals(0.8315410, output.get(0), 0.000001);
+  }
+
+  @Test
+  public void testNeuralNetwork() throws IOException {
+    testNeuralNetwork("testNeuralNetworkXORLocal", false, false, 10000);
+    testNeuralNetwork("testNeuralNetworkXORWithMomentum", true, false, 5000);
+    testNeuralNetwork("testNeuralNetworkXORWithRegularization", true, true, 5000);
+  }
+
+  private void testNeuralNetwork(String modelFilename, boolean useMomentum,
+                                 boolean useRegularization, int iterations) throws IOException {
+    NeuralNetwork ann = new MultilayerPerceptron();
+    ann.addLayer(2, false, "Sigmoid");
+    ann.addLayer(3, false, "Sigmoid");
+    ann.addLayer(1, true, "Sigmoid");
+    ann.setCostFunction("Minus_Squared").setLearningRate(0.1);
+
+    if (useMomentum) {
+      ann.setMomentumWeight(0.6);
+    }
+
+    if (useRegularization) {
+      ann.setRegularizationWeight(0.01);
+    }
+
+    double[][] instances = {{0, 1, 1}, {0, 0, 0}, {1, 0, 1}, {1, 1, 0}};
+    for (int i = 0; i < iterations; ++i) {
+      for (double[] instance : instances) {
+        ann.trainOnline(new DenseVector(instance));
+      }
+    }
+
+    for (double[] instance : instances) {
+      Vector input = new DenseVector(instance).viewPart(0, instance.length - 1);
+      // the expected output is the last element in array
+      double actual = instance[2];
+      double expected = ann.getOutput(input).get(0);
+      assertTrue(actual < 0.5 && expected < 0.5 || actual >= 0.5 && expected >= 0.5);
+    }
+
+    // write model into file and read out
+    File tmpModelFile = this.getTestTempFile(modelFilename);
+    ann.setModelPath(tmpModelFile.getAbsolutePath());
+    ann.writeModelToFile();
+
+    NeuralNetwork annCopy = new MultilayerPerceptron(tmpModelFile.getAbsolutePath());
+    // test on instances
+    for (double[] instance : instances) {
+      Vector input = new DenseVector(instance).viewPart(0, instance.length - 1);
+      // the expected output is the last element in array
+      double actual = instance[2];
+      double expected = annCopy.getOutput(input).get(0);
+      assertTrue(actual < 0.5 && expected < 0.5 || actual >= 0.5 && expected >= 0.5);
+    }
+  }
+
+  @Test
+  public void testWithCancerDataSet() throws IOException {
+    String dataSetPath = "src/test/resources/cancer.csv";
+    List<Vector> records = Lists.newArrayList();
+    // Returns a mutable list of the data
+    List<String> cancerDataSetList = Files.readLines(new File(dataSetPath), Charsets.UTF_8);
+    // skip the header line, hence remove the first element in the list
+    cancerDataSetList.remove(0);
+    for (String line : cancerDataSetList) {
+      String[] tokens = CSVUtils.parseLine(line);
+      double[] values = new double[tokens.length];
+      for (int i = 0; i < tokens.length; ++i) {
+        values[i] = Double.parseDouble(tokens[i]);
+      }
+      records.add(new DenseVector(values));
+    }
+
+    int splitPoint = (int) (records.size() * 0.8);
+    List<Vector> trainingSet = records.subList(0, splitPoint);
+    List<Vector> testSet = records.subList(splitPoint, records.size());
+
+    // initialize neural network model
+    NeuralNetwork ann = new MultilayerPerceptron();
+    int featureDimension = records.get(0).size() - 1;
+    ann.addLayer(featureDimension, false, "Sigmoid");
+    ann.addLayer(featureDimension * 2, false, "Sigmoid");
+    ann.addLayer(1, true, "Sigmoid");
+    ann.setLearningRate(0.05).setMomentumWeight(0.5).setRegularizationWeight(0.001);
+
+    int iteration = 2000;
+    for (int i = 0; i < iteration; ++i) {
+      for (Vector trainingInstance : trainingSet) {
+        ann.trainOnline(trainingInstance);
+      }
+    }
+
+    int correctInstances = 0;
+    for (Vector testInstance : testSet) {
+      Vector res = ann.getOutput(testInstance.viewPart(0, testInstance.size() - 1));
+      double actual = res.get(0);
+      double expected = testInstance.get(testInstance.size() - 1);
+      if (Math.abs(actual - expected) <= 0.1) {
+        ++correctInstances;
+      }
+    }
+    double accuracy = (double) correctInstances / testSet.size() * 100;
+    assertTrue("The classifier is even worse than a random guesser!", accuracy > 50);
+    System.out.printf("Cancer DataSet. Classification precision: %d/%d = %f%%\n", correctInstances, testSet.size(), accuracy);
+  }
+
+  @Test
+  public void testWithIrisDataSet() throws IOException {
+    String dataSetPath = "src/test/resources/iris.csv";
+    int numOfClasses = 3;
+    List<Vector> records = Lists.newArrayList();
+    // Returns a mutable list of the data
+    List<String> irisDataSetList = Files.readLines(new File(dataSetPath), Charsets.UTF_8);
+    // skip the header line, hence remove the first element in the list
+    irisDataSetList.remove(0);
+
+    for (String line : irisDataSetList) {
+      String[] tokens = CSVUtils.parseLine(line);
+      // last three dimensions represent the labels
+      double[] values = new double[tokens.length + numOfClasses - 1];
+      Arrays.fill(values, 0.0);
+      for (int i = 0; i < tokens.length - 1; ++i) {
+        values[i] = Double.parseDouble(tokens[i]);
+      }
+      // add label values
+      String label = tokens[tokens.length - 1];
+      if (label.equalsIgnoreCase("setosa")) {
+        values[values.length - 3] = 1;
+      } else if (label.equalsIgnoreCase("versicolor")) {
+        values[values.length - 2] = 1;
+      } else { // label 'virginica'
+        values[values.length - 1] = 1;
+      }
+      records.add(new DenseVector(values));
+    }
+
+    Collections.shuffle(records);
+
+    int splitPoint = (int) (records.size() * 0.8);
+    List<Vector> trainingSet = records.subList(0, splitPoint);
+    List<Vector> testSet = records.subList(splitPoint, records.size());
+
+    // initialize neural network model
+    NeuralNetwork ann = new MultilayerPerceptron();
+    int featureDimension = records.get(0).size() - numOfClasses;
+    ann.addLayer(featureDimension, false, "Sigmoid");
+    ann.addLayer(featureDimension * 2, false, "Sigmoid");
+    ann.addLayer(3, true, "Sigmoid"); // 3-class classification
+    ann.setLearningRate(0.05).setMomentumWeight(0.4).setRegularizationWeight(0.005);
+
+    int iteration = 2000;
+    for (int i = 0; i < iteration; ++i) {
+      for (Vector trainingInstance : trainingSet) {
+        ann.trainOnline(trainingInstance);
+      }
+    }
+
+    int correctInstances = 0;
+    for (Vector testInstance : testSet) {
+      Vector res = ann.getOutput(testInstance.viewPart(0, testInstance.size() - numOfClasses));
+      double[] actualLabels = new double[numOfClasses];
+      for (int i = 0; i < numOfClasses; ++i) {
+        actualLabels[i] = res.get(i);
+      }
+      double[] expectedLabels = new double[numOfClasses];
+      for (int i = 0; i < numOfClasses; ++i) {
+        expectedLabels[i] = testInstance.get(testInstance.size() - numOfClasses + i);
+      }
+
+      boolean allCorrect = true;
+      for (int i = 0; i < numOfClasses; ++i) {
+        if (Math.abs(expectedLabels[i] - actualLabels[i]) >= 0.1) {
+          allCorrect = false;
+          break;
+        }
+      }
+      if (allCorrect) {
+        ++correctInstances;
+      }
+    }
+    
+    double accuracy = (double) correctInstances / testSet.size() * 100;
+    assertTrue("The model is even worse than a random guesser.", accuracy > 50);
+    
+    System.out.printf("Iris DataSet. Classification precision: %d/%d = %f%%\n", correctInstances, testSet.size(), accuracy);
+  }
+  
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegressionTest.java mahout/core/src/test/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegressionTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegressionTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/sgd/AdaptiveLogisticRegressionTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -24,10 +24,13 @@
 import org.apache.mahout.math.jet.random.Exponential;
 import org.junit.Test;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
+
 import java.util.Random;
 
 public final class AdaptiveLogisticRegressionTest extends MahoutTestCase {
 
+  @ThreadLeakLingering(linger=1000)
   @Test
   public void testTrain() {
 
@@ -49,25 +52,25 @@
       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
       cl.train(r);
       if (i % 1000 == 0) {
-//        cl.close();
         System.out.printf("%10d %10.3f\n", i, cl.getLearner().auc());
       }
     }
     assertEquals(1, cl.getLearner().auc(), 0.1);
 
-    AdaptiveLogisticRegression x = new AdaptiveLogisticRegression(2, 200, new L1());
-    x.setInterval(1000);
+    AdaptiveLogisticRegression adaptiveLogisticRegression = new AdaptiveLogisticRegression(2, 200, new L1());
+    adaptiveLogisticRegression.setInterval(1000);
 
     for (int i = 0; i < 20000; i++) {
       AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);
-      x.train(r.getKey(), r.getActual(), r.getInstance());
-      if (i % 1000 == 0 && x.getBest() != null) {
+      adaptiveLogisticRegression.train(r.getKey(), r.getActual(), r.getInstance());
+      if (i % 1000 == 0 && adaptiveLogisticRegression.getBest() != null) {
         System.out.printf("%10d %10.4f %10.8f %.3f\n",
-                          i, x.auc(),
-                          Math.log10(x.getBest().getMappedParams()[0]), x.getBest().getMappedParams()[1]);
+                          i, adaptiveLogisticRegression.auc(),
+                          Math.log10(adaptiveLogisticRegression.getBest().getMappedParams()[0]), adaptiveLogisticRegression.getBest().getMappedParams()[1]);
       }
     }
-    assertEquals(1, x.auc(), 0.1);
+    assertEquals(1, adaptiveLogisticRegression.auc(), 0.1);
+    adaptiveLogisticRegression.close();
   }
 
   private static AdaptiveLogisticRegression.TrainingExample getExample(int i, Random gen, Vector beta) {
@@ -146,6 +149,7 @@
   }
 
   @Test
+  @ThreadLeakLingering(linger = 1000)
   public void constantStep() {
     AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());
     lr.setInterval(5000);
@@ -153,10 +157,12 @@
     assertEquals(20000, lr.nextStep(15001));
     assertEquals(20000, lr.nextStep(16500));
     assertEquals(20000, lr.nextStep(19999));
+    lr.close(); 
   }
     
 
   @Test
+  @ThreadLeakLingering(linger = 1000)
   public void growingStep() {
     AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());
     lr.setInterval(2000, 10000);
@@ -175,5 +181,6 @@
     for (int i = 50000; i < 500000; i += 10000) {
       assertEquals(i + 10000, lr.nextStep(i));
     }
+    lr.close();
   }
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/classifier/sgd/ModelSerializerTest.java mahout/core/src/test/java/org/apache/mahout/classifier/sgd/ModelSerializerTest.java
--- mahout/core/src/test/java/org/apache/mahout/classifier/sgd/ModelSerializerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/classifier/sgd/ModelSerializerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.util.Random;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
 import com.google.common.io.Closeables;
 import org.apache.hadoop.io.Writable;
 import org.apache.mahout.classifier.OnlineLearner;
@@ -87,6 +88,8 @@
     train(olr3, 100);
 
     assertEquals(0, olr.getBeta().minus(olr3.getBeta()).aggregate(Functions.MAX, Functions.IDENTITY), 1.0e-6);
+    olr.close();
+    olr3.close();
   }
 
   @Test
@@ -107,8 +110,11 @@
     assertEquals(learner.auc(), olr3.auc(), 0.02);
     double auc2 = learner.auc();
     assertTrue(auc2 > auc1);
+    learner.close();
+    olr3.close();
   }
 
+  @ThreadLeakLingering(linger = 1000)
   @Test
   public void adaptiveLogisticRegressionRoundTrip() throws IOException {
     AdaptiveLogisticRegression learner = new AdaptiveLogisticRegression(2, 5, new L1());
@@ -128,6 +134,8 @@
     assertEquals(learner.auc(), olr3.auc(), 0.005);
     double auc2 = learner.auc();
     assertTrue(String.format("%.3f > %.3f", auc2, auc1), auc2 > auc1);
+    learner.close();
+    olr3.close();
   }
 
   private static void train(OnlineLearner olr, int n) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/TestClusterInterface.java mahout/core/src/test/java/org/apache/mahout/clustering/TestClusterInterface.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/TestClusterInterface.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/TestClusterInterface.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,7 +18,6 @@
 package org.apache.mahout.clustering;
 
 import org.apache.mahout.clustering.canopy.Canopy;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopy;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
@@ -110,44 +109,4 @@
     assertEquals("CL-123{n=0 c=[0:1.100, 2:3.300] r=[]}", formatString);
   }
 
-  @Test
-  public void testMSCanopyAsFormatString() {
-    double[] d = { 1.1, 2.2, 3.3 };
-    Vector m = new DenseVector(d);
-    Cluster cluster = new MeanShiftCanopy(m, 123, measure);
-    String formatString = cluster.asFormatString(null);
-    assertEquals("MSC-123{n=0 c=[1.100, 2.200, 3.300] r=[]}", formatString);
-  }
-
-  @Test
-  public void testMSCanopyAsFormatStringSparse() {
-    double[] d = { 1.1, 0.0, 3.3 };
-    Vector m = new SequentialAccessSparseVector(3);
-    m.assign(d);
-    Cluster cluster = new MeanShiftCanopy(m, 123, measure);
-    String formatString = cluster.asFormatString(null);
-    assertEquals("MSC-123{n=0 c=[0:1.100, 2:3.300] r=[]}", formatString);
-  }
-
-  @Test
-  public void testMSCanopyAsFormatStringWithBindings() {
-    double[] d = { 1.1, 2.2, 3.3 };
-    Vector m = new DenseVector(d);
-    Cluster cluster = new MeanShiftCanopy(m, 123, measure);
-    String[] bindings = { "fee", null, "foo" };
-    String formatString = cluster.asFormatString(bindings);
-    assertEquals("MSC-123{n=0 c=[fee:1.100, 1:2.200, foo:3.300] r=[]}", formatString);
-  }
-
-  @Test
-  public void testMSCanopyAsFormatStringSparseWithBindings() {
-    double[] d = { 1.1, 0.0, 3.3 };
-    Vector m = new SequentialAccessSparseVector(3);
-    m.assign(d);
-    Cluster cluster = new MeanShiftCanopy(m, 123, measure);
-    String[] bindings = { "fee", null, "foo" };
-    String formatString = cluster.asFormatString(bindings);
-    assertEquals("MSC-123{n=0 c=[fee:1.100, foo:3.300] r=[]}", formatString);
-  }
-
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/TestGaussianAccumulators.java mahout/core/src/test/java/org/apache/mahout/clustering/TestGaussianAccumulators.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/TestGaussianAccumulators.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/TestGaussianAccumulators.java	2014-03-29 01:03:14.000000000 -0700
@@ -19,7 +19,6 @@
 import java.util.Collection;
 
 import com.google.common.collect.Lists;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.Vector;
@@ -68,7 +67,7 @@
 
   /**
    * Generate random samples and add them to the sampleData
-   * 
+   *
    * @param num
    *          int number of samples to generate
    * @param mx
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java mahout/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/canopy/TestCanopyCreation.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,6 +21,7 @@
 import java.util.List;
 import java.util.Set;
 
+import com.google.common.collect.Iterables;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -258,17 +259,16 @@
 
     List<VectorWritable> points = getPointsWritable();
     reducer.reduce(new Text("centroid"), points, context);
-    Set<Text> keys = writer.getKeys();
-    assertEquals("Number of centroids", 3, keys.size());
+    Iterable<Text> keys = writer.getKeysInInsertionOrder();
+    assertEquals("Number of centroids", 3, Iterables.size(keys));
     int i = 0;
     for (Text key : keys) {
       List<ClusterWritable> data = writer.getValue(key);
       ClusterWritable clusterWritable = data.get(0);
-	  Canopy canopy = (Canopy)clusterWritable.getValue();
-	  assertEquals(manhattanCentroids.get(i).asFormatString()
-          + " is not equal to "
-          + canopy.computeCentroid().asFormatString(), manhattanCentroids
-          .get(i), canopy.computeCentroid());
+      Canopy canopy = (Canopy) clusterWritable.getValue();
+      assertEquals(manhattanCentroids.get(i).asFormatString() + " is not equal to "
+          + canopy.computeCentroid().asFormatString(),
+          manhattanCentroids.get(i), canopy.computeCentroid());
       i++;
     }
   }
@@ -282,29 +282,27 @@
   public void testCanopyReducerEuclidean() throws Exception {
     CanopyReducer reducer = new CanopyReducer();
     Configuration conf = getConfiguration();
-    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY,
-        "org.apache.mahout.common.distance.EuclideanDistanceMeasure");
+    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, "org.apache.mahout.common.distance.EuclideanDistanceMeasure");
     conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));
     conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));
     conf.set(CanopyConfigKeys.CF_KEY, "0");
     DummyRecordWriter<Text, ClusterWritable> writer = new DummyRecordWriter<Text, ClusterWritable>();
-    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context = DummyRecordWriter
-        .build(reducer, conf, writer, Text.class, VectorWritable.class);
+    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context =
+        DummyRecordWriter.build(reducer, conf, writer, Text.class, VectorWritable.class);
     reducer.setup(context);
 
     List<VectorWritable> points = getPointsWritable();
     reducer.reduce(new Text("centroid"), points, context);
-    Set<Text> keys = writer.getKeys();
-    assertEquals("Number of centroids", 3, keys.size());
+    Iterable<Text> keys = writer.getKeysInInsertionOrder();
+    assertEquals("Number of centroids", 3, Iterables.size(keys));
     int i = 0;
     for (Text key : keys) {
       List<ClusterWritable> data = writer.getValue(key);
       ClusterWritable clusterWritable = data.get(0);
-      Canopy canopy = (Canopy)clusterWritable.getValue();
-      assertEquals(euclideanCentroids.get(i).asFormatString()
-          + " is not equal to "
-          + canopy.computeCentroid().asFormatString(), euclideanCentroids
-          .get(i), canopy.computeCentroid());
+      Canopy canopy = (Canopy) clusterWritable.getValue();
+      assertEquals(euclideanCentroids.get(i).asFormatString() + " is not equal to "
+          + canopy.computeCentroid().asFormatString(),
+          euclideanCentroids.get(i), canopy.computeCentroid());
       i++;
     }
   }
@@ -333,29 +331,27 @@
     try {
       Writable key = new Text();
       ClusterWritable clusterWritable = new ClusterWritable();
-	  assertTrue("more to come", reader.next(key, clusterWritable));
+      assertTrue("more to come", reader.next(key, clusterWritable));
       assertEquals("1st key", "C-0", key.toString());
 
       List<Pair<Double,Double>> refCenters = Lists.newArrayList();
       refCenters.add(new Pair<Double,Double>(1.5,1.5));
       refCenters.add(new Pair<Double,Double>(4.333333333333334,4.333333333333334));
-	  Pair<Double,Double> c = new Pair<Double,Double>(clusterWritable.getValue() .getCenter().get(0),
-			clusterWritable.getValue().getCenter().get(1));
+      Pair<Double,Double> c = new Pair<Double,Double>(clusterWritable.getValue() .getCenter().get(0),
+      clusterWritable.getValue().getCenter().get(1));
       assertTrue("center "+c+" not found", findAndRemove(c, refCenters, EPSILON));
       assertTrue("more to come", reader.next(key, clusterWritable));
       assertEquals("2nd key", "C-1", key.toString());
       c = new Pair<Double,Double>(clusterWritable.getValue().getCenter().get(0),
-    		  clusterWritable.getValue().getCenter().get(1));
-      assertTrue("center "+c+" not found", findAndRemove(c, refCenters, EPSILON));
+          clusterWritable.getValue().getCenter().get(1));
+      assertTrue("center " + c + " not found", findAndRemove(c, refCenters, EPSILON));
       assertFalse("more to come", reader.next(key, clusterWritable));
     } finally {
       Closeables.close(reader, true);
     }
   }
 
-  static boolean findAndRemove(Pair<Double, Double> target,
-                               Collection<Pair<Double, Double>> list,
-                               double epsilon) {
+  static boolean findAndRemove(Pair<Double, Double> target, Collection<Pair<Double, Double>> list, double epsilon) {
     for (Pair<Double,Double> curr : list) {
       if ( (Math.abs(target.getFirst() - curr.getFirst()) < epsilon) 
            && (Math.abs(target.getSecond() - curr.getSecond()) < epsilon) ) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java mahout/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/classify/ClusterClassificationDriverTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -37,6 +37,7 @@
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
+import org.apache.mahout.math.NamedVector;
 import org.apache.mahout.math.RandomAccessSparseVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
@@ -128,7 +129,7 @@
     clusteringOutputPath = getTestTempDirPath("output");
     classifiedOutputPath = getTestTempDirPath("classify");
 
-    conf = new Configuration();
+    conf = getConfiguration();
 
     ClusteringTestUtils.writePointsToFile(points,
         new Path(pointsPath, "file1"), fs, conf);
@@ -167,7 +168,7 @@
       SequenceFile.Reader classifiedVectors = new SequenceFile.Reader(fs,
           partFile.getPath(), conf);
       Writable clusterIdAsKey = new IntWritable();
-      WeightedVectorWritable point = new WeightedVectorWritable();
+      WeightedPropertyVectorWritable point = new WeightedPropertyVectorWritable();
       while (classifiedVectors.next(clusterIdAsKey, point)) {
         collectVector(clusterIdAsKey.toString(), point.getVector());
       }
@@ -235,9 +236,15 @@
       } else {
         singletonCnt++;
         assertEquals("expecting only singleton clusters; got size=" + vList.size(), 1, vList.size());
-        Assert.assertTrue("not expecting cluster:" + vList.get(0).asFormatString(),
-                          reference.contains(vList.get(0).asFormatString()));
-        reference.remove(vList.get(0).asFormatString());
+        if (vList.get(0).getClass().equals(NamedVector.class)) {
+          Assert.assertTrue("not expecting cluster:" + ((NamedVector) vList.get(0)).getDelegate().asFormatString(),
+                  reference.contains(((NamedVector)  vList.get(0)).getDelegate().asFormatString()));
+          reference.remove(((NamedVector)vList.get(0)).getDelegate().asFormatString());
+        } else if (vList.get(0).getClass().equals(RandomAccessSparseVector.class)) {
+          Assert.assertTrue("not expecting cluster:" + vList.get(0).asFormatString(),
+                  reference.contains(vList.get(0).asFormatString()));
+          reference.remove(vList.get(0).asFormatString());
+        }
       }
     }
     Assert.assertEquals("Different number of empty clusters than expected!", 1, emptyCnt);
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDirichletClustering.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet;
-
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.mahout.clustering.ClusteringTestUtils;
-import org.apache.mahout.clustering.dirichlet.models.DistanceMeasureClusterDistribution;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Before;
-import org.junit.Test;
-
-import com.google.common.collect.Lists;
-
-@Deprecated
-public final class TestDirichletClustering extends MahoutTestCase {
-
-  private List<VectorWritable> sampleData;
-  
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    sampleData = Lists.newArrayList();
-  }
-
-  /**
-   * Generate random samples and add them to the sampleData
-   *
-   * @param num int number of samples to generate
-   * @param mx  double x-value of the sample mean
-   * @param my  double y-value of the sample mean
-   * @param sd  double standard deviation of the samples
-   * @param card int cardinality of the generated sample vectors
-   */
-  private void generateSamples(int num, double mx, double my, double sd, int card) {
-    System.out.println("Generating " + num + " samples m=[" + mx + ", " + my + "] sd=" + sd);
-    for (int i = 0; i < num; i++) {
-      DenseVector v = new DenseVector(card);
-      for (int j = 0; j < card; j++) {
-        v.set(j, UncommonDistributions.rNorm(mx, sd));
-      }
-      sampleData.add(new VectorWritable(v));
-    }
-  }
-
-  /**
-   * Generate 2-d samples for backwards compatibility with existing tests
-   * @param num int number of samples to generate
-   * @param mx  double x-value of the sample mean
-   * @param my  double y-value of the sample mean
-   * @param sd  double standard deviation of the samples
-   */
-  private void generateSamples(int num, double mx, double my, double sd) {
-    generateSamples(num, mx, my, sd, 2);
-  }
-
-  @Test
-  public void testDirichletClusteringSeq() throws Exception {
-    Path output = getTestTempDirPath("output");
-    Configuration conf = getConfiguration();
-    FileSystem fs = FileSystem.get(getConfiguration());
-    
-    generateSamples(40, 1, 1, 3);
-    generateSamples(30, 1, 0, 0.1);
-    generateSamples(30, 0, 1, 0.1);
-
-    ClusteringTestUtils.writePointsToFile(sampleData,
-            getTestTempFilePath("testdata/file1"), fs, conf);
-
-    DenseVector prototype = (DenseVector) sampleData.get(0).get();
-    
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(),
-        RandomAccessSparseVector.class.getName(),
-        ManhattanDistanceMeasure.class.getName(), prototype.size());
-    
-    DirichletDriver.run(conf, getTestTempDirPath("testdata"), output,
-        description, 10, 1, 1.0, true, true, 0, true);
-    
-    Path path = new Path(output, "clusteredPoints/part-m-0");
-    long count = HadoopUtil.countRecords(path, conf);
-    assertEquals("number of points", sampleData.size(), count);
-  }
-  
-  @Test
-  public void testDirichletClusteringMR() throws Exception {
-    Path output = getTestTempDirPath("output");
-    Configuration conf = getConfiguration();
-    FileSystem fs = FileSystem.get(getConfiguration());
-    
-    generateSamples(40, 1, 1, 3);
-    generateSamples(30, 1, 0, 0.1);
-
-    ClusteringTestUtils.writePointsToFile(sampleData, true,
-            getTestTempFilePath("testdata/file1"), fs, conf);
-
-    DenseVector prototype = (DenseVector) sampleData.get(0).get();
-    
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(),
-        RandomAccessSparseVector.class.getName(),
-        ManhattanDistanceMeasure.class.getName(), prototype.size());
-    
-    DirichletDriver.run(conf, getTestTempDirPath("testdata"), output,
-        description, 10, 1, 1.0, true, true, 0, false);
-    
-    Path path = new Path(output, "clusteredPoints/part-m-00000");
-    long count = HadoopUtil.countRecords(path, conf);
-    assertEquals("number of points", sampleData.size(), count);
-  }
-  
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDistributions.java mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDistributions.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDistributions.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestDistributions.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,131 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet;
-
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.junit.Test;
-
-@Deprecated
-public final class TestDistributions extends MahoutTestCase {
-
-  @Test
-  public void testRbeta() {
-    for (double i = 0.01; i < 20; i += 0.25) {
-      System.out.println("rBeta(6,1," + i + ")="
-          + UncommonDistributions.rBeta(6, 1, i).asFormatString());
-    }
-  }
-
-  @Test
-  public void testRchisq() {
-    for (int i = 0; i < 50; i++) {
-      System.out
-          .println("rChisq(" + i + ")=" + UncommonDistributions.rChisq(i));
-    }
-  }
-
-  @Test
-  public void testRnorm() {
-    for (int i = 1; i < 50; i++) {
-      System.out.println("rNorm(6,1," + i + ")="
-          + UncommonDistributions.rNorm(1, i));
-    }
-  }
-
-  @Test
-  public void testDnorm() {
-    for (int i = -30; i < 30; i++) {
-      double d = i * 0.1;
-      double dnorm = UncommonDistributions.dNorm(d, 0, 1);
-      char[] bar = new char[(int) (dnorm * 100)];
-      for (int j = 0; j < bar.length; j++) {
-        bar[j] = '*';
-      }
-      String baz = new String(bar);
-      System.out.println(baz);
-    }
-  }
-
-  @Test
-  public void testDnorm2() {
-    for (int i = -30; i < 30; i++) {
-      double d = i * 0.1;
-      double dnorm = UncommonDistributions.dNorm(d, 0, 2);
-      char[] bar = new char[(int) (dnorm * 100)];
-      for (int j = 0; j < bar.length; j++) {
-        bar[j] = '*';
-      }
-      String baz = new String(bar);
-      System.out.println(baz);
-    }
-  }
-
-  @Test
-  public void testDnorm1() {
-    for (int i = -10; i < 10; i++) {
-      double d = i * 0.1;
-      double dnorm = UncommonDistributions.dNorm(d, 0, 0.2);
-      char[] bar = new char[(int) (dnorm * 100)];
-      for (int j = 0; j < bar.length; j++) {
-        bar[j] = '*';
-      }
-      String baz = new String(bar);
-      System.out.println(baz);
-    }
-  }
-
-  @Test
-  public void testRmultinom1() {
-    double[] b = {0.4, 0.6};
-    Vector v = new DenseVector(b);
-    Vector t = v.like();
-    for (int i = 1; i <= 100; i++) {
-      Vector multinom = UncommonDistributions.rMultinom(100, v);
-      t = t.plus(multinom);
-    }
-    System.out.println("sum(rMultinom(" + 100 + ", [0.4, 0.6]))/100="
-        + t.divide(100).asFormatString());
-
-  }
-
-  @Test
-  public void testRmultinom2() {
-    double[] b = {0.1, 0.2, 0.7};
-    Vector v = new DenseVector(b);
-    Vector t = v.like();
-    for (int i = 1; i <= 100; i++) {
-      Vector multinom = UncommonDistributions.rMultinom(100, v);
-      t = t.plus(multinom);
-    }
-    System.out.println("sum(rMultinom(" + 100 + ", [ 0.1, 0.2, 0.7 ]))/100="
-        + t.divide(100).asFormatString());
-
-  }
-
-  @Test
-  public void testRmultinom() {
-    double[] b = {0.1, 0.2, 0.8};
-    Vector v = new DenseVector(b);
-    for (int i = 1; i <= 100; i++) {
-      System.out.println("rMultinom(" + 100 + ", [0.1, 0.2, 0.8])="
-          + UncommonDistributions.rMultinom(100, v).asFormatString());
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/dirichlet/TestMapReduce.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,324 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.clustering.dirichlet;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.ClusteringTestUtils;
-import org.apache.mahout.clustering.classify.ClusterClassifier;
-import org.apache.mahout.clustering.dirichlet.models.DistanceMeasureClusterDistribution;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.common.distance.MahalanobisDistanceMeasure;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.MatrixWritable;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Before;
-import org.junit.Test;
-
-import com.google.common.collect.Lists;
-import com.google.common.io.Closeables;
-
-@Deprecated
-public final class TestMapReduce extends MahoutTestCase {
-  
-  private final Collection<VectorWritable> sampleData = Lists.newArrayList();
-  
-  private FileSystem fs;
-  
-  private Configuration conf;
-  
-  private void addSample(double[] values) {
-    Vector v = new DenseVector(2);
-    for (int j = 0; j < values.length; j++) {
-      v.setQuick(j, values[j]);
-    }
-    sampleData.add(new VectorWritable(v));
-  }
-  
-  /**
-   * Generate random samples and add them to the sampleData
-   * 
-   * @param num
-   *          int number of samples to generate
-   * @param mx
-   *          double x-value of the sample mean
-   * @param my
-   *          double y-value of the sample mean
-   * @param sd
-   *          double standard deviation of the samples
-   */
-  private void generateSamples(int num, double mx, double my, double sd) {
-    System.out.println("Generating " + num + " samples m=[" + mx + ", " + my + "] sd=" + sd);
-    for (int i = 0; i < num; i++) {
-      addSample(new double[] {UncommonDistributions.rNorm(mx, sd), UncommonDistributions.rNorm(my, sd)});
-    }
-  }
-  
-  /**
-   * Generate random samples with asymmetric standard deviations and add them to the sampleData
-   * 
-   * @param num
-   *          int number of samples to generate
-   * @param mx
-   *          double x-value of the sample mean
-   * @param my
-   *          double y-value of the sample mean
-   * @param sdx
-   *          double standard deviation in x of the samples
-   * @param sdy
-   *          double standard deviation in y of the samples
-   */
-  private void generateAsymmetricSamples(int num, double mx, double my, double sdx, double sdy) {
-    System.out.println("Generating " + num + " samples m=[" + mx + ", " + my + "] sd=[" + sdx + ", " + sdy + ']');
-    for (int i = 0; i < num; i++) {
-      addSample(new double[] {UncommonDistributions.rNorm(mx, sdx), UncommonDistributions.rNorm(my, sdy)});
-    }
-  }
-  
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    conf = getConfiguration(); 
-    fs = FileSystem.get(conf);
-  }
-  
-  /** Test the Mapper and Reducer using the Driver in sequential execution mode */
-  @Test
-  public void testDriverIterationsSeq() throws Exception {
-    generateSamples(100, 0, 0, 0.5);
-    generateSamples(100, 2, 0, 0.2);
-    generateSamples(100, 0, 2, 0.3);
-    generateSamples(100, 2, 2, 1);
-    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("input/data.txt"), fs, conf);
-    // Now run the driver using the run() method. Others can use runJob() as
-    // before
-    Integer maxIterations = 5;
-    DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-        DenseVector.class.getName(), null, 2);
-    Path outputPath = getTestTempDirPath("output");
-    String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("input").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
-        optKey(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION), description.getModelFactory(),
-        optKey(DirichletDriver.MODEL_PROTOTYPE_CLASS_OPTION), description.getModelPrototype(),
-        optKey(DefaultOptionCreator.NUM_CLUSTERS_OPTION), "20", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION),
-        maxIterations.toString(), optKey(DirichletDriver.ALPHA_OPTION), "1.0",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.CLUSTERING_OPTION),
-        optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD};
-    ToolRunner.run(conf, new DirichletDriver(), args);
-    // and inspect results
-    printModels(getClusters(outputPath, maxIterations));
-  }
-  
-  /** Test the Mapper and Reducer using the Driver in mapreduce mode */
-  @Test
-  public void testDriverIterationsMR() throws Exception {
-    generateSamples(100, 0, 0, 0.5);
-    generateSamples(100, 2, 0, 0.2);
-    generateSamples(100, 0, 2, 0.3);
-    generateSamples(100, 2, 2, 1);
-    ClusteringTestUtils.writePointsToFile(sampleData, true, getTestTempFilePath("input/data.txt"), fs, conf);
-    // Now run the driver using the run() method. Others can use runJob() as
-    // before
-    Integer maxIterations = 5;
-    DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-        DenseVector.class.getName(), null, 2);
-    Path outputPath = getTestTempDirPath("output");
-    String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("input").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
-        optKey(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION), description.getModelFactory(),
-        optKey(DirichletDriver.MODEL_PROTOTYPE_CLASS_OPTION), description.getModelPrototype(),
-        optKey(DefaultOptionCreator.NUM_CLUSTERS_OPTION), "20", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION),
-        maxIterations.toString(), optKey(DirichletDriver.ALPHA_OPTION), "1.0",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.CLUSTERING_OPTION)};
-    ToolRunner.run(conf, new DirichletDriver(), args);
-    // and inspect results
-    printModels(getClusters(outputPath, maxIterations));
-  }
-  
-  /**
-   * Test the Driver in sequential execution mode using MahalanobisDistanceMeasure
-   */
-  @Test
-  public void testDriverIterationsMahalanobisSeq() throws Exception {
-    generateAsymmetricSamples(100, 0, 0, 0.5, 3.0);
-    generateAsymmetricSamples(100, 0, 3, 0.3, 4.0);
-    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("input/data.txt"), fs, conf);
-    // Now run the driver using the run() method. Others can use runJob() as
-    // before
-    MahalanobisDistanceMeasure measure = new MahalanobisDistanceMeasure();
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(), DenseVector.class.getName(),
-        MahalanobisDistanceMeasure.class.getName(), 2);
-    
-    Vector meanVector = new DenseVector(new double[] {0.0, 0.0});
-    measure.setMeanVector(meanVector);
-    Matrix m = new DenseMatrix(new double[][] { {0.5, 0.0}, {0.0, 4.0}});
-    measure.setCovarianceMatrix(m);
-    
-    Path inverseCovarianceFile = new Path(getTestTempDirPath("mahalanobis"),
-        "MahalanobisDistanceMeasureInverseCovarianceFile");
-    conf.set("MahalanobisDistanceMeasure.inverseCovarianceFile", inverseCovarianceFile.toString());
-    FileSystem fs = FileSystem.get(inverseCovarianceFile.toUri(), conf);
-    MatrixWritable inverseCovarianceMatrix = new MatrixWritable(measure.getInverseCovarianceMatrix());
-    DataOutputStream out = fs.create(inverseCovarianceFile);
-    try {
-      inverseCovarianceMatrix.write(out);
-    } finally {
-      Closeables.close(out, true);
-    }
-    
-    Path meanVectorFile = new Path(getTestTempDirPath("mahalanobis"), "MahalanobisDistanceMeasureMeanVectorFile");
-    conf.set("MahalanobisDistanceMeasure.meanVectorFile", meanVectorFile.toString());
-    fs = FileSystem.get(meanVectorFile.toUri(), conf);
-    VectorWritable meanVectorWritable = new VectorWritable(meanVector);
-    out = fs.create(meanVectorFile);
-    try {
-      meanVectorWritable.write(out);
-    } finally {
-      Closeables.close(out, true);
-    }
-    
-    conf.set("MahalanobisDistanceMeasure.maxtrixClass", MatrixWritable.class.getName());
-    conf.set("MahalanobisDistanceMeasure.vectorClass", VectorWritable.class.getName());
-    
-    Integer maxIterations = 5;
-    Path outputPath = getTestTempDirPath("output");
-    String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("input").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
-        optKey(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION), description.getModelFactory(),
-        optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), description.getDistanceMeasure(),
-        optKey(DirichletDriver.MODEL_PROTOTYPE_CLASS_OPTION), description.getModelPrototype(),
-        optKey(DefaultOptionCreator.NUM_CLUSTERS_OPTION), "20", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION),
-        maxIterations.toString(), optKey(DirichletDriver.ALPHA_OPTION), "1.0",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.CLUSTERING_OPTION),
-        optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD};
-    DirichletDriver dirichletDriver = new DirichletDriver();
-    dirichletDriver.setConf(conf);
-    dirichletDriver.run(args);
-    // and inspect results
-    printModels(getClusters(outputPath, maxIterations));
-  }
-  
-  /** Test the Mapper and Reducer using the Driver in mapreduce mode */
-  @Test
-  public void testDriverIterationsMahalanobisMR() throws Exception {
-    generateAsymmetricSamples(100, 0, 0, 0.5, 3.0);
-    generateAsymmetricSamples(100, 0, 3, 0.3, 4.0);
-    ClusteringTestUtils.writePointsToFile(sampleData, true, getTestTempFilePath("input/data.txt"), fs, conf);
-    // Now run the driver using the run() method. Others can use runJob() as
-    // before
-    
-    MahalanobisDistanceMeasure measure = new MahalanobisDistanceMeasure();
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(), DenseVector.class.getName(),
-        MahalanobisDistanceMeasure.class.getName(), 2);
-    
-    Vector meanVector = new DenseVector(new double[] {0.0, 0.0});
-    measure.setMeanVector(meanVector);
-    Matrix m = new DenseMatrix(new double[][] { {0.5, 0.0}, {0.0, 4.0}});
-    measure.setCovarianceMatrix(m);
-    
-    Path inverseCovarianceFile = new Path(getTestTempDirPath("mahalanobis"),
-        "MahalanobisDistanceMeasureInverseCovarianceFile");
-    conf.set("MahalanobisDistanceMeasure.inverseCovarianceFile", inverseCovarianceFile.toString());
-    FileSystem fs = FileSystem.get(inverseCovarianceFile.toUri(), conf);
-    MatrixWritable inverseCovarianceMatrix = new MatrixWritable(measure.getInverseCovarianceMatrix());
-    DataOutputStream out = fs.create(inverseCovarianceFile);
-    try {
-      inverseCovarianceMatrix.write(out);
-    } finally {
-      Closeables.close(out, false);
-    }
-    
-    Path meanVectorFile = new Path(getTestTempDirPath("mahalanobis"), "MahalanobisDistanceMeasureMeanVectorFile");
-    conf.set("MahalanobisDistanceMeasure.meanVectorFile", meanVectorFile.toString());
-    fs = FileSystem.get(meanVectorFile.toUri(), conf);
-    VectorWritable meanVectorWritable = new VectorWritable(meanVector);
-    out = fs.create(meanVectorFile);
-    try {
-      meanVectorWritable.write(out);
-    } finally {
-      Closeables.close(out, false);
-    }
-    
-    conf.set("MahalanobisDistanceMeasure.maxtrixClass", MatrixWritable.class.getName());
-    conf.set("MahalanobisDistanceMeasure.vectorClass", VectorWritable.class.getName());
-    
-    Integer maxIterations = 5;
-    Path outputPath = getTestTempDirPath("output");
-    String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("input").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
-        optKey(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION), description.getModelFactory(),
-        optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), description.getDistanceMeasure(),
-        optKey(DirichletDriver.MODEL_PROTOTYPE_CLASS_OPTION), description.getModelPrototype(),
-        optKey(DefaultOptionCreator.NUM_CLUSTERS_OPTION), "20", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION),
-        maxIterations.toString(), optKey(DirichletDriver.ALPHA_OPTION), "1.0",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.CLUSTERING_OPTION)};
-    Tool dirichletDriver = new DirichletDriver();
-    dirichletDriver.setConf(conf);
-    ToolRunner.run(conf, dirichletDriver, args);
-    // and inspect results
-    printModels(getClusters(outputPath, maxIterations));
-  }
-  
-  private static void printModels(Iterable<List<Cluster>> result) {
-    int row = 0;
-    StringBuilder models = new StringBuilder(100);
-    for (List<Cluster> r : result) {
-      models.append("sample[").append(row++).append("]= ");
-      for (int k = 0; k < r.size(); k++) {
-        Cluster model = r.get(k);
-        models.append('m').append(k).append(model.asFormatString(null)).append(", ");
-      }
-      models.append('\n');
-    }
-    models.append('\n');
-    System.out.println(models.toString());
-  }
-  
-  private Iterable<List<Cluster>> getClusters(Path output, int numIterations) throws IOException {
-    List<List<Cluster>> result = Lists.newArrayList();
-    for (int i = 1; i <= numIterations; i++) {
-      ClusterClassifier posterior = new ClusterClassifier();
-      String name = i == numIterations ? "clusters-" + i + "-final" : "clusters-" + i;
-      posterior.readFromSeqFiles(conf, new Path(output, name));
-      List<Cluster> clusters = Lists.newArrayList();
-      for (Cluster cluster : posterior.getModels()) {
-        clusters.add(cluster);
-      }
-      result.add(clusters);
-    }
-    return result;
-  }
-  
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java mahout/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/iterator/TestClusterClassifier.java	2014-03-29 01:03:14.000000000 -0700
@@ -28,10 +28,8 @@
 import org.apache.mahout.clustering.ClusteringTestUtils;
 import org.apache.mahout.clustering.canopy.Canopy;
 import org.apache.mahout.clustering.classify.ClusterClassifier;
-import org.apache.mahout.clustering.dirichlet.models.GaussianCluster;
 import org.apache.mahout.clustering.fuzzykmeans.SoftCluster;
 import org.apache.mahout.clustering.kmeans.TestKmeansClustering;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopy;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.distance.CosineDistanceMeasure;
 import org.apache.mahout.common.distance.DistanceMeasure;
@@ -81,14 +79,6 @@
     return new ClusterClassifier(models, new FuzzyKMeansClusteringPolicy());
   }
   
-  private static ClusterClassifier newGaussianClassifier() {
-    List<Cluster> models = Lists.newArrayList();
-    models.add(new GaussianCluster(new DenseVector(2).assign(1), new DenseVector(2).assign(1), 0));
-    models.add(new GaussianCluster(new DenseVector(2), new DenseVector(2).assign(1), 1));
-    models.add(new GaussianCluster(new DenseVector(2).assign(-1), new DenseVector(2).assign(1), 2));
-    return new ClusterClassifier(models, new DirichletClusteringPolicy(3, 1.0));
-  }
-  
   private ClusterClassifier writeAndRead(ClusterClassifier classifier) throws IOException {
     Path path = new Path(getTestTempDirPath(), "output");
     classifier.writeToSeqFiles(path);
@@ -129,17 +119,6 @@
     assertEquals("[2,2]", "[0.493, 0.296, 0.211]", AbstractCluster.formatVector(pdf, null));
   }
   
-  @Test(expected = UnsupportedOperationException.class)
-  public void testMSCanopyClassification() {
-    List<Cluster> models = Lists.newArrayList();
-    DistanceMeasure measure = new ManhattanDistanceMeasure();
-    models.add(new MeanShiftCanopy(new DenseVector(2).assign(1), 0, measure));
-    models.add(new MeanShiftCanopy(new DenseVector(2), 1, measure));
-    models.add(new MeanShiftCanopy(new DenseVector(2).assign(-1), 2, measure));
-    ClusterClassifier classifier = new ClusterClassifier(models, new MeanShiftClusteringPolicy());
-    classifier.classify(new DenseVector(2));
-  }
-  
   @Test
   public void testSoftClusterClassification() {
     ClusterClassifier classifier = newSoftClusterClassifier();
@@ -150,15 +129,6 @@
   }
   
   @Test
-  public void testGaussianClusterClassification() {
-    ClusterClassifier classifier = newGaussianClassifier();
-    Vector pdf = classifier.classify(new DenseVector(2));
-    assertEquals("[0,0]", "[0.212, 0.576, 0.212]", AbstractCluster.formatVector(pdf, null));
-    pdf = classifier.classify(new DenseVector(2).assign(2));
-    assertEquals("[2,2]", "[0.952, 0.047, 0.000]", AbstractCluster.formatVector(pdf, null));
-  }
-  
-  @Test
   public void testDMClassifierSerialization() throws Exception {
     ClusterClassifier classifier = newDMClassifier();
     ClusterClassifier classifierOut = writeAndRead(classifier);
@@ -186,15 +156,6 @@
   }
   
   @Test
-  public void testGaussianClassifierSerialization() throws Exception {
-    ClusterClassifier classifier = newGaussianClassifier();
-    ClusterClassifier classifierOut = writeAndRead(classifier);
-    assertEquals(classifier.getModels().size(), classifierOut.getModels().size());
-    assertEquals(classifier.getModels().get(0).getClass().getName(), classifierOut.getModels().get(0).getClass()
-        .getName());
-  }
-  
-  @Test
   public void testClusterIteratorKMeans() {
     List<Vector> data = TestKmeansClustering.getPoints(TestKmeansClustering.REFERENCE);
     ClusterClassifier prior = newKlusterClassifier();
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestEigenSeedGenerator.java mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestEigenSeedGenerator.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestEigenSeedGenerator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestEigenSeedGenerator.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,99 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.kmeans;
-
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.ClusteringTestUtils;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Before;
-import org.junit.Test;
-
-import com.google.common.collect.Lists;
-
-public final class TestEigenSeedGenerator extends MahoutTestCase {
-
-  private
-   static final double[][] RAW = {{1, 0, 0}, {1, 0, 0}, {0, 1, 0}, {0, 1, 0},
-                                  {0, 1, 0}, {0, 0, 1}, {0, 0, 1}};
-
-  private FileSystem fs;
-
-  private static List<VectorWritable> getPoints() {
-    List<VectorWritable> points = Lists.newArrayList();
-    for (double[] fr : RAW) {
-      Vector vec = new RandomAccessSparseVector(fr.length);
-      vec.assign(fr);
-      points.add(new VectorWritable(vec));
-    }
-    return points;
-  }
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    Configuration conf = new Configuration();
-    fs = FileSystem.get(conf);
-  }
-
-  @Test
-  public void testEigenSeedGenerator() throws Exception {
-    List<VectorWritable> points = getPoints();
-    Job job = new Job();
-    Configuration conf = job.getConfiguration();
-    job.setMapOutputValueClass(VectorWritable.class);
-    Path input = getTestTempFilePath("eigen-input");
-    Path output = getTestTempDirPath("eigen-output");
-    ClusteringTestUtils.writePointsToFile(points, input, fs, conf);
-
-    EigenSeedGenerator.buildFromEigens(conf, input, output, 3, new ManhattanDistanceMeasure());
-
-    int clusterCount = 0;
-    Collection<Integer> set = new HashSet<Integer>();
-    Vector v[] = new Vector[3];
-    for (ClusterWritable clusterWritable :
-         new SequenceFileValueIterable<ClusterWritable>(
-             new Path(output, "part-eigenSeed"), true, conf)) {
-      Cluster cluster = clusterWritable.getValue();
-      int id = cluster.getId();
-      assertTrue(set.add(id)); // validate unique id's
-      v[id] = cluster.getCenter();
-      clusterCount++;
-    }
-    assertEquals(3, clusterCount); // validate sample count
-    // validate pair-wise orthogonality
-    assertEquals(0, v[0].dot(v[1]), 1E-10);
-    assertEquals(0, v[1].dot(v[2]), 1E-10);
-    assertEquals(0, v[0].dot(v[2]), 1E-10);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestKmeansClustering.java	2014-03-29 01:03:14.000000000 -0700
@@ -29,7 +29,7 @@
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.clustering.ClusteringTestUtils;
 import org.apache.mahout.clustering.canopy.CanopyDriver;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.DummyOutputCollector;
 import org.apache.mahout.common.MahoutTestCase;
@@ -39,6 +39,7 @@
 import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
 import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
+import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.RandomAccessSparseVector;
 import org.apache.mahout.math.SequentialAccessSparseVector;
 import org.apache.mahout.math.Vector;
@@ -76,6 +77,16 @@
     return points;
   }
   
+  public static List<VectorWritable> getPointsWritableDenseVector(double[][] raw) {
+    List<VectorWritable> points = Lists.newArrayList();
+    for (double[] fr : raw) {
+      Vector vec = new DenseVector(fr.length);
+      vec.assign(fr);
+      points.add(new VectorWritable(vec));
+    }
+    return points;
+  }
+  
   public static List<Vector> getPoints(double[][] raw) {
     List<Vector> points = Lists.newArrayList();
     for (double[] fr : raw) {
@@ -95,30 +106,30 @@
   public void testRunKMeansIterationConvergesInOneRunWithGivenDistanceThreshold() {
     double[][] rawPoints = { {0, 0}, {0, 0.25}, {0, 0.75}, {0, 1}};
     List<Vector> points = getPoints(rawPoints);
-    
+
     ManhattanDistanceMeasure distanceMeasure = new ManhattanDistanceMeasure();
     List<Kluster> clusters = Arrays.asList(new Kluster(points.get(0), 0, distanceMeasure), new Kluster(points.get(3),
         3, distanceMeasure));
-    
+
     // To converge in a single run, the given distance threshold should be
     // greater than or equal to 0.125,
     // since 0.125 will be the distance between center and centroid for the
     // initial two clusters after one run.
     double distanceThreshold = 0.25;
-    
+
     boolean converged = KMeansClusterer.runKMeansIteration(points, clusters, distanceMeasure, distanceThreshold);
-    
+
     Vector cluster1Center = clusters.get(0).getCenter();
     assertEquals(0, cluster1Center.get(0), EPSILON);
     assertEquals(0.125, cluster1Center.get(1), EPSILON);
-    
+
     Vector cluster2Center = clusters.get(1).getCenter();
     assertEquals(0, cluster2Center.get(0), EPSILON);
     assertEquals(0.875, cluster2Center.get(1), EPSILON);
-    
+
     assertTrue("KMeans iteration should be converged after a single run", converged);
   }*/
-  
+
   /** Story: User wishes to run kmeans job on reference data */
   @Test
   public void testKMeansSeqJob() throws Exception {
@@ -150,8 +161,6 @@
       }
       // now run the Job
       Path outputPath = getTestTempDirPath("output");
-      // KMeansDriver.runJob(pointsPath, clustersPath, outputPath,
-      // EuclideanDistanceMeasure.class.getName(), 0.001, 10, k + 1, true);
       String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(),
           optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(),
           optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
@@ -165,9 +174,63 @@
       // now compare the expected clusters with actual
       Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");
       int[] expect = EXPECTED_NUM_POINTS[k];
-      DummyOutputCollector<IntWritable,WeightedVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedVectorWritable>();
+      DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable>();
+      // The key is the clusterId, the value is the weighted vector
+      for (Pair<IntWritable,WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedPropertyVectorWritable>(
+          new Path(clusteredPointsPath, "part-m-0"), conf)) {
+        collector.collect(record.getFirst(), record.getSecond());
+      }
+      assertEquals("clusters[" + k + ']', expect.length, collector.getKeys().size());
+    }
+  }
+  
+  /** Story: User wishes to run kmeans job on reference data (DenseVector test) */
+  @Test
+  public void testKMeansSeqJobDenseVector() throws Exception {
+    DistanceMeasure measure = new EuclideanDistanceMeasure();
+    List<VectorWritable> points = getPointsWritableDenseVector(REFERENCE);
+    
+    Path pointsPath = getTestTempDirPath("points");
+    Path clustersPath = getTestTempDirPath("clusters");
+    Configuration conf = getConfiguration();
+    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);
+    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);
+    for (int k = 1; k < points.size(); k++) {
+      System.out.println("testKMeansMRJob k= " + k);
+      // pick k initial cluster centers at random
+      Path path = new Path(clustersPath, "part-00000");
+      FileSystem fs = FileSystem.get(path.toUri(), conf);
+      SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, Kluster.class);
+      try {
+        for (int i = 0; i < k + 1; i++) {
+          Vector vec = points.get(i).get();
+          
+          Kluster cluster = new Kluster(vec, i, measure);
+          // add the center so the centroid will be correct upon output
+          cluster.observe(cluster.getCenter(), 1);
+          writer.append(new Text(cluster.getIdentifier()), cluster);
+        }
+      } finally {
+        Closeables.close(writer, false);
+      }
+      // now run the Job
+      Path outputPath = getTestTempDirPath("output");
+      String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(),
+          optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(),
+          optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
+          optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(),
+          optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001",
+          optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(DefaultOptionCreator.CLUSTERING_OPTION),
+          optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION),
+          DefaultOptionCreator.SEQUENTIAL_METHOD};
+      ToolRunner.run(conf, new KMeansDriver(), args);
+      
+      // now compare the expected clusters with actual
+      Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");
+      int[] expect = EXPECTED_NUM_POINTS[k];
+      DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable>();
       // The key is the clusterId, the value is the weighted vector
-      for (Pair<IntWritable,WeightedVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedVectorWritable>(
+      for (Pair<IntWritable,WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedPropertyVectorWritable>(
           new Path(clusteredPointsPath, "part-m-0"), conf)) {
         collector.collect(record.getFirst(), record.getSecond());
       }
@@ -207,8 +270,6 @@
       }
       // now run the Job
       Path outputPath = getTestTempDirPath("output");
-      // KMeansDriver.runJob(pointsPath, clustersPath, outputPath,
-      // EuclideanDistanceMeasure.class.getName(), 0.001, 10, k + 1, true);
       String[] args = {optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(),
           optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(),
           optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(),
@@ -222,9 +283,9 @@
       Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");
       // assertEquals("output dir files?", 4, outFiles.length);
       int[] expect = EXPECTED_NUM_POINTS[k];
-      DummyOutputCollector<IntWritable,WeightedVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedVectorWritable>();
+      DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable>();
       // The key is the clusterId, the value is the weighted vector
-      for (Pair<IntWritable,WeightedVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedVectorWritable>(
+      for (Pair<IntWritable,WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedPropertyVectorWritable>(
           new Path(clusteredPointsPath, "part-m-00000"), conf)) {
         collector.collect(record.getFirst(), record.getSecond());
       }
@@ -288,40 +349,36 @@
 
     // now run the KMeans job
     Path kmeansOutput = new Path(outputPath, "kmeans");
-	KMeansDriver.run(getConfiguration(), pointsPath, new Path(outputPath, "clusters-0-final"), kmeansOutput, new EuclideanDistanceMeasure(),
-        0.001, 10, true, 0.0, false);
+	  KMeansDriver.run(getConfiguration(), pointsPath, new Path(outputPath, "clusters-0-final"), kmeansOutput,
+      0.001, 10, true, 0.0, false);
     
     // now compare the expected clusters with actual
     Path clusteredPointsPath = new Path(kmeansOutput, "clusteredPoints");
-    DummyOutputCollector<IntWritable,WeightedVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedVectorWritable>();
+    DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable> collector = new DummyOutputCollector<IntWritable,WeightedPropertyVectorWritable>();
     
     // The key is the clusterId, the value is the weighted vector
-    for (Pair<IntWritable,WeightedVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedVectorWritable>(
+    for (Pair<IntWritable,WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable,WeightedPropertyVectorWritable>(
         new Path(clusteredPointsPath, "part-m-00000"), conf)) {
       collector.collect(record.getFirst(), record.getSecond());
     }
     
-    //boolean gotLowClust = false;  // clusters should be [1, *] and [2, *]
-    //boolean gotHighClust = false; // vs [3 , *],  [4 , *] and [5, *]
     for (IntWritable k : collector.getKeys()) {
-      List<WeightedVectorWritable> wvList = collector.getValue(k);
-      assertTrue("empty cluster!", !wvList.isEmpty());
-      if (wvList.get(0).getVector().get(0) <= 2.0) {
-        for (WeightedVectorWritable wv : wvList) {
+      List<WeightedPropertyVectorWritable> wpvList = collector.getValue(k);
+      assertTrue("empty cluster!", !wpvList.isEmpty());
+      if (wpvList.get(0).getVector().get(0) <= 2.0) {
+        for (WeightedPropertyVectorWritable wv : wpvList) {
           Vector v = wv.getVector();
           int idx = v.maxValueIndex();
           assertTrue("bad cluster!", v.get(idx) <= 2.0);
         }
-        assertEquals("Wrong size cluster", 4, wvList.size());
-        //gotLowClust= true;
+        assertEquals("Wrong size cluster", 4, wpvList.size());
       } else {
-        for (WeightedVectorWritable wv : wvList) {
+        for (WeightedPropertyVectorWritable wv : wpvList) {
           Vector v = wv.getVector();
           int idx = v.minValueIndex();
           assertTrue("bad cluster!", v.get(idx) > 2.0);
         }
-        assertEquals("Wrong size cluster", 5, wvList.size());
-        //gotHighClust= true;
+        assertEquals("Wrong size cluster", 5, wpvList.size());
       }
     }
   }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestRandomSeedGenerator.java mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestRandomSeedGenerator.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestRandomSeedGenerator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/kmeans/TestRandomSeedGenerator.java	2014-03-29 01:03:14.000000000 -0700
@@ -60,7 +60,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
   
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java mahout/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/meanshift/TestMeanShift.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,552 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.meanshift;
-
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.ClusteringTestUtils;
-import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Before;
-import org.junit.Test;
-
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-
-@Deprecated
-public final class TestMeanShift extends MahoutTestCase {
-
-  private Vector[] raw = null;
-
-  // DistanceMeasure manhattanDistanceMeasure = new ManhattanDistanceMeasure();
-
-  private final DistanceMeasure euclideanDistanceMeasure = new EuclideanDistanceMeasure();
-
-  private final IKernelProfile kernelProfile = new TriangularKernelProfile();
-
-  /**
-   * Print the canopies to the transcript
-   *
-   * @param canopies
-   *          a List<Canopy>
-   */
-  private static void printCanopies(Iterable<MeanShiftCanopy> canopies) {
-    for (MeanShiftCanopy canopy : canopies) {
-      System.out.println(canopy.asFormatString(null));
-    }
-  }
-
-  /**
-   * Print a graphical representation of the clustered image points as a 10x10
-   * character mask
-   */
-  private void printImage(Iterable<MeanShiftCanopy> canopies) {
-    char[][] out = new char[10][10];
-    for (int i = 0; i < out.length; i++) {
-      for (int j = 0; j < out[0].length; j++) {
-        out[i][j] = ' ';
-      }
-    }
-    for (MeanShiftCanopy canopy : canopies) {
-      int ch = 'A' + canopy.getId();
-      for (int pid : canopy.getBoundPoints().toList()) {
-        Vector pt = raw[pid];
-        out[(int) pt.getQuick(0)][(int) pt.getQuick(1)] = (char) ch;
-      }
-    }
-    for (char[] anOut : out) {
-      System.out.println(anOut);
-    }
-  }
-
-  private List<MeanShiftCanopy> getInitialCanopies() {
-    int nextCanopyId = 0;
-    List<MeanShiftCanopy> canopies = Lists.newArrayList();
-    for (Vector point : raw) {
-      canopies.add(new MeanShiftCanopy(point, nextCanopyId++,
-          euclideanDistanceMeasure));
-    }
-    return canopies;
-  }
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    raw = new Vector[100];
-    for (int i = 0; i < 10; i++) {
-      for (int j = 0; j < 10; j++) {
-        int ix = i * 10 + j;
-        Vector v = new DenseVector(3);
-        v.setQuick(0, i);
-        v.setQuick(1, j);
-        if (i == j) {
-          v.setQuick(2, 9);
-        } else if (i + j == 9) {
-          v.setQuick(2, 4.5);
-        }
-        raw[ix] = v;
-      }
-    }
-  }
-
-  /**
-   * Story: User can exercise the reference implementation to verify that the
-   * test datapoints are clustered in a reasonable manner.
-   */
-  @Test
-  public void testReferenceImplementation() {
-    MeanShiftCanopyClusterer clusterer = new MeanShiftCanopyClusterer(
-        new EuclideanDistanceMeasure(), new TriangularKernelProfile(), 4.0,
-        1.0, 0.5, true);
-    List<MeanShiftCanopy> canopies = Lists.newArrayList();
-    // add all points to the canopies
-    int nextCanopyId = 0;
-    for (Vector aRaw : raw) {
-      clusterer.mergeCanopy(new MeanShiftCanopy(aRaw, nextCanopyId++,
-          euclideanDistanceMeasure), canopies);
-    }
-    boolean done = false;
-    int iter = 1;
-    while (!done) {// shift canopies to their centroids
-      done = true;
-      List<MeanShiftCanopy> migratedCanopies = Lists.newArrayList();
-      for (MeanShiftCanopy canopy : canopies) {
-        done = clusterer.shiftToMean(canopy) && done;
-        clusterer.mergeCanopy(canopy, migratedCanopies);
-      }
-      canopies = migratedCanopies;
-      printCanopies(canopies);
-      printImage(canopies);
-      System.out.println(iter++);
-    }
-  }
-
-  /**
-   * Test the MeanShiftCanopyClusterer's reference implementation. Should
-   * produce the same final output as above.
-   */
-  @Test
-  public void testClustererReferenceImplementation() {
-    Iterable<Vector> points = Lists.newArrayList(raw);
-    List<MeanShiftCanopy> canopies = MeanShiftCanopyClusterer.clusterPoints(
-        points, euclideanDistanceMeasure, kernelProfile, 0.5, 4, 1, 10);
-    printCanopies(canopies);
-    printImage(canopies);
-  }
-
-  /**
-   * Story: User can produce initial canopy centers using a
-   * EuclideanDistanceMeasure and a CanopyMapper/Combiner which clusters input
-   * points to produce an output set of canopies.
-   */
-  @Test
-  public void testCanopyMapperEuclidean() throws Exception {
-    MeanShiftCanopyClusterer clusterer = new MeanShiftCanopyClusterer(
-        euclideanDistanceMeasure, kernelProfile, 4, 1, 0.5, true);
-    // get the initial canopies
-    List<MeanShiftCanopy> canopies = getInitialCanopies();
-    // build the reference set
-    Collection<MeanShiftCanopy> refCanopies = Lists.newArrayList();
-    int nextCanopyId = 0;
-    for (Vector aRaw : raw) {
-      clusterer.mergeCanopy(new MeanShiftCanopy(aRaw, nextCanopyId++,
-          euclideanDistanceMeasure), refCanopies);
-    }
-
-    Configuration conf = getConfiguration();
-    conf.set(MeanShiftCanopyConfigKeys.DISTANCE_MEASURE_KEY, EuclideanDistanceMeasure.class.getName());
-    conf.set(MeanShiftCanopyConfigKeys.KERNEL_PROFILE_KEY, TriangularKernelProfile.class.getName());
-    conf.set(MeanShiftCanopyConfigKeys.T1_KEY, "4");
-    conf.set(MeanShiftCanopyConfigKeys.T2_KEY, "1");
-    conf.set(MeanShiftCanopyConfigKeys.CLUSTER_CONVERGENCE_KEY, "0.5");
-
-    // map the data
-    MeanShiftCanopyMapper mapper = new MeanShiftCanopyMapper();
-    DummyRecordWriter<Text, ClusterWritable> mapWriter = new DummyRecordWriter<Text, ClusterWritable>();
-    Mapper<WritableComparable<?>, ClusterWritable, Text, ClusterWritable>.Context mapContext =
-        DummyRecordWriter.build(mapper, conf, mapWriter);
-    mapper.setup(mapContext);
-    for (MeanShiftCanopy canopy : canopies) {
-    	ClusterWritable clusterWritable = new ClusterWritable();
-    	clusterWritable.setValue(canopy);
-        mapper.map(new Text(), clusterWritable, mapContext);
-    }
-    mapper.cleanup(mapContext);
-
-    // now verify the output
-    assertEquals("Number of map results", 1, mapWriter.getData().size());
-    List<ClusterWritable> data = mapWriter.getValue(new Text("0"));
-    assertEquals("Number of canopies", refCanopies.size(), data.size());
-
-    // add all points to the reference canopies
-    Map<String, MeanShiftCanopy> refCanopyMap = Maps.newHashMap();
-    for (MeanShiftCanopy canopy : refCanopies) {
-      clusterer.shiftToMean(canopy);
-      refCanopyMap.put(canopy.getIdentifier(), canopy);
-    }
-    // build a map of the combiner output
-    Map<String, MeanShiftCanopy> canopyMap = Maps.newHashMap();
-    for (ClusterWritable d : data) {
-      MeanShiftCanopy canopy = (MeanShiftCanopy)d.getValue();
-	  canopyMap.put(canopy.getIdentifier(), canopy);
-    }
-    // compare the maps
-    for (Map.Entry<String, MeanShiftCanopy> stringMeanShiftCanopyEntry : refCanopyMap
-        .entrySet()) {
-      MeanShiftCanopy ref = stringMeanShiftCanopyEntry.getValue();
-
-      MeanShiftCanopy canopy = canopyMap.get((ref.isConverged() ? "MSV-"
-          : "MSC-")
-          + ref.getId());
-      assertEquals("ids", ref.getId(), canopy.getId());
-      assertEquals("centers(" + ref.getIdentifier() + ')', ref.getCenter()
-          .asFormatString(), canopy.getCenter().asFormatString());
-      assertEquals("bound points", ref.getBoundPoints().toList().size(), canopy
-          .getBoundPoints().toList().size());
-      assertEquals("num bound points", ref.getNumObservations(), canopy
-          .getNumObservations());
-    }
-  }
-
-  /**
-   * Story: User can produce final canopy centers using a
-   * EuclideanDistanceMeasure and a CanopyReducer which clusters input centroid
-   * points to produce an output set of final canopy centroid points.
-   */
-  @Test
-  public void testCanopyReducerEuclidean() throws Exception {
-    MeanShiftCanopyClusterer clusterer = new MeanShiftCanopyClusterer(
-        euclideanDistanceMeasure, kernelProfile, 4, 1, 0.5, true);
-    // get the initial canopies
-    List<MeanShiftCanopy> canopies = getInitialCanopies();
-    // build the mapper output reference set
-    Collection<MeanShiftCanopy> mapperReference = Lists.newArrayList();
-    int nextCanopyId = 0;
-    for (Vector aRaw : raw) {
-      clusterer.mergeCanopy(new MeanShiftCanopy(aRaw, nextCanopyId++,
-          euclideanDistanceMeasure), mapperReference);
-    }
-    for (MeanShiftCanopy canopy : mapperReference) {
-      clusterer.shiftToMean(canopy);
-    }
-    // build the reducer reference output set
-    Collection<MeanShiftCanopy> reducerReference = Lists.newArrayList();
-    for (MeanShiftCanopy canopy : mapperReference) {
-      clusterer.mergeCanopy(canopy, reducerReference);
-    }
-    for (MeanShiftCanopy canopy : reducerReference) {
-      clusterer.shiftToMean(canopy);
-    }
-
-    Configuration conf = getConfiguration();
-    conf.set(MeanShiftCanopyConfigKeys.DISTANCE_MEASURE_KEY, EuclideanDistanceMeasure.class.getName());
-    conf.set(MeanShiftCanopyConfigKeys.KERNEL_PROFILE_KEY, TriangularKernelProfile.class.getName());
-    conf.set(MeanShiftCanopyConfigKeys.T1_KEY, "4");
-    conf.set(MeanShiftCanopyConfigKeys.T2_KEY, "1");
-    conf.set(MeanShiftCanopyConfigKeys.CLUSTER_CONVERGENCE_KEY, "0.5");
-    conf.set(MeanShiftCanopyConfigKeys.CONTROL_PATH_KEY, "output/control");
-
-    MeanShiftCanopyMapper mapper = new MeanShiftCanopyMapper();
-    DummyRecordWriter<Text, ClusterWritable> mapWriter = new DummyRecordWriter<Text, ClusterWritable>();
-    Mapper<WritableComparable<?>, ClusterWritable, Text, ClusterWritable>.Context mapContext = DummyRecordWriter
-        .build(mapper, conf, mapWriter);
-    mapper.setup(mapContext);
-
-    // map the data
-    for (MeanShiftCanopy canopy : canopies) {
-    	ClusterWritable clusterWritable = new ClusterWritable();
-    	clusterWritable.setValue(canopy);
-        mapper.map(new Text(), clusterWritable, mapContext);
-    }
-    mapper.cleanup(mapContext);
-
-    assertEquals("Number of map results", 1, mapWriter.getData().size());
-    // now reduce the mapper output
-    MeanShiftCanopyReducer reducer = new MeanShiftCanopyReducer();
-    DummyRecordWriter<Text, ClusterWritable> reduceWriter = new DummyRecordWriter<Text, ClusterWritable>();
-    Reducer<Text, ClusterWritable, Text, ClusterWritable>.Context reduceContext = DummyRecordWriter
-        .build(reducer, conf, reduceWriter, Text.class, ClusterWritable.class);
-    reducer.setup(reduceContext);
-    reducer.reduce(new Text("0"), mapWriter.getValue(new Text("0")),
-        reduceContext);
-    reducer.cleanup(reduceContext);
-
-    // now verify the output
-    assertEquals("Number of canopies", reducerReference.size(), reduceWriter
-        .getKeys().size());
-
-    // add all points to the reference canopy maps
-    Map<String, MeanShiftCanopy> reducerReferenceMap = Maps.newHashMap();
-    for (MeanShiftCanopy canopy : reducerReference) {
-      reducerReferenceMap.put(canopy.getIdentifier(), canopy);
-    }
-    // compare the maps
-    for (Map.Entry<String, MeanShiftCanopy> mapEntry : reducerReferenceMap
-        .entrySet()) {
-      MeanShiftCanopy refCanopy = mapEntry.getValue();
-
-      List<ClusterWritable> values = reduceWriter.getValue(new Text((refCanopy
-          .isConverged() ? "MSV-" : "MSC-")
-          + refCanopy.getId()));
-      assertEquals("values", 1, values.size());
-      ClusterWritable clusterWritable = values.get(0);
-	  MeanShiftCanopy reducerCanopy = (MeanShiftCanopy) clusterWritable.getValue();
-      assertEquals("ids", refCanopy.getId(), reducerCanopy.getId());
-      long refNumPoints = refCanopy.getNumObservations();
-      long reducerNumPoints = reducerCanopy.getNumObservations();
-      assertEquals("numPoints", refNumPoints, reducerNumPoints);
-      String refCenter = refCanopy.getCenter().asFormatString();
-      String reducerCenter = reducerCanopy.getCenter().asFormatString();
-      assertEquals("centers(" + mapEntry.getKey() + ')', refCenter,
-          reducerCenter);
-      assertEquals("bound points", refCanopy.getBoundPoints().toList().size(),
-          reducerCanopy.getBoundPoints().toList().size());
-      assertEquals("num bound points", refCanopy.getNumObservations(), reducerCanopy
-          .getNumObservations());
-    }
-  }
-
-  /**
-   * Story: User can produce final point clustering using a Hadoop map/reduce
-   * job and a EuclideanDistanceMeasure.
-   */
-  @Test
-  public void testCanopyEuclideanMRJob() throws Exception {
-    Path input = getTestTempDirPath("testdata");
-    Configuration conf = getConfiguration();
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    Collection<VectorWritable> points = Lists.newArrayList();
-    // TODO fix test so it doesn't need this random seed!
-    Random r = new Random(123);
-    Vector[] permutedRaw = new Vector[raw.length];
-    System.arraycopy(raw, 0, permutedRaw, 0, raw.length);
-    for (int i = 0; i < permutedRaw.length; i++) {
-      permutedRaw[i] = permutedRaw[i + r.nextInt(raw.length - i)];
-    }
-    for (Vector v : permutedRaw) {
-      points.add(new VectorWritable(v));
-    }
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file1"), fs, conf);
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file2"), fs, conf);
-    // now run the Job using the run() command. Other tests can continue to use
-    // runJob().
-    Path output = getTestTempDirPath("output");
-    // MeanShiftCanopyDriver.runJob(input, output,
-    // EuclideanDistanceMeasure.class.getName(), 4, 1, 0.5, 10, false, false);
-    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION),
-        getTestTempDirPath("testdata").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(),
-        optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION),
-        EuclideanDistanceMeasure.class.getName(),
-        optKey(DefaultOptionCreator.KERNEL_PROFILE_OPTION),
-        TriangularKernelProfile.class.getName(),
-        optKey(DefaultOptionCreator.T1_OPTION), "4",
-        optKey(DefaultOptionCreator.T2_OPTION), "1",
-        optKey(DefaultOptionCreator.CLUSTERING_OPTION),
-        optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "7",
-        optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.2",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION) };
-    ToolRunner.run(conf, new MeanShiftCanopyDriver(), args);
-    FileStatus[] outParts = FileSystem.get(conf).globStatus(
-        new Path(output, "clusters-?-final/part-r-*"));
-    assertEquals("Wrong number of matching final parts", 1, outParts.length);
-    long count = HadoopUtil.countRecords(outParts[0].getPath(), conf);
-    assertEquals("count", 5, count);
-    Path outPart = new Path(output, "clusters-0/part-m-00000");
-    Iterator<?> iterator = new SequenceFileValueIterator<Writable>(outPart,
-        true, conf);
-    // now test the initial clusters to ensure the type of their centers has
-    // been retained
-    while (iterator.hasNext()) {
-      ClusterWritable clusterWritable = (ClusterWritable)iterator.next();
-	  MeanShiftCanopy canopy = (MeanShiftCanopy) clusterWritable.getValue();
-      assertTrue(canopy.getCenter() instanceof DenseVector);
-      assertFalse(canopy.getBoundPoints().isEmpty());
-    }
-  }
-
-  /**
-   * Story: User can produce final point clustering using a Hadoop map/reduce
-   * job and a EuclideanDistanceMeasure.
-   */
-  @Test
-  public void testCanopyEuclideanSeqJob() throws Exception {
-    Path input = getTestTempDirPath("testdata");
-    Configuration conf = getConfiguration();
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    Collection<VectorWritable> points = Lists.newArrayList();
-    for (Vector v : raw) {
-      points.add(new VectorWritable(v));
-    }
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file1"), fs, conf);
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file2"), fs, conf);
-    // now run the Job using the run() command. Other tests can continue to use
-    // runJob().
-    Path output = getTestTempDirPath("output");
-    System.out.println("Output Path: " + output);
-    // MeanShiftCanopyDriver.runJob(input, output,
-    // EuclideanDistanceMeasure.class.getName(), 4, 1, 0.5, 10, false, false);
-    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION),
-        getTestTempDirPath("testdata").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(),
-        optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION),
-        EuclideanDistanceMeasure.class.getName(),
-        optKey(DefaultOptionCreator.KERNEL_PROFILE_OPTION),
-        TriangularKernelProfile.class.getName(),
-        optKey(DefaultOptionCreator.T1_OPTION), "4",
-        optKey(DefaultOptionCreator.T2_OPTION), "1",
-        optKey(DefaultOptionCreator.CLUSTERING_OPTION),
-        optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "7",
-        optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.2",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION),
-        optKey(DefaultOptionCreator.METHOD_OPTION),
-        DefaultOptionCreator.SEQUENTIAL_METHOD };
-    ToolRunner.run(getConfiguration(), new MeanShiftCanopyDriver(), args);
-    Path outPart = new Path(output, "clusters-7-final/part-r-00000");
-    long count = HadoopUtil.countRecords(outPart, conf);
-    assertEquals("count", 3, count);
-  }
-
-  /**
-   * Story: User can produce final point clustering using a Hadoop map/reduce
-   * job and a EuclideanDistanceMeasure.
-   */
-  @Test
-  public void testCanopyEuclideanMRJobNoClustering() throws Exception {
-    Path input = getTestTempDirPath("testdata");
-    Configuration conf = getConfiguration();
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    Collection<VectorWritable> points = Lists.newArrayList();
-    for (Vector v : raw) {
-      points.add(new VectorWritable(v));
-    }
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file1"), fs, conf);
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file2"), fs, conf);
-    // now run the Job using the run() command. Other tests can continue to use
-    // runJob().
-    Path output = getTestTempDirPath("output");
-    // MeanShiftCanopyDriver.runJob(input, output,
-    // EuclideanDistanceMeasure.class.getName(), 4, 1, 0.5, 10, false, false);
-    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION),
-        getTestTempDirPath("testdata").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(),
-        optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION),
-        EuclideanDistanceMeasure.class.getName(),
-        optKey(DefaultOptionCreator.KERNEL_PROFILE_OPTION),
-        TriangularKernelProfile.class.getName(),
-        optKey(DefaultOptionCreator.T1_OPTION), "4",
-        optKey(DefaultOptionCreator.T2_OPTION), "1",
-        optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "7",
-        optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.2",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION) };
-    ToolRunner.run(conf, new MeanShiftCanopyDriver(), args);
-    Path outPart = new Path(output, "clusters-3-final/part-r-00000");
-    long count = HadoopUtil.countRecords(outPart, conf);
-    assertEquals("count", 3, count);
-    Iterator<?> iterator = new SequenceFileValueIterator<Writable>(outPart,
-        true, conf);
-    while (iterator.hasNext()) {
-      ClusterWritable next = (ClusterWritable)iterator.next();
-	  MeanShiftCanopy canopy = (MeanShiftCanopy) next.getValue();
-      assertTrue(canopy.getCenter() instanceof DenseVector);
-      assertEquals(1, canopy.getBoundPoints().size());
-    }
-  }
-
-  /**
-   * Story: User can produce final point clustering using a Hadoop map/reduce
-   * job and a EuclideanDistanceMeasure.
-   */
-  @Test
-  public void testCanopyEuclideanSeqJobNoClustering() throws Exception {
-    Path input = getTestTempDirPath("testdata");
-    Configuration conf = getConfiguration();
-    FileSystem fs = FileSystem.get(input.toUri(), conf);
-    Collection<VectorWritable> points = Lists.newArrayList();
-    for (Vector v : raw) {
-      points.add(new VectorWritable(v));
-    }
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file1"), fs, conf);
-    ClusteringTestUtils.writePointsToFile(points,
-        getTestTempFilePath("testdata/file2"), fs, conf);
-    // now run the Job using the run() command. Other tests can continue to use
-    // runJob().
-    Path output = getTestTempDirPath("output");
-    System.out.println("Output Path: " + output);
-    // MeanShiftCanopyDriver.runJob(input, output,
-    // EuclideanDistanceMeasure.class.getName(), 4, 1, 0.5, 10, false, false);
-    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION),
-        getTestTempDirPath("testdata").toString(),
-        optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(),
-        optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION),
-        EuclideanDistanceMeasure.class.getName(),
-        optKey(DefaultOptionCreator.KERNEL_PROFILE_OPTION),
-        TriangularKernelProfile.class.getName(),
-        optKey(DefaultOptionCreator.T1_OPTION), "4",
-        optKey(DefaultOptionCreator.T2_OPTION), "1",
-        optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "7",
-        optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.2",
-        optKey(DefaultOptionCreator.OVERWRITE_OPTION),
-        optKey(DefaultOptionCreator.METHOD_OPTION),
-        DefaultOptionCreator.SEQUENTIAL_METHOD };
-    ToolRunner.run(getConfiguration(), new MeanShiftCanopyDriver(), args);
-    Path outPart = new Path(output, "clusters-7-final/part-r-00000");
-    long count = HadoopUtil.countRecords(outPart, conf);
-    assertEquals("count", 3, count);
-    Iterator<?> iterator = new SequenceFileValueIterator<Writable>(outPart,
-        true, conf);
-    while (iterator.hasNext()) {
-      ClusterWritable next = (ClusterWritable)iterator.next();
-	  MeanShiftCanopy canopy = (MeanShiftCanopy) next.getValue();
-      assertEquals(1, canopy.getBoundPoints().size());
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java mahout/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/minhash/TestMinHashClustering.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,230 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.clustering.minhash;
-
-import com.google.common.collect.Lists;
-import com.google.common.collect.Sets;
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.minhash.HashFactory.HashType;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
-import org.apache.mahout.math.SequentialAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-import java.util.Set;
-
-@Deprecated
-public final class TestMinHashClustering extends MahoutTestCase {
-  
-  private static final double[][] REFERENCE = { {0, 0, 3, 4, 5}, {0, 0, 3, 6, 7}, {0, 7, 6, 11, 8, 9},
-                                              {0, 7, 8, 9, 6, 0}, {5, 8, 10, 0, 0}, {6, 17, 14, 15},
-                                              {8, 9, 11, 0, 12, 0, 7}, {10, 13, 9, 7, 0, 6, 0},
-                                              {0, 0, 7, 9, 0, 11}, {13, 7, 6, 8, 0}};
-
-  private Path input;
-  private Path output;
-  
-  public static List<VectorWritable> getPointsWritable(double[][] raw) {
-    List<VectorWritable> points = Lists.newArrayList();
-    for (double[] fr : raw) {
-      Vector vec = new SequentialAccessSparseVector(fr.length);
-      vec.assign(fr);
-      points.add(new VectorWritable(vec));
-    }
-    return points;
-  }
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    Configuration conf = getConfiguration();
-    List<VectorWritable> points = getPointsWritable(REFERENCE);
-    input = getTestTempDirPath("points");
-    output = new Path(getTestTempDirPath(), "output");
-    Path pointFile = new Path(input, "file1");
-    FileSystem fs = FileSystem.get(pointFile.toUri(), conf);
-    SequenceFile.Writer writer = null;
-    try {
-      writer = new SequenceFile.Writer(fs, conf, pointFile, Text.class, VectorWritable.class);
-      int id = 0;
-      for (VectorWritable point : points) {
-        writer.append(new Text("Id-" + id++), point);
-      }
-    } finally {
-      Closeables.close(writer, false);
-    }
-  }
-  
-  private String[] makeArguments(String dimensionToHash, int minClusterSize,
-                                 int minVectorSize,
-                                 int numHashFunctions,
-                                 int keyGroups,
-                                 String hashType) {
-    return new String[] {optKey(DefaultOptionCreator.INPUT_OPTION), input.toString(),
-                         optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(),
-                         optKey(MinHashDriver.VECTOR_DIMENSION_TO_HASH), dimensionToHash,
-                         optKey(MinHashDriver.MIN_CLUSTER_SIZE), String.valueOf(minClusterSize),
-                         optKey(MinHashDriver.MIN_VECTOR_SIZE), String.valueOf(minVectorSize),
-                         optKey(MinHashDriver.HASH_TYPE), hashType,
-                         optKey(MinHashDriver.NUM_HASH_FUNCTIONS), String.valueOf(numHashFunctions),
-                         optKey(MinHashDriver.KEY_GROUPS), String.valueOf(keyGroups),
-                         optKey(MinHashDriver.NUM_REDUCERS), String.valueOf(1),
-                         optKey(MinHashDriver.DEBUG_OUTPUT)};
-  }
-  
-  private static Set<Integer> getValues(Vector vector, String dimensionToHash) {
-    Set<Integer> values = Sets.newHashSet();
-    if ("value".equalsIgnoreCase(dimensionToHash)) {
-      for (Vector.Element e : vector.nonZeroes()) {
-        values.add((int) e.get());
-      }
-    } else {
-      for (Vector.Element e : vector.nonZeroes()) {
-        values.add(e.index());
-      }
-    }
-    return values;
-  }
-
-  private static void runPairwiseSimilarity(List<Vector> clusteredItems, double simThreshold,
-                                            String dimensionToHash, String msg) {
-    if (clusteredItems.size() > 1) {
-      for (int i = 0; i < clusteredItems.size(); i++) {
-        Set<Integer> itemSet1 = getValues(clusteredItems.get(i), dimensionToHash);
-        for (int j = i + 1; j < clusteredItems.size(); j++) {
-          Set<Integer> itemSet2 = getValues(clusteredItems.get(j), dimensionToHash);
-          Collection<Integer> union = Sets.newHashSet();
-          union.addAll(itemSet1);
-          union.addAll(itemSet2);
-          Collection<Integer> intersect = Sets.newHashSet();
-          intersect.addAll(itemSet1);
-          intersect.retainAll(itemSet2);
-          double similarity = intersect.size() / (double) union.size();
-          assertTrue(msg + " - Sets failed min similarity test, Set1: " + itemSet1 + " Set2: " + itemSet2
-                     + ", similarity:" + similarity, similarity >= simThreshold);
-        }
-      }
-    }
-  }
-  
-  private void verify(Path output, double simThreshold, String dimensionToHash, String msg) throws IOException {
-    Configuration conf = getConfiguration();
-    Path outputFile = new Path(output, "part-r-00000");
-    List<Vector> clusteredItems = Lists.newArrayList();
-    String prevClusterId = "";
-    for (Pair<Writable,VectorWritable> record : new SequenceFileIterable<Writable,VectorWritable>(outputFile, conf)) {
-      Writable clusterId = record.getFirst();
-      VectorWritable point = record.getSecond();
-      if (prevClusterId.equals(clusterId.toString())) {
-        clusteredItems.add(point.get());
-      } else {
-        runPairwiseSimilarity(clusteredItems, simThreshold, dimensionToHash, msg);
-        clusteredItems.clear();
-        prevClusterId = clusterId.toString();
-        clusteredItems.add(point.get());
-      }
-    }
-    runPairwiseSimilarity(clusteredItems, simThreshold, dimensionToHash, msg);
-  }
-
-
-  @Test
-  public void testFailOnNonExistingHashType() throws Exception {
-    String[] args = makeArguments("value", 2, 3, 20, 4, "xKrot37");
-    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
-    assertEquals(-1, ret);
-  }
-
-  @Test
-  public void testLinearMinHashMRJob() throws Exception {
-    String[] args = makeArguments("value", 2, 3, 20, 4, HashType.LINEAR.toString());
-    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Hash value Job failed for " + HashType.LINEAR, 0, ret);
-    verify(output, 0.2, "value", "Hash Type: LINEAR");
-  }
-  
-  @Test
-  public void testPolynomialMinHashMRJob() throws Exception {
-    String[] args = makeArguments("value", 2, 3, 20, 3, HashType.POLYNOMIAL.toString());
-    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash value failed for " + HashType.POLYNOMIAL, 0, ret);
-    verify(output, 0.27, "value", "Hash Type: POLYNOMIAL");
-  }
-  
-  @Test
-  public void testMurmurMinHashMRJob() throws Exception {
-    String[] args = makeArguments("value", 2, 3, 20, 4, HashType.MURMUR.toString());
-    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash value failed for " + HashType.MURMUR, 0, ret);
-    verify(output, 0.2, "value", "Hash Type: MURMUR");
-  }
-
-  @Test
-  public void testMurmur3MinHashMRJob() throws Exception {
-    String[] args = makeArguments("value", 2, 3, 20, 4, HashType.MURMUR3.toString());
-    int ret = ToolRunner.run(getConfiguration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash value failed for " + HashType.MURMUR3, 0, ret);
-    verify(output, 0.2, "value", "Hash Type: MURMUR");
-  }
-
-  @Test
-  public void testLinearMinHashMRJobHashIndex() throws Exception {
-    String[] args = makeArguments("index", 2, 3, 20, 3, HashType.LINEAR.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash Index failed for " + HashType.LINEAR, 0, ret);
-    verify(output, 0.2, "index", "Hash Type: LINEAR");
-  }
-
-  @Test
-  public void testPolynomialMinHashMRJobHashIndex() throws Exception {
-    String[] args = makeArguments("index", 2, 3, 20, 3, HashType.POLYNOMIAL.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash Index failed for " + HashType.POLYNOMIAL, 0, ret);
-    verify(output, 0.3, "index", "Hash Type: POLYNOMIAL");
-  }
-
-  @Test
-  public void testMurmurMinHashMRJobHashIndex() throws Exception {
-    String[] args = makeArguments("index", 2, 3, 20, 4, HashType.MURMUR.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash Index failed for " + HashType.MURMUR, 0, ret);
-    verify(output, 0.3, "index", "Hash Type: MURMUR");
-  }
-
-  @Test
-  public void testMurmur3MinHashMRJobHashIndex() throws Exception {
-    String[] args = makeArguments("index", 2, 3, 20, 4, HashType.MURMUR3.toString());
-    int ret = ToolRunner.run(new Configuration(), new MinHashDriver(), args);
-    assertEquals("MinHash MR Job Hash Index failed for " + HashType.MURMUR3, 0, ret);
-    verify(output, 0.3, "index", "Hash Type: MURMUR");
-  }
-
-}
\ No newline at end of file
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestAffinityMatrixInputJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestAffinityMatrixInputJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestAffinityMatrixInputJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestAffinityMatrixInputJob.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,145 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.common.DummyRecordWriter;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.apache.mahout.math.hadoop.DistributedRowMatrix.MatrixEntryWritable;
+import org.junit.Test;
+
+/**
+ * <p>Tests the affinity matrix input M/R task.</p>
+ * 
+ * <p>The tricky item with this task is that the format of the input
+ * must be correct; it must take the form of a graph input, and for the
+ * current implementation, the input must be symmetric, e.g. the weight
+ * from node A to B = the weight from node B to A. This is not explicitly
+ * enforced within the task itself (since, as of the time these tests were 
+ * written, we have not yet decided on a final rule regarding the 
+ * symmetry/non-symmetry of the affinity matrix, so we are unofficially 
+ * enforcing symmetry). Input looks something like this:</p>
+ * 
+ * <pre>0, 0, 0
+ * 0, 1, 10
+ * 0, 2, 20
+ * ...
+ * 1, 0, 10
+ * 2, 0, 20
+ * ...</pre>
+ * 
+ * <p>The mapper's task is simply to convert each line of text into a
+ * DistributedRowMatrix entry, allowing the reducer to join each entry
+ * of the same row into a VectorWritable.</p>
+ * 
+ * <p>Exceptions are thrown in cases of bad input format: if there are
+ * more or fewer than 3 numbers per line, or any of the numbers are missing.
+ */
+public class TestAffinityMatrixInputJob extends MahoutTestCase {
+  
+  private static final String [] RAW = {"0,0,0", "0,1,5", "0,2,10", "1,0,5", "1,1,0",
+                                        "1,2,20", "2,0,10", "2,1,20", "2,2,0"};
+  private static final int RAW_DIMENSIONS = 3;
+
+  @Test
+  public void testAffinityMatrixInputMapper() throws Exception {
+    AffinityMatrixInputMapper mapper = new AffinityMatrixInputMapper();
+    Configuration conf = getConfiguration();
+    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
+    
+    // set up the dummy writer and the M/R context
+    DummyRecordWriter<IntWritable, MatrixEntryWritable> writer =
+      new DummyRecordWriter<IntWritable, MatrixEntryWritable>();
+    Mapper<LongWritable, Text, IntWritable, MatrixEntryWritable>.Context 
+      context = DummyRecordWriter.build(mapper, conf, writer);
+
+    // loop through all the points and test each one is converted
+    // successfully to a DistributedRowMatrix.MatrixEntry
+    for (String s : RAW) {
+      mapper.map(new LongWritable(), new Text(s), context);
+    }
+
+    // test the data was successfully constructed
+    assertEquals("Number of map results", RAW_DIMENSIONS, writer.getData().size());
+    Set<IntWritable> keys = writer.getData().keySet();
+    for (IntWritable i : keys) {
+      List<MatrixEntryWritable> row = writer.getData().get(i);
+      assertEquals("Number of items in row", RAW_DIMENSIONS, row.size());
+    }
+  }
+  
+  @Test
+  public void testAffinitymatrixInputReducer() throws Exception {
+    AffinityMatrixInputMapper mapper = new AffinityMatrixInputMapper();
+    Configuration conf = getConfiguration();
+    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
+    
+    // set up the dummy writer and the M/R context
+    DummyRecordWriter<IntWritable, MatrixEntryWritable> mapWriter =
+      new DummyRecordWriter<IntWritable, MatrixEntryWritable>();
+    Mapper<LongWritable, Text, IntWritable, MatrixEntryWritable>.Context
+      mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);
+
+    // loop through all the points and test each one is converted
+    // successfully to a DistributedRowMatrix.MatrixEntry
+    for (String s : RAW) {
+      mapper.map(new LongWritable(), new Text(s), mapContext);
+    }
+    // store the data for checking later
+    Map<IntWritable, List<MatrixEntryWritable>> map = mapWriter.getData();
+
+    // now reduce the data
+    AffinityMatrixInputReducer reducer = new AffinityMatrixInputReducer();
+    DummyRecordWriter<IntWritable, VectorWritable> redWriter = 
+      new DummyRecordWriter<IntWritable, VectorWritable>();
+    Reducer<IntWritable, MatrixEntryWritable,
+      IntWritable, VectorWritable>.Context redContext = DummyRecordWriter
+      .build(reducer, conf, redWriter, IntWritable.class, MatrixEntryWritable.class);
+    for (IntWritable key : mapWriter.getKeys()) {
+      reducer.reduce(key, mapWriter.getValue(key), redContext);
+    }
+    
+    // check that all the elements are correctly ordered
+    assertEquals("Number of reduce results", RAW_DIMENSIONS, redWriter.getData().size());
+    for (IntWritable row : redWriter.getKeys()) {
+      List<VectorWritable> list = redWriter.getValue(row);
+      assertEquals("Should only be one vector", 1, list.size());
+      // check that the elements in the array are correctly ordered
+      Vector v = list.get(0).get();
+      for (Vector.Element e : v.all()) {
+        // find this value in the original map
+        MatrixEntryWritable toCompare = new MatrixEntryWritable();
+        toCompare.setRow(-1);
+        toCompare.setCol(e.index());
+        toCompare.setVal(e.get());
+        assertTrue("This entry was correctly placed in its row", map.get(row).contains(toCompare));
+      }
+    }
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestMatrixDiagonalizeJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestMatrixDiagonalizeJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestMatrixDiagonalizeJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestMatrixDiagonalizeJob.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.mahout.clustering.spectral.MatrixDiagonalizeJob.MatrixDiagonalizeMapper;
+import org.apache.mahout.clustering.spectral.MatrixDiagonalizeJob.MatrixDiagonalizeReducer;
+import org.apache.mahout.common.DummyRecordWriter;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.math.RandomAccessSparseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Test;
+
+/**
+ * <p>The MatrixDiagonalize task is pretty simple: given a matrix,
+ * it sums the elements of the row, and sticks the sum in position (i, i) 
+ * of a new matrix of identical dimensions to the original.</p>
+ */
+public class TestMatrixDiagonalizeJob extends MahoutTestCase {
+  
+  private static final double[][] RAW = { {1, 2, 3}, {4, 5, 6}, {7, 8, 9} };
+  private static final int RAW_DIMENSIONS = 3;
+  
+  private static double rowSum(double [] row) {
+    double sum = 0;
+    for (double r : row) {
+      sum += r;
+    }
+    return sum;
+  }
+
+  @Test
+  public void testMatrixDiagonalizeMapper() throws Exception {
+    MatrixDiagonalizeMapper mapper = new MatrixDiagonalizeMapper();
+    Configuration conf = getConfiguration();
+    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
+    
+    // set up the dummy writers
+    DummyRecordWriter<NullWritable, IntDoublePairWritable> writer =
+      new DummyRecordWriter<NullWritable, IntDoublePairWritable>();
+    Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable>.Context 
+      context = DummyRecordWriter.build(mapper, conf, writer);
+    
+    // perform the mapping
+    for (int i = 0; i < RAW_DIMENSIONS; i++) {
+      RandomAccessSparseVector toAdd = new RandomAccessSparseVector(RAW_DIMENSIONS);
+      toAdd.assign(RAW[i]);
+      mapper.map(new IntWritable(i), new VectorWritable(toAdd), context);
+    }
+    
+    // check the number of the results
+    assertEquals("Number of map results", RAW_DIMENSIONS,
+        writer.getValue(NullWritable.get()).size());
+  }
+  
+  @Test
+ public void testMatrixDiagonalizeReducer() throws Exception {
+    MatrixDiagonalizeMapper mapper = new MatrixDiagonalizeMapper();
+    Configuration conf = getConfiguration();
+    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
+    
+    // set up the dummy writers
+    DummyRecordWriter<NullWritable, IntDoublePairWritable> mapWriter = 
+      new DummyRecordWriter<NullWritable, IntDoublePairWritable>();
+    Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable>.Context 
+      mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);
+    
+    // perform the mapping
+    for (int i = 0; i < RAW_DIMENSIONS; i++) {
+      RandomAccessSparseVector toAdd = new RandomAccessSparseVector(RAW_DIMENSIONS);
+      toAdd.assign(RAW[i]);
+      mapper.map(new IntWritable(i), new VectorWritable(toAdd), mapContext);
+    }
+    
+    // now perform the reduction
+    MatrixDiagonalizeReducer reducer = new MatrixDiagonalizeReducer();
+    DummyRecordWriter<NullWritable, VectorWritable> redWriter = new
+      DummyRecordWriter<NullWritable, VectorWritable>();
+    Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable>.Context
+      redContext = DummyRecordWriter.build(reducer, conf, redWriter, 
+      NullWritable.class, IntDoublePairWritable.class);
+    
+    // only need one reduction
+    reducer.reduce(NullWritable.get(), mapWriter.getValue(NullWritable.get()), redContext);
+    
+    // first, make sure there's only one result
+    List<VectorWritable> list = redWriter.getValue(NullWritable.get());
+    assertEquals("Only a single resulting vector", 1, list.size());
+    Vector v = list.get(0).get();
+    for (int i = 0; i < v.size(); i++) {
+      assertEquals("Element sum is correct", rowSum(RAW[i]), v.get(i),0.01);
+    }
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestUnitVectorizerJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestUnitVectorizerJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestUnitVectorizerJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestUnitVectorizerJob.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.clustering.spectral.UnitVectorizerJob.UnitVectorizerMapper;
+import org.apache.mahout.common.DummyRecordWriter;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.math.RandomAccessSparseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Test;
+
+public class TestUnitVectorizerJob extends MahoutTestCase {
+
+  private static final double [][] RAW = { {1, 2, 3}, {4, 5, 6}, {7, 8, 9} };
+
+  @Test
+  public void testUnitVectorizerMapper() throws Exception {
+    UnitVectorizerMapper mapper = new UnitVectorizerMapper();
+    Configuration conf = getConfiguration();
+    
+    // set up the dummy writers
+    DummyRecordWriter<IntWritable, VectorWritable> writer = new
+      DummyRecordWriter<IntWritable, VectorWritable>();
+    Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>.Context 
+      context = DummyRecordWriter.build(mapper, conf, writer);
+    
+    // perform the mapping
+    for (int i = 0; i < RAW.length; i++) {
+      Vector vector = new RandomAccessSparseVector(RAW[i].length);
+      vector.assign(RAW[i]);
+      mapper.map(new IntWritable(i), new VectorWritable(vector), context);
+    }
+    
+    // check the results
+    assertEquals("Number of map results", RAW.length, writer.getData().size());
+    for (int i = 0; i < RAW.length; i++) {
+      IntWritable key = new IntWritable(i);
+      List<VectorWritable> list = writer.getValue(key);
+      assertEquals("Only one element per row", 1, list.size());
+      Vector v = list.get(0).get();
+      assertTrue("Unit vector sum is 1 or differs by 0.0001", Math.abs(v.norm(2) - 1) < 0.000001);
+    }
+  } 
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorCache.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorCache.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorCache.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorCache.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,110 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.net.URI;
+
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Writable;
+import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Test;
+
+public class TestVectorCache extends MahoutTestCase {
+
+  private static final double [] VECTOR = { 1, 2, 3, 4 };
+  
+  @Test
+  public void testSave() throws Exception {
+    Configuration conf = getConfiguration();
+    Writable key = new IntWritable(0);
+    Vector value = new DenseVector(VECTOR);
+    Path path = getTestTempDirPath("output");
+    
+    // write the vector out
+    VectorCache.save(key, value, path, conf, true, true);
+    
+    // can we read it from here?
+    SequenceFileValueIterator<VectorWritable> iterator =
+        new SequenceFileValueIterator<VectorWritable>(path, true, conf);
+    try {
+      VectorWritable old = iterator.next();
+      // test if the values are identical
+      assertEquals("Saved vector is identical to original", old.get(), value);
+    } finally {
+      Closeables.close(iterator, true);
+    }
+  }
+  
+  @Test
+  public void testLoad() throws Exception {
+    // save a vector manually
+    Configuration conf = getConfiguration();
+    Writable key = new IntWritable(0);
+    Vector value = new DenseVector(VECTOR);
+    Path path = getTestTempDirPath("output");
+
+    FileSystem fs = FileSystem.get(path.toUri(), conf);
+    // write the vector
+    path = fs.makeQualified(path);
+    fs.deleteOnExit(path);
+    HadoopUtil.delete(conf, path);
+    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class);
+    try {
+      writer.append(key, new VectorWritable(value));
+    } finally {
+      Closeables.close(writer, false);
+    }
+    DistributedCache.setCacheFiles(new URI[] {path.toUri()}, conf);
+
+    // load it
+    Vector result = VectorCache.load(conf);
+    
+    // are they the same?
+    assertNotNull("Vector is null", result);
+    assertEquals("Loaded vector is not identical to original", result, value);
+  }
+  
+  @Test
+  public void testAll() throws Exception {
+    Configuration conf = getConfiguration();
+    Vector v = new DenseVector(VECTOR);
+    Path toSave = getTestTempDirPath("output");
+    Writable key = new IntWritable(0);
+    
+    // save it
+    VectorCache.save(key, v, toSave, conf);
+    
+    // now, load it back
+    Vector v2 = VectorCache.load(conf);
+    
+    // are they the same?
+    assertNotNull("Vector is null", v2);
+    assertEquals("Vectors are not identical", v2, v);
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorMatrixMultiplicationJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorMatrixMultiplicationJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorMatrixMultiplicationJob.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/TestVectorMatrixMultiplicationJob.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral;
+
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.mahout.clustering.spectral.VectorMatrixMultiplicationJob.VectorMatrixMultiplicationMapper;
+import org.apache.mahout.common.DummyRecordWriter;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.RandomAccessSparseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Test;
+
+/**
+ * <p>This test ensures that a Vector can be successfully multiplied
+ * with a matrix.</p>
+ */
+public class TestVectorMatrixMultiplicationJob extends MahoutTestCase {
+  
+  private static final double [][] MATRIX = { {1, 1}, {2, 3} };
+  private static final double [] VECTOR = {9, 16};
+  
+  @Test
+  public void testVectorMatrixMultiplicationMapper() throws Exception {
+    VectorMatrixMultiplicationMapper mapper = new VectorMatrixMultiplicationMapper();
+    Configuration conf = getConfiguration();
+    
+    // set up all the parameters for the job
+    Vector toSave = new DenseVector(VECTOR);
+    DummyRecordWriter<IntWritable, VectorWritable> writer = new 
+      DummyRecordWriter<IntWritable, VectorWritable>();
+    Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>.Context
+      context = DummyRecordWriter.build(mapper, conf, writer);
+    mapper.setup(toSave);
+    
+    // run the job
+    for (int i = 0; i < MATRIX.length; i++) {
+      Vector v = new RandomAccessSparseVector(MATRIX[i].length);
+      v.assign(MATRIX[i]);
+      mapper.map(new IntWritable(i), new VectorWritable(v), context);
+    }
+    
+    // check the results
+    assertEquals("Number of map results", MATRIX.length, writer.getData().size());
+    for (int i = 0; i < MATRIX.length; i++) {
+      List<VectorWritable> list = writer.getValue(new IntWritable(i));
+      assertEquals("Only one vector per key", 1, list.size());
+      Vector v = list.get(0).get();
+      for (int j = 0; j < MATRIX[i].length; j++) {
+        double total = Math.sqrt(VECTOR[i]) * Math.sqrt(VECTOR[j]) * MATRIX[i][j];
+        assertEquals("Product matrix elements", total, v.get(j),EPSILON);
+      }
+    }
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestAffinityMatrixInputJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestAffinityMatrixInputJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestAffinityMatrixInputJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestAffinityMatrixInputJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,146 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.hadoop.DistributedRowMatrix.MatrixEntryWritable;
-import org.junit.Test;
-
-/**
- * <p>Tests the affinity matrix input M/R task.</p>
- * 
- * <p>The tricky item with this task is that the format of the input
- * must be correct; it must take the form of a graph input, and for the
- * current implementation, the input must be symmetric, e.g. the weight
- * from node A to B = the weight from node B to A. This is not explicitly
- * enforced within the task itself (since, as of the time these tests were 
- * written, we have not yet decided on a final rule regarding the 
- * symmetry/non-symmetry of the affinity matrix, so we are unofficially 
- * enforcing symmetry). Input looks something like this:</p>
- * 
- * <pre>0, 0, 0
- * 0, 1, 10
- * 0, 2, 20
- * ...
- * 1, 0, 10
- * 2, 0, 20
- * ...</pre>
- * 
- * <p>The mapper's task is simply to convert each line of text into a
- * DistributedRowMatrix entry, allowing the reducer to join each entry
- * of the same row into a VectorWritable.</p>
- * 
- * <p>Exceptions are thrown in cases of bad input format: if there are
- * more or fewer than 3 numbers per line, or any of the numbers are missing.
- */
-public class TestAffinityMatrixInputJob extends MahoutTestCase {
-  
-  private static final String [] RAW = {"0,0,0", "0,1,5", "0,2,10", "1,0,5", "1,1,0",
-                                        "1,2,20", "2,0,10", "2,1,20", "2,2,0"};
-  private static final int RAW_DIMENSIONS = 3;
-
-  @Test
-  public void testAffinityMatrixInputMapper() throws Exception {
-    AffinityMatrixInputMapper mapper = new AffinityMatrixInputMapper();
-    Configuration conf = new Configuration();
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
-    
-    // set up the dummy writer and the M/R context
-    DummyRecordWriter<IntWritable, MatrixEntryWritable> writer =
-      new DummyRecordWriter<IntWritable, MatrixEntryWritable>();
-    Mapper<LongWritable, Text, IntWritable, MatrixEntryWritable>.Context 
-      context = DummyRecordWriter.build(mapper, conf, writer);
-
-    // loop through all the points and test each one is converted
-    // successfully to a DistributedRowMatrix.MatrixEntry
-    for (String s : RAW) {
-      mapper.map(new LongWritable(), new Text(s), context);
-    }
-
-    // test the data was successfully constructed
-    assertEquals("Number of map results", RAW_DIMENSIONS, writer.getData().size());
-    Set<IntWritable> keys = writer.getData().keySet();
-    for (IntWritable i : keys) {
-      List<MatrixEntryWritable> row = writer.getData().get(i);
-      assertEquals("Number of items in row", RAW_DIMENSIONS, row.size());
-    }
-  }
-  
-  @Test
-  public void testAffinitymatrixInputReducer() throws Exception {
-    AffinityMatrixInputMapper mapper = new AffinityMatrixInputMapper();
-    Configuration conf = new Configuration();
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
-    
-    // set up the dummy writer and the M/R context
-    DummyRecordWriter<IntWritable, MatrixEntryWritable> mapWriter =
-      new DummyRecordWriter<IntWritable, MatrixEntryWritable>();
-    Mapper<LongWritable, Text, IntWritable, MatrixEntryWritable>.Context
-      mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);
-
-    // loop through all the points and test each one is converted
-    // successfully to a DistributedRowMatrix.MatrixEntry
-    for (String s : RAW) {
-      mapper.map(new LongWritable(), new Text(s), mapContext);
-    }
-    // store the data for checking later
-    Map<IntWritable, List<MatrixEntryWritable>> map = mapWriter.getData();
-
-    // now reduce the data
-    AffinityMatrixInputReducer reducer = new AffinityMatrixInputReducer();
-    DummyRecordWriter<IntWritable, VectorWritable> redWriter = 
-      new DummyRecordWriter<IntWritable, VectorWritable>();
-    Reducer<IntWritable, MatrixEntryWritable,
-      IntWritable, VectorWritable>.Context redContext = DummyRecordWriter
-      .build(reducer, conf, redWriter, IntWritable.class, MatrixEntryWritable.class);
-    for (IntWritable key : mapWriter.getKeys()) {
-      reducer.reduce(key, mapWriter.getValue(key), redContext);
-    }
-    
-    // check that all the elements are correctly ordered
-    assertEquals("Number of reduce results", RAW_DIMENSIONS, redWriter.getData().size());
-    for (IntWritable row : redWriter.getKeys()) {
-      List<VectorWritable> list = redWriter.getValue(row);
-      assertEquals("Should only be one vector", 1, list.size());
-      // check that the elements in the array are correctly ordered
-      Vector v = list.get(0).get();
-      for (Vector.Element e : v.all()) {
-        // find this value in the original map
-        MatrixEntryWritable toCompare = new MatrixEntryWritable();
-        toCompare.setRow(-1);
-        toCompare.setCol(e.index());
-        toCompare.setVal(e.get());
-        assertTrue("This entry was correctly placed in its row", map.get(row).contains(toCompare));
-      }
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestMatrixDiagonalizeJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestMatrixDiagonalizeJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestMatrixDiagonalizeJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestMatrixDiagonalizeJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob.MatrixDiagonalizeMapper;
-import org.apache.mahout.clustering.spectral.common.MatrixDiagonalizeJob.MatrixDiagonalizeReducer;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsKeys;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-/**
- * <p>The MatrixDiagonalize task is pretty simple: given a matrix,
- * it sums the elements of the row, and sticks the sum in position (i, i) 
- * of a new matrix of identical dimensions to the original.</p>
- */
-public class TestMatrixDiagonalizeJob extends MahoutTestCase {
-  
-  private static final double[][] RAW = { {1, 2, 3}, {4, 5, 6}, {7, 8, 9} };
-  private static final int RAW_DIMENSIONS = 3;
-  
-  private static double rowSum(double [] row) {
-    double sum = 0;
-    for (double r : row) {
-      sum += r;
-    }
-    return sum;
-  }
-
-  @Test
-  public void testMatrixDiagonalizeMapper() throws Exception {
-    MatrixDiagonalizeMapper mapper = new MatrixDiagonalizeMapper();
-    Configuration conf = new Configuration();
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
-    
-    // set up the dummy writers
-    DummyRecordWriter<NullWritable, IntDoublePairWritable> writer = 
-      new DummyRecordWriter<NullWritable, IntDoublePairWritable>();
-    Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable>.Context 
-      context = DummyRecordWriter.build(mapper, conf, writer);
-    
-    // perform the mapping
-    for (int i = 0; i < RAW_DIMENSIONS; i++) {
-      RandomAccessSparseVector toAdd = new RandomAccessSparseVector(RAW_DIMENSIONS);
-      toAdd.assign(RAW[i]);
-      mapper.map(new IntWritable(i), new VectorWritable(toAdd), context);
-    }
-    
-    // check the number of the results
-    assertEquals("Number of map results", RAW_DIMENSIONS,
-        writer.getValue(NullWritable.get()).size());
-  }
-  
-  @Test
- public void testMatrixDiagonalizeReducer() throws Exception {
-    MatrixDiagonalizeMapper mapper = new MatrixDiagonalizeMapper();
-    Configuration conf = new Configuration();
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);
-    
-    // set up the dummy writers
-    DummyRecordWriter<NullWritable, IntDoublePairWritable> mapWriter = 
-      new DummyRecordWriter<NullWritable, IntDoublePairWritable>();
-    Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable>.Context 
-      mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);
-    
-    // perform the mapping
-    for (int i = 0; i < RAW_DIMENSIONS; i++) {
-      RandomAccessSparseVector toAdd = new RandomAccessSparseVector(RAW_DIMENSIONS);
-      toAdd.assign(RAW[i]);
-      mapper.map(new IntWritable(i), new VectorWritable(toAdd), mapContext);
-    }
-    
-    // now perform the reduction
-    MatrixDiagonalizeReducer reducer = new MatrixDiagonalizeReducer();
-    DummyRecordWriter<NullWritable, VectorWritable> redWriter = new
-      DummyRecordWriter<NullWritable, VectorWritable>();
-    Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable>.Context
-      redContext = DummyRecordWriter.build(reducer, conf, redWriter, 
-      NullWritable.class, IntDoublePairWritable.class);
-    
-    // only need one reduction
-    reducer.reduce(NullWritable.get(), mapWriter.getValue(NullWritable.get()), redContext);
-    
-    // first, make sure there's only one result
-    List<VectorWritable> list = redWriter.getValue(NullWritable.get());
-    assertEquals("Only a single resulting vector", 1, list.size());
-    Vector v = list.get(0).get();
-    for (int i = 0; i < v.size(); i++) {
-      assertEquals("Element sum is correct", rowSum(RAW[i]), v.get(i),0.01);
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestUnitVectorizerJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestUnitVectorizerJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestUnitVectorizerJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestUnitVectorizerJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,65 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.spectral.common.UnitVectorizerJob.UnitVectorizerMapper;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-public class TestUnitVectorizerJob extends MahoutTestCase {
-
-  private static final double [][] RAW = { {1, 2, 3}, {4, 5, 6}, {7, 8, 9} };
-
-  @Test
-  public void testUnitVectorizerMapper() throws Exception {
-    UnitVectorizerMapper mapper = new UnitVectorizerMapper();
-    Configuration conf = new Configuration();
-    
-    // set up the dummy writers
-    DummyRecordWriter<IntWritable, VectorWritable> writer = new
-      DummyRecordWriter<IntWritable, VectorWritable>();
-    Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>.Context 
-      context = DummyRecordWriter.build(mapper, conf, writer);
-    
-    // perform the mapping
-    for (int i = 0; i < RAW.length; i++) {
-      Vector vector = new RandomAccessSparseVector(RAW[i].length);
-      vector.assign(RAW[i]);
-      mapper.map(new IntWritable(i), new VectorWritable(vector), context);
-    }
-    
-    // check the results
-    assertEquals("Number of map results", RAW.length, writer.getData().size());
-    for (int i = 0; i < RAW.length; i++) {
-      IntWritable key = new IntWritable(i);
-      List<VectorWritable> list = writer.getValue(key);
-      assertEquals("Only one element per row", 1, list.size());
-      Vector v = list.get(0).get();
-      assertTrue("Unit vector sum is 1 or differs by 0.0001", Math.abs(v.norm(2) - 1) < 0.000001);
-    }
-  } 
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorCache.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorCache.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorCache.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorCache.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.net.URI;
-
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.filecache.DistributedCache;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Writable;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-public class TestVectorCache extends MahoutTestCase {
-
-  private static final double [] VECTOR = { 1, 2, 3, 4 };
-  
-  @Test
-  public void testSave() throws Exception {
-    Configuration conf = new Configuration();
-    Writable key = new IntWritable(0);
-    Vector value = new DenseVector(VECTOR);
-    Path path = getTestTempDirPath("output");
-    
-    // write the vector out
-    VectorCache.save(key, value, path, conf, true, true);
-    
-    // can we read it from here?
-    SequenceFileValueIterator<VectorWritable> iterator =
-        new SequenceFileValueIterator<VectorWritable>(path, true, conf);
-    try {
-      VectorWritable old = iterator.next();
-      // test if the values are identical
-      assertEquals("Saved vector is identical to original", old.get(), value);
-    } finally {
-      Closeables.close(iterator, true);
-    }
-  }
-  
-  @Test
-  public void testLoad() throws Exception {
-    // save a vector manually
-    Configuration conf = new Configuration();
-    Writable key = new IntWritable(0);
-    Vector value = new DenseVector(VECTOR);
-    Path path = getTestTempDirPath("output");
-
-    FileSystem fs = FileSystem.get(path.toUri(), conf);
-    // write the vector
-    path = fs.makeQualified(path);
-    fs.deleteOnExit(path);
-    HadoopUtil.delete(conf, path);
-    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class);
-    try {
-      writer.append(key, new VectorWritable(value));
-    } finally {
-      Closeables.close(writer, false);
-    }
-    DistributedCache.setCacheFiles(new URI[] {path.toUri()}, conf);
-
-    // load it
-    Vector result = VectorCache.load(conf);
-    
-    // are they the same?
-    assertNotNull("Vector is null", result);
-    assertEquals("Loaded vector is not identical to original", result, value);
-  }
-  
-  @Test
-  public void testAll() throws Exception {
-    Configuration conf = new Configuration();
-    Vector v = new DenseVector(VECTOR);
-    Path toSave = getTestTempDirPath("output");
-    Writable key = new IntWritable(0);
-    
-    // save it
-    VectorCache.save(key, v, toSave, conf);
-    
-    // now, load it back
-    Vector v2 = VectorCache.load(conf);
-    
-    // are they the same?
-    assertNotNull("Vector is null", v2);
-    assertEquals("Vectors are not identical", v2, v);
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorMatrixMultiplicationJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorMatrixMultiplicationJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorMatrixMultiplicationJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/common/TestVectorMatrixMultiplicationJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.common;
-
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.spectral.common.VectorMatrixMultiplicationJob.VectorMatrixMultiplicationMapper;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-/**
- * <p>This test ensures that a Vector can be successfully multiplied
- * with a matrix.</p>
- */
-public class TestVectorMatrixMultiplicationJob extends MahoutTestCase {
-  
-  private static final double [][] MATRIX = { {1, 1}, {2, 3} };
-  private static final double [] VECTOR = {9, 16};
-  
-  @Test
-  public void testVectorMatrixMultiplicationMapper() throws Exception {
-    VectorMatrixMultiplicationMapper mapper = new VectorMatrixMultiplicationMapper();
-    Configuration conf = new Configuration();
-    
-    // set up all the parameters for the job
-    Vector toSave = new DenseVector(VECTOR);
-    DummyRecordWriter<IntWritable, VectorWritable> writer = new 
-      DummyRecordWriter<IntWritable, VectorWritable>();
-    Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>.Context
-      context = DummyRecordWriter.build(mapper, conf, writer);
-    mapper.setup(toSave);
-    
-    // run the job
-    for (int i = 0; i < MATRIX.length; i++) {
-      Vector v = new RandomAccessSparseVector(MATRIX[i].length);
-      v.assign(MATRIX[i]);
-      mapper.map(new IntWritable(i), new VectorWritable(v), context);
-    }
-    
-    // check the results
-    assertEquals("Number of map results", MATRIX.length, writer.getData().size());
-    for (int i = 0; i < MATRIX.length; i++) {
-      List<VectorWritable> list = writer.getValue(new IntWritable(i));
-      assertEquals("Only one vector per key", 1, list.size());
-      Vector v = list.get(0).get();
-      for (int j = 0; j < MATRIX[i].length; j++) {
-        double total = Math.sqrt(VECTOR[i]) * Math.sqrt(VECTOR[j]) * MATRIX[i][j];
-        assertEquals("Product matrix elements", total, v.get(j),EPSILON);
-      }
-    }
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsAffinityCutsJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsAffinityCutsJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsAffinityCutsJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsAffinityCutsJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,318 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.util.List;
-import java.util.Map;
-
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.clustering.spectral.common.VertexWritable;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsAffinityCutsJob.EigencutsAffinityCutsCombiner;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsAffinityCutsJob.EigencutsAffinityCutsMapper;
-import org.apache.mahout.clustering.spectral.eigencuts.EigencutsAffinityCutsJob.EigencutsAffinityCutsReducer;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-/**
- * <p>Tests the Eigencuts affinity matrix "cut" ability, the core functionality
- * of the algorithm responsible for making the clusterings.</p>
- * 
- * <p>Due to the complexity of this section, and the amount of data required,
- * there are three steps: the mapper essentially reads in the affinity/cut
- * matrices and creating "vertices" of points, the combiner performs the 
- * actual checks on the sensitivities and zeroes out the necessary affinities,
- * and at last the reducer reforms the affinity matrix.</p>
- */
-@Deprecated
-public class TestEigencutsAffinityCutsJob extends MahoutTestCase {
-  
-  private final double [][] affinity = { {0, 10, 2, 1}, {10, 0, 2, 2},
-                                  {2, 2, 0, 10}, {1, 2, 10, 0} };
-  private final double [][] sensitivity = { {0, 0, 1, 1}, {0, 0, 1, 1},
-                                {1, 1, 0, 0}, {1, 1, 0, 0} };
-
-  /**
-   * Testing the mapper is fairly straightforward: there are two matrices
-   * to be processed simultaneously (cut matrix of sensitivities, and the
-   * affinity matrix), and since both are symmetric, two entries from each
-   * will be grouped together with the same key (or, in the case of an
-   * entry along the diagonal, only two entries).
-   * 
-   * The correct grouping of these quad or pair vertices is the only
-   * output of the mapper.
-   * 
-   * @throws Exception
-   */
-  @Test
-  public void testEigencutsAffinityCutsMapper() throws Exception {
-    EigencutsAffinityCutsMapper mapper = new EigencutsAffinityCutsMapper();
-    Configuration conf = new Configuration();
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, this.affinity.length);
-    
-    // set up the writer
-    DummyRecordWriter<Text, VertexWritable> writer = 
-      new DummyRecordWriter<Text, VertexWritable>();
-    Mapper<IntWritable, VectorWritable, Text, VertexWritable>.Context context = 
-      DummyRecordWriter.build(mapper, conf, writer);
-    
-    // perform the maps
-    for (int i = 0; i < this.affinity.length; i++) {
-      VectorWritable aff = new VectorWritable(new DenseVector(this.affinity[i]));
-      VectorWritable sens = new VectorWritable(new DenseVector(this.sensitivity[i]));
-      IntWritable key = new IntWritable(i);
-      mapper.map(key, aff, context);
-      mapper.map(key, sens, context);
-    }
-    
-    // were the vertices constructed correctly? if so, then for two 4x4
-    // matrices, there should be 10 unique keys with 56 total entries
-    assertEquals("Number of keys", 10, writer.getKeys().size());
-    for (int i = 0; i < this.affinity.length; i++) {
-      for (int j = 0; j < this.affinity.length; j++) {
-        Text key = new Text(Math.max(i, j) + "_" + Math.min(i,j));
-        List<VertexWritable> values = writer.getValue(key);
-        
-        // if we're on a diagonal, there should only be 2 entries
-        // otherwise, there should be 4
-        if (i == j) {
-          assertEquals("Diagonal entry", 2, values.size());
-          for (VertexWritable v : values) {
-            assertFalse("Diagonal values are zero", v.getValue() > 0);
-          }
-        } else {
-          assertEquals("Off-diagonal entry", 4, values.size());
-          if (i + j == 3) { // all have values greater than 0
-            for (VertexWritable v : values) {
-              assertTrue("Off-diagonal non-zero entries", v.getValue() > 0);
-            }
-          }
-        }
-      }
-    }
-  }
-  
-  /**
-   * This is by far the trickiest step. However, an easy condition is if 
-   * we have only two vertices - indicating vertices on the diagonal of the
-   * two matrices - then we simply exit (since the algorithm does not operate
-   * on the diagonal; it makes no sense to perform cuts by isolating data
-   * points from themselves).
-   * 
-   * If there are four points, then first we must separate the two which
-   * belong to the affinity matrix from the two that are sensitivities. In theory,
-   * each pair should have exactly the same value (symmetry). If the sensitivity
-   * is below a certain threshold, then we set the two values of the affinity
-   * matrix to 0 (but not before adding the affinity values to the diagonal, so
-   * as to maintain the overall sum of the row of the affinity matrix).
-   * 
-   * @throws Exception
-   */
-  @Test
-  public void testEigencutsAffinityCutsCombiner() throws Exception {
-    Configuration conf = new Configuration();
-    Path affinity = new Path("affinity");
-    Path sensitivity = new Path("sensitivity");
-    conf.set(EigencutsKeys.AFFINITY_PATH, affinity.getName());
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, this.affinity.length);
-    
-    // since we need the working paths to distinguish the vertex types, 
-    // we can't use the mapper (since we have no way of manually setting
-    // the Context.workingPath() )
-    Map<Text, List<VertexWritable>> data = buildMapData(affinity, sensitivity, this.sensitivity);
-     
-    // now, set up the combiner
-    EigencutsAffinityCutsCombiner combiner = new EigencutsAffinityCutsCombiner();
-    DummyRecordWriter<Text, VertexWritable> redWriter =
-      new DummyRecordWriter<Text, VertexWritable>();
-    Reducer<Text, VertexWritable, Text, VertexWritable>.Context 
-      redContext = DummyRecordWriter.build(combiner, conf, redWriter, Text.class,
-      VertexWritable.class);
-    
-    // perform the combining
-    for (Map.Entry<Text, List<VertexWritable>> entry : data.entrySet()) {
-      combiner.reduce(entry.getKey(), entry.getValue(), redContext);
-    }
-    
-    // test the number of cuts, there should be 2
-    assertEquals("Number of cuts detected", 4, 
-        redContext.getCounter(EigencutsAffinityCutsJob.CUTSCOUNTER.NUM_CUTS).getValue());
-    
-    // loop through all the results; let's see if they match up to our
-    // affinity matrix (and all the cuts appear where they should
-    Map<Text, List<VertexWritable>> results = redWriter.getData();
-    for (Map.Entry<Text, List<VertexWritable>> entry : results.entrySet()) {
-      List<VertexWritable> row = entry.getValue();
-      IntWritable key = new IntWritable(Integer.parseInt(entry.getKey().toString()));
-      
-      double calcDiag = 0.0;
-      double trueDiag = sumOfRowCuts(key.get(), this.sensitivity);
-      for (VertexWritable e : row) {
-
-        // should the value have been cut, e.g. set to 0?
-        if (key.get() == e.getCol()) {
-          // we have our diagonal
-          calcDiag += e.getValue();
-        } else if (this.sensitivity[key.get()][e.getCol()] == 0.0) {
-          // no, corresponding affinity should have same value as before
-          assertEquals("Preserved affinity value", 
-              this.affinity[key.get()][e.getCol()], e.getValue(),EPSILON);
-        } else {
-          // yes, corresponding affinity value should be 0
-          assertEquals("Cut affinity value", 0.0, e.getValue(),EPSILON);
-        }
-      }
-      // check the diagonal has the correct sum
-      assertEquals("Diagonal sum from cuts", trueDiag, calcDiag,EPSILON);
-    }
-  }
-  
-  /**
-   * Fairly straightforward: the task here is to reassemble the rows of the
-   * affinity matrix. The tricky part is that any specific element in the list
-   * of elements which does NOT lay on the diagonal will be so because it
-   * did not drop below the sensitivity threshold, hence it was not "cut". 
-   * 
-   * On the flip side, there will be many entries whose coordinate is now
-   * set to the diagonal, indicating they were previously affinity entries
-   * whose sensitivities were below the threshold, and hence were "cut" - 
-   * set to 0 at their original coordinates, and had their values added to
-   * the diagonal entry (hence the numerous entries with the coordinate of
-   * the diagonal).
-   * 
-   * @throws Exception
-   */
-  @Test
-  public void testEigencutsAffinityCutsReducer() throws Exception {
-    Configuration conf = new Configuration();
-    Path affinity = new Path("affinity");
-    Path sensitivity = new Path("sensitivity");
-    conf.set(EigencutsKeys.AFFINITY_PATH, affinity.getName());
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, this.affinity.length);
-    
-    // since we need the working paths to distinguish the vertex types, 
-    // we can't use the mapper (since we have no way of manually setting
-    // the Context.workingPath() )
-    Map<Text, List<VertexWritable>> data = buildMapData(affinity, sensitivity, this.sensitivity);
-     
-    // now, set up the combiner
-    EigencutsAffinityCutsCombiner combiner = new EigencutsAffinityCutsCombiner();
-    DummyRecordWriter<Text, VertexWritable> comWriter =
-      new DummyRecordWriter<Text, VertexWritable>();
-    Reducer<Text, VertexWritable, Text, VertexWritable>.Context 
-      comContext = DummyRecordWriter.build(combiner, conf, comWriter, Text.class,
-      VertexWritable.class);
-    
-    // perform the combining
-    for (Map.Entry<Text, List<VertexWritable>> entry : data.entrySet()) {
-      combiner.reduce(entry.getKey(), entry.getValue(), comContext);
-    }
-    
-    // finally, set up the reduction writers
-    EigencutsAffinityCutsReducer reducer = new EigencutsAffinityCutsReducer();
-    DummyRecordWriter<IntWritable, VectorWritable> redWriter = new
-      DummyRecordWriter<IntWritable, VectorWritable>();
-    Reducer<Text, VertexWritable, IntWritable, VectorWritable>.Context 
-      redContext = DummyRecordWriter.build(reducer, conf, redWriter, 
-      Text.class, VertexWritable.class);
-    
-    // perform the reduction
-    for (Text key : comWriter.getKeys()) {
-      reducer.reduce(key, comWriter.getValue(key), redContext);
-    }
-    
-    // now, check that the affinity matrix is correctly formed
-    for (IntWritable row : redWriter.getKeys()) {
-      List<VectorWritable> results = redWriter.getValue(row);
-      // there should only be 1 vector
-      assertEquals("Only one vector with a given row number", 1, results.size());
-      Vector therow = results.get(0).get();
-      for (Vector.Element e : therow.all()) {
-        // check the diagonal
-        if (row.get() == e.index()) {
-          assertEquals("Correct diagonal sum of cuts", sumOfRowCuts(row.get(), 
-              this.sensitivity), e.get(),EPSILON);
-        } else {
-          // not on the diagonal...if it was an element labeled to be cut,
-          // it should have a value of 0. Otherwise, it should have kept its
-          // previous value
-          if (this.sensitivity[row.get()][e.index()] == 0.0) {
-            // should be what it was originally
-            assertEquals("Preserved element", this.affinity[row.get()][e.index()], e.get(), EPSILON);
-          } else {
-            // should be 0
-            assertEquals("Cut element", 0.0, e.get(), EPSILON);
-          }
-        }
-      }
-    }
-  }
-  
-  /**
-   * Utility method for simulating the Mapper behavior.
-   * @param affinity
-   * @param sensitivity
-   * @param array
-   * @return
-   */
-  private Map<Text, List<VertexWritable>> buildMapData(Path affinity, 
-      Path sensitivity, double [][] array) {
-    Map<Text, List<VertexWritable>> map = Maps.newHashMap();
-    for (int i = 0; i < this.affinity.length; i++) {
-      for (int j = 0; j < this.affinity[i].length; j++) {
-        Text key = new Text(Math.max(i, j) + "_" + Math.min(i, j));
-        List<VertexWritable> toAdd = Lists.newArrayList();
-        if (map.containsKey(key)) {
-          toAdd = map.get(key);
-          map.remove(key);
-        }
-        toAdd.add(new VertexWritable(i, j, this.affinity[i][j], affinity.getName()));
-        toAdd.add(new VertexWritable(i, j, array[i][j], sensitivity.getName()));
-        map.put(key, toAdd);
-      }
-    }
-    return map;
-  }
-  
-  /**
-   * Utility method for calculating the new diagonal on the specified row of the
-   * affinity matrix after a single iteration, given the specified cut matrix
-   * @param row
-   * @param cuts
-   * @return
-   */
-  private double sumOfRowCuts(int row, double [][] cuts) {
-    double retval = 0.0;
-    for (int j = 0; j < this.affinity[row].length; j++) {
-      if (cuts[row][j] != 0.0) {
-        retval += this.affinity[row][j];
-      }
-    }
-    return retval;
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsSensitivityJob.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsSensitivityJob.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsSensitivityJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/eigencuts/TestEigencutsSensitivityJob.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.spectral.eigencuts;
-
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.mahout.common.DummyRecordWriter;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.junit.Test;
-
-/**
- * <p>Tests the Eigencuts M/R process for generating perturbation sensitivities
- * in a weighted graph.</p>
- * 
- * <p>This process requires a lot of inputs. Please read the 
- * EigencutsSensitivityJob javadocs for more information on these variables.
- * For now, 
- *
- */
-@Deprecated
-public class TestEigencutsSensitivityJob extends MahoutTestCase {
-  
-  /*
-  private final double [][] affinity = { {0, 0.9748, 0.6926, 0.6065},
-                                         {0.9748, 0, 0.7178, 0.6350},
-                                         {0.6926, 0.7178, 0, 0.9898},
-                                         {0.6065, 0.6350, 0.9898, 0} };
-  */
-  private final double [] diagonal = {2.2739, 2.3276, 2.4002, 2.2313};
-  
-  private final double [][] eigenvectors = { {-0.4963, -0.5021, -0.5099, -0.4916},
-                                             {-0.5143, -0.4841, 0.4519, 0.5449},
-                                             {-0.6858, 0.7140, -0.1146, 0.0820},
-                                             {0.1372, -0.0616, -0.7230, 0.6743} };
-  private final double [] eigenvalues = {1.000, -0.1470, -0.4238, -0.4293};
-
-  /**
-   * This is the toughest step, primarily because of the intensity of 
-   * the calculations that are performed and the amount of data required.
-   * Four parameters in particular - the list of eigenvalues, the 
-   * vector representing the diagonal matrix, and the scalars beta0 and 
-   * epsilon - must be set here prior to the start of the mapper. Once
-   * the mapper is executed, it iterates over a matrix of all corresponding
-   * eigenvectors.
-   * @throws Exception
-   */
-@Test
-public void testEigencutsSensitivityMapper() throws Exception {
-    EigencutsSensitivityMapper mapper = new EigencutsSensitivityMapper();
-    Configuration conf = new Configuration();
-
-    // construct the writers
-    DummyRecordWriter<IntWritable, EigencutsSensitivityNode> writer = 
-      new DummyRecordWriter<IntWritable, EigencutsSensitivityNode>();
-    Mapper<IntWritable, VectorWritable, IntWritable, EigencutsSensitivityNode>.Context 
-      context = DummyRecordWriter.build(mapper, conf, writer);
-    mapper.setup(2.0, 0.25, new DenseVector(eigenvalues), new DenseVector(diagonal));
-    
-    // perform the mapping
-    for (int i = 0; i < eigenvectors.length; i++) {
-      VectorWritable row = new VectorWritable(new DenseVector(eigenvectors[i]));
-      mapper.map(new IntWritable(i), row, context);
-    }
-    
-    // the results line up
-    for (IntWritable key : writer.getKeys()) {
-      List<EigencutsSensitivityNode> list = writer.getValue(key);
-      assertEquals("Only one result per row", 1, list.size());
-      EigencutsSensitivityNode item = list.get(0);
-      assertTrue("Sensitivity values are correct", Math.abs(item.getSensitivity() + 0.48) < 0.01);
-    }
-  }
-  
-  /**
-   * This step will simply assemble sensitivities into one coherent matrix.
-   * @throws Exception
-   */
-@Test
-  public void testEigencutsSensitivityReducer() throws Exception {
-    EigencutsSensitivityMapper mapper = new EigencutsSensitivityMapper();
-    Configuration conf = new Configuration();
-    conf.setInt(EigencutsKeys.AFFINITY_DIMENSIONS, eigenvectors.length);
-    
-    // construct the writers
-    DummyRecordWriter<IntWritable, EigencutsSensitivityNode> mapWriter = 
-      new DummyRecordWriter<IntWritable, EigencutsSensitivityNode>();
-    Mapper<IntWritable, VectorWritable, IntWritable, EigencutsSensitivityNode>.Context 
-      mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);
-    mapper.setup(2.0, 0.25, new DenseVector(eigenvalues), new DenseVector(diagonal));
-    
-    // perform the mapping
-    for (int i = 0; i < eigenvectors.length; i++) {
-      VectorWritable row = new VectorWritable(new DenseVector(eigenvectors[i]));
-      mapper.map(new IntWritable(i), row, mapContext);
-    }
-    
-    // set up the values for the reducer
-    conf.set(EigencutsKeys.DELTA, "1.0");
-    conf.set(EigencutsKeys.TAU, "-0.1");
-    
-    EigencutsSensitivityReducer reducer = new EigencutsSensitivityReducer();
-    // set up the writers
-    DummyRecordWriter<IntWritable, VectorWritable> redWriter = new
-      DummyRecordWriter<IntWritable, VectorWritable>();
-    Reducer<IntWritable, EigencutsSensitivityNode, IntWritable, VectorWritable>.Context
-      redContext = DummyRecordWriter.build(reducer, conf, redWriter, 
-      IntWritable.class, EigencutsSensitivityNode.class);
-    
-    // perform the reduction
-    for (IntWritable key : mapWriter.getKeys()) {
-      reducer.reduce(key, mapWriter.getValue(key), redContext);
-    }
-    
-    // since all the sensitivities were below the threshold,
-    // each of them should have survived
-    for (IntWritable key : redWriter.getKeys()) {
-      List<VectorWritable> list = redWriter.getValue(key);
-      assertEquals("One item in the list", 1, list.size());
-      Vector item = list.get(0).get();
-      
-      // should only be one non-zero item
-      assertTrue("One non-zero item in the array", Math.abs(item.zSum() + 0.48) < 0.01);
-    }
-
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/spectral/kmeans/TestEigenSeedGenerator.java mahout/core/src/test/java/org/apache/mahout/clustering/spectral/kmeans/TestEigenSeedGenerator.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/spectral/kmeans/TestEigenSeedGenerator.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/clustering/spectral/kmeans/TestEigenSeedGenerator.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.clustering.spectral.kmeans;
+
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.mahout.clustering.Cluster;
+import org.apache.mahout.clustering.ClusteringTestUtils;
+import org.apache.mahout.clustering.iterator.ClusterWritable;
+import org.apache.mahout.clustering.spectral.kmeans.EigenSeedGenerator;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
+import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
+import org.apache.mahout.math.RandomAccessSparseVector;
+import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Before;
+import org.junit.Test;
+
+import com.google.common.collect.Lists;
+
+public final class TestEigenSeedGenerator extends MahoutTestCase {
+
+  private
+   static final double[][] RAW = {{1, 0, 0}, {1, 0, 0}, {0, 1, 0}, {0, 1, 0},
+                                  {0, 1, 0}, {0, 0, 1}, {0, 0, 1}};
+
+  private FileSystem fs;
+
+  private static List<VectorWritable> getPoints() {
+    List<VectorWritable> points = Lists.newArrayList();
+    for (double[] fr : RAW) {
+      Vector vec = new RandomAccessSparseVector(fr.length);
+      vec.assign(fr);
+      points.add(new VectorWritable(vec));
+    }
+    return points;
+  }
+
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    Configuration conf = getConfiguration();
+    fs = FileSystem.get(conf);
+  }
+
+  @Test
+  public void testEigenSeedGenerator() throws Exception {
+    List<VectorWritable> points = getPoints();
+    Job job = new Job();
+    Configuration conf = job.getConfiguration();
+    job.setMapOutputValueClass(VectorWritable.class);
+    Path input = getTestTempFilePath("eigen-input");
+    Path output = getTestTempDirPath("eigen-output");
+    ClusteringTestUtils.writePointsToFile(points, input, fs, conf);
+
+    EigenSeedGenerator.buildFromEigens(conf, input, output, 3, new ManhattanDistanceMeasure());
+
+    int clusterCount = 0;
+    Collection<Integer> set = new HashSet<Integer>();
+    Vector v[] = new Vector[3];
+    for (ClusterWritable clusterWritable :
+         new SequenceFileValueIterable<ClusterWritable>(
+             new Path(output, "part-eigenSeed"), true, conf)) {
+      Cluster cluster = clusterWritable.getValue();
+      int id = cluster.getId();
+      assertTrue(set.add(id)); // validate unique id's
+      v[id] = cluster.getCenter();
+      clusterCount++;
+    }
+    assertEquals(3, clusterCount); // validate sample count
+    // validate pair-wise orthogonality
+    assertEquals(0, v[0].dot(v[1]), 1E-10);
+    assertEquals(0, v[1].dot(v[2]), 1E-10);
+    assertEquals(0, v[0].dot(v[2]), 1E-10);
+  }
+
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/BallKMeansTest.java mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/BallKMeansTest.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/BallKMeansTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/BallKMeansTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -22,6 +22,7 @@
 import com.google.common.collect.Lists;
 import org.apache.mahout.clustering.ClusteringUtils;
 import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
 import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
 import org.apache.mahout.math.Centroid;
@@ -40,6 +41,8 @@
 import org.apache.mahout.math.random.MultiNormal;
 import org.apache.mahout.math.random.WeightedThing;
 import org.apache.mahout.math.stats.OnlineSummarizer;
+import org.junit.Before;
+import org.junit.BeforeClass;
 import org.junit.Test;
 
 import static org.apache.mahout.clustering.ClusteringUtils.totalWeight;
@@ -52,10 +55,17 @@
   private static final int NUM_ITERATIONS = 20;
   private static final double DISTRIBUTION_RADIUS = 0.01;
 
-  private static Pair<List<Centroid>, List<Centroid>> syntheticData =
-      DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, DISTRIBUTION_RADIUS);
+  @BeforeClass
+  public static void setUp() {
+    RandomUtils.useTestSeed();
+    syntheticData = DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, DISTRIBUTION_RADIUS);
+
+  }
+
+  private static Pair<List<Centroid>, List<Centroid>> syntheticData;
   private static final int K1 = 100;
 
+
   @Test
   public void testClusteringMultipleRuns() {
     for (int i = 1; i <= 10; ++i) {
@@ -80,9 +90,18 @@
     BallKMeans clusterer = new BallKMeans(searcher, 1 << NUM_DIMENSIONS, NUM_ITERATIONS);
 
     long startTime = System.currentTimeMillis();
-    clusterer.cluster(syntheticData.getFirst());
+    Pair<List<Centroid>, List<Centroid>> data = syntheticData;
+    clusterer.cluster(data.getFirst());
     long endTime = System.currentTimeMillis();
 
+    long hash = 0;
+    for (Centroid centroid : data.getFirst()) {
+      for (Vector.Element element : centroid.all()) {
+        hash = 31 * hash + 17 * element.index() + Double.toHexString(element.get()).hashCode();
+      }
+    }
+    System.out.printf("Hash = %08x\n", hash);
+
     assertEquals("Total weight not preserved", totalWeight(syntheticData.getFirst()), totalWeight(clusterer), 1.0e-9);
 
     // Verify that each corner of the cube has a centroid very nearby.
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/DataUtils.java mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/DataUtils.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/DataUtils.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/DataUtils.java	2014-03-29 01:03:14.000000000 -0700
@@ -41,6 +41,11 @@
    * A hypercube of numDimensions has 2^numDimensions vertices. Keep this in mind when clustering
    * the data.
    *
+   * Note that it is almost always the case that you want to call RandomUtils.useTestSeed() before
+   * generating test data.  This means that you can't generate data in the declaration of a static
+   * variable because such initializations happen before any @BeforeClass or @Before setup methods
+   * are called.
+   *
    *
    * @param numDimensions number of dimensions of the vectors to be generated.
    * @param numDatapoints number of data points to be generated.
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/StreamingKMeansTest.java mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/StreamingKMeansTest.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/StreamingKMeansTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/streaming/cluster/StreamingKMeansTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -23,6 +23,7 @@
 
 import org.apache.mahout.clustering.ClusteringUtils;
 import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
 import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
 import org.apache.mahout.math.Centroid;
@@ -33,6 +34,7 @@
 import org.apache.mahout.math.neighborhood.Searcher;
 import org.apache.mahout.math.neighborhood.UpdatableSearcher;
 import org.apache.mahout.math.random.WeightedThing;
+import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
@@ -49,8 +51,14 @@
   private static final int NUM_PROJECTIONS = 2;
   private static final int SEARCH_SIZE = 10;
 
-  private static final Pair<List<Centroid>, List<Centroid>> syntheticData =
+  private static Pair<List<Centroid>, List<Centroid>> syntheticData ;
+
+  @Before
+  public void setUp() {
+    RandomUtils.useTestSeed();
+    syntheticData =
       DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS);
+  }
 
   private UpdatableSearcher searcher;
   private boolean allAtOnce;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansTestMR.java mahout/core/src/test/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansTestMR.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansTestMR.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansTestMR.java	2014-03-29 01:03:14.000000000 -0700
@@ -34,7 +34,9 @@
 import org.apache.mahout.clustering.ClusteringUtils;
 import org.apache.mahout.clustering.streaming.cluster.DataUtils;
 import org.apache.mahout.clustering.streaming.cluster.StreamingKMeans;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
 import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
@@ -46,6 +48,7 @@
 import org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearch;
 import org.apache.mahout.math.neighborhood.ProjectionSearch;
 import org.apache.mahout.math.random.WeightedThing;
+import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
@@ -54,7 +57,7 @@
 import static org.junit.Assert.assertTrue;
 
 @RunWith(Parameterized.class)
-public class StreamingKMeansTestMR {
+public class StreamingKMeansTestMR extends MahoutTestCase {
   private static final int NUM_DATA_POINTS = 1 << 15;
   private static final int NUM_DIMENSIONS = 8;
   private static final int NUM_PROJECTIONS = 3;
@@ -62,8 +65,14 @@
   private static final int MAX_NUM_ITERATIONS = 10;
   private static final double DISTANCE_CUTOFF = 1.0e-6;
 
-  private static final Pair<List<Centroid>, List<Centroid>> syntheticData =
+  private static Pair<List<Centroid>, List<Centroid>> syntheticData;
+
+  @Before
+  public void setUp() {
+    RandomUtils.useTestSeed();
+    syntheticData =
       DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, 1.0e-4);
+  }
 
   private final String searcherClassName;
   private final String distanceMeasureClassName;
@@ -224,7 +233,7 @@
 
   @Test
   public void testHypercubeMapReduceRunSequentially() throws Exception {
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     configure(configuration);
     configuration.set(DefaultOptionCreator.METHOD_OPTION, DefaultOptionCreator.SEQUENTIAL_METHOD);
 
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReaderTest.java mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReaderTest.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReaderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterCountReaderTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -56,7 +56,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
   
@@ -79,7 +79,7 @@
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     
     Path pointsPath = getTestTempDirPath("points");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file2"), fs, conf);
     
@@ -101,7 +101,7 @@
     CanopyDriver.run(conf, pointsPath, outputPathForCanopy, measure, 4.0, 3.0, true, 0.0, true);
     Path clustersIn = new Path(outputPathForCanopy, new Path(Cluster.CLUSTERS_DIR + '0'
                                                                    + Cluster.FINAL_ITERATION_SUFFIX));
-    KMeansDriver.run(conf, pointsPath, clustersIn, outputPathForKMeans, measure, 1, 1, true, 0.0, true);
+    KMeansDriver.run(conf, pointsPath, clustersIn, outputPathForKMeans, 1, 1, true, 0.0, true);
   }
   
   private static void verifyThatNumberOfClustersIsCorrect(Configuration conf, Path clusteredPointsPath) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorTest.java mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorTest.java
--- mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/clustering/topdown/postprocessor/ClusterOutputPostProcessorTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -66,7 +66,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     fs = FileSystem.get(conf);
   }
 
@@ -89,7 +89,7 @@
     List<VectorWritable> points = getPointsWritable(REFERENCE);
 
     Path pointsPath = getTestTempDirPath("points");
-    conf = new Configuration();
+    conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file2"), fs, conf);
 
@@ -202,4 +202,4 @@
     CanopyDriver.run(conf, pointsPath, outputPath, new ManhattanDistanceMeasure(), 3.1, 2.1, true, 0.0, true);
   }
 
-}
\ No newline at end of file
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/common/AbstractJobTest.java mahout/core/src/test/java/org/apache/mahout/common/AbstractJobTest.java
--- mahout/core/src/test/java/org/apache/mahout/common/AbstractJobTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/common/AbstractJobTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -57,7 +57,7 @@
       }
     };
     
-    // testFlag will only be present if speciied on the command-line
+    // testFlag will only be present if specified on the command-line
     
     ToolRunner.run(fact.getJob(), new String[0]);
     assertFalse("test map for absent flag", testMap.containsKey("--testFlag"));
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java
--- mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,15 +18,10 @@
 package org.apache.mahout.common;
 
 import com.google.common.collect.Lists;
-
-import java.lang.reflect.Constructor;
-import java.lang.reflect.Method;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
-
+import com.google.common.collect.Maps;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.MapContext;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.RecordWriter;
@@ -35,18 +30,60 @@
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 
-public final class DummyRecordWriter<K, V> extends RecordWriter<K, V> {
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.Method;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+public final class DummyRecordWriter<K extends Writable, V extends Writable> extends RecordWriter<K, V> {
 
-  private final Map<K, List<V>> data = new TreeMap<K, List<V>>();
+  private final List<K> keysInInsertionOrder = Lists.newArrayList();
+  private final Map<K, List<V>> data = Maps.newHashMap();
 
   @Override
   public void write(K key, V value) {
-    List<V> points = data.get(key);
-    if (points == null) {
-      points = Lists.newArrayList();
-      data.put(key, points);
+
+    // if the user reuses the same writable class, we need to create a new one
+    // otherwise the Map content will be modified after the insert
+    try {
+
+      K keyToUse = key instanceof NullWritable ? key : (K) cloneWritable(key);
+      V valueToUse = (V) cloneWritable(value);
+
+      keysInInsertionOrder.add(keyToUse);
+
+      List<V> points = data.get(key);
+      if (points == null) {
+        points = Lists.newArrayList();
+        data.put(keyToUse, points);
+      }
+      points.add(valueToUse);
+
+    } catch (IOException e) {
+      throw new RuntimeException(e.getMessage(), e);
     }
-    points.add(value);
+  }
+
+  private Writable cloneWritable(Writable original) throws IOException {
+
+    Writable clone;
+    try {
+      clone = original.getClass().asSubclass(Writable.class).newInstance();
+    } catch (Exception e) {
+      throw new RuntimeException("Unable to instantiate writable!", e);
+    }
+    ByteArrayOutputStream bytes = new ByteArrayOutputStream();
+
+    original.write(new DataOutputStream(bytes));
+    clone.readFields(new DataInputStream(new ByteArrayInputStream(bytes.toByteArray())));
+
+    return clone;
   }
 
   @Override
@@ -65,6 +102,10 @@
     return data.keySet();
   }
 
+  public Iterable<K> getKeysInInsertionOrder() {
+    return keysInInsertionOrder;
+  }
+
   public static <K1, V1, K2, V2> Mapper<K1, V1, K2, V2>.Context build(Mapper<K1, V1, K2, V2> mapper,
                                                                       Configuration configuration,
                                                                       RecordWriter<K2, V2> output) {
@@ -101,13 +142,13 @@
     }
   }
 
-  @SuppressWarnings({ "unchecked", "rawtypes" })
+  @SuppressWarnings({"unchecked", "rawtypes"})
   private static <K1, V1, K2, V2> Mapper<K1, V1, K2, V2>.Context buildNewMapperContext(
-      Configuration configuration, RecordWriter<K2, V2> output) throws Exception {
+    Configuration configuration, RecordWriter<K2, V2> output) throws Exception {
     Class<?> mapContextImplClass = Class.forName("org.apache.hadoop.mapreduce.task.MapContextImpl");
     Constructor<?> cons = mapContextImplClass.getConstructors()[0];
     Object mapContextImpl = cons.newInstance(configuration,
-        new TaskAttemptID(), null, output, null, new DummyStatusReporter(), null);
+      new TaskAttemptID(), null, output, null, new DummyStatusReporter(), null);
 
     Class<?> wrappedMapperClass = Class.forName("org.apache.hadoop.mapreduce.lib.map.WrappedMapper");
     Object wrappedMapper = wrappedMapperClass.getConstructor().newInstance();
@@ -115,20 +156,20 @@
     return (Mapper.Context) getMapContext.invoke(wrappedMapper, mapContextImpl);
   }
 
-  @SuppressWarnings({ "unchecked", "rawtypes" })
+  @SuppressWarnings({"unchecked", "rawtypes"})
   private static <K1, V1, K2, V2> Mapper<K1, V1, K2, V2>.Context buildOldMapperContext(
-      Mapper<K1, V1, K2, V2> mapper, Configuration configuration,
-      RecordWriter<K2, V2> output) throws Exception {
+    Mapper<K1, V1, K2, V2> mapper, Configuration configuration,
+    RecordWriter<K2, V2> output) throws Exception {
     Constructor<?> cons = getNestedContextConstructor(mapper.getClass());
     // first argument to the constructor is the enclosing instance
     return (Mapper.Context) cons.newInstance(mapper, configuration,
-        new TaskAttemptID(), null, output, null, new DummyStatusReporter(), null);
+      new TaskAttemptID(), null, output, null, new DummyStatusReporter(), null);
   }
 
-  @SuppressWarnings({ "unchecked", "rawtypes" })
+  @SuppressWarnings({"unchecked", "rawtypes"})
   private static <K1, V1, K2, V2> Reducer<K1, V1, K2, V2>.Context buildNewReducerContext(
-      Configuration configuration, RecordWriter<K2, V2> output, Class<K1> keyClass,
-      Class<V1> valueClass) throws Exception {
+    Configuration configuration, RecordWriter<K2, V2> output, Class<K1> keyClass,
+    Class<V1> valueClass) throws Exception {
     Class<?> reduceContextImplClass = Class.forName("org.apache.hadoop.mapreduce.task.ReduceContextImpl");
     Constructor<?> cons = reduceContextImplClass.getConstructors()[0];
     Object reduceContextImpl = cons.newInstance(configuration,
@@ -148,26 +189,26 @@
     Method getReducerContext = wrappedReducerClass.getMethod("getReducerContext", ReduceContext.class);
     return (Reducer.Context) getReducerContext.invoke(wrappedReducer, reduceContextImpl);
   }
-  
-  @SuppressWarnings({ "unchecked", "rawtypes" })
+
+  @SuppressWarnings({"unchecked", "rawtypes"})
   private static <K1, V1, K2, V2> Reducer<K1, V1, K2, V2>.Context buildOldReducerContext(
-      Reducer<K1, V1, K2, V2> reducer, Configuration configuration,
-      RecordWriter<K2, V2> output, Class<K1> keyClass,
-      Class<V1> valueClass) throws Exception {
+    Reducer<K1, V1, K2, V2> reducer, Configuration configuration,
+    RecordWriter<K2, V2> output, Class<K1> keyClass,
+    Class<V1> valueClass) throws Exception {
     Constructor<?> cons = getNestedContextConstructor(reducer.getClass());
     // first argument to the constructor is the enclosing instance
     return (Reducer.Context) cons.newInstance(reducer,
-        configuration,
-        new TaskAttemptID(),
-        new MockIterator(),
-        null,
-        null,
-        output,
-        null,
-        new DummyStatusReporter(),
-        null,
-        keyClass,
-        valueClass);
+      configuration,
+      new TaskAttemptID(),
+      new MockIterator(),
+      null,
+      null,
+      output,
+      null,
+      new DummyStatusReporter(),
+      null,
+      keyClass,
+      valueClass);
   }
 
   private static Constructor<?> getNestedContextConstructor(Class<?> outerClass) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriterTest.java mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriterTest.java
--- mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriterTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/common/DummyRecordWriterTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.common;
+
+import org.apache.hadoop.io.IntWritable;
+import org.apache.mahout.math.DenseVector;
+import org.apache.mahout.math.VectorWritable;
+import org.junit.Assert;
+import org.junit.Test;
+
+public class DummyRecordWriterTest {
+
+  @Test
+  public void testWrite() {
+    DummyRecordWriter<IntWritable, VectorWritable> writer = 
+        new DummyRecordWriter<IntWritable, VectorWritable>();
+    IntWritable reusableIntWritable = new IntWritable();
+    VectorWritable reusableVectorWritable = new VectorWritable();
+    reusableIntWritable.set(0);
+    reusableVectorWritable.set(new DenseVector(new double[] { 1, 2, 3 }));
+    writer.write(reusableIntWritable, reusableVectorWritable);
+    reusableIntWritable.set(1);
+    reusableVectorWritable.set(new DenseVector(new double[] { 4, 5, 6 }));
+    writer.write(reusableIntWritable, reusableVectorWritable);
+
+    Assert.assertEquals(
+        "The writer must remember the two keys that is written to it", 2,
+        writer.getKeys().size());
+  }
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java mahout/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java
--- mahout/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/common/MahoutTestCase.java	2014-03-29 01:03:14.000000000 -0700
@@ -32,7 +32,7 @@
 import org.junit.After;
 import org.junit.Before;
 
-public abstract class MahoutTestCase extends org.apache.mahout.math.MahoutTestCase {
+public class MahoutTestCase extends org.apache.mahout.math.MahoutTestCase {
 
   /** "Close enough" value for floating-point comparisons. */
   public static final double EPSILON = 0.000001;
@@ -63,16 +63,16 @@
     }
     super.tearDown();
   }
-  
-  protected final Configuration getConfiguration() throws IOException {
-	Configuration conf = new Configuration();
+
+  public final Configuration getConfiguration() throws IOException {
+    Configuration conf = new Configuration();
     conf.set("hadoop.tmp.dir", getTestTempDir("hadoop" + Math.random()).getAbsolutePath());
     return conf;
   }
 
   protected final Path getTestTempDirPath() throws IOException {
     if (testTempDirPath == null) {
-      fs = FileSystem.get(new Configuration());
+      fs = FileSystem.get(getConfiguration());
       long simpleRandomLong = (long) (Long.MAX_VALUE * Math.random());
       testTempDirPath = fs.makeQualified(
           new Path("/tmp/mahout-" + getClass().getSimpleName() + '-' + simpleRandomLong));
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/ep/EvolutionaryProcessTest.java mahout/core/src/test/java/org/apache/mahout/ep/EvolutionaryProcessTest.java
--- mahout/core/src/test/java/org/apache/mahout/ep/EvolutionaryProcessTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/ep/EvolutionaryProcessTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -52,6 +52,7 @@
       System.out.printf("%10.3f %.3f\n", best.getValue(), best.getOmni());
     }
 
+    ep.close();
     assertNotNull(best);
     assertEquals(0.0, best.getValue(), 0.02);
   }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -35,7 +35,6 @@
 
 import com.google.common.io.Resources;
 
-@Deprecated
 public final class FPGrowthRetailDataTest extends MahoutTestCase {
 
   @Test
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTest2.java	2014-03-29 01:03:14.000000000 -0700
@@ -29,13 +29,11 @@
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.FileLineIterable;
 import org.apache.mahout.common.iterator.StringRecordIterator;
-import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
 import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
 import org.junit.Test;
 
 import com.google.common.io.Resources;
 
-@Deprecated
 public final class FPGrowthRetailDataTest2 extends MahoutTestCase {
 
   @Test
@@ -76,10 +74,6 @@
           }
         }
         
-      }, new StatusUpdater() {
-        
-        @Override
-        public void update(String status) {}
       });
     
     assertEquals(Long.valueOf(pattern_41_36_39), results.get(returnableFeatures));
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthRetailDataTestVs.java	2014-03-29 01:03:14.000000000 -0700
@@ -40,7 +40,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
 public final class FPGrowthRetailDataTestVs extends MahoutTestCase {
 
   private static final Logger log = LoggerFactory.getLogger(PFPGrowthRetailDataTestVs.class);
@@ -120,7 +119,7 @@
       fp2.generateFList(new StringRecordIterator(new FileLineIterable(Resources.getResource(inputFilename)
            .openStream()), "\\s+"), minSupport), minSupport, 100000,
         Sets.<String>newHashSet(),
-      new MapCollector(initialResults2), new DummyUpdater());
+      new MapCollector(initialResults2));
 
     Map<Set<String>, Long> results2;
     if (returnableFeatures.isEmpty()) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthSyntheticDataTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -38,7 +38,6 @@
 
 import com.google.common.io.Resources;
 
-@Deprecated
 public final class FPGrowthSyntheticDataTest extends MahoutTestCase {
 
   @Test
@@ -97,11 +96,7 @@
                                         }
                                       }
         
-                                    }, new StatusUpdater() {
-        
-                                        @Override
-                                          public void update(String status) {}
-                                      });
+                                    });
 
     assertEquals(patternCnt_10_13, highestSupport(results, features_10_13));
     assertEquals(patternCnt_10_13_1669, highestSupport(results, returnableFeatures));
@@ -192,11 +187,7 @@
                                          }
                                        }
         
-                                     }, new StatusUpdater() {
-        
-                                         @Override
-                                           public void update(String status) {}
-                                       });
+                                     });
 
     Map<Set<String>, Long> results2;
     if (returnableFeatures.isEmpty()) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -39,7 +39,6 @@
 import org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth;
 import org.junit.Test;
 
-@Deprecated
 public final class FPGrowthTest extends MahoutTestCase {
 
   @Test
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/FPGrowthTest2.java	2014-03-29 01:03:14.000000000 -0700
@@ -32,14 +32,12 @@
 import org.apache.hadoop.mapred.OutputCollector;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
-import org.apache.mahout.fpm.pfpgrowth.convertors.ContextStatusUpdater;
 import org.apache.mahout.fpm.pfpgrowth.convertors.SequenceFileOutputCollector;
 import org.apache.mahout.fpm.pfpgrowth.convertors.string.StringOutputConverter;
 import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
 import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
 import org.junit.Test;
 
-@Deprecated
 public final class FPGrowthTest2 extends MahoutTestCase {
 
   @Test
@@ -70,8 +68,8 @@
         3,
         100,
         Sets.<String>newHashSet(),
-        new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
-        new ContextStatusUpdater(null));
+        new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer))
+    );
     } finally {
       Closeables.close(writer, false);
     }
@@ -113,8 +111,8 @@
           2,
           100,
           Sets.<String>newHashSet(),
-          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
-          new ContextStatusUpdater(null));
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer))
+      );
     } finally {
       Closeables.close(writer, false);
     }
@@ -151,8 +149,8 @@
           2,
           100,
           Sets.<String>newHashSet(),
-          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer)),
-          new ContextStatusUpdater(null));
+          new StringOutputConverter(new SequenceFileOutputCollector<Text,TopKStringPatterns>(writer))
+      );
     } finally {
       Closeables.close(writer, false);
     }
@@ -187,7 +185,7 @@
         3,
         100,
         null,
-        noOutput,
-        new ContextStatusUpdater(null));
+        noOutput
+    );
   }
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -45,7 +45,6 @@
 
 import com.google.common.io.Resources;
 
-@Deprecated
 public class PFPGrowthRetailDataTest extends MahoutTestCase {
   
   private final Parameters params = new Parameters();
@@ -55,7 +54,7 @@
   public void setUp() throws Exception {
     super.setUp();
     params.set(PFPGrowth.MIN_SUPPORT, "100");
-    params.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
     params.set(PFPGrowth.NUM_GROUPS, "50");
     params.set(PFPGrowth.ENCODING, "UTF-8");
     File inputDir = getTestTempDir("transactions");
@@ -159,7 +158,7 @@
       expectedResults.put(Sets.newHashSet(items), support);
     }
     Configuration conf = getConfiguration();
-    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelCounting(params, conf);
 
     List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
@@ -173,7 +172,7 @@
     params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
 
     PFPGrowth.startParallelFPGrowth(params, conf);
-    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startAggregating(params, conf);
     List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
     
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTest2.java	2014-03-29 01:03:14.000000000 -0700
@@ -45,7 +45,6 @@
 
 import com.google.common.io.Resources;
 
-@Deprecated
 public class PFPGrowthRetailDataTest2 extends MahoutTestCase {
   
   private final Parameters params = new Parameters();
@@ -55,7 +54,7 @@
   public void setUp() throws Exception {
     super.setUp();
     params.set(PFPGrowth.MIN_SUPPORT, "100");
-    params.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
     params.set(PFPGrowth.NUM_GROUPS, "50");
     params.set(PFPGrowth.ENCODING, "UTF-8");
     params.set(PFPGrowth.USE_FPG2, "true");
@@ -157,7 +156,7 @@
       expectedResults.put(Sets.newHashSet(items), support);
     }
     Configuration conf = new Configuration();
-    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelCounting(params, conf);
 
     List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
@@ -171,9 +170,9 @@
     }
     params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
 
-    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelFPGrowth(params, conf);
-    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startAggregating(params, conf);
     List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
     
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthRetailDataTestVs.java	2014-03-29 01:03:14.000000000 -0700
@@ -42,7 +42,6 @@
 import com.google.common.io.Files;
 import com.google.common.io.Resources;
 
-@Deprecated
 public final class PFPGrowthRetailDataTestVs extends MahoutTestCase {
 
   private final Parameters paramsImpl1 = new Parameters();
@@ -56,13 +55,13 @@
     File input = new File(inputDir, "test.txt");
 
     paramsImpl1.set(PFPGrowth.MIN_SUPPORT, "100");
-    paramsImpl1.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    paramsImpl1.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
     paramsImpl1.set(PFPGrowth.NUM_GROUPS, "50");
     paramsImpl1.set(PFPGrowth.ENCODING, "UTF-8");
     paramsImpl1.set(PFPGrowth.INPUT, input.getAbsolutePath());
 
     paramsImpl2.set(PFPGrowth.MIN_SUPPORT, "100");
-    paramsImpl2.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    paramsImpl2.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
     paramsImpl2.set(PFPGrowth.NUM_GROUPS, "50");
     paramsImpl2.set(PFPGrowth.ENCODING, "UTF-8");
     paramsImpl2.set(PFPGrowth.INPUT, input.getAbsolutePath());
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthSynthDataTest2.java	2014-03-29 01:03:14.000000000 -0700
@@ -38,7 +38,6 @@
 import org.apache.mahout.common.Parameters;
 import org.apache.mahout.common.iterator.FileLineIterable;
 import org.apache.mahout.common.iterator.StringRecordIterator;
-import org.apache.mahout.fpm.pfpgrowth.convertors.StatusUpdater;
 import org.apache.mahout.fpm.pfpgrowth.convertors.string.TopKStringPatterns;
 import org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthObj;
 import org.junit.Test;
@@ -47,7 +46,6 @@
 
 import com.google.common.io.Resources;
 
-@Deprecated
 public class PFPGrowthSynthDataTest2 extends MahoutTestCase {
   
   private final Parameters params = new Parameters();
@@ -57,7 +55,7 @@
   public void setUp() throws Exception {
     super.setUp();
     params.set(PFPGrowth.MIN_SUPPORT, "100");
-    params.set(PFPGrowth.MAX_HEAPSIZE, "10000");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "10000");
     params.set(PFPGrowth.NUM_GROUPS, "50");
     params.set(PFPGrowth.ENCODING, "UTF-8");
     params.set(PFPGrowth.USE_FPG2, "true");
@@ -131,10 +129,6 @@
           }
         }
         
-      }, new StatusUpdater() {
-        
-        @Override
-        public void update(String status) {}
       });
 
     for (Entry<Set<String>,Long> entry : parallelResult.entrySet()) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -36,7 +36,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
 public final class PFPGrowthTest extends MahoutTestCase {
   
   private static final Logger log = LoggerFactory.getLogger(PFPGrowthTest.class);
@@ -47,7 +46,7 @@
   public void setUp() throws Exception {
     super.setUp();
     params.set(PFPGrowth.MIN_SUPPORT, "3");
-    params.set(PFPGrowth.MAX_HEAPSIZE, "4");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "4");
     params.set(PFPGrowth.NUM_GROUPS, "2");
     params.set(PFPGrowth.ENCODING, "UTF-8");
     File inputDir = getTestTempDir("transactions");
@@ -104,9 +103,9 @@
   @Test
   public void testStartParallelFPGrowthInSteps() throws Exception {
     Configuration conf = getConfiguration();
-    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelCounting(params, conf);
-    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
     log.info("{}", fList);
     assertEquals("[(B,6), (D,6), (A,5), (E,4), (C,3)]", fList.toString());
@@ -120,9 +119,9 @@
     }
     params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
 
-    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelFPGrowth(params, conf);
-    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startAggregating(params, conf);
     List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
     assertEquals("[(A,([A],5), ([D, A],4), ([B, A],4), ([A, E],4)), "
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest2.java	2014-03-29 01:03:14.000000000 -0700
@@ -36,7 +36,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
 public final class PFPGrowthTest2 extends MahoutTestCase {
   
   private static final Logger log = LoggerFactory.getLogger(PFPGrowthTest.class);
@@ -47,7 +46,7 @@
   public void setUp() throws Exception {
     super.setUp();
     params.set(PFPGrowth.MIN_SUPPORT, "3");
-    params.set(PFPGrowth.MAX_HEAPSIZE, "4");
+    params.set(PFPGrowth.MAX_HEAP_SIZE, "4");
     params.set(PFPGrowth.NUM_GROUPS, "2");
     params.set(PFPGrowth.ENCODING, "UTF-8");
     params.set(PFPGrowth.USE_FPG2, "true");
@@ -105,9 +104,9 @@
   @Test
   public void testStartParallelFPGrowthInSteps() throws Exception {
     Configuration conf = new Configuration();
-    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel Counting Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelCounting(params, conf);
-    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Reading fList Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     List<Pair<String,Long>> fList = PFPGrowth.readFList(params);
     log.info("{}", fList);
     assertEquals("[(B,6), (D,6), (A,5), (E,4), (C,3)]", fList.toString());
@@ -121,9 +120,9 @@
     }
     params.set(PFPGrowth.MAX_PER_GROUP, Integer.toString(maxPerGroup));
 
-    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Parallel FPGrowth Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startParallelFPGrowth(params, conf);
-    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAPSIZE));
+    log.info("Starting Pattern Aggregation Test: {}", params.get(PFPGrowth.MAX_HEAP_SIZE));
     PFPGrowth.startAggregating(params, conf);
 
     List<Pair<String,TopKStringPatterns>> frequentPatterns = PFPGrowth.readFrequentPattern(params);
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/TransactionTreeTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -29,7 +29,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Deprecated
 public final class TransactionTreeTest extends MahoutTestCase {
 
   private static final Logger log = LoggerFactory.getLogger(TransactionTreeTest.class);
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java
--- mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/fpm/pfpgrowth/fpgrowth/FrequentPatternMaxHeapTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -26,7 +26,6 @@
 import org.apache.mahout.common.RandomUtils;
 import org.junit.Test;
 
-@Deprecated
 public final class FrequentPatternMaxHeapTest extends MahoutTestCase {
 
   @Test
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/TestDistributedRowMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -378,8 +378,8 @@
     return saveToFs(c, baseTmpDirPath);
   }
 
-  private static DistributedRowMatrix saveToFs(final Matrix m, Path baseTmpDirPath) throws IOException {
-    Configuration conf = new Configuration();
+  private DistributedRowMatrix saveToFs(final Matrix m, Path baseTmpDirPath) throws IOException {
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(baseTmpDirPath.toUri(), conf);
 
     ClusteringTestUtils.writePointsToFile(new Iterable<VectorWritable>() {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/TestVectorDistanceSimilarityJob.java	2014-03-29 01:03:14.000000000 -0700
@@ -170,7 +170,7 @@
     List<VectorWritable> points = getPointsWritable(REFERENCE);
     List<VectorWritable> seeds = getPointsWritable(SEEDS);
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);
     ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);
 
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/similarity/cooccurrence/RowSimilarityJobTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -24,6 +24,7 @@
 import org.apache.mahout.math.Matrix;
 import org.apache.mahout.math.hadoop.MathHelper;
 import org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.TanimotoCoefficientSimilarity;
+import org.apache.mahout.math.map.OpenIntIntHashMap;
 import org.junit.Test;
 
 import java.io.File;
@@ -73,6 +74,15 @@
         "--numberOfColumns", String.valueOf(5), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(),
         "--tempDir", tmpDir.getAbsolutePath() });
 
+
+    OpenIntIntHashMap observationsPerColumn =
+        Vectors.readAsIntMap(new Path(tmpDir.getAbsolutePath(), "observationsPerColumn.bin"), conf);
+    assertEquals(4, observationsPerColumn.size());
+    assertEquals(1, observationsPerColumn.get(0));
+    assertEquals(2, observationsPerColumn.get(2));
+    assertEquals(2, observationsPerColumn.get(3));
+    assertEquals(1, observationsPerColumn.get(4));
+
     Matrix similarityMatrix = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "part-r-00000"), 3, 3);
 
     assertNotNull(similarityMatrix);
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCADenseTest.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCADenseTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCADenseTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCADenseTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,191 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.hadoop.stochasticsvd;
-
-import java.io.Closeable;
-import java.io.File;
-import java.io.IOException;
-import java.util.Deque;
-import java.util.LinkedList;
-import java.util.Random;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.SequenceFile.CompressionType;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.mahout.common.IOUtils;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.SequentialAccessSparseVector;
-import org.apache.mahout.math.SingularValueDecomposition;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.function.Functions;
-import org.junit.Test;
-
-import com.google.common.io.Closeables;
-
-public class LocalSSVDPCADenseTest extends MahoutTestCase {
-
-  private static final double s_epsilon = 1.0E-10d;
-
-  @Test
-  public void runPCATest1() throws IOException {
-    runSSVDSolver(1);
-  }
-
-  public void runSSVDSolver(int q) throws IOException {
-
-    Configuration conf = new Configuration();
-    conf.set("mapred.job.tracker", "local");
-    conf.set("fs.default.name", "file:///");
-
-    // conf.set("mapred.job.tracker","localhost:11011");
-    // conf.set("fs.default.name","hdfs://localhost:11010/");
-
-    Deque<Closeable> closeables = Lists.newLinkedList();
-    Random rnd = RandomUtils.getRandom();
-
-    File tmpDir = getTestTempDir("svdtmp");
-    conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
-
-    Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
-
-    // create distributed row matrix-like struct
-    SequenceFile.Writer w =
-      SequenceFile.createWriter(FileSystem.getLocal(conf),
-                                conf,
-                                aLocPath,
-                                IntWritable.class,
-                                VectorWritable.class,
-                                CompressionType.BLOCK,
-                                new DefaultCodec());
-    closeables.addFirst(w);
-
-    int n = 100;
-    int m = 2000;
-    double percent = 5;
-
-    VectorWritable vw = new VectorWritable();
-    IntWritable roww = new IntWritable();
-
-    Vector xi = new DenseVector(n);
-
-    double muAmplitude = 50.0;
-    for (int i = 0; i < m; i++) {
-      Vector dv = new SequentialAccessSparseVector(n);
-      for (int j = 0; j < n * percent / 100; j++) {
-        dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.25));
-      }
-      roww.set(i);
-      vw.set(dv);
-      w.append(roww, vw);
-      xi.assign(dv, Functions.PLUS);
-    }
-    closeables.remove(w);
-    Closeables.close(w, false);
-
-    // TODO fix test so that 1.0/m works as intended!
-    xi.assign(Functions.mult(1 / m));
-
-    FileSystem fs = FileSystem.get(conf);
-
-    Path tempDirPath = getTestTempDirPath("svd-proc");
-    Path aPath = new Path(tempDirPath, "A/A.seq");
-    fs.copyFromLocalFile(aLocPath, aPath);
-    Path xiPath = new Path(tempDirPath, "xi/xi.seq");
-    SSVDHelper.saveVector(xi, xiPath, conf);
-
-    Path svdOutPath = new Path(tempDirPath, "SSVD-out");
-
-    // make sure we wipe out previous test results, just a convenience
-    fs.delete(svdOutPath, true);
-
-    // Solver starts here:
-    System.out.println("Input prepared, starting solver...");
-
-    int ablockRows = 867;
-    int p = 60;
-    int k = 40;
-    SSVDSolver ssvd =
-      new SSVDSolver(conf,
-                     new Path[] { aPath },
-                     svdOutPath,
-                     ablockRows,
-                     k,
-                     p,
-                     3);
-    ssvd.setOuterBlockHeight(500);
-    ssvd.setAbtBlockHeight(251);
-    ssvd.setPcaMeanPath(xiPath);
-
-    /*
-     * removing V,U jobs from this test to reduce running time. i will keep them
-     * put in the dense test though.
-     */
-    ssvd.setComputeU(false);
-    ssvd.setComputeV(false);
-
-    ssvd.setOverwrite(true);
-    ssvd.setQ(q);
-    ssvd.setBroadcast(true);
-    ssvd.run();
-
-    Vector stochasticSValues = ssvd.getSingularValues();
-    System.out.println("--SSVD solver singular values:");
-    LocalSSVDSolverSparseSequentialTest.dumpSv(stochasticSValues);
-    System.out.println("--Colt SVD solver singular values:");
-
-    // try to run the same thing without stochastic algo
-    double[][] a = SSVDHelper.loadDistributedRowMatrix(fs, aPath, conf);
-
-    // subtract pseudo pca mean
-    for (int i = 0; i < m; i++) {
-      for (int j = 0; j < n; j++) {
-        a[i][j] -= xi.getQuick(j);
-      }
-    }
-
-    SingularValueDecomposition svd2 =
-      new SingularValueDecomposition(new DenseMatrix(a));
-
-    Vector svalues2 = new DenseVector(svd2.getSingularValues());
-    LocalSSVDSolverSparseSequentialTest.dumpSv(svalues2);
-
-    for (int i = 0; i < k + p; i++) {
-      assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);
-    }
-
-    double[][] mQ =
-      SSVDHelper.loadDistributedRowMatrix(fs, new Path(svdOutPath, "Bt-job/"
-          + BtJob.OUTPUT_Q + "-*"), conf);
-
-    SSVDCommonTest.assertOrthonormality(new DenseMatrix(mQ),
-                                           false,
-                                           s_epsilon);
-
-    IOUtils.close(closeables);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCASparseTest.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCASparseTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCASparseTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDPCASparseTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,296 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.hadoop.stochasticsvd;
+
+import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.mahout.common.IOUtils;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.*;
+import org.apache.mahout.math.function.DoubleFunction;
+import org.apache.mahout.math.function.Functions;
+import org.apache.mahout.math.function.VectorFunction;
+import org.junit.Test;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.IOException;
+import java.util.Deque;
+import java.util.Iterator;
+import java.util.Random;
+
+public class LocalSSVDPCASparseTest extends MahoutTestCase {
+
+  private static final double s_epsilon = 1.0E-10d;
+
+  @Test
+  public void testOmegaTRightMultiply() {
+    final Random rnd = RandomUtils.getRandom();
+    final long seed = rnd.nextLong();
+    final int n = 2000;
+
+    final int kp = 100;
+
+    final Omega omega = new Omega(seed, kp);
+    final Matrix materializedOmega = new DenseMatrix(n, kp);
+    for (int i = 0; i < n; i++)
+      for (int j = 0; j < kp; j++)
+        materializedOmega.setQuick(i, j, omega.getQuick(i, j));
+    Vector xi = new DenseVector(n);
+    xi.assign(new DoubleFunction() {
+      @Override
+      public double apply(double x) {
+        return rnd.nextDouble() * 100;
+      }
+    });
+
+    Vector s_o = omega.mutlithreadedTRightMultiply(xi);
+
+    Matrix xiVector = new DenseMatrix(n, 1);
+    xiVector.assignColumn(0, xi);
+
+    Vector s_o_control = materializedOmega.transpose().times(xiVector).viewColumn(0);
+
+    assertEquals(0, s_o.minus(s_o_control).aggregate(Functions.PLUS, Functions.ABS), 1e-10);
+
+    System.out.printf("s_omega=\n%s\n", s_o);
+    System.out.printf("s_omega_control=\n%s\n", s_o_control);
+  }
+
+  @Test
+  public void runPCATest1() throws IOException {
+    runSSVDSolver(1);
+  }
+
+//  @Test
+  public void runPCATest0() throws IOException {
+    runSSVDSolver(0);
+  }
+
+
+  public void runSSVDSolver(int q) throws IOException {
+
+    Configuration conf = new Configuration();
+    conf.set("mapred.job.tracker", "local");
+    conf.set("fs.default.name", "file:///");
+
+    // conf.set("mapred.job.tracker","localhost:11011");
+    // conf.set("fs.default.name","hdfs://localhost:11010/");
+
+    Deque<Closeable> closeables = Lists.newLinkedList();
+    try {
+      Random rnd = RandomUtils.getRandom();
+
+      File tmpDir = getTestTempDir("svdtmp");
+      conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());
+
+      Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");
+
+      // create distributed row matrix-like struct
+      SequenceFile.Writer w =
+        SequenceFile.createWriter(FileSystem.getLocal(conf),
+                                  conf,
+                                  aLocPath,
+                                  Text.class,
+                                  VectorWritable.class,
+                                  CompressionType.BLOCK,
+                                  new DefaultCodec());
+      closeables.addFirst(w);
+
+      int n = 100;
+      int m = 2000;
+      double percent = 5;
+
+      VectorWritable vw = new VectorWritable();
+      Text rkey = new Text();
+
+      Vector xi = new DenseVector(n);
+
+      double muAmplitude = 50.0;
+      for (int i = 0; i < m; i++) {
+        Vector dv = new SequentialAccessSparseVector(n);
+        String rowname = "row-"+i;
+        NamedVector namedRow = new NamedVector(dv, rowname);
+        for (int j = 0; j < n * percent / 100; j++) {
+          dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.25));
+        }
+        rkey.set("row-i"+i);
+        vw.set(namedRow);
+        w.append(rkey, vw);
+        xi.assign(dv, Functions.PLUS);
+      }
+      closeables.remove(w);
+      Closeables.close(w, false);
+
+      xi.assign(Functions.mult(1.0 / m));
+
+      FileSystem fs = FileSystem.get(conf);
+
+      Path tempDirPath = getTestTempDirPath("svd-proc");
+      Path aPath = new Path(tempDirPath, "A/A.seq");
+      fs.copyFromLocalFile(aLocPath, aPath);
+      Path xiPath = new Path(tempDirPath, "xi/xi.seq");
+      SSVDHelper.saveVector(xi, xiPath, conf);
+
+      Path svdOutPath = new Path(tempDirPath, "SSVD-out");
+
+      // make sure we wipe out previous test results, just a convenience
+      fs.delete(svdOutPath, true);
+
+      // Solver starts here:
+      System.out.println("Input prepared, starting solver...");
+
+      int ablockRows = 867;
+      int p = 60;
+      int k = 40;
+      SSVDSolver ssvd =
+        new SSVDSolver(conf,
+                       new Path[]{aPath},
+                       svdOutPath,
+                       ablockRows,
+                       k,
+                       p,
+                       3);
+      ssvd.setOuterBlockHeight(500);
+      ssvd.setAbtBlockHeight(251);
+      ssvd.setPcaMeanPath(xiPath);
+
+    /*
+     * Removing V,U jobs from this test to reduce running time. i will keep them
+     * put in the dense test though.
+     *
+     * For PCA test, we also want to request U*Sigma output and check it for named
+     * vector propagation.
+     */
+      ssvd.setComputeU(false);
+      ssvd.setComputeV(false);
+      ssvd.setcUSigma(true);
+
+      ssvd.setOverwrite(true);
+      ssvd.setQ(q);
+      ssvd.setBroadcast(true);
+      ssvd.run();
+
+      Vector stochasticSValues = ssvd.getSingularValues();
+
+      // try to run the same thing without stochastic algo
+      Matrix a = SSVDHelper.drmLoadAsDense(fs, aPath, conf);
+
+      verifyInternals(svdOutPath, a, new Omega(ssvd.getOmegaSeed(), k + p), k + p, q);
+
+      // subtract pseudo pca mean
+      for (int i = 0; i < m; i++) {
+        a.viewRow(i).assign(xi, Functions.MINUS);
+      }
+
+      SingularValueDecomposition svd2 =
+        new SingularValueDecomposition(a);
+
+      Vector svalues2 = new DenseVector(svd2.getSingularValues());
+
+      System.out.println("--SSVD solver singular values:");
+      LocalSSVDSolverSparseSequentialTest.dumpSv(stochasticSValues);
+      System.out.println("--SVD solver singular values:");
+      LocalSSVDSolverSparseSequentialTest.dumpSv(svalues2);
+
+      for (int i = 0; i < k + p; i++) {
+        assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);
+      }
+
+      DenseMatrix mQ =
+        SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/"
+          + BtJob.OUTPUT_Q + "-*"), conf);
+
+      SSVDCommonTest.assertOrthonormality(mQ,
+                                          false,
+                                          s_epsilon);
+
+      // assert name propagation
+      for (Iterator<Pair<Writable, Vector>> iter = SSVDHelper.drmIterator(fs,
+                                                                          new Path(ssvd.getuSigmaPath()+"/*"),
+                                                                          conf,
+                                                                          closeables); iter.hasNext(); ) {
+        Pair<Writable, Vector> pair = iter.next();
+        Writable key = pair.getFirst();
+        Vector v = pair.getSecond();
+
+        assertTrue(v instanceof NamedVector);
+        assertTrue(key instanceof Text);
+      }
+
+    } finally {
+      IOUtils.close(closeables);
+    }
+  }
+
+  private void verifyInternals(Path tempDir, Matrix a, Omega omega, int kp, int q) {
+    int m = a.numRows();
+    int n = a.numCols();
+
+    Vector xi = a.aggregateColumns(new VectorFunction() {
+      @Override
+      public double apply(Vector v) {
+        return v.zSum() / v.size();
+      }
+    });
+
+    // materialize omega
+    Matrix momega = new DenseMatrix(n, kp);
+    for (int i = 0; i < n; i++)
+      for (int j = 0; j < kp; j++)
+        momega.setQuick(i, j, omega.getQuick(i, j));
+
+    Vector s_o = omega.mutlithreadedTRightMultiply(xi);
+
+    System.out.printf("s_omega=\n%s\n", s_o);
+
+    Matrix y = a.times(momega);
+    for (int i = 0; i < n; i++) y.viewRow(i).assign(s_o, Functions.MINUS);
+
+    QRDecomposition qr = new QRDecomposition(y);
+    Matrix qm = qr.getQ();
+
+    Vector s_q = qm.aggregateColumns(new VectorFunction() {
+      @Override
+      public double apply(Vector v) {
+        return v.zSum();
+      }
+    });
+
+    System.out.printf("s_q=\n%s\n", s_q);
+
+    Matrix b = qm.transpose().times(a);
+
+    Vector s_b = b.times(xi);
+
+    System.out.printf("s_b=\n%s\n", s_b);
+
+
+  }
+
+}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverDenseTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -69,7 +69,7 @@
 
   public void runSSVDSolver(int q) throws IOException {
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set("mapred.job.tracker", "local");
     conf.set("fs.default.name", "file:///");
 
@@ -150,7 +150,7 @@
      * System.out.println("--Colt SVD solver singular values:"); // try to run
      * 
      * the same thing without stochastic algo double[][] a =
-     * SSVDSolver.loadDistributedRowMatrix(fs, aPath, conf);
+     * SSVDSolver.drmLoadAsDense(fs, aPath, conf);
      * 
      * 
      * 
@@ -173,26 +173,25 @@
           / singularValues.getQuick(i)) <= s_precisionPct / 100);
     }
 
-    double[][] mQ =
-      SSVDHelper.loadDistributedRowMatrix(fs, new Path(svdOutPath, "Bt-job/"
-          + BtJob.OUTPUT_Q + "-*"), conf);
-
-    SSVDCommonTest.assertOrthonormality(new DenseMatrix(mQ),
-                                           false,
-                                           s_epsilon);
-
-    double[][] u =
-      SSVDHelper.loadDistributedRowMatrix(fs,
-                                          new Path(svdOutPath, "U/[^_]*"),
-                                          conf);
-
-    SSVDCommonTest.assertOrthonormality(new DenseMatrix(u), false, s_epsilon);
-    double[][] v =
-      SSVDHelper.loadDistributedRowMatrix(fs,
-                                          new Path(svdOutPath, "V/[^_]*"),
-                                          conf);
-
-    SSVDCommonTest.assertOrthonormality(new DenseMatrix(v), false, s_epsilon);
+    DenseMatrix mQ =
+      SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/"
+        + BtJob.OUTPUT_Q + "-*"), conf);
+
+    SSVDCommonTest.assertOrthonormality(mQ,
+                                        false,
+                                        s_epsilon);
+
+    DenseMatrix u =
+      SSVDHelper.drmLoadAsDense(fs,
+                                new Path(svdOutPath, "U/*"),
+                                conf);
+    SSVDCommonTest.assertOrthonormality(u, false, s_epsilon);
+
+    DenseMatrix v =
+      SSVDHelper.drmLoadAsDense(fs,
+                                new Path(svdOutPath, "V/*"),
+                                conf);
+    SSVDCommonTest.assertOrthonormality(v, false, s_epsilon);
   }
 
   static void dumpSv(Vector s) {
@@ -204,13 +203,4 @@
 
   }
 
-  static void dump(double[][] matrix) {
-    for (double[] aMatrix : matrix) {
-      for (double anAMatrix : aMatrix) {
-        System.out.printf("%f  ", anAMatrix);
-      }
-      System.out.println();
-    }
-  }
-
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/LocalSSVDSolverSparseSequentialTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.Deque;
-import java.util.LinkedList;
 import java.util.Random;
 
 import com.google.common.collect.Lists;
@@ -72,7 +71,7 @@
 
   public void runSSVDSolver(int q) throws IOException {
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set("mapred.job.tracker", "local");
     conf.set("fs.default.name", "file:///");
 
@@ -164,12 +163,12 @@
     System.out.println("--Colt SVD solver singular values:");
 
     // try to run the same thing without stochastic algo
-    double[][] a = SSVDHelper.loadDistributedRowMatrix(fs, aPath, conf);
+    DenseMatrix a = SSVDHelper.drmLoadAsDense(fs, aPath, conf);
 
     // SingularValueDecompositionImpl svd=new SingularValueDecompositionImpl(new
     // Array2DRowRealMatrix(a));
     SingularValueDecomposition svd2 =
-      new SingularValueDecomposition(new DenseMatrix(a));
+      new SingularValueDecomposition(a);
 
     Vector svalues2 = new DenseVector(svd2.getSingularValues());
     dumpSv(svalues2);
@@ -178,13 +177,13 @@
       assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);
     }
 
-    double[][] mQ =
-      SSVDHelper.loadDistributedRowMatrix(fs, new Path(svdOutPath, "Bt-job/"
-          + BtJob.OUTPUT_Q + "-*"), conf);
-
-    SSVDCommonTest.assertOrthonormality(new DenseMatrix(mQ),
-                                           false,
-                                           s_epsilon);
+    DenseMatrix mQ =
+      SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/"
+        + BtJob.OUTPUT_Q + "-*"), conf);
+
+    SSVDCommonTest.assertOrthonormality(mQ,
+                                        false,
+                                        s_epsilon);
 
     IOUtils.close(closeables);
   }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDTestsHelper.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDTestsHelper.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDTestsHelper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDTestsHelper.java	2014-03-29 01:03:14.000000000 -0700
@@ -26,6 +26,7 @@
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.hadoop.io.Writable;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.math.DenseMatrix;
 import org.apache.mahout.math.DenseVector;
@@ -115,7 +116,8 @@
   // do not use. for internal consumption only.
   public static void main(String[] args) throws Exception {
     // create 1Gb input for distributed tests.
-    Configuration conf = new Configuration();
+    MahoutTestCase ca = new MahoutTestCase();
+    Configuration conf = ca.getConfiguration();
     FileSystem dfs = FileSystem.getLocal(conf);
     Path outputDir=new Path("/tmp/DRM");
     dfs.mkdirs(outputDir);
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangularTest.java mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangularTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangularTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/hadoop/stochasticsvd/UpperTriangularTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,58 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.hadoop.stochasticsvd;
-
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.math.DenseMatrix;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Matrix;
-import org.apache.mahout.math.function.Functions;
-import org.junit.Test;
-
-public class UpperTriangularTest extends MahoutTestCase {
-  @Test
-  public void testBasics() {
-    Matrix a = new UpperTriangular(new double[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, false);
-    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[]{1, 5, 8, 10})).norm(1), 1.0e-10);
-    assertEquals(0, a.viewPart(0, 3, 1, 3).viewDiagonal().minus(
-        new DenseVector(new double[]{2, 6, 9})).norm(1), 1.0e-10);
-    assertEquals(4, a.get(0, 3), 1.0e-10);
-    print(a);
-    Matrix m = new DenseMatrix(4, 4).assign(a);
-    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
-    print(m);
-
-    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(
-        Functions.PLUS, Functions.ABS), 1.0e-10);
-    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
-  }
-
-  private static void print(Matrix m) {
-    for (int i = 0; i < m.rowSize(); i++) {
-      for (int j = 0; j < m.columnSize(); j++) {
-        if (Math.abs(m.get(i, j)) > 1.0e-10) {
-          System.out.printf("%10.3f ", m.get(i, j));
-        } else {
-          System.out.printf("%10s ", (i + j) % 3 == 0 ? "." : "");
-        }
-      }
-      System.out.printf("\n");
-    }
-    System.out.printf("\n");
-  }
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchQualityTest.java mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchQualityTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchQualityTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchQualityTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.neighborhood;
 
 import java.util.Arrays;
@@ -153,7 +170,7 @@
   static class StripWeight implements Function<WeightedThing<Vector>, Vector> {
     @Override
     public Vector apply(WeightedThing<Vector> input) {
-      Preconditions.checkArgument(input != null);
+      Preconditions.checkArgument(input != null, "input is null");
       //noinspection ConstantConditions
       return input.getValue();
     }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchSanityTest.java mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchSanityTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchSanityTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/neighborhood/SearchSanityTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,12 +17,15 @@
 
 package org.apache.mahout.math.neighborhood;
 
+
 import java.util.Arrays;
 import java.util.List;
 
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
+
 import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
 import org.apache.mahout.math.DenseMatrix;
 import org.apache.mahout.math.DenseVector;
@@ -36,6 +39,9 @@
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.lessThanOrEqualTo;
+
 @RunWith(Parameterized.class)
 public class SearchSanityTest extends MahoutTestCase {
   private static final int NUM_DATA_POINTS = 1 << 13;
@@ -57,6 +63,7 @@
 
   @Parameterized.Parameters
   public static List<Object[]> generateData() {
+    RandomUtils.useTestSeed();
     Matrix dataPoints = multiNormalRandomData(NUM_DATA_POINTS, NUM_DIMENSIONS);
     return Arrays.asList(new Object[][]{
         {new ProjectionSearch(new EuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), dataPoints},
@@ -205,6 +212,17 @@
   }
 
   @Test
+  public void testSearchLimiting() {
+    searcher.clear();
+    searcher.addAll(dataPoints);
+    for (Vector datapoint : dataPoints) {
+      List<WeightedThing<Vector>> firstTwo = searcher.search(datapoint, 2);
+
+      assertThat("Search limit isn't respected", firstTwo.size(), is(lessThanOrEqualTo(2)));
+    }
+  }
+
+  @Test
   public void testRemove() {
     searcher.clear();
     for (int i = 0; i < dataPoints.rowSize(); ++i) {
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/ConditionalEntropyTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,80 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import java.util.Iterator;
-
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
-import org.junit.Test;
-
-@Deprecated
-public final class ConditionalEntropyTest extends MahoutTestCase {
-
-  @Test
-  public void testConditionalEntropy() throws Exception {
-
-    Configuration configuration = getConfiguration();
-    Path input = getTestTempFilePath("input");
-    Path output = getTestTempFilePath("output");
-    FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
-
-    // create input
-    String[] keys = { "Math", "History", "CS", "Math", "Math", "CS", "History", "Math" };
-    String[] values = { "Yes", "No", "Yes", "No", "No", "Yes", "No", "Yes" };
-    SequenceFile.Writer writer = new SequenceFile.Writer(fileSystem, configuration, input, Text.class, Text.class);
-    try {
-      for (int i = 0; i < keys.length; i++) {
-        writer.append(new Text(keys[i]), new Text(values[i]));
-      }
-    } finally {
-      Closeables.close(writer, false);
-    }
-
-    // run the job
-    Tool job = new ConditionalEntropy();
-    String[] args = { "-i", input.toString(), "-o", output.toString(),
-        "--tempDir", getTestTempDirPath("tmp").toString() };
-    ToolRunner.run(configuration, job, args);
-
-    // check the output
-    Iterator<DoubleWritable> iteratorNodes =
-        new SequenceFileDirValueIterator<DoubleWritable>(output,
-                                                         PathType.LIST,
-                                                         PathFilters.logsCRCFilter(),
-                                                         null,
-                                                         false,
-                                                         getConfiguration());
-    while (iteratorNodes.hasNext()) {
-      assertEquals(0.5, iteratorNodes.next().get(), EPSILON);
-    }
-
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/EntropyTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,95 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import java.util.Iterator;
-
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator;
-import org.junit.Test;
-
-@Deprecated
-public final class EntropyTest extends MahoutTestCase {
-
-  @Test
-  public void testLetters() throws Exception {
-    String[] content = { "A", "A", "A", "A", "A", "B", "B", "C", "D", "E" };
-    calculateEntropy(content, 1.96096405, "key");
-  }
-
-  @Test
-  public void testYN() throws Exception {
-    String[] content = { "Yes", "No", "Yes", "No", "No", "Yes", "No", "Yes" };
-    calculateEntropy(content, 1.0, "value");
-  }
-
-  private void calculateEntropy(String[] content, double expected, String source) throws Exception {
-
-    Configuration configuration = getConfiguration();
-    Path input = getTestTempFilePath("input");
-    Path output = getTestTempFilePath("output");
-    FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
-
-    // write content into test text file
-    SequenceFile.Writer writer = new SequenceFile.Writer(fileSystem, configuration, input, Text.class, Text.class);
-    Writable empty = new Text();
-    try {
-      for (String item : content) {
-        if ("key".equals(source)) {
-          writer.append(new Text(item), empty);
-        } else {
-          writer.append(empty, new Text(item));
-        }
-
-      }
-    } finally {
-      Closeables.close(writer, false);
-    }
-
-    // run the job
-    String[] args = { "-i", input.toString(), "-o", output.toString(), "-s", source,
-        "--tempDir", getTestTempDirPath("tmp").toString() };
-    Entropy job = new Entropy();
-    ToolRunner.run(configuration, job, args);
-
-    assertEquals(content.length, job.getNumberItems());
-
-    // check output
-    Iterator<DoubleWritable> iteratorNodes =
-        new SequenceFileDirValueIterator<DoubleWritable>(output,
-                                                         PathType.LIST,
-                                                         PathFilters.logsCRCFilter(),
-                                                         null,
-                                                         false,
-                                                         getConfiguration());
-    assertTrue(iteratorNodes.hasNext());
-    assertEquals(expected, iteratorNodes.next().get(), EPSILON);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainRatioTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.MahoutTestCase;
-import org.junit.Test;
-
-@Deprecated
-public final class InformationGainRatioTest extends MahoutTestCase {
-
-  @Test
-  public void testInformationGain() throws Exception {
-
-    Configuration configuration = getConfiguration();
-    Path input = getTestTempFilePath("input");
-    FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
-
-    // create input
-    String[] keys = { "Math", "History", "CS", "Math", "Math", "CS", "History", "Math" };
-    String[] values = { "Yes", "No", "Yes", "No", "No", "Yes", "No", "Yes" };
-    SequenceFile.Writer writer = new SequenceFile.Writer(fileSystem, configuration, input, Text.class, Text.class);
-    try {
-      for (int i = 0; i < keys.length; i++) {
-        writer.append(new Text(keys[i]), new Text(values[i]));
-      }
-    } finally {
-      Closeables.close(writer, false);
-    }
-
-    // run the job
-    InformationGainRatio job = new InformationGainRatio();
-    String[] args = { "-i", input.toString(), "--tempDir", getTestTempDirPath("tmp").toString() };
-    ToolRunner.run(configuration, job, args);
-
-    // check the output
-    assertEquals(1.0, job.getEntropy(), EPSILON);
-    assertEquals(0.5, job.getInformationGain(), EPSILON);
-    assertEquals(0.5, job.getInformationGainRatio(), EPSILON);
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java
--- mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/math/stats/entropy/InformationGainTest.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.math.stats.entropy;
-
-import com.google.common.io.Closeables;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.common.MahoutTestCase;
-import org.junit.Test;
-
-@Deprecated
-public final class InformationGainTest extends MahoutTestCase {
-
-  @Test
-  public void testInformationGain() throws Exception {
-
-    Configuration configuration = getConfiguration();
-    Path input = getTestTempFilePath("input");
-    FileSystem fileSystem = FileSystem.get(input.toUri(), configuration);
-
-    // create input
-    String[] keys = { "Math", "History", "CS", "Math", "Math", "CS", "History", "Math" };
-    String[] values = { "Yes", "No", "Yes", "No", "No", "Yes", "No", "Yes" };
-    SequenceFile.Writer writer = new SequenceFile.Writer(fileSystem, configuration, input, Text.class, Text.class);
-    try {
-      for (int i = 0; i < keys.length; i++) {
-        writer.append(new Text(keys[i]), new Text(values[i]));
-      }
-    } finally {
-      Closeables.close(writer, false);
-    }
-
-    // run the job
-    InformationGain job = new InformationGain();
-    String[] args = { "-i", input.toString(), "--tempDir", getTestTempDirPath("tmp").toString() };
-    ToolRunner.run(configuration, job, args);
-
-    // check the output
-    assertEquals(1.0, job.getEntropy(), EPSILON);
-    assertEquals(0.5, job.getConditionalEntropy(), EPSILON);
-    assertEquals(0.5, job.getInformationGain(), EPSILON);
-
-  }
-
-}
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/DictionaryVectorizerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,6 +20,8 @@
 import java.io.IOException;
 import java.util.List;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
 import com.google.common.io.Closeables;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -45,6 +47,7 @@
 /**
  * Test the dictionary Vector
  */
+@ThreadLeakScope(ThreadLeakScope.Scope.NONE)
 public final class DictionaryVectorizerTest extends MahoutTestCase {
 
   private static final int NUM_DOCS = 100;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/DocumentProcessorTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -19,6 +19,7 @@
 
 import java.util.Arrays;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
 import com.google.common.io.Closeables;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -38,6 +39,7 @@
  * by the {@link DocumentProcessor} into {@link SequenceFile}s of document ID and tokens (as
  * {@link StringTuple}).
  */
+@ThreadLeakScope(ThreadLeakScope.Scope.NONE)
 public class DocumentProcessorTest extends MahoutTestCase {
 
   @Test
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/EncodedVectorsFromSequenceFilesTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,6 @@
 
 package org.apache.mahout.vectorizer;
 
-import java.util.LinkedList;
 import java.util.List;
 
 import com.google.common.collect.Lists;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/HighDFWordsPrunerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -16,6 +16,7 @@
  * limitations under the License.
  */
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
 import com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -37,9 +38,9 @@
 import org.junit.Test;
 
 import java.util.Arrays;
-import java.util.LinkedList;
 import java.util.List;
 
+@ThreadLeakScope(ThreadLeakScope.Scope.NONE)
 public class HighDFWordsPrunerTest extends MahoutTestCase {
   private static final int NUM_DOCS = 100;
 
@@ -94,8 +95,6 @@
     argList.add(inputPath.toString());
     argList.add("-o");
     argList.add(outputPath.toString());
-    argList.add("--mapred");
-    argList.add(getTestTempDir("mapred" + Math.random()).getAbsolutePath());
     if (prune) {
       argList.add("-xs");
       argList.add("3"); // we prune all words that are outside 3*sigma
@@ -109,7 +108,7 @@
 
     String[] args = argList.toArray(new String[argList.size()]);
 
-    ToolRunner.run(getConfiguration(), new SparseVectorsFromSequenceFiles(), args);
+    ToolRunner.run(conf, new SparseVectorsFromSequenceFiles(), args);
 
     Path dictionary = new Path(outputPath, "dictionary.file-0");
     Path tfVectors = new Path(outputPath, "tf-vectors");
@@ -136,7 +135,8 @@
     return highDFWordsDictionaryIndices;
   }
 
-  private void validateVectors(Path vectorPath, int[] highDFWordsDictionaryIndices, boolean prune) {
+  private void validateVectors(Path vectorPath, int[] highDFWordsDictionaryIndices, boolean prune) throws Exception {
+    assertTrue("Path does not exist", vectorPath.getFileSystem(conf).exists(vectorPath));
     for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(vectorPath, PathType.LIST, PathFilters
             .partFilter(), null, true, conf)) {
       Vector v = ((NamedVector) value.get()).getDelegate();
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFilesTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,9 +18,9 @@
 package org.apache.mahout.vectorizer;
 
 import java.io.IOException;
-import java.util.LinkedList;
 import java.util.List;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
 import com.google.common.collect.Lists;
 import com.google.common.io.Closeables;
 import org.apache.hadoop.conf.Configuration;
@@ -37,6 +37,7 @@
 import org.apache.mahout.math.VectorWritable;
 import org.junit.Test;
 
+@ThreadLeakScope(ThreadLeakScope.Scope.NONE)
 public class SparseVectorsFromSequenceFilesTest extends MahoutTestCase {
 
   private static final int NUM_DOCS = 100;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocMapperTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocMapperTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocMapperTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocMapperTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -72,7 +72,7 @@
                           {"t_of", "worst of"},};
     // set up expectations for mocks. ngram max size = 2
     
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CollocMapper.MAX_SHINGLE_SIZE, "2");
     EasyMock.expect(context.getConfiguration()).andReturn(conf);
     
@@ -134,7 +134,7 @@
                                          {"u_times", "times"},};
 
     // set up expectations for mocks. ngram max size = 2
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(CollocMapper.MAX_SHINGLE_SIZE, "2");
     conf.setBoolean(CollocDriver.EMIT_UNIGRAMS, true);
     EasyMock.expect(context.getConfiguration()).andReturn(conf);
@@ -177,18 +177,4 @@
     
     EasyMock.verify(context);
   }
-  
-  /** A lucene 2.9 standard analyzer with no stopwords. */
- /* public static class TestAnalyzer extends DefaultAnalyzer {
-    private final Analyzer a;
-    
-    public TestAnalyzer() {
-      a = new StandardAnalyzer(Version.LUCENE_40, Collections.emptySet());
-    }
-    
-    @Override
-    public TokenStream tokenStream(String arg0, Reader arg1) {
-      return a.tokenStream(arg0, arg1);
-    }
- }*/
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocReducerTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocReducerTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocReducerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/CollocReducerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -19,7 +19,6 @@
 
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.LinkedList;
 
 import com.google.common.collect.Lists;
 import org.apache.hadoop.mapreduce.Reducer;
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/LLRReducerTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/LLRReducerTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/LLRReducerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/collocations/llr/LLRReducerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -19,7 +19,6 @@
 
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.LinkedList;
 
 import com.google.common.collect.Lists;
 import org.apache.hadoop.conf.Configuration;
@@ -91,7 +90,7 @@
                             {1, 0, 1, 5}  // worst of
     };
     
-    Configuration config = new Configuration();
+    Configuration config = getConfiguration();
     config.set(LLRReducer.NGRAM_TOTAL, "7");
     EasyMock.expect(context.getConfiguration()).andReturn(config);
     
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoderTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoderTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/InteractionValueEncoderTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,12 @@
 
 package org.apache.mahout.vectorizer.encoders;
 
+import java.util.Map;
+import java.util.Set;
+
+import static com.google.common.collect.Iterables.getFirst;
+
+import com.google.common.collect.Maps;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.Vector;
@@ -73,5 +79,25 @@
     // should interact "a" with each of "some","text" and "here"
     assertEquals((float) k*3, v1.norm(1), 0);
   }
+  
+  @Test
+  public void testTraceDictionary() {
+    StaticWordValueEncoder encoder1 = new StaticWordValueEncoder("first");
+    StaticWordValueEncoder encoder2 = new StaticWordValueEncoder("second");
+    
+    Map<String, Set<Integer>> traceDictionary = Maps.newHashMap();
+
+    InteractionValueEncoder interactions = new InteractionValueEncoder("interactions", encoder1, encoder2);
+    interactions.setProbes(1);
+    interactions.setTraceDictionary(traceDictionary);
+    
+    Vector v = new DenseVector(10);
+    interactions.addInteractionToVector("a", "b", 1, v);
+    
+    assertEquals(1, v.getNumNonZeroElements());
+    assertEquals(1, traceDictionary.size());
+    assertEquals("interactions=a:b", getFirst(traceDictionary.keySet(), null));
+
+  }
 
 }
diff -uNar -x .git mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/TextValueEncoderTest.java mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/TextValueEncoderTest.java
--- mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/TextValueEncoderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/core/src/test/java/org/apache/mahout/vectorizer/encoders/TextValueEncoderTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -70,7 +70,7 @@
   @Test
   public void testLuceneEncoding() throws Exception {
     LuceneTextValueEncoder enc = new LuceneTextValueEncoder("text");
-    enc.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_43));
+    enc.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_46));
     Vector v1 = new DenseVector(200);
     enc.addToVector("test1 and more", v1);
     enc.flush(1, v1);
diff -uNar -x .git mahout/distribution/KEYS mahout/distribution/KEYS
--- mahout/distribution/KEYS	2014-03-29 01:04:48.000000000 -0700
+++ mahout/distribution/KEYS	2014-03-29 01:03:14.000000000 -0700
@@ -826,4 +826,63 @@
 aQNM1OGlRMbmORowHGkymnSLoj3UnifvlPMEDSdK69xSHYrB4JwPrzl1Q2ebXriF
 P2Zv+KoST7zypBalLlnh
 =Bwm8
------END PGP PUBLIC KEY BLOCK-----
\ No newline at end of file
+-----END PGP PUBLIC KEY BLOCK-----
+pub   4096R/D3541808 2014-01-09
+uid                  Suneel Marthi (CODE SIGNING KEY) <smarthi@apache.org>
+sig 3        D3541808 2014-01-09  Suneel Marthi (CODE SIGNING KEY) <smarthi@apache.org>
+sub   4096R/AF46E2DE 2014-01-09
+sig          D3541808 2014-01-09  Suneel Marthi (CODE SIGNING KEY) <smarthi@apache.org>
+
+-----BEGIN PGP PUBLIC KEY BLOCK-----
+Version: GnuPG/MacGPG2 v2.0.22 (Darwin)
+Comment: GPGTools - https://gpgtools.org
+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+=f8kJ
+-----END PGP PUBLIC KEY BLOCK-----
diff -uNar -x .git mahout/distribution/pom.xml mahout/distribution/pom.xml
--- mahout/distribution/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/distribution/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -20,7 +20,7 @@
   <parent>
     <groupId>org.apache.mahout</groupId>
     <artifactId>mahout</artifactId>
-    <version>0.8</version>
+    <version>1.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
   <artifactId>mahout-distribution</artifactId>
diff -uNar -x .git mahout/distribution/src/main/assembly/bin.xml mahout/distribution/src/main/assembly/bin.xml
--- mahout/distribution/src/main/assembly/bin.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/distribution/src/main/assembly/bin.xml	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
 <assembly xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
 	  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
 	  xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd">
@@ -5,7 +22,6 @@
   <formats>
     <format>dir</format>
     <format>tar.gz</format>
-    <format>tar.bz2</format>
     <format>zip</format>
   </formats>
 
@@ -18,6 +34,7 @@
       <excludes>
         <exclude>mahout-*</exclude>
         <exclude>hadoop-*</exclude>
+		<exclude>junit-*</exclude>
       </excludes>
       <outputDirectory>lib</outputDirectory>
     </fileSet>
@@ -117,7 +134,7 @@
         <include>**/*.properties</include>
       </includes>
       <excludes>
-        <exclude>target/**</exclude>
+        <exclude>**/target/**</exclude>
       </excludes>
     </fileSet>
     <fileSet>
@@ -127,6 +144,7 @@
         <include>mahout</include>
       </includes>
       <fileMode>0755</fileMode>
+      <directoryMode>0755</directoryMode>
     </fileSet>
     <fileSet>
       <directory>${project.basedir}/../src/conf</directory>
@@ -138,6 +156,7 @@
       <directory>${project.basedir}/../examples/bin</directory>
       <outputDirectory>examples/bin</outputDirectory>
       <fileMode>0755</fileMode>
+      <directoryMode>0755</directoryMode>
       <excludes>
         <exclude>work</exclude>
         <exclude>work/**</exclude>
diff -uNar -x .git mahout/distribution/src/main/assembly/src.xml mahout/distribution/src/main/assembly/src.xml
--- mahout/distribution/src/main/assembly/src.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/distribution/src/main/assembly/src.xml	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
 <assembly xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd">
@@ -5,7 +22,6 @@
   <formats>
     <format>dir</format>
     <format>tar.gz</format>
-    <format>tar.bz2</format>
     <format>zip</format>
   </formats>
   <fileSets>
@@ -24,7 +40,7 @@
         <include>**/*.properties</include>
       </includes>
       <excludes>
-        <exclude>target/**</exclude>
+        <exclude>**/target/**</exclude>
       </excludes>
     </fileSet>
     <fileSet>
@@ -32,12 +48,14 @@
       <outputDirectory>bin</outputDirectory>
       <useDefaultExcludes>true</useDefaultExcludes>
       <fileMode>0755</fileMode>
+      <directoryMode>0755</directoryMode>
     </fileSet>
     <fileSet>
       <directory>${project.basedir}/../examples/bin</directory>
       <outputDirectory>examples/bin</outputDirectory>
       <useDefaultExcludes>true</useDefaultExcludes>
       <fileMode>0755</fileMode>
+      <directoryMode>0755</directoryMode>
       <excludes>
         <exclude>work</exclude>
         <exclude>work/**</exclude>
diff -uNar -x .git mahout/doap_Mahout.rdf mahout/doap_Mahout.rdf
--- mahout/doap_Mahout.rdf	2014-03-29 01:04:48.000000000 -0700
+++ mahout/doap_Mahout.rdf	2014-03-29 01:03:14.000000000 -0700
@@ -29,8 +29,8 @@
     <asfext:pmc rdf:resource="http://mahout.apache.org" />
     <shortdesc>Scalable machine learning library</shortdesc>
     <bug-database rdf:resource="https://issues.apache.org/jira/browse/MAHOUT" />
-    <mailing-list rdf:resource="https://cwiki.apache.org/confluence/display/MAHOUT/Mailing+Lists" />
-    <download-page rdf:resource="https://cwiki.apache.org/confluence/display/MAHOUT/Downloads" />
+    <mailing-list rdf:resource="https://mahout.apache.org/general/mailing-lists,-irc-and-archives.html" />
+    <download-page rdf:resource="http://mahout.apache.org/general/downloads.html" />
     <programming-language>Java</programming-language>
     <category rdf:resource="http://projects.apache.org/category/library" />
     <release>
diff -uNar -x .git mahout/examples/bin/README.txt mahout/examples/bin/README.txt
--- mahout/examples/bin/README.txt	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/README.txt	2014-03-29 01:03:14.000000000 -0700
@@ -2,16 +2,9 @@
 
 Here's a description of what each does:
 
-asf-email-examples.sh -- Recommend, classify and cluster the ASF Email Public Dataset, as hosted on Amazon (http://aws.amazon.com/datasets/7791434387204566).  Requires download.  Demonstrates a number of Mahout algorithms.
 classify-20newsgroups.sh -- Run SGD and Bayes classifiers over the classic 20 News Groups.  Downloads the data set automatically.
 cluster-reuters.sh -- Cluster the Reuters data set using a variety of algorithms.  Downloads the data set automatically.
 cluster-syntheticcontrol.sh -- Cluster the Synthetic Control data set.  Downloads the data set automatically.
 factorize-movielens-1m.sh -- Run the Alternating Least Squares Recommender on the Grouplens data set (size 1M).
 factorize-netflix.sh -- Run the ALS Recommender on the Netflix data set
-
-
-If you are looking for the build-* scripts (build-asf-email.sh, build-reuters.sh), they have been renamed to better signify what they do.  See https://issues.apache.org/jira/browse/MAHOUT-868 for more information.  These have been renamed to:
-
-build-asf-email.sh -> asf-email-examples.sh
-build-cluster-syntheticcontrol.sh -> cluster-syntheticcontrol.sh
-build-reuters.sh -> cluster-reuters.sh
+run-rf.sh -- Create some synthetic data, build a random forest, and test performance.
\ No newline at end of file
diff -uNar -x .git mahout/examples/bin/asf-email-examples.sh mahout/examples/bin/asf-email-examples.sh
--- mahout/examples/bin/asf-email-examples.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/asf-email-examples.sh	1969-12-31 16:00:00.000000000 -0800
@@ -1,262 +0,0 @@
-#!/bin/bash
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# You will need to download or otherwise obtain some or all of the Amazon ASF Email Public Dataset (http://aws.amazon.com/datasets/7791434387204566) to use this script.
-# To obtain a full copy you will need to launch an EC2 instance and mount the dataset to download it, otherwise you can get a sample of it at
-# http://www.lucidimagination.com/devzone/technical-articles/scaling-mahout
-# Also, see http://www.ibm.com/developerworks/java/library/j-mahout-scaling/ for more info
-
-function fileExists() {
-  if ([ "$MAHOUT_LOCAL" != "" ] && [ ! -e "$1" ]) || ([ "$MAHOUT_LOCAL" == "" ] && ! hadoop fs -test -e /user/$USER/$1); then
-    return 1 # file doesn't exist
-  else
-    return 0 # file exists
-  fi
-}
-
-function removeFolder() {
-  if [ "$MAHOUT_LOCAL" == "" ]; then
-    rm -rf $1
-  else
-    if fileExists "$1"; then
-      hadoop fs -rmr /user/$USER/$1
-    fi
-  fi	
-}
-
-if [ "$1" = "--help" ] || [ "$1" = "--?" ]; then
-  echo "This script runs recommendation, classification and clustering of the ASF Email Public Dataset, as hosted on Amazon (http://aws.amazon.com/datasets/7791434387204566).  Requires download."
-  exit
-fi
-
-if [ -z "$2" ]; then
-  echo "Usage: asf-email-examples.sh input_path output_path"
-  exit
-fi
-
-SCRIPT_PATH=${0%/*}
-if [ "$0" != "$SCRIPT_PATH" ] && [ "$SCRIPT_PATH" != "" ]; then
-  cd $SCRIPT_PATH
-fi
-START_PATH=`pwd`
-MAHOUT="../../bin/mahout"
-ASF_ARCHIVES=$1
-OUT=$2
-
-algorithm=( recommender clustering classification clean )
-if [ -n "$3" ]; then
-  choice=$3
-else
-  echo "Please select a number to choose the corresponding algorithm to run"
-  echo "1. ${algorithm[0]}"
-  echo "2. ${algorithm[1]}"
-  echo "3. ${algorithm[2]}"
-  echo "4. ${algorithm[3]} -- cleans up the work area -- all files under the work area will be deleted"
-  read -p "Enter your choice : " choice
-fi
-echo "ok. You chose $choice and we'll use ${algorithm[$choice-1]}"
-alg=${algorithm[$choice-1]}
-
-if [ "x$alg" == "xrecommender" ]; then
-  # convert the mail to seq files
-  MAIL_OUT="$OUT/prefs/seq-files"
-  if ! fileExists "$MAIL_OUT/chunk-0"; then
-    echo "Converting Mail files to Sequence Files"
-    $MAHOUT org.apache.mahout.text.SequenceFilesFromMailArchives --charset "UTF-8" --from --references --input $ASF_ARCHIVES --output $MAIL_OUT --separator " ::: "
-  fi
-  PREFS="$OUT/prefs/input"
-  PREFS_TMP="$OUT/prefs/tmp"
-  PREFS_REC_INPUT="$OUT/prefs/input/recInput"
-  RECS_OUT=$"$OUT/prefs/recommendations"
-  # prep for recs
-  if ! fileExists "$PREFS/fromIds-dictionary-0"; then
-    echo "Prepping Sequence files for Recommender"
-    $MAHOUT org.apache.mahout.cf.taste.example.email.MailToPrefsDriver --input $MAIL_OUT --output $PREFS --overwrite --separator " ::: "
-  fi
-  removeFolder "$PREFS_TMP"
-  removeFolder "$RECS_OUT"
-  # run the recs
-  echo "Run the recommender"
-  $MAHOUT recommenditembased --input $PREFS_REC_INPUT --output $RECS_OUT --tempDir $PREFS_TMP --similarityClassname SIMILARITY_LOGLIKELIHOOD
-
-#clustering
-elif [ "x$alg" == "xclustering" ]; then
-  MAIL_OUT="$OUT/clustering/seq-files"
-  SEQ2SP="$OUT/clustering/seq2sparse"
-  algorithm=( kmeans dirichlet minhash )
-
-  if [ -n "$4" ]; then
-    choice=$4
-  else
-    echo "Please select a number to choose the corresponding algorithm to run"
-    echo "1. ${algorithm[0]}"
-    echo "2. ${algorithm[1]}"
-    echo "3. ${algorithm[2]}"
-    read -p "Enter your choice : " choice
-  fi
-
-  echo "ok. You chose $choice and we'll use ${algorithm[$choice-1]}"
-  nbalg=${algorithm[$choice-1]}
-  if [ "x$nbalg" == "xkmeans"  ] || [ "x$nbalg" == "xdirichlet" ]; then
-    if [ -n "$5" ]; then
-      numClusters=$5
-    else
-      echo "How many clusters would you like to generate:"
-      read -p "Enter your choice : " numClusters
-    fi
-  fi
-  if ! fileExists "$MAIL_OUT/chunk-0"; then
-    echo "Converting Mail files to Sequence Files"
-    $MAHOUT org.apache.mahout.text.SequenceFilesFromMailArchives --charset "UTF-8" --subject --body --input $ASF_ARCHIVES --output $MAIL_OUT
-  fi
-
-  #convert to sparse vectors -- use the 2 norm (Euclidean distance) and lop of some of the common terms
-
-  if ! fileExists "$SEQ2SP/dictionary.file-0"; then
-    echo "Converting the files to sparse vectors"
-    $MAHOUT seq2sparse --input $MAIL_OUT --output $SEQ2SP --norm 2 --weight TFIDF --namedVector --maxDFPercent 90 --minSupport 2 --analyzerName org.apache.mahout.text.MailArchivesClusteringAnalyzer
-  fi
-  if [ "x$nbalg" == "xkmeans" ]; then
-    CLUST_OUT="$OUT/clustering/kmeans"
-    echo "Running K-Means with K = $numClusters"
-    $MAHOUT kmeans --input "$SEQ2SP/tfidf-vectors" --output $CLUST_OUT -k $numClusters --maxIter 20 --distanceMeasure org.apache.mahout.common.distance.CosineDistanceMeasure --clustering --method mapreduce --clusters "$CLUST_OUT/clusters"
-  elif [ "x$nbalg" == "xdirichlet"  ]; then
-    CLUST_OUT="$OUT/clustering/dirichlet"
-    echo "Running Dirichlet with K = $numClusters"
-    $MAHOUT dirichlet --input "$SEQ2SP/tfidf-vectors" --output $CLUST_OUT -k $numClusters --maxIter 20 --distanceMeasure org.apache.mahout.common.distance.CosineDistanceMeasure --method mapreduce
-  elif [ "x$nbalg" == "xminhash"  ]; then
-    CLUST_OUT="$OUT/clustering/minhash"
-    echo "Running Minhash"
-    $MAHOUT minhash --input "$SEQ2SP/tfidf-vectors" --output $CLUST_OUT
-  fi
-
-#classification
-elif [ "x$alg" == "xclassification" ]; then
-  algorithm=( standard complementary sgd )
-  echo ""
-  if [ -n "$4" ]; then
-    choice=$4
-  else
-    echo "Please select a number to choose the corresponding algorithm to run"
-    echo "1. ${algorithm[0]}"
-    echo "2. ${algorithm[1]}"
-    echo "3. ${algorithm[2]}"
-    read -p "Enter your choice : " choice
-  fi
-  
-  echo "ok. You chose $choice and we'll use ${algorithm[$choice-1]}"
-  classAlg=${algorithm[$choice-1]}
-
-  if [ "x$classAlg" == "xsgd"  ]; then
-    if [ -n "$5" ]; then
-      numLabels=$5
-    else
-      echo "How many labels/projects are there in the data set:"
-      read -p "Enter your choice : " numLabels
-    fi
-  fi
-  #Convert mail to be formatted as:
-  # label\ttext
-  # One per line
-  # the label is the project_name_mailing_list, as in tomcat.apache.org_dev
-  #Convert to vectors
-  if [ "x$classAlg" == "xstandard" ] || [ "x$classAlg" == "xcomplementary" ]; then
-	set -x
-    CLASS="$OUT/classification/bayesian"
-    MAIL_OUT="$CLASS/seq-files"
-    SEQ2SP="$CLASS/seq2sparse"
-    SPLIT="$CLASS/splits"
-    TRAIN="$SPLIT/train"
-    TEST="$SPLIT/test"
-    TEST_OUT="$CLASS/test-results"
-    LABEL="$SPLIT/labels"
-    if ! fileExists "$MAIL_OUT/chunk-0"; then
-      echo "Converting Mail files to Sequence Files"
-      $MAHOUT org.apache.mahout.text.SequenceFilesFromMailArchives --charset "UTF-8" --subject --body --input $ASF_ARCHIVES --output $MAIL_OUT -chunk 768 --stripQuoted
-    fi
-    if ! fileExists "$SEQ2SP/dictionary.file-0"; then
-      echo "Converting the files to sparse vectors"
-      # $MAHOUT seq2sparse --input $MAIL_OUT --output $SEQ2SP --norm 2 --weight TFIDF --namedVector -lnorm --maxDFPercent 90 --minSupport 2 --analyzerName org.apache.mahout.text.MailArchivesClusteringAnalyzer -chunk 1000
-      $MAHOUT seq2encoded --input $MAIL_OUT --output $SEQ2SP --analyzerName org.apache.mahout.text.MailArchivesClusteringAnalyzer --cardinality 100000 -ow
-	fi
-    if ! fileExists "$TRAIN/part-m-00000"; then
-      #setup train/test files
-      echo "Creating training and test inputs"
-      $MAHOUT split --input $SEQ2SP --trainingOutput $TRAIN --testOutput $TEST --randomSelectionPct 20 --overwrite --sequenceFiles -xm sequential
-    fi
-    MODEL="$CLASS/model"
-    if [ "x$classAlg" == "xstandard" ]; then
-      echo "Running Standard Training"
-      $MAHOUT trainnb -i $TRAIN -o $MODEL --extractLabels --labelIndex $LABEL --overwrite
-      echo "Running Test"
-      $MAHOUT testnb -i $TEST -o $TEST_OUT -m $MODEL --labelIndex $LABEL --overwrite
-
-    elif [ "x$classAlg" == "xcomplementary"  ]; then
-      echo "Running Complementary Training"
-      $MAHOUT trainnb -i $TRAIN -o $MODEL --extractLabels --labelIndex $LABEL --overwrite --trainComplementary
-      echo "Running Complementary Test"
-      $MAHOUT testnb -i $TEST -o $TEST_OUT -m $MODEL --labelIndex $LABEL --overwrite --testComplementary
-    fi
-  elif [ "x$classAlg" == "xsgd"  ]; then
-    CLASS="$OUT/classification/sgd"
-    MAIL_OUT="$CLASS/seq-files"
-    SEQ2SP="$CLASS/seq2encoded"
-    SEQ2SPLABEL="$CLASS/labeled"
-    SPLIT="$CLASS/splits"
-    TRAIN="$SPLIT/train"
-    TEST="$SPLIT/test"
-    MAPREDOUT="$SPLIT/mapRedOut"
-    TEST_OUT="$CLASS/test-results"
-    MODELS="$CLASS/models"
-    LABEL="$SPLIT/labels"
-    if ! fileExists "$MAIL_OUT/chunk-0"; then
-      echo "Converting Mail files to Sequence Files"
-      $MAHOUT org.apache.mahout.text.SequenceFilesFromMailArchives --charset "UTF-8" --subject --body --input $ASF_ARCHIVES --output $MAIL_OUT --stripQuoted -chunk 768
-    fi
-    echo "Converting the files to sparse vectors in $SEQ2SP"
-    if ! fileExists "$SEQ2SP/part-m-00000"; then
-      $MAHOUT seq2encoded --input $MAIL_OUT --output $SEQ2SP --analyzerName org.apache.mahout.text.MailArchivesClusteringAnalyzer --cardinality 100000
-    fi
-    #We need to modify the vectors to have a better label
-    echo "Converting vector labels"
-    $MAHOUT org.apache.mahout.classifier.email.PrepEmailVectorsDriver --input "$SEQ2SP" --output $SEQ2SPLABEL --overwrite
-    if ! fileExists "$MAPREDOUT/training-r-00000"; then
-      #setup train/test files
-      echo "Creating training and test inputs from $SEQ2SPLABEL"
-      $MAHOUT split --input $SEQ2SPLABEL --mapRedOutputDir $MAPREDOUT  --randomSelectionPct 20 --overwrite --sequenceFiles --method mapreduce
-    fi
-    MODEL="$MODELS/asf.model"
-
-
-    echo "Running SGD Training"
-    $MAHOUT org.apache.mahout.classifier.sgd.TrainASFEmail -i $MAPREDOUT/ -o $MODELS --categories $numLabels --cardinality 100000
-    echo "Running Test"
-    $MAHOUT org.apache.mahout.classifier.sgd.TestASFEmail --input $MAPREDOUT/ --model $MODEL
-
-  fi
-elif [ "x$alg" == "xclean" ]; then
-  echo "Are you sure you really want to remove all files under $OUT:"
-  read -p "Enter your choice (y/n): " answer
-  if [ "x$answer" == "xy" ] || [ "x$answer" == "xY" ]; then
-    echo "Cleaning out $OUT";
-	removeFolder "$OUT"
-  fi
-fi
-
-
diff -uNar -x .git mahout/examples/bin/build-asf-email.sh mahout/examples/bin/build-asf-email.sh
--- mahout/examples/bin/build-asf-email.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/build-asf-email.sh	1969-12-31 16:00:00.000000000 -0800
@@ -1,28 +0,0 @@
-#!/bin/bash
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-echo "Please call asf-email-examples.sh directly next time, as this file is deprecated"
-SCRIPT_PATH=${0%/*}
-if [ "$0" != "$SCRIPT_PATH" ] && [ "$SCRIPT_PATH" != "" ]; then
-  cd $SCRIPT_PATH
-fi
-START_PATH=`pwd`
-
-./asf-email-examples.sh $@
-
-
diff -uNar -x .git mahout/examples/bin/build-cluster-syntheticcontrol.sh mahout/examples/bin/build-cluster-syntheticcontrol.sh
--- mahout/examples/bin/build-cluster-syntheticcontrol.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/build-cluster-syntheticcontrol.sh	1969-12-31 16:00:00.000000000 -0800
@@ -1,33 +0,0 @@
-#!/bin/bash
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# Downloads the Synthetic control dataset and prepares it for clustering
-#
-# To run:  change into the mahout directory and type:
-#  examples/bin/cluster-syntheticcontrol.sh
-
-echo "Please call cluster-syntheticcontrol.sh directly next time.  This file is going away."
-
-SCRIPT_PATH=${0%/*}
-if [ "$0" != "$SCRIPT_PATH" ] && [ "$SCRIPT_PATH" != "" ]; then
-  cd $SCRIPT_PATH
-fi
-START_PATH=`pwd`
-
-./cluster-syntheticcontrol.sh $@
diff -uNar -x .git mahout/examples/bin/build-reuters.sh mahout/examples/bin/build-reuters.sh
--- mahout/examples/bin/build-reuters.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/build-reuters.sh	1969-12-31 16:00:00.000000000 -0800
@@ -1,25 +0,0 @@
-#!/bin/bash
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# Downloads the Reuters dataset and prepares it for clustering
-#
-# To run:  change into the mahout directory and type:
-#  examples/bin/build-reuters.sh
-echo "Please call cluster-reuters.sh directly next time.  This file is going away."
-./cluster-reuters.sh 
diff -uNar -x .git mahout/examples/bin/classify-20newsgroups.sh mahout/examples/bin/classify-20newsgroups.sh
--- mahout/examples/bin/classify-20newsgroups.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/classify-20newsgroups.sh	2014-03-29 01:03:14.000000000 -0700
@@ -33,6 +33,14 @@
 fi
 START_PATH=`pwd`
 
+if [ "$HADOOP_HOME" != "" ] && [ "$MAHOUT_LOCAL" == "" ] ; then
+  HADOOP="$HADOOP_HOME/bin/hadoop"
+  if [ ! -e $HADOOP ]; then
+    echo "Can't find hadoop in $HADOOP, exiting"
+    exit 1
+  fi
+fi
+
 WORK_DIR=/tmp/mahout-work-${USER}
 algorithm=( cnaivebayes naivebayes sgd clean)
 if [ -n "$1" ]; then
@@ -49,18 +57,20 @@
 echo "ok. You chose $choice and we'll use ${algorithm[$choice-1]}"
 alg=${algorithm[$choice-1]}
 
-echo "creating work directory at ${WORK_DIR}"
+if [ "x$alg" != "xclean" ]; then
+  echo "creating work directory at ${WORK_DIR}"
 
-mkdir -p ${WORK_DIR}
-if [ ! -e ${WORK_DIR}/20news-bayesinput ]; then
-  if [ ! -e ${WORK_DIR}/20news-bydate ]; then
-    if [ ! -f ${WORK_DIR}/20news-bydate.tar.gz ]; then
-      echo "Downloading 20news-bydate"
-      curl http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz -o ${WORK_DIR}/20news-bydate.tar.gz
+  mkdir -p ${WORK_DIR}
+  if [ ! -e ${WORK_DIR}/20news-bayesinput ]; then
+    if [ ! -e ${WORK_DIR}/20news-bydate ]; then
+      if [ ! -f ${WORK_DIR}/20news-bydate.tar.gz ]; then
+        echo "Downloading 20news-bydate"
+        curl http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz -o ${WORK_DIR}/20news-bydate.tar.gz
+      fi
+      mkdir -p ${WORK_DIR}/20news-bydate
+      echo "Extracting..."
+      cd ${WORK_DIR}/20news-bydate && tar xzf ../20news-bydate.tar.gz && cd .. && cd ..
     fi
-    mkdir -p ${WORK_DIR}/20news-bydate
-    echo "Extracting..."
-    cd ${WORK_DIR}/20news-bydate && tar xzf ../20news-bydate.tar.gz && cd .. && cd ..
   fi
 fi
 #echo $START_PATH
@@ -82,6 +92,14 @@
   mkdir ${WORK_DIR}/20news-all
   cp -R ${WORK_DIR}/20news-bydate/*/* ${WORK_DIR}/20news-all
 
+  if [ "$HADOOP_HOME" != "" ] && [ "$MAHOUT_LOCAL" == "" ] ; then
+    echo "Copying 20newsgroups data to HDFS"
+    set +e
+    $HADOOP dfs -rmr ${WORK_DIR}/20news-all
+    set -e
+    $HADOOP dfs -put ${WORK_DIR}/20news-all ${WORK_DIR}/20news-all
+  fi
+
   echo "Creating sequence files from 20newsgroups data"
   ./bin/mahout seqdirectory \
     -i ${WORK_DIR}/20news-all \
diff -uNar -x .git mahout/examples/bin/cluster-reuters.sh mahout/examples/bin/cluster-reuters.sh
--- mahout/examples/bin/cluster-reuters.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/cluster-reuters.sh	2014-03-29 01:03:14.000000000 -0700
@@ -39,7 +39,7 @@
   exit 1
 fi
 
-algorithm=( kmeans fuzzykmeans dirichlet lda minhash)
+algorithm=( kmeans fuzzykmeans lda streamingkmeans)
 if [ -n "$1" ]; then
   choice=$1
 else
@@ -47,8 +47,7 @@
   echo "1. ${algorithm[0]} clustering"
   echo "2. ${algorithm[1]} clustering"
   echo "3. ${algorithm[2]} clustering"
-  echo "4. ${algorithm[3]} clustering" 
-  echo "5. ${algorithm[4]} clustering"
+  echo "4. ${algorithm[3]} clustering"
   read -p "Enter your choice : " choice
 fi
 
@@ -103,7 +102,7 @@
     fi
   fi
   echo "Converting to Sequence Files from Directory"
-  $MAHOUT seqdirectory -i ${WORK_DIR}/reuters-out -o ${WORK_DIR}/reuters-out-seqdir -c UTF-8 -chunk 5
+  $MAHOUT seqdirectory -i ${WORK_DIR}/reuters-out -o ${WORK_DIR}/reuters-out-seqdir -c UTF-8 -chunk 64 -xm sequential
 fi
 
 if [ "x$clustertype" == "xkmeans" ]; then
@@ -145,25 +144,6 @@
     -dt sequencefile -b 100 -n 20 -sp 0 \
     && \
   cat ${WORK_DIR}/reuters-fkmeans/clusterdump
-elif [ "x$clustertype" == "xdirichlet" ]; then
-  $MAHOUT seq2sparse \
-    -i ${WORK_DIR}/reuters-out-seqdir/ \
-    -o ${WORK_DIR}/reuters-out-seqdir-sparse-dirichlet  --maxDFPercent 85 --namedVector \
-  && \
-  $MAHOUT dirichlet \
-    -i ${WORK_DIR}/reuters-out-seqdir-sparse-dirichlet/tfidf-vectors \
-    -o ${WORK_DIR}/reuters-dirichlet -k 20 -ow -x 20 -a0 2 \
-    -md org.apache.mahout.clustering.dirichlet.models.DistanceMeasureClusterDistribution \
-    -mp org.apache.mahout.math.DenseVector \
-    -dm org.apache.mahout.common.distance.CosineDistanceMeasure \
-  && \
-  $MAHOUT clusterdump \
-    -i ${WORK_DIR}/reuters-dirichlet/clusters-*-final \
-    -o ${WORK_DIR}/reuters-dirichlet/clusterdump \
-    -d ${WORK_DIR}/reuters-out-seqdir-sparse-dirichlet/dictionary.file-0 \
-    -dt sequencefile -b 100 -n 20 -sp 0 \
-    && \
-  cat ${WORK_DIR}/reuters-dirichlet/clusterdump
 elif [ "x$clustertype" == "xlda" ]; then
   $MAHOUT seq2sparse \
     -i ${WORK_DIR}/reuters-out-seqdir/ \
@@ -190,14 +170,27 @@
     -dt sequencefile -sort ${WORK_DIR}/reuters-lda-topics/part-m-00000 \
     && \
   cat ${WORK_DIR}/reuters-lda/vectordump
-elif [ "x$clustertype" == "xminhash" ]; then
+elif [ "x$clustertype" == "xstreamingkmeans" ]; then
   $MAHOUT seq2sparse \
     -i ${WORK_DIR}/reuters-out-seqdir/ \
-    -o ${WORK_DIR}/reuters-out-seqdir-sparse-minhash --maxDFPercent 85 --namedVector \
+    -o ${WORK_DIR}/reuters-out-seqdir-sparse-streamingkmeans -ow --maxDFPercent 85 --namedVector \
   && \
-  $MAHOUT org.apache.mahout.clustering.minhash.MinHashDriver \
-    -i ${WORK_DIR}/reuters-out-seqdir-sparse-minhash/tfidf-vectors \
-    -o ${WORK_DIR}/reuters-minhash --overwrite
+  rm -rf ${WORK_DIR}/reuters-streamingkmeans \
+  && \
+  $MAHOUT streamingkmeans \
+    -i ${WORK_DIR}/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors/ \
+    --tempDir ${WORK_DIR}/tmp \
+    -o ${WORK_DIR}/reuters-streamingkmeans \
+    -sc org.apache.mahout.math.neighborhood.FastProjectionSearch \
+    -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure \
+    -k 10 -km 100 -ow \
+  && \
+  $MAHOUT qualcluster \
+    -i ${WORK_DIR}/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors/part-r-00000 \
+    -c ${WORK_DIR}/reuters-streamingkmeans/part-r-00000   \
+    -o ${WORK_DIR}/reuters-cluster-distance.csv \
+    && \
+  cat ${WORK_DIR}/reuters-cluster-distance.csv
 else 
   echo "unknown cluster type: $clustertype"
 fi 
diff -uNar -x .git mahout/examples/bin/cluster-syntheticcontrol.sh mahout/examples/bin/cluster-syntheticcontrol.sh
--- mahout/examples/bin/cluster-syntheticcontrol.sh	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/bin/cluster-syntheticcontrol.sh	2014-03-29 01:03:14.000000000 -0700
@@ -27,7 +27,7 @@
   exit
 fi
 
-algorithm=( canopy kmeans fuzzykmeans dirichlet meanshift )
+algorithm=( canopy kmeans fuzzykmeans )
 if [ -n "$1" ]; then
   choice=$1
 else
@@ -35,8 +35,6 @@
   echo "1. ${algorithm[0]} clustering"
   echo "2. ${algorithm[1]} clustering"
   echo "3. ${algorithm[2]} clustering"
-  echo "4. ${algorithm[3]} clustering"
-  echo "5. ${algorithm[4]} clustering"
   read -p "Enter your choice : " choice
 fi
 echo "ok. You chose $choice and we'll use ${algorithm[$choice-1]} Clustering"
diff -uNar -x .git mahout/examples/bin/create-rf-data.sh mahout/examples/bin/create-rf-data.sh
--- mahout/examples/bin/create-rf-data.sh	1969-12-31 16:00:00.000000000 -0800
+++ mahout/examples/bin/create-rf-data.sh	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,36 @@
+#!/bin/bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+#
+# Create synthetic data set with four numeric fields and a boolean label.
+#
+# Requires scala, and is run from run-rf.sh.
+
+exec scala "$0" "$@"
+!#
+val r = new scala.util.Random()
+val pw = new java.io.PrintWriter(args(1))
+val numRows = args(0).toInt
+(1 to numRows).foreach(e =>
+  pw.println(r.nextDouble() + "," +
+  r.nextDouble() + "," +
+  r.nextDouble() + "," +
+  r.nextDouble() + "," +
+  (if (r.nextBoolean()) 1 else 0))
+)
+pw.close()
+
diff -uNar -x .git mahout/examples/bin/run-rf.sh mahout/examples/bin/run-rf.sh
--- mahout/examples/bin/run-rf.sh	1969-12-31 16:00:00.000000000 -0800
+++ mahout/examples/bin/run-rf.sh	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,89 @@
+#!/bin/bash
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+#
+# Requires scala, and for $HADOOP_HOME to be set.
+#
+# Creates test data for random forest classifier, splits data into train 
+# and test sets, trains the classifier on the train set, then tests 
+# model on test set.
+#
+# To run:  change into the mahout directory and type:
+# ./examples/bin/run-rf.sh <num-rows>
+WORK_DIR=/tmp/mahout-work-${USER}/
+input="rf-input.csv"
+
+
+# Remove old files
+echo
+echo "Removing old temp files if they exist; this will mention they're not there if not."
+echo
+$HADOOP_HOME/bin/hadoop fs -rmr -skipTrash $WORK_DIR forest
+$HADOOP_HOME/bin/hadoop fs -mkdir $WORK_DIR
+
+# Create test data
+numrows=$1
+echo
+echo "Writing random data to $input"
+./examples/bin/create-rf-data.sh $numrows $input
+
+# Put the test file in HDFS
+$HADOOP_HOME/bin/hadoop fs -rmr -skipTrash ${WORK_DIR}
+$HADOOP_HOME/bin/hadoop fs -mkdir -p ${WORK_DIR}/input
+if [ "$HADOOP_HOME" != "" ] && [ "$MAHOUT_LOCAL" == "" ] ; then
+  HADOOP="$HADOOP_HOME/bin/hadoop"
+  if [ ! -e $HADOOP ]; then
+    echo "Can't find hadoop in $HADOOP, exiting"
+    exit 1
+  fi
+fi
+if [ "$HADOOP_HOME" != "" ] && [ "$MAHOUT_LOCAL" == "" ] ; then
+  echo "Copying random data to HDFS"
+  set +e
+  $HADOOP dfs -rmr ${WORK_DIR}
+  set -e
+  $HADOOP dfs -put $input ${WORK_DIR}/input/$input
+fi
+
+# Split original file into train and test
+echo "Creating training and holdout set with a random 60-40 split of the generated vector dataset"
+./bin/mahout split \
+  -i ${WORK_DIR}/input \
+  --trainingOutput ${WORK_DIR}/train.csv \
+  --testOutput ${WORK_DIR}/test.csv \
+  --randomSelectionPct 40 --overwrite -xm sequential
+
+# Describe input file schema
+# Note:  "-d 4 N L" indicates four numerical fields and one label, as built by the step above.
+./bin/mahout describe -p $WORK_DIR/input/$input -f $WORK_DIR/info -d 4 N L
+
+# Train rf model
+echo
+echo "Training random forest."
+echo
+./bin/mahout buildforest -DXmx10000m -Dmapred.max.split.size=1000000 -d $WORK_DIR/train.csv -ds $WORK_DIR/info -sl 7 -p -t 500 -o $WORK_DIR/forest
+
+# Test predictions
+echo
+echo "Testing predictions on test set."
+echo
+./bin/mahout testforest -DXmx10000m -Dmapred.output.compress=false -i $WORK_DIR/test.csv -ds $WORK_DIR/info -m $WORK_DIR/forest -a -mr -o $WORK_DIR/predictions
+
+# Remove old files
+$HADOOP_HOME/bin/hadoop fs -rmr -skipTrash $WORK_DIR
+rm $input
+
diff -uNar -x .git mahout/examples/pom.xml mahout/examples/pom.xml
--- mahout/examples/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -23,7 +23,7 @@
   <parent>
     <groupId>org.apache.mahout</groupId>
     <artifactId>mahout</artifactId>
-    <version>0.8</version>
+    <version>1.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
 
@@ -93,6 +93,12 @@
       <plugin>
         <artifactId>maven-source-plugin</artifactId>
       </plugin>
+
+      <plugin>
+        <groupId>org.mortbay.jetty</groupId>
+        <artifactId>maven-jetty-plugin</artifactId>
+        <version>6.1.26</version>
+      </plugin>
     </plugins>
 
   </build>
@@ -135,26 +141,39 @@
     </dependency>
 
     <dependency>
+      <groupId>com.carrotsearch.randomizedtesting</groupId>
+      <artifactId>randomizedtesting-runner</artifactId>
+    </dependency>
+
+    <dependency>
       <groupId>org.easymock</groupId>
       <artifactId>easymock</artifactId>
-      <scope>test</scope>
     </dependency>
     
     <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
-      <scope>test</scope>
     </dependency>
 
     <dependency>
       <groupId>org.slf4j</groupId>
       <artifactId>slf4j-api</artifactId>
     </dependency>
-
     <dependency>
       <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-jcl</artifactId>
-      <scope>runtime</scope>
+      <artifactId>slf4j-log4j12</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>jcl-over-slf4j</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-logging</groupId>
+      <artifactId>commons-logging</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>log4j</groupId>
+      <artifactId>log4j</artifactId>
     </dependency>
 
   </dependencies>
diff -uNar -x .git mahout/examples/src/main/assembly/job.xml mahout/examples/src/main/assembly/job.xml
--- mahout/examples/src/main/assembly/job.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/assembly/job.xml	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
 <assembly
   xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
@@ -22,11 +39,6 @@
       <useTransitiveFiltering>true</useTransitiveFiltering>
       <excludes>
         <exclude>org.apache.hadoop:hadoop-core</exclude>
-        <!-- This jar contains a LICENSE file in the combined package. Another JAR includes
-          a licenses/ directory. That's OK except when unpacked on case-insensitive file
-          systems like Mac HFS+. Since this isn't really needed, we just remove it. -->
-        <exclude>com.github.stephenc.high-scale-lib:high-scale-lib</exclude>
-
       </excludes>
     </dependencySet>
   </dependencySets>
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/bookcrossing/BookCrossingDataModel.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/bookcrossing/BookCrossingDataModel.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/bookcrossing/BookCrossingDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/bookcrossing/BookCrossingDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -27,7 +27,7 @@
 
 import com.google.common.base.Charsets;
 import com.google.common.io.Closeables;
-import org.apache.mahout.cf.taste.example.grouplens.GroupLensDataModel;
+import org.apache.mahout.cf.taste.similarity.precompute.example.GroupLensDataModel;
 import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
 import org.apache.mahout.common.iterator.FileLineIterable;
 
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensDataModel.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensDataModel.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensDataModel.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,102 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.grouplens;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
-import java.net.URL;
-import java.util.regex.Pattern;
-
-import com.google.common.base.Charsets;
-import com.google.common.io.Closeables;
-import com.google.common.io.Files;
-import com.google.common.io.InputSupplier;
-import com.google.common.io.Resources;
-import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
-import org.apache.mahout.common.iterator.FileLineIterable;
-
-public final class GroupLensDataModel extends FileDataModel {
-  
-  private static final String COLON_DELIMTER = "::";
-  private static final Pattern COLON_DELIMITER_PATTERN = Pattern.compile(COLON_DELIMTER);
-  
-  public GroupLensDataModel() throws IOException {
-    this(readResourceToTempFile("/org/apache/mahout/cf/taste/example/grouplens/ratings.dat"));
-  }
-  
-  /**
-   * @param ratingsFile GroupLens ratings.dat file in its native format
-   * @throws IOException if an error occurs while reading or writing files
-   */
-  public GroupLensDataModel(File ratingsFile) throws IOException {
-    super(convertGLFile(ratingsFile));
-  }
-  
-  private static File convertGLFile(File originalFile) throws IOException {
-    // Now translate the file; remove commas, then convert "::" delimiter to comma
-    File resultFile = new File(new File(System.getProperty("java.io.tmpdir")), "ratings.txt");
-    if (resultFile.exists()) {
-      resultFile.delete();
-    }
-    Writer writer = null;
-    try {
-      writer = new OutputStreamWriter(new FileOutputStream(resultFile), Charsets.UTF_8);
-      for (String line : new FileLineIterable(originalFile, false)) {
-        int lastDelimiterStart = line.lastIndexOf(COLON_DELIMTER);
-        if (lastDelimiterStart < 0) {
-          throw new IOException("Unexpected input format on line: " + line);
-        }
-        String subLine = line.substring(0, lastDelimiterStart);
-        String convertedLine = COLON_DELIMITER_PATTERN.matcher(subLine).replaceAll(",");
-        writer.write(convertedLine);
-        writer.write('\n');
-      }
-    } catch (IOException ioe) {
-      resultFile.delete();
-      throw ioe;
-    } finally {
-      Closeables.close(writer, false);
-    }
-    return resultFile;
-  }
-
-  public static File readResourceToTempFile(String resourceName) throws IOException {
-    InputSupplier<? extends InputStream> inSupplier;
-    try {
-      URL resourceURL = Resources.getResource(GroupLensRecommender.class, resourceName);
-      inSupplier = Resources.newInputStreamSupplier(resourceURL);
-    } catch (IllegalArgumentException iae) {
-      File resourceFile = new File("src/main/java" + resourceName);
-      inSupplier = Files.newInputStreamSupplier(resourceFile);
-    }
-    File tempFile = File.createTempFile("taste", null);
-    tempFile.deleteOnExit();
-    Files.copy(inSupplier, tempFile);
-    return tempFile;
-  }
-
-  @Override
-  public String toString() {
-    return "GroupLensDataModel";
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommender.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommender.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,95 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.grouplens;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.recommender.CachingRecommender;
-import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.IDRescorer;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-
-/**
- * A simple {@link Recommender} implemented for the GroupLens demo.
- */
-public final class GroupLensRecommender implements Recommender {
-  
-  private final Recommender recommender;
-  
-  public GroupLensRecommender() throws IOException, TasteException {
-    this(new GroupLensDataModel());
-  }
-  
-  /**
-   * <p>Alternate constructor that takes a {@link DataModel} argument, which allows this {@link Recommender}
-   * to be used with the {@link org.apache.mahout.cf.taste.eval.RecommenderEvaluator} framework.</p>
-   *
-   * @param dataModel data model
-   * @throws TasteException if an error occurs while initializing this
-   */
-  public GroupLensRecommender(DataModel dataModel) throws TasteException {
-    recommender = new CachingRecommender(new SlopeOneRecommender(dataModel));
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException {
-    return recommender.recommend(userID, howMany);
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException {
-    return recommender.recommend(userID, howMany, rescorer);
-  }
-  
-  @Override
-  public float estimatePreference(long userID, long itemID) throws TasteException {
-    return recommender.estimatePreference(userID, itemID);
-  }
-  
-  @Override
-  public void setPreference(long userID, long itemID, float value) throws TasteException {
-    recommender.setPreference(userID, itemID, value);
-  }
-  
-  @Override
-  public void removePreference(long userID, long itemID) throws TasteException {
-    recommender.removePreference(userID, itemID);
-  }
-  
-  @Override
-  public DataModel getDataModel() {
-    return recommender.getDataModel();
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    recommender.refresh(alreadyRefreshed);
-  }
-  
-  @Override
-  public String toString() {
-    return "GroupLensRecommender[recommender:" + recommender + ']';
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderBuilder.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderBuilder.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderBuilder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderBuilder.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.grouplens;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.eval.RecommenderBuilder;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-
-final class GroupLensRecommenderBuilder implements RecommenderBuilder {
-  
-  @Override
-  public Recommender buildRecommender(DataModel dataModel) throws TasteException {
-    return new GroupLensRecommender(dataModel);
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderEvaluatorRunner.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderEvaluatorRunner.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderEvaluatorRunner.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/grouplens/GroupLensRecommenderEvaluatorRunner.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.grouplens;
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.commons.cli2.OptionException;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.eval.RecommenderEvaluator;
-import org.apache.mahout.cf.taste.example.TasteOptionParser;
-import org.apache.mahout.cf.taste.impl.eval.AverageAbsoluteDifferenceRecommenderEvaluator;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * <p>A simple example "runner" class which will evaluate the performance of the current
- * implementation of {@link GroupLensRecommender}.</p>
- */
-public final class GroupLensRecommenderEvaluatorRunner {
-  
-  private static final Logger log = LoggerFactory.getLogger(GroupLensRecommenderEvaluatorRunner.class);
-  
-  private GroupLensRecommenderEvaluatorRunner() {
-    // do nothing
-  }
-  
-  public static void main(String... args) throws IOException, TasteException, OptionException {
-    RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();
-    File ratingsFile = TasteOptionParser.getRatings(args);
-    DataModel model = ratingsFile == null ? new GroupLensDataModel() : new GroupLensDataModel(ratingsFile);
-    double evaluation = evaluator.evaluate(new GroupLensRecommenderBuilder(),
-      null,
-      model,
-      0.9,
-      0.3);
-    log.info(String.valueOf(evaluation));
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterDataModel.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterDataModel.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterDataModel.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,88 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.jester;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.regex.Pattern;
-
-import com.google.common.collect.Lists;
-import org.apache.mahout.cf.taste.example.grouplens.GroupLensDataModel;
-import org.apache.mahout.cf.taste.impl.common.FastByIDMap;
-import org.apache.mahout.cf.taste.impl.model.GenericDataModel;
-import org.apache.mahout.cf.taste.impl.model.GenericPreference;
-import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.model.Preference;
-import org.apache.mahout.common.iterator.FileLineIterator;
-
-public final class JesterDataModel extends FileDataModel {
-
-  private static final Pattern COMMA_PATTERN = Pattern.compile(",");
-
-  private long userBeingRead;
-  
-  public JesterDataModel() throws IOException {
-    this(GroupLensDataModel.readResourceToTempFile("/org/apache/mahout/cf/taste/example/jester/jester-data-1.csv"));
-  }
-  
-  /**
-   * @param ratingsFile Jester ratings file in CSV format
-   * @throws IOException if an error occurs while reading or writing files
-   */
-  public JesterDataModel(File ratingsFile) throws IOException {
-    super(ratingsFile);
-  }
-  
-  @Override
-  public void reload() {
-    userBeingRead = 0;
-    super.reload();
-  }
-  
-  @Override
-  protected DataModel buildModel() throws IOException {
-    FastByIDMap<Collection<Preference>> data = new FastByIDMap<Collection<Preference>>();
-    FileLineIterator iterator = new FileLineIterator(getDataFile(), false);
-    FastByIDMap<FastByIDMap<Long>> timestamps = new FastByIDMap<FastByIDMap<Long>>();
-    processFile(iterator, data, timestamps, false);
-    return new GenericDataModel(GenericDataModel.toDataMap(data, true));
-  }
-  
-  @Override
-  protected void processLine(String line,
-                             FastByIDMap<?> rawData,
-                             FastByIDMap<FastByIDMap<Long>> timestamps,
-                             boolean fromPriorData) {
-    FastByIDMap<Collection<Preference>> data = (FastByIDMap<Collection<Preference>>) rawData;
-    String[] jokePrefs = COMMA_PATTERN.split(line);
-    int count = Integer.parseInt(jokePrefs[0]);
-    Collection<Preference> prefs = Lists.newArrayListWithCapacity(count);
-    for (int itemID = 1; itemID < jokePrefs.length; itemID++) { // yes skip first one, just a count
-      String jokePref = jokePrefs[itemID];
-      if (!"99".equals(jokePref)) {
-        float jokePrefValue = Float.parseFloat(jokePref);
-        prefs.add(new GenericPreference(userBeingRead, itemID, jokePrefValue));
-      }
-    }
-    data.put(userBeingRead, prefs);
-    userBeingRead++;
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommender.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommender.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommender.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommender.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,84 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.jester;
-
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.recommender.CachingRecommender;
-import org.apache.mahout.cf.taste.impl.recommender.slopeone.SlopeOneRecommender;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.IDRescorer;
-import org.apache.mahout.cf.taste.recommender.RecommendedItem;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-
-/**
- * A simple {@link Recommender} implemented for the Jester Online Joke Recommender data set demo.
- * See the <a href="http://eigentaste.berkeley.edu/dataset/">Jester site</a>.
- */
-public final class JesterRecommender implements Recommender {
-  
-  private final Recommender recommender;
-  
-  public JesterRecommender(DataModel dataModel) throws TasteException {
-    recommender = new CachingRecommender(new SlopeOneRecommender(dataModel));
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException {
-    return recommender.recommend(userID, howMany);
-  }
-  
-  @Override
-  public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException {
-    return recommender.recommend(userID, howMany, rescorer);
-  }
-  
-  @Override
-  public float estimatePreference(long userID, long itemID) throws TasteException {
-    return recommender.estimatePreference(userID, itemID);
-  }
-  
-  @Override
-  public void setPreference(long userID, long itemID, float value) throws TasteException {
-    recommender.setPreference(userID, itemID, value);
-  }
-  
-  @Override
-  public void removePreference(long userID, long itemID) throws TasteException {
-    recommender.removePreference(userID, itemID);
-  }
-  
-  @Override
-  public DataModel getDataModel() {
-    return recommender.getDataModel();
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    recommender.refresh(alreadyRefreshed);
-  }
-  
-  @Override
-  public String toString() {
-    return "JesterRecommender[recommender:" + recommender + ']';
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderBuilder.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderBuilder.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderBuilder.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderBuilder.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.jester;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.eval.RecommenderBuilder;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.apache.mahout.cf.taste.recommender.Recommender;
-
-final class JesterRecommenderBuilder implements RecommenderBuilder {
-  
-  @Override
-  public Recommender buildRecommender(DataModel dataModel) throws TasteException {
-    return new JesterRecommender(dataModel);
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderEvaluatorRunner.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderEvaluatorRunner.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderEvaluatorRunner.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/JesterRecommenderEvaluatorRunner.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.example.jester;
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.commons.cli2.OptionException;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.eval.RecommenderEvaluator;
-import org.apache.mahout.cf.taste.example.TasteOptionParser;
-import org.apache.mahout.cf.taste.impl.eval.AverageAbsoluteDifferenceRecommenderEvaluator;
-import org.apache.mahout.cf.taste.model.DataModel;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public final class JesterRecommenderEvaluatorRunner {
-  
-  private static final Logger log = LoggerFactory.getLogger(JesterRecommenderEvaluatorRunner.class);
-  
-  private JesterRecommenderEvaluatorRunner() {
-    // do nothing
-  }
-  
-  public static void main(String... args) throws IOException, TasteException, OptionException {
-    RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();
-    File ratingsFile = TasteOptionParser.getRatings(args);
-    DataModel model = ratingsFile == null ? new JesterDataModel() : new JesterDataModel(ratingsFile);
-    double evaluation = evaluator.evaluate(new JesterRecommenderBuilder(),
-      null,
-      model,
-      0.9,
-      0.3);
-    log.info(String.valueOf(evaluation));
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/README mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/README
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/README	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/jester/README	1969-12-31 16:00:00.000000000 -0800
@@ -1,10 +0,0 @@
-Code works with Jester data set, which is not included in this distribution but is downloadable from
-http://eigentaste.berkeley.edu/dataset/
-
-Note that the code here works on data set 1, after being converted to CSV format.
-
-Data set originated from:
-
-Eigentaste: A Constant Time Collaborative Filtering Algorithm.
- Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins.
- Information Retrieval, 4(2), 133-151. July 2001.
\ No newline at end of file
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/kddcup/KDDCupDataModel.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/kddcup/KDDCupDataModel.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/kddcup/KDDCupDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/example/kddcup/KDDCupDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -65,7 +65,8 @@
    */
   public KDDCupDataModel(File dataFile, boolean storeDates, double samplingRate) throws IOException {
 
-    Preconditions.checkArgument(!Double.isNaN(samplingRate) && samplingRate > 0.0 && samplingRate <= 1.0);
+    Preconditions.checkArgument(!Double.isNaN(samplingRate) && samplingRate > 0.0 && samplingRate <= 1.0,
+        "Must be: 0.0 < samplingRate <= 1.0");
 
     dataFileDirectory = dataFile.getParentFile();
 
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/BatchItemSimilaritiesGroupLens.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/BatchItemSimilaritiesGroupLens.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/BatchItemSimilaritiesGroupLens.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/BatchItemSimilaritiesGroupLens.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,6 @@
 
 package org.apache.mahout.cf.taste.similarity.precompute.example;
 
-import org.apache.mahout.cf.taste.example.grouplens.GroupLensDataModel;
 import org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender;
 import org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity;
 import org.apache.mahout.cf.taste.impl.similarity.precompute.FileSimilarItemsWriter;
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/GroupLensDataModel.java mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/GroupLensDataModel.java
--- mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/GroupLensDataModel.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/examples/src/main/java/org/apache/mahout/cf/taste/similarity/precompute/example/GroupLensDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,102 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.cf.taste.similarity.precompute.example;
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.net.URL;
+import java.util.regex.Pattern;
+
+import com.google.common.base.Charsets;
+import com.google.common.io.Closeables;
+import com.google.common.io.Files;
+import com.google.common.io.InputSupplier;
+import com.google.common.io.Resources;
+import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;
+import org.apache.mahout.common.iterator.FileLineIterable;
+
+public final class GroupLensDataModel extends FileDataModel {
+  
+  private static final String COLON_DELIMTER = "::";
+  private static final Pattern COLON_DELIMITER_PATTERN = Pattern.compile(COLON_DELIMTER);
+  
+  public GroupLensDataModel() throws IOException {
+    this(readResourceToTempFile("/org/apache/mahout/cf/taste/example/grouplens/ratings.dat"));
+  }
+  
+  /**
+   * @param ratingsFile GroupLens ratings.dat file in its native format
+   * @throws IOException if an error occurs while reading or writing files
+   */
+  public GroupLensDataModel(File ratingsFile) throws IOException {
+    super(convertGLFile(ratingsFile));
+  }
+  
+  private static File convertGLFile(File originalFile) throws IOException {
+    // Now translate the file; remove commas, then convert "::" delimiter to comma
+    File resultFile = new File(new File(System.getProperty("java.io.tmpdir")), "ratings.txt");
+    if (resultFile.exists()) {
+      resultFile.delete();
+    }
+    Writer writer = null;
+    try {
+      writer = new OutputStreamWriter(new FileOutputStream(resultFile), Charsets.UTF_8);
+      for (String line : new FileLineIterable(originalFile, false)) {
+        int lastDelimiterStart = line.lastIndexOf(COLON_DELIMTER);
+        if (lastDelimiterStart < 0) {
+          throw new IOException("Unexpected input format on line: " + line);
+        }
+        String subLine = line.substring(0, lastDelimiterStart);
+        String convertedLine = COLON_DELIMITER_PATTERN.matcher(subLine).replaceAll(",");
+        writer.write(convertedLine);
+        writer.write('\n');
+      }
+    } catch (IOException ioe) {
+      resultFile.delete();
+      throw ioe;
+    } finally {
+      Closeables.close(writer, false);
+    }
+    return resultFile;
+  }
+
+  public static File readResourceToTempFile(String resourceName) throws IOException {
+    InputSupplier<? extends InputStream> inSupplier;
+    try {
+      URL resourceURL = Resources.getResource(GroupLensDataModel.class, resourceName);
+      inSupplier = Resources.newInputStreamSupplier(resourceURL);
+    } catch (IllegalArgumentException iae) {
+      File resourceFile = new File("src/main/java" + resourceName);
+      inSupplier = Files.newInputStreamSupplier(resourceFile);
+    }
+    File tempFile = File.createTempFile("taste", null);
+    tempFile.deleteOnExit();
+    Files.copy(inSupplier, tempFile);
+    return tempFile;
+  }
+
+  @Override
+  public String toString() {
+    return "GroupLensDataModel";
+  }
+  
+}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/classifier/NewsgroupHelper.java mahout/examples/src/main/java/org/apache/mahout/classifier/NewsgroupHelper.java
--- mahout/examples/src/main/java/org/apache/mahout/classifier/NewsgroupHelper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/classifier/NewsgroupHelper.java	2014-03-29 01:03:14.000000000 -0700
@@ -60,7 +60,7 @@
   private static final long WEEK = 7 * 24 * 3600;
   
   private final Random rand = RandomUtils.getRandom();  
-  private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43);
+  private final Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);
   private final FeatureVectorEncoder encoder = new StaticWordValueEncoder("body");
   private final FeatureVectorEncoder bias = new ConstantValueEncoder("Intercept");
   
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/classifier/df/mapreduce/BuildForest.java mahout/examples/src/main/java/org/apache/mahout/classifier/df/mapreduce/BuildForest.java
--- mahout/examples/src/main/java/org/apache/mahout/classifier/df/mapreduce/BuildForest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/classifier/df/mapreduce/BuildForest.java	2014-03-29 01:03:14.000000000 -0700
@@ -226,6 +226,9 @@
     long time = System.currentTimeMillis();
     
     DecisionForest forest = forestBuilder.build(nbTrees);
+    if (forest == null) {
+      return;
+    }
     
     time = System.currentTimeMillis() - time;
     log.info("Build Time: {}", DFUtils.elapsedTime(time));
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/classifier/sequencelearning/hmm/PosTagger.java mahout/examples/src/main/java/org/apache/mahout/classifier/sequencelearning/hmm/PosTagger.java
--- mahout/examples/src/main/java/org/apache/mahout/classifier/sequencelearning/hmm/PosTagger.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/classifier/sequencelearning/hmm/PosTagger.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,7 +20,6 @@
 import java.io.IOException;
 import java.net.URL;
 import java.util.Arrays;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Pattern;
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayClustering.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayClustering.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayClustering.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayClustering.java	2014-03-29 01:03:14.000000000 -0700
@@ -44,7 +44,7 @@
 import org.apache.mahout.clustering.AbstractCluster;
 import org.apache.mahout.clustering.Cluster;
 import org.apache.mahout.clustering.classify.WeightedVectorWritable;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
+import org.apache.mahout.clustering.UncommonDistributions;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.RandomUtils;
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayDirichlet.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayDirichlet.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayDirichlet.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayDirichlet.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,125 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.display;
-
-import java.awt.Graphics;
-import java.awt.Graphics2D;
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.Model;
-import org.apache.mahout.clustering.ModelDistribution;
-import org.apache.mahout.clustering.classify.ClusterClassifier;
-import org.apache.mahout.clustering.dirichlet.DirichletDriver;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
-import org.apache.mahout.clustering.iterator.ClusterIterator;
-import org.apache.mahout.clustering.iterator.DirichletClusteringPolicy;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.VectorWritable;
-
-import com.google.common.collect.Lists;
-
-public class DisplayDirichlet extends DisplayClustering {
-  
-  public DisplayDirichlet() {
-    initialize();
-    setTitle("Dirichlet Process Clusters - Normal Distribution (>" + (int) (significance * 100) + "% of population)");
-  }
-
-  @Override
-  public void paint(Graphics g) {
-    plotSampleData((Graphics2D) g);
-    plotClusters((Graphics2D) g);
-  }
-  
-  protected static void generateResults(Path input, Path output, ModelDistribution<VectorWritable> modelDist,
-      int numClusters, int numIterations, double alpha0, int thin, int burnin) throws IOException,
-      ClassNotFoundException, InterruptedException {
-    boolean runClusterer = true;
-    if (runClusterer) {
-      runSequentialDirichletClusterer(input, output, modelDist, numClusters, numIterations, alpha0);
-    } else {
-      runSequentialDirichletClassifier(input, output, modelDist, numClusters, numIterations, alpha0);
-    }
-    for (int i = 1; i <= numIterations; i++) {
-      ClusterClassifier posterior = new ClusterClassifier();
-      String name = i == numIterations ? "clusters-" + i + "-final" : "clusters-" + i;
-      posterior.readFromSeqFiles(new Configuration(), new Path(output, name));
-      List<Cluster> clusters = Lists.newArrayList();
-      for (Cluster cluster : posterior.getModels()) {
-        if (isSignificant(cluster)) {
-          clusters.add(cluster);
-        }
-      }
-      CLUSTERS.add(clusters);
-    }
-  }
-  
-  private static void runSequentialDirichletClassifier(Path input, Path output,
-      ModelDistribution<VectorWritable> modelDist, int numClusters, int numIterations, double alpha0)
-    throws IOException {
-    List<Cluster> models = Lists.newArrayList();
-    for (Model<VectorWritable> cluster : modelDist.sampleFromPrior(numClusters)) {
-      models.add((Cluster) cluster);
-    }
-    ClusterClassifier prior = new ClusterClassifier(models, new DirichletClusteringPolicy(numClusters, alpha0));
-    Path priorPath = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);
-    prior.writeToSeqFiles(priorPath);
-    Configuration conf = new Configuration();
-    ClusterIterator.iterateSeq(conf, input, priorPath, output, numIterations);
-  }
-  
-  private static void runSequentialDirichletClusterer(Path input, Path output,
-      ModelDistribution<VectorWritable> modelDist, int numClusters, int numIterations, double alpha0)
-    throws IOException, ClassNotFoundException, InterruptedException {
-    DistributionDescription description = new DistributionDescription(modelDist.getClass().getName(),
-        RandomAccessSparseVector.class.getName(), ManhattanDistanceMeasure.class.getName(), 2);
-    
-    DirichletDriver.run(new Configuration(), input, output, description, numClusters, numIterations, alpha0, true,
-        true, 0, true);
-  }
-  
-  public static void main(String[] args) throws Exception {
-    VectorWritable modelPrototype = new VectorWritable(new DenseVector(2));
-    ModelDistribution<VectorWritable> modelDist = new GaussianClusterDistribution(modelPrototype);
-    Configuration conf = new Configuration();
-    Path output = new Path("output");
-    HadoopUtil.delete(conf, output);
-    Path samples = new Path("samples");
-    HadoopUtil.delete(conf, samples);
-    RandomUtils.useTestSeed();
-    generateSamples();
-    writeSampleData(samples);
-    int numIterations = 20;
-    int numClusters = 10;
-    int alpha0 = 1;
-    int thin = 3;
-    int burnin = 5;
-    generateResults(samples, output, modelDist, numClusters, numIterations, alpha0, thin, burnin);
-    new DisplayDirichlet();
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayFuzzyKMeans.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayFuzzyKMeans.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayFuzzyKMeans.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayFuzzyKMeans.java	2014-03-29 01:03:14.000000000 -0700
@@ -102,7 +102,7 @@
       ClassNotFoundException, InterruptedException {
     Path clustersIn = new Path(output, "random-seeds");
     RandomSeedGenerator.buildRandom(conf, samples, clustersIn, 3, measure);
-    FuzzyKMeansDriver.run(samples, clustersIn, output, measure, threshold, maxIterations, m, true, true, threshold,
+    FuzzyKMeansDriver.run(samples, clustersIn, output, threshold, maxIterations, m, true, true, threshold,
         true);
     
     loadClustersWritable(output);
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayKMeans.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayKMeans.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayKMeans.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayKMeans.java	2014-03-29 01:03:14.000000000 -0700
@@ -93,7 +93,7 @@
     throws IOException, InterruptedException, ClassNotFoundException {
     Path clustersIn = new Path(output, "random-seeds");
     RandomSeedGenerator.buildRandom(conf, samples, clustersIn, numClusters, measure);
-    KMeansDriver.run(samples, clustersIn, output, measure, convergenceDelta, maxIterations, true, 0.0, true);
+    KMeansDriver.run(samples, clustersIn, output, convergenceDelta, maxIterations, true, 0.0, true);
     loadClustersWritable(output);
   }
   
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMeanShift.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMeanShift.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMeanShift.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMeanShift.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,129 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.display;
-
-import java.awt.Color;
-import java.awt.Graphics;
-import java.awt.Graphics2D;
-import java.awt.geom.AffineTransform;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopy;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.math.function.Functions;
-
-public final class DisplayMeanShift extends DisplayClustering {
-  
-  private static double t1;
-  
-  private static double t2;
-  
-  private DisplayMeanShift() {
-    initialize();
-    this.setTitle("Mean Shift Canopy Clusters (>" + (int) (significance * 100)
-        + "% of population)");
-  }
-  
-  @Override
-  public void paint(Graphics g) {
-    Graphics2D g2 = (Graphics2D) g;
-    double sx = (double) res / DS;
-    g2.setTransform(AffineTransform.getScaleInstance(sx, sx));
-    
-    // plot the axes
-    g2.setColor(Color.BLACK);
-    Vector dv = new DenseVector(2).assign(SIZE / 2.0);
-    Vector dv1 = new DenseVector(2).assign(t1);
-    Vector dv2 = new DenseVector(2).assign(t2);
-    DisplayClustering.plotRectangle(g2, new DenseVector(2).assign(2), dv);
-    DisplayClustering.plotRectangle(g2, new DenseVector(2).assign(-2), dv);
-    
-    // plot the sample data
-    g2.setColor(Color.DARK_GRAY);
-    dv.assign(0.03);
-    for (VectorWritable v : SAMPLE_DATA) {
-      DisplayClustering.plotRectangle(g2, v.get(), dv);
-    }
-    int i = 0;
-    for (Cluster cluster : CLUSTERS.get(CLUSTERS.size() - 1)) {
-      MeanShiftCanopy canopy = (MeanShiftCanopy) cluster;
-      if (canopy.getMass() >= significance
-          * DisplayClustering.SAMPLE_DATA.size()) {
-        g2.setColor(COLORS[Math.min(i++, DisplayClustering.COLORS.length - 1)]);
-        int count = 0;
-        Vector center = new DenseVector(2);
-        for (int vix : canopy.getBoundPoints().toList()) {
-          Vector v = SAMPLE_DATA.get(vix).get();
-          count++;
-          center.assign(v, Functions.PLUS);
-          DisplayClustering.plotRectangle(g2, v, dv);
-        }
-        center = center.divide(count);
-        DisplayClustering.plotEllipse(g2, center, dv1);
-        DisplayClustering.plotEllipse(g2, center, dv2);
-      }
-    }
-  }
-  
-  public static void main(String[] args) throws Exception {
-    t1 = 1.5;
-    t2 = 0.5;
-    DistanceMeasure measure = new EuclideanDistanceMeasure();
-    IKernelProfile kernelProfile = new TriangularKernelProfile();
-    significance = 0.02;
-    
-    Path samples = new Path("samples");
-    Path output = new Path("output");
-    Configuration conf = new Configuration();
-    HadoopUtil.delete(conf, samples);
-    HadoopUtil.delete(conf, output);
-    
-    RandomUtils.useTestSeed();
-    DisplayClustering.generateSamples();
-    writeSampleData(samples);
-    // boolean b = true;
-    // if (b) {
-    MeanShiftCanopyDriver.run(conf, samples, output, measure, kernelProfile,
-        t1, t2, 0.005, 20, false, true, true);
-    loadClustersWritable(output);
-    // } else {
-    // Collection<Vector> points = new ArrayList<Vector>();
-    // for (VectorWritable sample : SAMPLE_DATA) {
-    // points.add(sample.get());
-    // }
-    // List<MeanShiftCanopy> canopies =
-    // MeanShiftCanopyClusterer.clusterPoints(points, measure, 0.005, t1, t2,
-    // 20);
-    // for (MeanShiftCanopy canopy : canopies) {
-    // log.info(canopy.toString());
-    // }
-    // }
-    new DisplayMeanShift();
-  }
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMinHash.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMinHash.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMinHash.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplayMinHash.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,371 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.display;
-
-import java.awt.Color;
-import java.awt.Font;
-import java.awt.Frame;
-import java.awt.Graphics;
-import java.awt.Graphics2D;
-import java.awt.event.ActionEvent;
-import java.awt.event.ActionListener;
-import java.awt.event.KeyEvent;
-import java.awt.event.KeyListener;
-import java.awt.geom.AffineTransform;
-import java.awt.geom.Line2D;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-
-import javax.swing.Timer;
-
-import com.google.common.collect.Lists;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.minhash.HashFactory;
-import org.apache.mahout.clustering.minhash.MinHashDriver;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.common.iterator.sequencefile.PathFilters;
-import org.apache.mahout.common.iterator.sequencefile.PathType;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * <p>
- * This class displays the work of the minimum hash algorithm. There are several
- * parameters which can be used:
- * </p>
- * <p>
- * With the first command line argument you can point the plot type of the
- * algorithm. The possible values are:<br>
- * -p: The algorithm displays the different clusters from the sample data as it
- * highlights the points in the separate clusters in a slide show. It can be
- * paused/resumed with the space key.<br>
- * -l: The algorithm draws the lines between every two points in every cluster.
- * Every cluster is indicated with different, randomly chosen color.<br>
- * -s: The algorithm draws the points from the sample data and maps every
- * cluster to a single character symbol which is displayed near the points which
- * belong to that cluster. However usually one point belongs to more than one
- * cluster and it is not very easy to recognize the clusters with that display.<br>
- * By default the algorithm will plot points highlight slide show.
- * </p>
- * <p>
- * With the second command line argument you can determine the time in seconds
- * in a cluster will be highlighted before the cluster is changed. This option
- * is relevant only when -p is passed as the first command line argument.
- * </p>
- **/
-public class DisplayMinHash extends DisplayClustering {
-
-  /**
-   * Enumeration of the possible plot types for the {@link DisplayMinHash}
-   * program.
-   */
-  public enum PlotType {
-    LINES, POINTS, SYMBOLS
-  }
-
-  private static final Logger log = LoggerFactory.getLogger(DisplayMinHash.class);
-
-  private static final int SYMBOLS_FONT_SIZE = 6;
-
-  private static final Map<String, List<Vector>> CLUSTERS = new HashMap<String, List<Vector>>();
-  private static Iterator<Entry<String, List<Vector>>> currentCluster;
-  private static List<Vector> currentClusterPoints;
-  private static int updatePeriodTime;
-  private static long lastUpdateTime;
-  private static boolean isSlideShowOnHold;
-
-  private PlotType plotType = PlotType.POINTS;
-
-  /**
-   * Creates a new instance.
-   * 
-   * @param type
-   *          The chosen {@link PlotType} option.
-   */
-  public DisplayMinHash(PlotType type) {
-    if (type == null) {
-      log.error("The PlotType passed should not be null. The program will use the default value - POINTS");
-    } else {
-      this.plotType = type;
-    }
-    initialize();
-    this.setTitle("Minhash Clusters (>" + (int) (significance * 100)
-        + "% of population)");
-  }
-
-  /**
-   * Draws the clusters in the minimum hash algorithm according to the chosen
-   * plot type.
-   * 
-   * @param g
-   *          The {@link Graphics} object used to plot the clusters.
-   */
-  @Override
-  public void paint(Graphics g) {
-    plotClusters((Graphics2D) g, plotType);
-  }
-
-  private static void plotClusters(Graphics2D g2, PlotType plotType) {
-    double sx = (double) res / DS;
-    g2.setTransform(AffineTransform.getScaleInstance(sx, sx));
-    Font f = new Font("Dialog", Font.PLAIN, SYMBOLS_FONT_SIZE);
-    g2.setFont(f);
-    switch (plotType) {
-      case LINES:
-        plotLines(g2);
-        break;
-      case SYMBOLS:
-        plotSymbols(g2);
-        break;
-      case POINTS:
-        plotPoints(g2);
-        break;
-      default:
-        throw new IllegalStateException("Unknown plot type: " + plotType);
-    }
-  }
-
-  private static void plotLines(Graphics2D g2) {
-    Random rand = RandomUtils.getRandom();
-    for (Map.Entry<String, List<Vector>> entry : CLUSTERS.entrySet()) {
-      List<Vector> vecs = entry.getValue();
-
-      g2.setColor(new Color(rand.nextInt()));
-
-      for (int i = 0; i < vecs.size(); i += 2) {
-        Vector vec1;
-        Vector vec2;
-        if (i < vecs.size() - 1) {
-          vec1 = vecs.get(i);
-          vec2 = vecs.get(i + 1);
-        } else {
-          // wrap around back to the beginning
-          vec1 = vecs.get(i);
-          vec2 = vecs.get(0);
-
-        }
-        plotLine(g2, vec1, vec2);
-      }
-    }
-  }
-
-  private static void plotSymbols(Graphics2D g2) {
-    char symbol = 0;
-    Random rand = RandomUtils.getRandom();
-    for (Map.Entry<String, List<Vector>> entry : CLUSTERS.entrySet()) {
-      List<Vector> vecs = entry.getValue();
-
-      g2.setColor(new Color(rand.nextInt()));
-      symbol++;
-
-      for (Vector vec : vecs) {
-        plotSymbols(g2, vec, symbol);
-      }
-    }
-  }
-
-  private static void plotPoints(Graphics2D g2) {
-    if (currentCluster == null || !currentCluster.hasNext()) {
-      currentCluster = CLUSTERS.entrySet().iterator();
-    }
-
-    if (System.currentTimeMillis() - lastUpdateTime > updatePeriodTime) {
-      plotSampleData(g2);
-      currentClusterPoints = currentCluster.next().getValue();
-      lastUpdateTime = System.currentTimeMillis();
-    }
-    plotSampleData(g2);
-    g2.setColor(Color.RED);
-    Vector dv = new DenseVector(2).assign(0.03);
-
-    for (Vector currentClusterPoint : currentClusterPoints) {
-      plotRectangle(g2, currentClusterPoint, dv);
-    }
-  }
-
-  private static void plotSymbols(Graphics2D g2, Vector vec, char symbol) {
-    double[] flip = { 1, -1 };
-    Vector v1 = vec.times(new DenseVector(flip));
-    int h = SIZE / 2;
-    double x1 = v1.get(0) + h;
-    double y1 = v1.get(1) + h;
-    g2.drawString(Character.toString(symbol), (int) (x1 * DS), (int) (y1 * DS));
-  }
-
-  private static void plotLine(Graphics2D g2, Vector vec1, Vector vec2) {
-    double[] flip = { 1, -1 };
-    Vector v1 = vec1.times(new DenseVector(flip));
-    Vector v2 = vec2.times(new DenseVector(flip));
-    int h = SIZE / 2;
-    double x1 = v1.get(0) + h;
-    double y1 = v1.get(1) + h;
-    double x2 = v2.get(0) + h;
-    double y2 = v2.get(1) + h;
-    g2.draw(new Line2D.Double(x1 * DS, y1 * DS, x2 * DS, y2 * DS));
-  }
-
-  /**
-   * The entry point to the program.
-   * 
-   * @param args
-   *          The command-line arguments.
-   * 
-   * @throws Exception
-   *           Thrown if an error occurs during the execution.
-   */
-  public static void main(String[] args) throws Exception {
-    Path samples = new Path("samples");
-    Path output = new Path("output", "minhash");
-
-    PlotType type = determinePlotType(args);
-    updatePeriodTime = determineUpdatePeriodTime(args);
-
-    Configuration conf = new Configuration();
-    HadoopUtil.delete(conf, samples);
-    HadoopUtil.delete(conf, output);
-    RandomUtils.useTestSeed();
-    generateSamples();
-    writeSampleData(samples);
-    runMinHash(conf, samples, output);
-    loadClusters(output);
-    logClusters();
-    final Frame f = new DisplayMinHash(type);
-
-    if (type == PlotType.POINTS) {
-      Timer timer = new Timer(updatePeriodTime, new ActionListener() {
-        @Override
-        public void actionPerformed(ActionEvent e) {
-          repaint(f);
-        }
-      });
-      timer.start();
-    }
-
-    f.addKeyListener(new KeyListener() {
-
-      @Override
-      public void keyTyped(KeyEvent arg0) {
-      }
-
-      @Override
-      public void keyReleased(KeyEvent arg0) {
-      }
-
-      @Override
-      public void keyPressed(KeyEvent arg0) {
-        if (arg0.getKeyCode() == KeyEvent.VK_SPACE) {
-          onSpacePressed();
-        }
-      }
-    });
-  }
-
-  private static PlotType determinePlotType(String[] args) {
-    PlotType type = PlotType.POINTS;
-    if (args.length != 0) {
-      if ("-p".equals(args[0])) {
-        type = PlotType.POINTS;
-      } else if ("-l".equals(args[0])) {
-        type = PlotType.LINES;
-      } else if ("-s".equals(args[0])) {
-        type = PlotType.SYMBOLS;
-      } else {
-        System.out.println("Wrong parameter: -p (plot points); -l (plot lines); -s (plot symbols)");
-      }
-    }
-    return type;
-  }
-
-  private static int determineUpdatePeriodTime(String[] args) {
-    if (args.length >= 2) {
-      try {
-        updatePeriodTime = Integer.parseInt(args[1]);
-      } catch (NumberFormatException nfe) {
-        System.out.println(args[1] + " isn't valid integer value. 1 second will be used.");
-      }
-    }
-    int updatePeriodTimeInMinutes = 1;
-    return updatePeriodTimeInMinutes * 1000;
-  }
-
-  private static void repaint(Frame f) {
-    if (!isSlideShowOnHold) {
-      f.repaint();
-    }
-
-  }
-
-  private static void onSpacePressed() {
-    isSlideShowOnHold = !isSlideShowOnHold;
-  }
-
-  private static void logClusters() {
-    int i = 0;
-    for (Map.Entry<String, List<Vector>> entry : CLUSTERS.entrySet()) {
-      StringBuilder logStr = new StringBuilder();
-      logStr.append("Cluster N:").append(++i).append(": ");
-      List<Vector> vecs = entry.getValue();
-      for (Vector vector : vecs) {
-        logStr.append(vector.get(0));
-        logStr.append(',');
-        logStr.append(vector.get(1));
-        logStr.append("; ");
-      }
-      log.info(logStr.toString());
-    }
-  }
-
-  protected static void loadClusters(Path output) throws IOException {
-    Configuration conf = new Configuration();
-    SequenceFileDirIterator<Text, VectorWritable> iterator = new SequenceFileDirIterator<Text, VectorWritable>(
-        output, PathType.LIST, PathFilters.partFilter(), null, false, conf);
-    while (iterator.hasNext()) {
-      Pair<Text, VectorWritable> next = iterator.next();
-      String key = next.getFirst().toString();
-      List<Vector> list = CLUSTERS.get(key);
-      if (list == null) {
-        list = Lists.newArrayList();
-        CLUSTERS.put(key, list);
-      }
-      list.add(next.getSecond().get());
-    }
-    log.info("Loaded: {} clusters", CLUSTERS.size());
-  }
-
-  private static void runMinHash(Configuration conf, Path samples, Path output) throws Exception {
-    ToolRunner.run(conf, new MinHashDriver(), new String[] { "--input", samples.toString(),
-      "--hashType", HashFactory.HashType.MURMUR3.toString(), "--output", output.toString(),
-      "--minVectorSize", "1", "--debugOutput"
-    });
-  }
-
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplaySpectralKMeans.java mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplaySpectralKMeans.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplaySpectralKMeans.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/display/DisplaySpectralKMeans.java	2014-03-29 01:03:14.000000000 -0700
@@ -83,6 +83,6 @@
 
   @Override
   public void paint(Graphics g) {
-    plotClusteredSampleData((Graphics2D) g, new Path(OUTPUT));
+    plotClusteredSampleData((Graphics2D) g, new Path(new Path(OUTPUT), "kmeans_out"));
   }
 }
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmClusterEvaluator.java mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmClusterEvaluator.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmClusterEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmClusterEvaluator.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,157 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.minhash;
-
-import java.text.NumberFormat;
-import java.util.Collection;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-
-import com.google.common.collect.Lists;
-import com.google.common.collect.Sets;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.mahout.common.Pair;
-import org.apache.mahout.common.RandomUtils;
-import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-public final class LastfmClusterEvaluator {
-
-  private LastfmClusterEvaluator() {
-  }
-
-  /* Calculate used JVM memory */
-  private static String usedMemory() {
-    Runtime runtime = Runtime.getRuntime();
-    return "Used Memory: [" + (runtime.totalMemory() - runtime.freeMemory()) / (1024 * 1024) + " MB] ";
-  }
-
-  /**
-   * Computer Jaccard coefficient over two sets. (A intersect B) / (A union B)
-   */
-  private static double computeSimilarity(Iterable<Integer> listenerVector1, Iterable<Integer> listenerVector2) {
-    Set<Integer> first = Sets.newHashSet();
-    for (Integer ele : listenerVector1) {
-      first.add(ele);
-    }
-    Collection<Integer> second = Sets.newHashSet();
-    for (Integer ele : listenerVector2) {
-      second.add(ele);
-    }
-
-    Collection<Integer> intersection = Sets.newHashSet();
-    intersection.retainAll(second);
-    double intersectSize = intersection.size();
-
-    first.addAll(second);
-    double unionSize = first.size();
-    return unionSize == 0 ? 0.0 : intersectSize / unionSize;
-  }
-
-  /**
-   * Calculate the overall cluster precision by sampling clusters. Precision is
-   * calculated as follows :-
-   * 
-   * 1. For a sample of all the clusters calculate the pair-wise similarity
-   * (Jaccard coefficient) for items in the same cluster.
-   * 
-   * 2. Count true positives as items whose similarity is above specified
-   * threshold.
-   * 
-   * 3. Precision = (true positives) / (total items in clusters sampled).
-   * 
-   * @param clusterFile
-   *          The file containing cluster information
-   * @param threshold
-   *          Similarity threshold for containing two items in a cluster to be
-   *          relevant. Must be between 0.0 and 1.0
-   * @param samplePercentage
-   *          Percentage of clusters to sample. Must be between 0.0 and 1.0
-   */
-  private static void testPrecision(Path clusterFile, double threshold, double samplePercentage) {
-    Configuration conf = new Configuration();
-    Random rand = RandomUtils.getRandom();
-    Text prevCluster = new Text();
-    List<List<Integer>> listenerVectors = Lists.newArrayList();
-    long similarListeners = 0;
-    long allListeners = 0;
-    int clustersProcessed = 0;
-    for (Pair<Text,VectorWritable> record
-        : new SequenceFileIterable<Text,VectorWritable>(clusterFile, true, conf)) {
-      Text cluster = record.getFirst();
-      VectorWritable point = record.getSecond();
-      if (!cluster.equals(prevCluster)) {
-        // We got a new cluster
-        prevCluster.set(cluster.toString());
-        // Should we check previous cluster ?
-        if (rand.nextDouble() > samplePercentage) {
-          listenerVectors.clear();
-          continue;
-        }
-        int numListeners = listenerVectors.size();
-        allListeners += numListeners;
-        for (int i = 0; i < numListeners; i++) {
-          List<Integer> listenerVector1 = listenerVectors.get(i);
-          for (int j = i + 1; j < numListeners; j++) {
-            List<Integer> listenerVector2 = listenerVectors.get(j);
-            double similarity = computeSimilarity(listenerVector1,
-                listenerVector2);
-            similarListeners += similarity >= threshold ? 1 : 0;
-          }
-        }
-        listenerVectors.clear();
-        clustersProcessed++;
-        System.out.print('\r' + usedMemory() + " Clusters processed: " + clustersProcessed);
-      }
-      List<Integer> listeners = Lists.newArrayList();
-      for (Vector.Element ele : point.get().nonZeroes()) {
-        listeners.add((int) ele.get());
-      }
-      listenerVectors.add(listeners);
-    }
-    System.out.println("\nTest Results");
-    System.out.println("=============");
-    System.out.println(" (A) Listeners in same cluster with simiarity above threshold ("
-                           + threshold + ") : " + similarListeners);
-    System.out.println(" (B) All listeners: " + allListeners);
-    NumberFormat format = NumberFormat.getInstance();
-    format.setMaximumFractionDigits(2);
-    double precision = (double) similarListeners / allListeners * 100.0;
-    System.out.println(" Average cluster precision: A/B = " + format.format(precision));
-  }
-
-  public static void main(String[] args) {
-    if (args.length < 3) {
-      System.out.println("LastfmClusterEvaluation <cluster-file> <threshold> <sample-percentage>");
-      System.out.println("      <cluster-file>: Absolute Path of file containing cluster information in DEBUG format");
-      System.out.println("         <threshold>: Minimum threshold for jaccard co-efficient for considering two items");
-      System.out.println("                      in a cluster to be really similar. Should be between 0.0 and 1.0");
-      System.out.println(" <sample-percentage>: Percentage of clusters to sample. Should be between 0.0 and 1.0");
-      return;
-    }
-    Path clusterFile = new Path(args[0]);
-    double threshold = Double.parseDouble(args[1]);
-    double samplePercentage = Double.parseDouble(args[2]);
-    testPrecision(clusterFile, threshold, samplePercentage);
-  }
-
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmDataConverter.java mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmDataConverter.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmDataConverter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/minhash/LastfmDataConverter.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,214 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.minhash;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.regex.Pattern;
-
-import com.google.common.base.Charsets;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-import com.google.common.io.Closeables;
-import com.google.common.io.Files;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.mahout.math.SequentialAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-
-public final class LastfmDataConverter {
-
-  private static final Pattern TAB_PATTERN = Pattern.compile("\t");
-
-  // we are clustering similar featureIdxs on the following dataset
-  // http://www.iua.upf.es/~ocelma/MusicRecommendationDataset/index.html
-  //
-  // Preparation of the data set means gettting the dataset to a format which
-  // can
-  // be read by the min hash algorithm;
-  //
-  enum Lastfm {
-    USERS_360K(17559530),
-    USERS_1K(19150868);
-    private final int totalRecords;
-    Lastfm(int totalRecords) {
-      this.totalRecords = totalRecords;
-    }
-    int getTotalRecords() {
-      return totalRecords;
-    }
-  }
-
-  private LastfmDataConverter() {
-  }
-
-  private static String usedMemory() {
-    Runtime runtime = Runtime.getRuntime();
-    return "Used Memory: [" + (runtime.totalMemory() - runtime.freeMemory()) / (1024 * 1024) + " MB] ";
-  }
-
-  /* Get the feature from the parsed record */
-  private static String getFeature(String[] fields, Lastfm dataSet) {
-    if (dataSet == Lastfm.USERS_360K) {
-      return fields[0];
-    } else {
-      return fields[2];
-    }
-  }
-
-  /* Get the item from the parsed record */
-  private static String getItem(String[] fields, Lastfm dataSet) {
-    if (dataSet == Lastfm.USERS_360K) {
-      return fields[2];
-    } else {
-      return fields[0];
-    }
-  }
-
-  /**
-   * Reads the LastFm dataset and constructs a Map of (item, features). For 360K
-   * Users dataset - (Item=Artist, Feature=User) For 1K Users dataset -
-   * (Item=User, Feature=Artist)
-   * 
-   * @param inputFile
-   *          Lastfm dataset file on the local file system.
-   * @param dataSet
-   *          Type of dataset - 360K Users or 1K Users
-   */
-  public static Map<String, List<Integer>> convertToItemFeatures(String inputFile, Lastfm dataSet) throws IOException {
-    long totalRecords = dataSet.getTotalRecords();
-    Map<String, Integer> featureIdxMap = Maps.newHashMap();
-    Map<String, List<Integer>> itemFeaturesMap = Maps.newHashMap();
-    String msg = usedMemory() + "Converting data to internal vector format: ";
-    BufferedReader br = Files.newReader(new File(inputFile), Charsets.UTF_8);
-    try {
-      System.out.print(msg);
-      int prevPercentDone = 1;
-      double percentDone = 0.0;
-      long parsedRecords = 0;
-      String line;
-      while ((line = br.readLine()) != null) {
-        String[] fields = TAB_PATTERN.split(line);
-        String feature = getFeature(fields, dataSet);
-        String item = getItem(fields, dataSet);
-        // get the featureIdx
-        Integer featureIdx = featureIdxMap.get(feature);
-        if (featureIdx == null) {
-          featureIdx = featureIdxMap.size() + 1;
-          featureIdxMap.put(feature, featureIdx);
-        }
-        // add it to the corresponding feature idx map
-        List<Integer> features = itemFeaturesMap.get(item);
-        if (features == null) {
-          features = Lists.newArrayList();
-          itemFeaturesMap.put(item, features);
-        }
-        features.add(featureIdx);
-        parsedRecords++;
-        // Update the progress
-        percentDone = parsedRecords * 100.0 / totalRecords;
-        msg = usedMemory() + "Converting data to internal vector format: ";
-        if (percentDone > prevPercentDone) {
-          System.out.print('\r' + msg + percentDone + '%');
-          prevPercentDone++;
-        }
-        parsedRecords++;
-      }
-      msg = usedMemory() + "Converting data to internal vector format: ";
-      System.out.print('\r' + msg + percentDone + "% Completed\n");
-    } finally {
-      Closeables.close(br, true);
-    }
-    return itemFeaturesMap;
-  }
-
-  /**
-   * Converts each record in (item,features) map into Mahout vector format and
-   * writes it into sequencefile for minhash clustering
-   */
-  public static boolean writeToSequenceFile(Map<String, List<Integer>> itemFeaturesMap, Path outputPath)
-    throws IOException {
-    Configuration conf = new Configuration();
-    FileSystem fs = FileSystem.get(conf);
-    fs.mkdirs(outputPath.getParent());
-    long totalRecords = itemFeaturesMap.size();
-    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, outputPath, Text.class, VectorWritable.class);
-    try {
-      String msg = "Now writing vectorized data in sequence file format: ";
-      System.out.print(msg);
-
-      Text itemWritable = new Text();
-      VectorWritable featuresWritable = new VectorWritable();
-
-      int doneRecords = 0;
-      int prevPercentDone = 1;
-
-      for (Map.Entry<String, List<Integer>> itemFeature : itemFeaturesMap.entrySet()) {
-        int numfeatures = itemFeature.getValue().size();
-        itemWritable.set(itemFeature.getKey());
-        Vector featureVector = new SequentialAccessSparseVector(numfeatures);
-        int i = 0;
-        for (Integer feature : itemFeature.getValue()) {
-          featureVector.setQuick(i++, feature);
-        }
-        featuresWritable.set(featureVector);
-        writer.append(itemWritable, featuresWritable);
-        // Update the progress
-        double percentDone = ++doneRecords * 100.0 / totalRecords;
-        if (percentDone > prevPercentDone) {
-          System.out.print('\r' + msg + percentDone + "% " + (percentDone >= 100 ? "Completed\n" : ""));
-          prevPercentDone++;
-        }
-      }
-    } finally {
-      Closeables.close(writer, false);
-    }
-    return true;
-  }
-
-  public static void main(String[] args) throws Exception {
-    if (args.length < 3) {
-      System.out.println("[Usage]: LastfmDataConverter <input> <output> <dataset>");
-      System.out.println("   <input>: Absolute path to the local file [usersha1-artmbid-artname-plays.tsv] ");
-      System.out.println("  <output>: Absolute path to the HDFS output file");
-      System.out.println(" <dataset>: Either of the two Lastfm public datasets. "
-          + "Must be either 'Users360K' or 'Users1K'");
-      System.out.println("Note:- Hadoop configuration pointing to HDFS namenode should be in classpath");
-      return;
-    }
-    Lastfm dataSet = Lastfm.valueOf(args[2]);
-    Map<String, List<Integer>> itemFeatures = convertToItemFeatures(args[0], dataSet);
-    if (itemFeatures.isEmpty()) {
-      throw new IllegalStateException("Error converting the data file: [" + args[0] + ']');
-    }
-    Path output = new Path(args[1]);
-    boolean status = writeToSequenceFile(itemFeatures, output);
-    if (status) {
-      System.out.println("Data converted and written successfully to HDFS location: [" + output + ']');
-    } else {
-      System.err.println("Error writing the converted data to HDFS location: [" + output + ']');
-    }
-  }
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ClusterQualitySummarizer.java mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ClusterQualitySummarizer.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ClusterQualitySummarizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ClusterQualitySummarizer.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,13 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.clustering.streaming.tools;
 
 import java.io.FileOutputStream;
 import java.io.IOException;
-import java.io.OutputStreamWriter;
 import java.io.PrintWriter;
 import java.util.List;
 
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
+import com.google.common.io.Closeables;
 import org.apache.commons.cli2.CommandLine;
 import org.apache.commons.cli2.Group;
 import org.apache.commons.cli2.Option;
@@ -21,6 +38,7 @@
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.clustering.ClusteringUtils;
 import org.apache.mahout.clustering.streaming.mapreduce.CentroidWritable;
+import org.apache.mahout.common.AbstractJob;
 import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure;
 import org.apache.mahout.common.iterator.sequencefile.PathType;
@@ -30,10 +48,9 @@
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.math.stats.OnlineSummarizer;
 
-public class ClusterQualitySummarizer {
+public class ClusterQualitySummarizer extends AbstractJob {
   private String outputFile;
 
-
   private PrintWriter fileOut;
 
   private String trainFile;
@@ -53,36 +70,36 @@
     double maxDistance = 0;
     for (int i = 0; i < summarizers.size(); ++i) {
       OnlineSummarizer summarizer = summarizers.get(i);
-      if (summarizer.getCount() == 0) {
-        System.out.printf("Cluster %d is empty\n", i);
-        continue;
-      }
-      maxDistance = Math.max(maxDistance, summarizer.getMax());
-      System.out.printf("Average distance in cluster %d [%d]: %f\n", i, summarizer.getCount(), summarizer.getMean());
-      // If there is just one point in the cluster, quartiles cannot be estimated. We'll just assume all the quartiles
-      // equal the only value.
-      boolean moreThanOne = summarizer.getCount() > 1;
-      if (fileOut != null) {
-        fileOut.printf("%d,%f,%f,%f,%f,%f,%f,%f,%d,%s\n", i, summarizer.getMean(),
-            summarizer.getSD(),
-            summarizer.getQuartile(0),
-            moreThanOne ? summarizer.getQuartile(1) : summarizer.getQuartile(0),
-            moreThanOne ? summarizer.getQuartile(2) : summarizer.getQuartile(0),
-            moreThanOne ? summarizer.getQuartile(3) : summarizer.getQuartile(0),
-            summarizer.getQuartile(4), summarizer.getCount(), type);
+      if (summarizer.getCount() > 1) {
+        maxDistance = Math.max(maxDistance, summarizer.getMax());
+        System.out.printf("Average distance in cluster %d [%d]: %f\n", i, summarizer.getCount(), summarizer.getMean());
+        // If there is just one point in the cluster, quartiles cannot be estimated. We'll just assume all the quartiles
+        // equal the only value.
+        if (fileOut != null) {
+          fileOut.printf("%d,%f,%f,%f,%f,%f,%f,%f,%d,%s\n", i, summarizer.getMean(),
+              summarizer.getSD(),
+              summarizer.getQuartile(0),
+              summarizer.getQuartile(1),
+              summarizer.getQuartile(2),
+              summarizer.getQuartile(3),
+              summarizer.getQuartile(4), summarizer.getCount(), type);
+        }
+      } else {
+        System.out.printf("Cluster %d is has %d data point. Need atleast 2 data points in a cluster for" +
+            " OnlineSummarizer.\n", i, summarizer.getCount());
       }
     }
     System.out.printf("Num clusters: %d; maxDistance: %f\n", summarizers.size(), maxDistance);
   }
 
-  public void run(String[] args) {
+  public int run(String[] args) throws IOException {
     if (!parseArgs(args)) {
-      return;
+      return -1;
     }
 
     Configuration conf = new Configuration();
     try {
-      Configuration.dumpConfiguration(conf, new OutputStreamWriter(System.out));
+//      Configuration.dumpConfiguration(conf, new OutputStreamWriter(System.out));
 
       fileOut = new PrintWriter(new FileOutputStream(outputFile));
       fileOut.printf("cluster,distance.mean,distance.sd,distance.q0,distance.q1,distance.q2,distance.q3,"
@@ -145,8 +162,7 @@
       }
       System.out.printf("[Dunn Index] First: %f", ClusteringUtils.dunnIndex(centroids, distanceMeasure, summaries));
       if (compareSummaries != null) {
-        System.out.printf(" Second: %f\n",
-            ClusteringUtils.dunnIndex(centroidsCompare, distanceMeasure, compareSummaries));
+        System.out.printf(" Second: %f\n", ClusteringUtils.dunnIndex(centroidsCompare, distanceMeasure, compareSummaries));
       } else {
         System.out.printf("\n");
       }
@@ -158,13 +174,12 @@
       } else {
         System.out.printf("\n");
       }
-
-      if (outputFile != null) {
-        fileOut.close();
-      }
     } catch (IOException e) {
       System.out.println(e.getMessage());
+    } finally {
+      Closeables.close(fileOut, false);
     }
+    return 0;
   }
 
   private boolean parseArgs(String[] args) {
@@ -260,7 +275,7 @@
     return true;
   }
 
-  public static void main(String[] args) {
+  public static void main(String[] args) throws IOException {
     new ClusterQualitySummarizer().run(args);
   }
 }
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/IOUtils.java mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/IOUtils.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/IOUtils.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/IOUtils.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.clustering.streaming.tools;
 
 import com.google.common.base.Function;
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ResplitSequenceFiles.java mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ResplitSequenceFiles.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ResplitSequenceFiles.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/streaming/tools/ResplitSequenceFiles.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.clustering.streaming.tools;
 
 import java.io.IOException;
@@ -5,6 +22,7 @@
 import java.io.PrintWriter;
 import java.util.Iterator;
 
+import com.google.common.base.Charsets;
 import com.google.common.collect.Iterables;
 import org.apache.commons.cli2.CommandLine;
 import org.apache.commons.cli2.Group;
@@ -14,7 +32,6 @@
 import org.apache.commons.cli2.builder.GroupBuilder;
 import org.apache.commons.cli2.commandline.Parser;
 import org.apache.commons.cli2.util.HelpFormatter;
-import org.apache.commons.io.Charsets;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/dirichlet/Job.java mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/dirichlet/Job.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/dirichlet/Job.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/dirichlet/Job.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,168 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.syntheticcontrol.dirichlet;
-
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.cli2.builder.ArgumentBuilder;
-import org.apache.commons.cli2.builder.DefaultOptionBuilder;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.conversion.InputDriver;
-import org.apache.mahout.clustering.dirichlet.DirichletDriver;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.utils.clustering.ClusterDumper;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public final class Job extends AbstractJob {
-  
-  private static final Logger log = LoggerFactory.getLogger(Job.class);
-  
-  private static final String DIRECTORY_CONTAINING_CONVERTED_INPUT = "data";
-  
-  private Job() {
-  }
-  
-  public static void main(String[] args) throws Exception {
-    if (args.length > 0) {
-      log.info("Running with only user-supplied arguments");
-      ToolRunner.run(new Configuration(), new Job(), args);
-    } else {
-      log.info("Running with default arguments");
-      Path output = new Path("output");
-      HadoopUtil.delete(new Configuration(), output);
-      DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-          RandomAccessSparseVector.class.getName(), null, 60);
-      run(new Path("testdata"), output, description, 10, 5, 1.0, true, 0);
-    }
-  }
-  
-  @Override
-  public int run(String[] args) throws Exception {
-    addInputOption();
-    addOutputOption();
-    addOption(DefaultOptionCreator.maxIterationsOption().create());
-    addOption(DefaultOptionCreator.numClustersOption().withRequired(true).create());
-    addOption(DefaultOptionCreator.overwriteOption().create());
-    addOption(new DefaultOptionBuilder()
-        .withLongName(DirichletDriver.ALPHA_OPTION)
-        .withRequired(false)
-        .withShortName("m")
-        .withArgument(
-            new ArgumentBuilder().withName(DirichletDriver.ALPHA_OPTION).withDefault("1.0").withMinimum(1)
-                .withMaximum(1).create())
-        .withDescription("The alpha0 value for the DirichletDistribution. Defaults to 1.0").create());
-    addOption(new DefaultOptionBuilder()
-        .withLongName(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION)
-        .withRequired(false)
-        .withShortName("md")
-        .withArgument(
-            new ArgumentBuilder().withName(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION)
-                .withDefault(GaussianClusterDistribution.class.getName()).withMinimum(1).withMaximum(1).create())
-        .withDescription("The ModelDistribution class name. Defaults to GaussianClusterDistribution").create());
-    addOption(new DefaultOptionBuilder()
-        .withLongName(DirichletDriver.MODEL_PROTOTYPE_CLASS_OPTION)
-        .withRequired(false)
-        .withShortName("mp")
-        .withArgument(
-            new ArgumentBuilder().withName("prototypeClass").withDefault(RandomAccessSparseVector.class.getName())
-                .withMinimum(1).withMaximum(1).create())
-        .withDescription("The ModelDistribution prototype Vector class name. Defaults to RandomAccessSparseVector")
-        .create());
-    addOption(DefaultOptionCreator.distanceMeasureOption().withRequired(false).create());
-    addOption(DefaultOptionCreator.emitMostLikelyOption().create());
-    addOption(DefaultOptionCreator.thresholdOption().create());
-    
-    Map<String,List<String>> argMap = parseArguments(args);
-    if (argMap == null) {
-      return -1;
-    }
-    
-    Path input = getInputPath();
-    Path output = getOutputPath();
-    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(getConf(), output);
-    }
-    String modelFactory = getOption(DirichletDriver.MODEL_DISTRIBUTION_CLASS_OPTION);
-    String modelPrototype = getOption(DirichletDriver.MODEL_PROTOTYPE_CLASS_OPTION);
-    String distanceMeasure = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
-    int numModels = Integer.parseInt(getOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION));
-    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
-    boolean emitMostLikely = Boolean.parseBoolean(getOption(DefaultOptionCreator.EMIT_MOST_LIKELY_OPTION));
-    double threshold = Double.parseDouble(getOption(DefaultOptionCreator.THRESHOLD_OPTION));
-    double alpha0 = Double.parseDouble(getOption(DirichletDriver.ALPHA_OPTION));
-    DistributionDescription description = new DistributionDescription(modelFactory, modelPrototype, distanceMeasure,
-        60);
-    
-    run(input, output, description, numModels, maxIterations, alpha0, emitMostLikely, threshold);
-    return 0;
-  }
-  
-  /**
-   * Run the job using supplied arguments, deleting the output directory if it exists beforehand
-   * 
-   * @param input
-   *          the directory pathname for input points
-   * @param output
-   *          the directory pathname for output points
-   * @param description
-   *          the model distribution description
-   * @param numModels
-   *          the number of Models
-   * @param maxIterations
-   *          the maximum number of iterations
-   * @param alpha0
-   *          the alpha0 value for the DirichletDistribution
-   */
-  public static void run(Path input, Path output, DistributionDescription description, int numModels,
-      int maxIterations, double alpha0, boolean emitMostLikely, double threshold) throws Exception {
-    Path directoryContainingConvertedInput = new Path(output, DIRECTORY_CONTAINING_CONVERTED_INPUT);
-    InputDriver.runJob(input, directoryContainingConvertedInput, "org.apache.mahout.math.RandomAccessSparseVector");
-    DirichletDriver.run(new Configuration(), directoryContainingConvertedInput, output, description, numModels,
-        maxIterations, alpha0, true, emitMostLikely, threshold, false);
-    // run ClusterDumper
-    ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-*-final"), new Path(output,
-        "clusteredPoints"));
-    clusterDumper.printClusters(null);
-  }
-  
-  /**
-   * Actually prints out the clusters
-   * 
-   * @param clusters
-   *          a List of Lists of DirichletClusters
-   * @param significant
-   *          the minimum number of samples to enable printing a model
-   */
-  /*
-   * private static void printClusters(Iterable<List<DirichletCluster>> clusters, int significant) { int row = 0;
-   * StringBuilder result = new StringBuilder(100); for (List<DirichletCluster> r : clusters) {
-   * result.append("sample=").append(row++).append("]= "); for (int k = 0; k < r.size(); k++) { Model<VectorWritable>
-   * model = r.get(k).getModel(); if (model.count() > significant) { int total = (int) r.get(k).getTotalCount();
-   * result.append('m').append(k).append('(').append(total).append(')').append(model).append(", "); } }
-   * result.append('\n'); } result.append('\n'); log.info(result.toString()); }
-   */
-}
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/fuzzykmeans/Job.java mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/fuzzykmeans/Job.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/fuzzykmeans/Job.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/fuzzykmeans/Job.java	2014-03-29 01:03:14.000000000 -0700
@@ -133,14 +133,12 @@
     InputDriver.runJob(input, directoryContainingConvertedInput, "org.apache.mahout.math.RandomAccessSparseVector");
     log.info("Running Canopy to get initial clusters");
     Path canopyOutput = new Path(output, "canopies");
-    CanopyDriver
-        .run(new Configuration(), directoryContainingConvertedInput, canopyOutput, measure, t1, t2, false, 0.0, false);
+    CanopyDriver.run(new Configuration(), directoryContainingConvertedInput, canopyOutput, measure, t1, t2, false, 0.0, false);
     log.info("Running FuzzyKMeans");
     FuzzyKMeansDriver.run(directoryContainingConvertedInput, new Path(canopyOutput, "clusters-0-final"), output,
-        measure, convergenceDelta, maxIterations, fuzziness, true, true, 0.0, false);
+        convergenceDelta, maxIterations, fuzziness, true, true, 0.0, false);
     // run ClusterDumper
-    ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-*-final"), new Path(output,
-        "clusteredPoints"));
+    ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-*-final"), new Path(output, "clusteredPoints"));
     clusterDumper.printClusters(null);
   }
 }
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java	2014-03-29 01:03:14.000000000 -0700
@@ -132,12 +132,11 @@
     Path clusters = new Path(output, "random-seeds");
     clusters = RandomSeedGenerator.buildRandom(conf, directoryContainingConvertedInput, clusters, k, measure);
     log.info("Running KMeans with k = {}", k);
-    KMeansDriver.run(conf, directoryContainingConvertedInput, clusters, output, measure, convergenceDelta,
+    KMeansDriver.run(conf, directoryContainingConvertedInput, clusters, output, convergenceDelta,
         maxIterations, true, 0.0, false);
     // run ClusterDumper
     Path outGlob = new Path(output, "clusters-*-final");
-    Path clusteredPoints = new Path(output,
-            "clusteredPoints");
+    Path clusteredPoints = new Path(output,"clusteredPoints");
     log.info("Dumping out clusters from clusters: {} and clusteredPoints: {}", outGlob, clusteredPoints);
     ClusterDumper clusterDumper = new ClusterDumper(outGlob, clusteredPoints);
     clusterDumper.printClusters(null);
@@ -179,7 +178,7 @@
         false);
     log.info("Running KMeans");
     KMeansDriver.run(conf, directoryContainingConvertedInput, new Path(canopyOutput, Cluster.INITIAL_CLUSTERS_DIR
-        + "-final"), output, measure, convergenceDelta, maxIterations, true, 0.0, false);
+        + "-final"), output, convergenceDelta, maxIterations, true, 0.0, false);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-*-final"), new Path(output,
         "clusteredPoints"));
diff -uNar -x .git mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/meanshift/Job.java mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/meanshift/Job.java
--- mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/meanshift/Job.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/meanshift/Job.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,149 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.syntheticcontrol.meanshift;
-
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.clustering.conversion.meanshift.InputDriver;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver;
-import org.apache.mahout.common.AbstractJob;
-import org.apache.mahout.common.ClassUtils;
-import org.apache.mahout.common.HadoopUtil;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.apache.mahout.common.distance.DistanceMeasure;
-import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
-import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
-import org.apache.mahout.utils.clustering.ClusterDumper;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public final class Job extends AbstractJob {
-  
-  private static final Logger log = LoggerFactory.getLogger(Job.class);
-  
-  private static final String DIRECTORY_CONTAINING_CONVERTED_INPUT = "data";
-  
-  private Job() {
-  }
-  
-  public static void main(String[] args) throws Exception {
-    if (args.length > 0) {
-      log.info("Running with only user-supplied arguments");
-      ToolRunner.run(new Configuration(), new Job(), args);
-    } else {
-      log.info("Running with default arguments");
-      Path output = new Path("output");
-      Configuration conf = new Configuration();
-      HadoopUtil.delete(conf, output);
-      run(conf, new Path("testdata"), output, new EuclideanDistanceMeasure(), new TriangularKernelProfile(), 47.6, 1,
-          0.5, 10);
-    }
-  }
-  
-  @Override
-  public int run(String[] args) throws Exception {
-    addInputOption();
-    addOutputOption();
-    addOption(DefaultOptionCreator.convergenceOption().create());
-    addOption(DefaultOptionCreator.maxIterationsOption().create());
-    addOption(DefaultOptionCreator.overwriteOption().create());
-    addOption(DefaultOptionCreator.inputIsCanopiesOption().create());
-    addOption(DefaultOptionCreator.distanceMeasureOption().create());
-    addOption(DefaultOptionCreator.kernelProfileOption().create());
-    addOption(DefaultOptionCreator.t1Option().create());
-    addOption(DefaultOptionCreator.t2Option().create());
-    addOption(DefaultOptionCreator.clusteringOption().create());
-    
-    Map<String,List<String>> argMap = parseArguments(args);
-    if (argMap == null) {
-      return -1;
-    }
-    
-    Path input = getInputPath();
-    Path output = getOutputPath();
-    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {
-      HadoopUtil.delete(new Configuration(), output);
-    }
-    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);
-    String kernelProfileClass = getOption(DefaultOptionCreator.KERNEL_PROFILE_OPTION);
-    double t1 = Double.parseDouble(getOption(DefaultOptionCreator.T1_OPTION));
-    double t2 = Double.parseDouble(getOption(DefaultOptionCreator.T2_OPTION));
-    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));
-    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));
-    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);
-    IKernelProfile kernelProfile = ClassUtils.instantiateAs(kernelProfileClass, IKernelProfile.class);
-    run(getConf(), input, output, measure, kernelProfile, t1, t2, convergenceDelta, maxIterations);
-    return 0;
-  }
-
-  /**
-   * Run the meanshift clustering job on an input dataset using the given
-   * distance measure, t1, t2 and iteration parameters. All output data will be
-   * written to the output directory, which will be initially deleted if it
-   * exists. The clustered points will reside in the path
-   * <output>/clustered-points. By default, the job expects the a file
-   * containing synthetic_control.data as obtained from
-   * http://archive.ics.uci.edu/ml/datasets/Synthetic+Control+Chart+Time+Series
-   * resides in a directory named "testdata", and writes output to a directory
-   * named "output".
-   * 
-   * @param input
-   *          the String denoting the input directory path
-   * @param output
-   *          the String denoting the output directory path
-   * @param measure
-   *          the DistanceMeasure to use
-   * @param kernelProfile
-   *          the IKernelProfile to use
-   * @param t1
-   *          the meanshift canopy T1 threshold
-   * @param t2
-   *          the meanshift canopy T2 threshold
-   * @param convergenceDelta
-   *          the double convergence criteria for iterations
-   * @param maxIterations
-   *          the int maximum number of iterations
-   */
-  public static void run(Configuration conf,
-                         Path input,
-                         Path output,
-                         DistanceMeasure measure,
-                         IKernelProfile kernelProfile,
-                         double t1,
-                         double t2,
-                         double convergenceDelta,
-                         int maxIterations)
-    throws Exception {
-    Path directoryContainingConvertedInput = new Path(output,
-        DIRECTORY_CONTAINING_CONVERTED_INPUT);
-    InputDriver.runJob(input, directoryContainingConvertedInput);
-    MeanShiftCanopyDriver.run(conf, directoryContainingConvertedInput, output,
-        measure, kernelProfile, t1, t2, convergenceDelta, maxIterations, true,
-        true, false);
-    // run ClusterDumper
-    ClusterDumper clusterDumper = new ClusterDumper(new Path(output,
-        "clusters-*-final"), new Path(output, "clusteredPoints"));
-    clusterDumper.printClusters(null);
-  }
-  
-}
diff -uNar -x .git mahout/examples/src/test/java/org/apache/mahout/clustering/display/ClustersFilterTest.java mahout/examples/src/test/java/org/apache/mahout/clustering/display/ClustersFilterTest.java
--- mahout/examples/src/test/java/org/apache/mahout/clustering/display/ClustersFilterTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/examples/src/test/java/org/apache/mahout/clustering/display/ClustersFilterTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -35,7 +35,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    configuration = new Configuration();
+    configuration = getConfiguration();
     output = getTestTempDirPath();
   }
 
diff -uNar -x .git mahout/integration/pom.xml mahout/integration/pom.xml
--- mahout/integration/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -24,7 +24,7 @@
   <parent>
     <groupId>org.apache.mahout</groupId>
     <artifactId>mahout</artifactId>
-    <version>0.8</version>
+    <version>1.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
 
@@ -35,26 +35,14 @@
 
   <packaging>jar</packaging>
 
+  <properties>
+    <hbase.version>0.98.0-hadoop2</hbase.version>
+  </properties>
+
   <build>
     <plugins>
       <plugin>
         <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-dependency-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>copy-dependencies</id>
-            <phase>package</phase>
-            <goals>
-              <goal>copy-dependencies</goal>
-            </goals>
-            <configuration>
-              <!-- configure the plugin here -->
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-remote-resources-plugin</artifactId>
         <configuration>
           <appendedResourcesDirectory>../src/main/appended-resources</appendedResourcesDirectory>
@@ -75,11 +63,6 @@
         <artifactId>maven-source-plugin</artifactId>
       </plugin>
 
-      <plugin>
-        <groupId>org.mortbay.jetty</groupId>
-        <artifactId>maven-jetty-plugin</artifactId>
-        <version>6.1.26</version>
-      </plugin>
     </plugins>
 
   </build>
@@ -108,76 +91,61 @@
       <scope>test</scope>
     </dependency>
 
-      <!-- 3rd party -->
+    <!-- 3rd party -->
 
     <dependency>
       <groupId>commons-dbcp</groupId>
       <artifactId>commons-dbcp</artifactId>
+      <optional>true</optional>
     </dependency>
 
     <dependency>
       <groupId>commons-pool</groupId>
       <artifactId>commons-pool</artifactId>
+      <optional>true</optional>
     </dependency>
 
     <dependency>
-      <groupId>org.apache.solr</groupId>
-      <artifactId>solr-commons-csv</artifactId>
-      <version>3.5.0</version>
+      <groupId>commons-io</groupId>
+      <artifactId>commons-io</artifactId>
     </dependency>
 
     <dependency>
-        <groupId>org.apache.lucene</groupId>
-        <artifactId>lucene-benchmark</artifactId>
-    </dependency>
-    <dependency>
-        <groupId>org.apache.lucene</groupId>
-        <artifactId>lucene-analyzers-common</artifactId>
+      <groupId>org.apache.solr</groupId>
+      <artifactId>solr-commons-csv</artifactId>
+      <version>3.5.0</version>
     </dependency>
 
     <dependency>
-      <groupId>commons-io</groupId>
-      <artifactId>commons-io</artifactId>
+      <groupId>org.apache.lucene</groupId>
+      <artifactId>lucene-benchmark</artifactId>
+      <optional>true</optional>
     </dependency>
-
     <dependency>
-      <groupId>javax.servlet</groupId>
-      <artifactId>servlet-api</artifactId>
-      <scope>provided</scope>
+      <groupId>org.apache.lucene</groupId>
+      <artifactId>lucene-analyzers-common</artifactId>
+      <optional>true</optional>
     </dependency>
     
     <dependency>
       <groupId>org.mongodb</groupId>
       <artifactId>mongo-java-driver</artifactId>
-      <version>2.11.1</version>
+      <version>2.11.2</version>
+      <optional>true</optional>
     </dependency>
 
     <dependency>
       <groupId>org.mongodb</groupId>
       <artifactId>bson</artifactId>
-      <version>2.11.1</version>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.cassandra</groupId>
-      <artifactId>cassandra-all</artifactId>
-      <version>1.2.5</version>
-      <exclusions>
-        <exclusion>
-          <groupId>org.slf4j</groupId>
-          <artifactId>slf4j-log4j12</artifactId>
-        </exclusion>
-        <exclusion>
-         <groupId>log4j</groupId>
-         <artifactId>log4j</artifactId>
-       </exclusion>
-      </exclusions>
+      <version>2.11.2</version>
+      <optional>true</optional>
     </dependency>
 
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-client</artifactId>
-      <version>0.95.0</version>
+      <version>${hbase.version}</version>
+      <optional>true</optional>
       <exclusions>
         <exclusion>
           <groupId>org.slf4j</groupId>
@@ -191,9 +159,10 @@
     </dependency>
 
     <dependency>
-      <groupId>me.prettyprint</groupId>
+      <groupId>org.hectorclient</groupId>
       <artifactId>hector-core</artifactId>
-      <version>1.0-5</version>
+      <version>1.1-4</version>
+      <optional>true</optional>
     </dependency>
 
     <dependency>
@@ -214,16 +183,31 @@
     </dependency>
 
     <dependency>
-      <groupId>org.easymock</groupId>
-      <artifactId>easymock</artifactId>
+      <groupId>com.carrotsearch.randomizedtesting</groupId>
+      <artifactId>randomizedtesting-runner</artifactId>
       <scope>test</scope>
     </dependency>
 
     <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-core</artifactId>
+      <groupId>org.easymock</groupId>
+      <artifactId>easymock</artifactId>
+      <scope>test</scope>
     </dependency>
 
-
   </dependencies>
+
+  <profiles>
+    <profile>
+    <id>hadoop1</id>
+    <activation>
+      <property>
+        <name>!hadoop2.version</name>
+      </property>
+    </activation>
+    <properties>
+      <hbase.version>0.95.1-hadoop1</hbase.version>
+    </properties>
+    </profile>
+  </profiles>
+
 </project>
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/BenchmarkRunner.java mahout/integration/src/main/java/org/apache/mahout/benchmark/BenchmarkRunner.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/BenchmarkRunner.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/BenchmarkRunner.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import java.util.Random;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/CloneBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/CloneBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/CloneBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/CloneBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import static org.apache.mahout.benchmark.VectorBenchmarks.DENSE_VECTOR;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/ClosestCentroidBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/ClosestCentroidBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/ClosestCentroidBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/ClosestCentroidBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import java.io.IOException;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/DistanceBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/DistanceBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/DistanceBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/DistanceBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import static org.apache.mahout.benchmark.VectorBenchmarks.DENSE_FN_RAND;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/DotBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/DotBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/DotBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/DotBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import static org.apache.mahout.benchmark.VectorBenchmarks.DENSE_FN_RAND;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/MinusBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/MinusBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/MinusBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/MinusBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import static org.apache.mahout.benchmark.VectorBenchmarks.DENSE_FN_RAND;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/PlusBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/PlusBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/PlusBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/PlusBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import static org.apache.mahout.benchmark.VectorBenchmarks.DENSE_FN_RAND;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/SerializationBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/SerializationBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/SerializationBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/SerializationBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import java.io.IOException;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/benchmark/TimesBenchmark.java mahout/integration/src/main/java/org/apache/mahout/benchmark/TimesBenchmark.java
--- mahout/integration/src/main/java/org/apache/mahout/benchmark/TimesBenchmark.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/benchmark/TimesBenchmark.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.benchmark;
 
 import static org.apache.mahout.benchmark.VectorBenchmarks.DENSE_FN_RAND;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/cassandra/CassandraDataModel.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/cassandra/CassandraDataModel.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/cassandra/CassandraDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/cassandra/CassandraDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -126,7 +126,7 @@
   public CassandraDataModel(String host, int port, String keyspaceName) {
     
     Preconditions.checkNotNull(host);
-    Preconditions.checkArgument(port > 0);
+    Preconditions.checkArgument(port > 0, "port must be greater then 0!");
     Preconditions.checkNotNull(keyspaceName);
 
     cluster = HFactory.getOrCreateCluster(CassandraDataModel.class.getSimpleName(), host + ':' + port);
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,7 +21,6 @@
 import java.io.IOException;
 import java.nio.ByteBuffer;
 import java.util.Collection;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.SortedMap;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLBooleanPrefJDBCDataModel.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLBooleanPrefJDBCDataModel.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLBooleanPrefJDBCDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLBooleanPrefJDBCDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -45,7 +45,7 @@
 
   /**
    * <p>
-   * Creates a  using the default {@link javax.sql.DataSource} (named
+   * Creates a {@link MySQLBooleanPrefJDBCDataModel} using the default {@link javax.sql.DataSource} (named
    * {@link #DEFAULT_DATASOURCE_NAME} and default table/column names.
    * </p>
    * 
@@ -58,7 +58,7 @@
   
   /**
    * <p>
-   * Creates a  using the default {@link javax.sql.DataSource} found
+   * Creates a {@link MySQLBooleanPrefJDBCDataModel} using the default {@link javax.sql.DataSource} found
    * under the given name, and using default table/column names.
    * </p>
    * 
@@ -77,7 +77,7 @@
   
   /**
    * <p>
-   * Creates a  using the given {@link javax.sql.DataSource} and default
+   * Creates a {@link MySQLBooleanPrefJDBCDataModel} using the given {@link javax.sql.DataSource} and default
    * table/column names.
    * </p>
    * 
@@ -94,7 +94,7 @@
   
   /**
    * <p>
-   * Creates a  using the given {@link javax.sql.DataSource} and default
+   * Creates a {@link MySQLBooleanPrefJDBCDataModel} using the given {@link javax.sql.DataSource} and default
    * table/column names.
    * </p>
    * 
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLJDBCDataModel.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLJDBCDataModel.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLJDBCDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/MySQLJDBCDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -129,7 +129,7 @@
   
   /**
    * <p>
-   * Creates a  using the default {@link DataSource} (named
+   * Creates a {@link MySQLJDBCDataModel} using the default {@link DataSource} (named
    * {@link #DEFAULT_DATASOURCE_NAME} and default table/column names.
    * </p>
    * 
@@ -142,7 +142,7 @@
   
   /**
    * <p>
-   * Creates a  using the default {@link DataSource} found under the given name, and
+   * Creates a {@link MySQLJDBCDataModel} using the default {@link DataSource} found under the given name, and
    * using default table/column names.
    * </p>
    * 
@@ -162,7 +162,7 @@
   
   /**
    * <p>
-   * Creates a  using the given {@link DataSource} and default table/column names.
+   * Creates a {@link MySQLJDBCDataModel} using the given {@link DataSource} and default table/column names.
    * </p>
    * 
    * @param dataSource
@@ -179,7 +179,7 @@
   
   /**
    * <p>
-   * Creates a  using the given {@link DataSource} and default table/column names.
+   * Creates a {@link MySQLJDBCDataModel} using the given {@link DataSource} and default table/column names.
    * </p>
    * 
    * @param dataSource
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -757,10 +757,10 @@
                          boolean add) throws NoSuchUserException, NoSuchItemException {
     Preconditions.checkNotNull(userID);
     Preconditions.checkNotNull(items);
-    Preconditions.checkArgument(!userID.isEmpty());
+    Preconditions.checkArgument(!userID.isEmpty(), "userID is empty");
     for (List<String> item : items) {
       Preconditions.checkNotNull(item.get(0));
-      Preconditions.checkArgument(!item.get(0).isEmpty());
+      Preconditions.checkArgument(!item.get(0).isEmpty(), "item is empty");
     }
     if (userIsObject && !ID_PATTERN.matcher(userID).matches()) {
       throw new IllegalArgumentException();
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/AbstractJDBCDiffStorage.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/AbstractJDBCDiffStorage.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/AbstractJDBCDiffStorage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/AbstractJDBCDiffStorage.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,514 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone.jdbc;
-
-import java.sql.Connection;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.Collection;
-import java.util.concurrent.Callable;
-
-import javax.sql.DataSource;
-
-import org.apache.mahout.cf.taste.common.Refreshable;
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.common.FastIDSet;
-import org.apache.mahout.cf.taste.impl.common.FixedRunningAverage;
-import org.apache.mahout.cf.taste.impl.common.FixedRunningAverageAndStdDev;
-import org.apache.mahout.cf.taste.impl.common.RefreshHelper;
-import org.apache.mahout.cf.taste.impl.common.RunningAverage;
-import org.apache.mahout.cf.taste.impl.common.jdbc.AbstractJDBCComponent;
-import org.apache.mahout.cf.taste.model.JDBCDataModel;
-import org.apache.mahout.cf.taste.model.PreferenceArray;
-import org.apache.mahout.cf.taste.recommender.slopeone.DiffStorage;
-import org.apache.mahout.common.IOUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Preconditions;
-
-/**
- * <p>
- * A {@link DiffStorage} which stores diffs in a database. Database-specific implementations subclass this
- * abstract class. Note that this implementation has a fairly particular dependence on the
- * {@link org.apache.mahout.cf.taste.model.DataModel} used; it needs a {@link JDBCDataModel} attached to the
- * same database since its efficient operation depends on accessing preference data in the database directly.
- * </p>
- */
-public abstract class AbstractJDBCDiffStorage extends AbstractJDBCComponent implements DiffStorage {
-  
-  private static final Logger log = LoggerFactory.getLogger(AbstractJDBCDiffStorage.class);
-  
-  public static final String DEFAULT_DIFF_TABLE = "taste_slopeone_diffs";
-  public static final String DEFAULT_ITEM_A_COLUMN = "item_id_a";
-  public static final String DEFAULT_ITEM_B_COLUMN = "item_id_b";
-  public static final String DEFAULT_COUNT_COLUMN = "count";
-  public static final String DEFAULT_AVERAGE_DIFF_COLUMN = "average_diff";
-  public static final String DEFAULT_STDEV_COLUMN = "standard_deviation";
-
-  private final JDBCDataModel dataModel;
-  private final DataSource dataSource;
-  private final String getDiffSQL;
-  private final String getDiffsSQL;
-  private final String getAverageItemPrefSQL;
-  private final String getDiffsAffectedByUserSQL;
-  private final String[] updateDiffSQLs;
-  private final String updateOneDiffSQL;
-  private final String addDiffSQL;
-  private final String removeDiffSQL;
-  private final String getRecommendableItemsSQL;
-  private final String deleteDiffsSQL;
-  private final String createDiffsSQL;
-  private final String diffsExistSQL;
-  private final int minDiffCount;
-  private final RefreshHelper refreshHelper;
-  
-  protected AbstractJDBCDiffStorage(JDBCDataModel dataModel,
-                                    String getDiffSQL,
-                                    String getDiffsSQL,
-                                    String getAverageItemPrefSQL,
-                                    String getDiffsAffectedByUserSQL,
-                                    String[] updateDiffSQLs,
-                                    String updateOneDiffSQL,
-                                    String addDiffSQL,
-                                    String removeDiffSQL,
-                                    String getRecommendableItemsSQL,
-                                    String deleteDiffsSQL,
-                                    String createDiffsSQL,
-                                    String diffsExistSQL,
-                                    int minDiffCount) throws TasteException {
-    
-    AbstractJDBCComponent.checkNotNullAndLog("dataModel", dataModel);
-    AbstractJDBCComponent.checkNotNullAndLog("getDiffSQL", getDiffSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("getDiffsSQL", getDiffsSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("getAverageItemPrefSQL", getAverageItemPrefSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("getDiffsAffectedByUserSQL", getDiffsAffectedByUserSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("updateDiffSQLs", updateDiffSQLs);
-    AbstractJDBCComponent.checkNotNullAndLog("updateOneDiffSQL", updateOneDiffSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("addDiffSQL", addDiffSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("removeDiffSQL", removeDiffSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("getRecommendableItemsSQL", getRecommendableItemsSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("deleteDiffsSQL", deleteDiffsSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("createDiffsSQL", createDiffsSQL);
-    AbstractJDBCComponent.checkNotNullAndLog("diffsExistSQL", diffsExistSQL);
-
-    Preconditions.checkArgument(minDiffCount >= 0, "minDiffCount is not positive");
-
-    this.dataModel = dataModel;
-    this.dataSource = dataModel.getDataSource();
-    this.getDiffSQL = getDiffSQL;
-    this.getDiffsSQL = getDiffsSQL;
-    this.getAverageItemPrefSQL = getAverageItemPrefSQL;
-    this.getDiffsAffectedByUserSQL = getDiffsAffectedByUserSQL;
-    this.updateDiffSQLs = updateDiffSQLs;
-    this.updateOneDiffSQL = updateOneDiffSQL;
-    this.addDiffSQL = addDiffSQL;
-    this.removeDiffSQL = removeDiffSQL;
-    this.getRecommendableItemsSQL = getRecommendableItemsSQL;
-    this.deleteDiffsSQL = deleteDiffsSQL;
-    this.createDiffsSQL = createDiffsSQL;
-    this.diffsExistSQL = diffsExistSQL;
-    this.minDiffCount = minDiffCount;
-    this.refreshHelper = new RefreshHelper(new Callable<Object>() {
-      @Override
-      public Object call() throws TasteException {
-        buildAverageDiffs();
-        return null;
-      }
-    });
-    refreshHelper.addDependency(dataModel);
-    if (isDiffsExist()) {
-      log.info("Diffs already exist in database; using them instead of recomputing");
-    } else {
-      log.info("No diffs exist in database; recomputing...");
-      buildAverageDiffs();
-    }
-  }
-  
-  @Override
-  public RunningAverage getDiff(long itemID1, long itemID2) throws TasteException {
-
-    boolean flipped = itemID1 > itemID2;
-    if (flipped) {
-      long temp = itemID1;
-      itemID1 = itemID2;
-      itemID2 = temp;
-    }
-
-    Connection conn = null;
-    PreparedStatement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getDiffSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      stmt.setLong(1, itemID1);
-      stmt.setLong(2, itemID2);
-      log.debug("Executing SQL query: {}", getDiffSQL);
-      rs = stmt.executeQuery();
-      if (rs.next()) {
-        double average = rs.getDouble(2);
-        if (flipped) {
-          average = -average;
-        }
-        return new FixedRunningAverageAndStdDev(average, rs.getDouble(3), rs.getInt(1));
-      } else {
-        return null;
-      }
-    } catch (SQLException sqle) {
-      log.warn("Exception while retrieving diff", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-  }
-  
-  @Override
-  public RunningAverage[] getDiffs(long userID, long itemID, PreferenceArray prefs) throws TasteException {
-    int size = prefs.length();
-    RunningAverage[] result = new RunningAverage[size];
-    Connection conn = null;
-    PreparedStatement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getDiffsSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      stmt.setLong(1, itemID);
-      stmt.setLong(2, userID);
-      stmt.setLong(3, itemID);
-      stmt.setLong(4, userID);
-      log.debug("Executing SQL query: {}", getDiffsSQL);
-      rs = stmt.executeQuery();
-      // We should have up to one result for each Preference in prefs
-      // They are both ordered by item. Step through and create a RunningAverage[]
-      // with nulls for Preferences that have no corresponding result row
-      int i = 0;
-      while (rs.next()) {
-        long nextResultItemID = rs.getLong(4);
-        while (i < size && prefs.getItemID(i) != nextResultItemID) {
-          i++;
-          // result[i] is null for these values of i
-        }
-        if (i == size) {
-          break;
-        }
-        result[i] = new FixedRunningAverageAndStdDev(rs.getDouble(2), rs.getDouble(3), rs.getInt(1));
-        i++;
-      }
-    } catch (SQLException sqle) {
-      log.warn("Exception while retrieving diff", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-    return result;
-  }
-  
-  @Override
-  public RunningAverage getAverageItemPref(long itemID) throws TasteException {
-    Connection conn = null;
-    PreparedStatement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getAverageItemPrefSQL, ResultSet.TYPE_FORWARD_ONLY,
-        ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      stmt.setLong(1, itemID);
-      log.debug("Executing SQL query: {}", getAverageItemPrefSQL);
-      rs = stmt.executeQuery();
-      if (rs.next()) {
-        int count = rs.getInt(1);
-        if (count > 0) {
-          return new FixedRunningAverage(rs.getDouble(2), count);
-        }
-      }
-      return null;
-    } catch (SQLException sqle) {
-      log.warn("Exception while retrieving average item pref", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-  }
-
-
-  @Override
-  public void addItemPref(long userID, long itemID, float prefValue) throws TasteException {
-
-    PreferenceArray prefs = dataModel.getPreferencesFromUser(userID);
-    FastIDSet unupdatedItemIDs = new FastIDSet();
-    for (long anItemID : prefs.getIDs()) {
-      unupdatedItemIDs.add(anItemID);
-    }
-
-    Connection conn = null;
-    PreparedStatement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getDiffsAffectedByUserSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      stmt.setLong(1, userID);
-      log.debug("Executing SQL query: {}", getDiffsAffectedByUserSQL);
-      rs = stmt.executeQuery();
-
-      while (rs.next()) {
-        int count = rs.getInt(1);
-        float average = rs.getFloat(2);
-        long itemIDA = rs.getLong(3);
-        long itemIDB = rs.getLong(4);
-        float currentOtherPrefValue = rs.getFloat(5);
-        float prefDelta;
-        long otherItemID;
-        if (itemID == itemIDA) {
-          prefDelta = currentOtherPrefValue - prefValue;
-          otherItemID = itemIDB;
-        } else {
-          prefDelta = prefValue - currentOtherPrefValue;
-          otherItemID = itemIDA;
-        }
-        float newAverage = (average * count + prefDelta) / (count + 1);
-        updateOneDiff(conn, count + 1, newAverage, itemIDA, itemIDB);
-        unupdatedItemIDs.remove(otherItemID);
-      }
-
-    } catch (SQLException sqle) {
-      log.warn("Exception while adding item diff", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-
-    // Catch antyhing that wasn't already covered in the diff table
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(addDiffSQL);
-      for (long unupdatedItemID : unupdatedItemIDs) {
-        if (unupdatedItemID < itemID) {
-          stmt.setLong(1, unupdatedItemID);
-          stmt.setLong(2, itemID);
-          stmt.setFloat(3, prefValue);
-        } else {
-          stmt.setLong(1, itemID);
-          stmt.setLong(2, unupdatedItemID);
-          stmt.setFloat(3, -prefValue);
-        }
-        log.debug("Executing SQL query: {}", getDiffsAffectedByUserSQL);
-        stmt.executeUpdate();
-      }
-    } catch (SQLException sqle) {
-      log.warn("Exception while adding item diff", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(null, stmt, conn);
-    }
-  }
-
-  private void updateOneDiff(Connection conn, int newCount, float newAverage, long itemIDA, long itemIDB)
-    throws SQLException {
-    PreparedStatement stmt = conn.prepareStatement(updateOneDiffSQL);
-    try {
-      stmt.setInt(1, newCount);
-      stmt.setFloat(2, newAverage);
-      stmt.setLong(3, itemIDA);
-      stmt.setLong(4, itemIDB);
-      log.debug("Executing SQL update: {}", updateOneDiffSQL);
-      stmt.executeUpdate();
-    } finally {
-      IOUtils.quietClose(stmt);
-    }
-  }
-
-  /**
-   * Note that this implementation does <em>not</em> update standard deviations. This would
-   * be expensive relative to the value of slightly adjusting these values, which are merely
-   * used as weighted. Rebuilding the diffs table will update standard deviations.
-   */
-  @Override
-  public void updateItemPref(long itemID, float prefDelta) throws TasteException {
-    Connection conn = null;
-    try {
-      conn = dataSource.getConnection();
-      doPartialUpdate(updateDiffSQLs[0], itemID, prefDelta, conn);
-      doPartialUpdate(updateDiffSQLs[1], itemID, prefDelta, conn);
-    } catch (SQLException sqle) {
-      log.warn("Exception while updating item diff", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(conn);
-    }
-  }
-
-  @Override
-  public void removeItemPref(long userID, long itemID, float prefValue) throws TasteException {
-    Connection conn = null;
-    PreparedStatement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getDiffsAffectedByUserSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      stmt.setLong(1, userID);
-      log.debug("Executing SQL query: {}", getDiffsAffectedByUserSQL);
-      rs = stmt.executeQuery();
-
-      while (rs.next()) {
-        int count = rs.getInt(1);
-        long itemIDA = rs.getLong(3);
-        long itemIDB = rs.getLong(4);
-        if (count == minDiffCount) {
-          // going to remove the diff
-          removeOneDiff(conn, itemIDA, itemIDB);
-        } else {
-          float average = rs.getFloat(2);
-          float currentOtherPrefValue = rs.getFloat(5);
-          float prefDelta;
-          if (itemID == itemIDA) {
-            prefDelta = currentOtherPrefValue - prefValue;
-          } else {
-            prefDelta = prefValue - currentOtherPrefValue;
-          }
-          float newAverage = (average * count - prefDelta) / (count - 1);
-          updateOneDiff(conn, count - 1, newAverage, itemIDA, itemIDB);
-        }
-      }
-    } catch (SQLException sqle) {
-      log.warn("Exception while removing item diff", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-  }
-
-  private void removeOneDiff(Connection conn, long itemIDA, long itemIDB)
-    throws SQLException {
-    PreparedStatement stmt = conn.prepareStatement(removeDiffSQL);
-    try {
-      stmt.setLong(1, itemIDA);
-      stmt.setLong(2, itemIDB);
-      log.debug("Executing SQL update: {}", removeDiffSQL);
-      stmt.executeUpdate();
-    } finally {
-      IOUtils.quietClose(stmt);
-    }
-  }
-  
-  private static void doPartialUpdate(String sql, long itemID, double prefDelta, Connection conn) throws SQLException {
-    PreparedStatement stmt = conn.prepareStatement(sql);
-    try {
-      stmt.setDouble(1, prefDelta);
-      stmt.setLong(2, itemID);
-      log.debug("Executing SQL update: {}", sql);
-      stmt.executeUpdate();
-    } finally {
-      IOUtils.quietClose(stmt);
-    }
-  }
-  
-  @Override
-  public FastIDSet getRecommendableItemIDs(long userID) throws TasteException {
-    Connection conn = null;
-    PreparedStatement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.prepareStatement(getRecommendableItemsSQL, ResultSet.TYPE_FORWARD_ONLY,
-        ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      stmt.setLong(1, userID);
-      stmt.setLong(2, userID);
-      stmt.setLong(3, userID);
-      log.debug("Executing SQL query: {}", getRecommendableItemsSQL);
-      rs = stmt.executeQuery();
-      FastIDSet itemIDs = new FastIDSet();
-      while (rs.next()) {
-        itemIDs.add(rs.getLong(1));
-      }
-      return itemIDs;
-    } catch (SQLException sqle) {
-      log.warn("Exception while retrieving recommendable items", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-  }
-  
-  private void buildAverageDiffs() throws TasteException {
-    Connection conn = null;
-    try {
-      conn = dataSource.getConnection();
-      PreparedStatement stmt = null;
-      try {
-        stmt = conn.prepareStatement(deleteDiffsSQL);
-        log.debug("Executing SQL update: {}", deleteDiffsSQL);
-        stmt.executeUpdate();
-      } finally {
-        IOUtils.quietClose(stmt);
-      }
-      try {
-        stmt = conn.prepareStatement(createDiffsSQL);
-        stmt.setInt(1, minDiffCount);
-        log.debug("Executing SQL update: {}", createDiffsSQL);
-        stmt.executeUpdate();
-      } finally {
-        IOUtils.quietClose(stmt);
-      }
-    } catch (SQLException sqle) {
-      log.warn("Exception while updating/deleting diffs", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(conn);
-    }
-  }
-  
-  private boolean isDiffsExist() throws TasteException {
-    Connection conn = null;
-    Statement stmt = null;
-    ResultSet rs = null;
-    try {
-      conn = dataSource.getConnection();
-      stmt = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      stmt.setFetchDirection(ResultSet.FETCH_FORWARD);
-      stmt.setFetchSize(getFetchSize());
-      log.debug("Executing SQL query: {}", diffsExistSQL);
-      rs = stmt.executeQuery(diffsExistSQL);
-      rs.next();
-      return rs.getInt(1) > 0;
-    } catch (SQLException sqle) {
-      log.warn("Exception while deleting diffs", sqle);
-      throw new TasteException(sqle);
-    } finally {
-      IOUtils.quietClose(rs, stmt, conn);
-    }
-  }
-  
-  @Override
-  public void refresh(Collection<Refreshable> alreadyRefreshed) {
-    refreshHelper.refresh(alreadyRefreshed);
-  }
-}
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/MySQLJDBCDiffStorage.java mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/MySQLJDBCDiffStorage.java
--- mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/MySQLJDBCDiffStorage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/cf/taste/impl/recommender/slopeone/jdbc/MySQLJDBCDiffStorage.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,190 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.cf.taste.impl.recommender.slopeone.jdbc;
-
-import org.apache.mahout.cf.taste.common.TasteException;
-import org.apache.mahout.cf.taste.impl.model.jdbc.AbstractJDBCDataModel;
-
-/**
- * <p>
- * MySQL-specific implementation. Should be used in conjunction with a
- * {@link org.apache.mahout.cf.taste.impl.model.jdbc.MySQLJDBCDataModel}. This
- * implementation stores item-item diffs in a MySQL database and encapsulates some other slope-one-specific
- * operations that are needed on the preference data in the database. It assumes the database has a schema
- * like:
- * </p>
- *
- * <table>
- * <tr>
- * <th>item_id_a</th>
- * <th>item_id_b</th>
- * <th>average_diff</th>
- * <th>standard_deviation</th>
- * <th>count</th>
- * </tr>
- * <tr>
- * <td>123</td>
- * <td>234</td>
- * <td>0.5</td>
- * <td>0.12</td>
- * <td>5</td>
- * </tr>
- * <tr>
- * <td>123</td>
- * <td>789</td>
- * <td>-1.33</td>
- * <td>0.2</td>
- * <td>3</td>
- * </tr>
- * <tr>
- * <td>234</td>
- * <td>789</td>
- * <td>2.1</td>
- * <td>1.03</td>
- * <td>1</td>
- * </tr>
- * </table>
- *
- * <p>
- * {@code item_id_a} and {@code item_id_b} should have types compatible with the long primitive
- * type. {@code average_diff} and {@code standard_deviation} must be compatible with
- * {@code float} and {@code count} must be compatible with {@code int}.
- * </p>
- *
- * <p>
- * The following command sets up a suitable table in MySQL:
- * </p>
- *
- * <p>
- *
- * <pre>
- * CREATE TABLE taste_slopeone_diffs (
- *   item_id_a BIGINT NOT NULL,
- *   item_id_b BIGINT NOT NULL,
- *   average_diff FLOAT NOT NULL,
- *   standard_deviation FLOAT NOT NULL,
- *   count INT NOT NULL,
- *   PRIMARY KEY (item_id_a, item_id_b),
- *   INDEX (item_id_a),
- *   INDEX (item_id_b)
- * )
- * </pre>
- *
- * </p>
- */
-public final class MySQLJDBCDiffStorage extends AbstractJDBCDiffStorage {
-  
-  private static final int DEFAULT_MIN_DIFF_COUNT = 2;
-  
-  public MySQLJDBCDiffStorage(AbstractJDBCDataModel dataModel) throws TasteException {
-    this(dataModel,
-         DEFAULT_DIFF_TABLE,
-         DEFAULT_ITEM_A_COLUMN,
-         DEFAULT_ITEM_B_COLUMN,
-         DEFAULT_COUNT_COLUMN,
-         DEFAULT_AVERAGE_DIFF_COLUMN,
-         DEFAULT_STDEV_COLUMN,
-         DEFAULT_MIN_DIFF_COUNT);
-  }
-  
-  public MySQLJDBCDiffStorage(AbstractJDBCDataModel dataModel,
-                              String diffsTable,
-                              String itemIDAColumn,
-                              String itemIDBColumn,
-                              String countColumn,
-                              String avgColumn,
-                              String stdevColumn,
-                              int minDiffCount) throws TasteException {
-    super(dataModel,
-        // getDiffSQL
-        "SELECT " + countColumn + ", " + avgColumn + ", " + stdevColumn + " FROM "
-            + diffsTable + " WHERE " + itemIDAColumn + "=? AND " + itemIDBColumn + "=?",
-        // getDiffsSQL
-        "(SELECT " + countColumn + ", " + avgColumn + ", " + stdevColumn + ", " + itemIDAColumn
-            + " FROM " + diffsTable + ", "
-            + dataModel.getPreferenceTable() + " WHERE " + itemIDBColumn + "=? AND " + itemIDAColumn + " = "
-            + dataModel.getItemIDColumn() + " AND " + dataModel.getUserIDColumn() + "=? ORDER BY "
-            + itemIDAColumn + ") UNION ("
-            + "SELECT " + countColumn + ", -" + avgColumn + ", " + stdevColumn + ", " + itemIDBColumn
-            + " FROM " + diffsTable + ", "
-            + dataModel.getPreferenceTable() + " WHERE " + itemIDAColumn + "=? AND " + itemIDBColumn + " = "
-            + dataModel.getItemIDColumn() + " AND " + dataModel.getUserIDColumn() + "=? ORDER BY "
-            + itemIDBColumn + ')',
-        // getAverageItemPrefSQL
-        "SELECT COUNT(1), AVG(" + dataModel.getPreferenceColumn() + ") FROM "
-            + dataModel.getPreferenceTable() + " WHERE " + dataModel.getItemIDColumn() + "=?",
-        // getDiffsAffectedByUserSQL
-        "SELECT diffs." + countColumn + ", diffs." + avgColumn + ", diffs." + itemIDAColumn
-            + ", diffs." + itemIDBColumn + ", prefs." + dataModel.getPreferenceColumn()
-            + " FROM " + diffsTable + " AS diffs, " + dataModel.getPreferenceTable() + " AS prefs WHERE prefs."
-            + dataModel.getUserIDColumn() + "=? AND (prefs." + dataModel.getItemIDColumn()
-            + " = diffs." + itemIDAColumn + " OR prefs." + dataModel.getItemIDColumn()
-            + " = diffs." + itemIDBColumn + ')',
-        // updateDiffSQLs
-        new String[] {
-          "UPDATE " + diffsTable + " SET "
-              + avgColumn + " = " + avgColumn + " - (? / " + countColumn
-              + ") WHERE " + itemIDAColumn + "=?",
-          "UPDATE " + diffsTable + " SET "
-              + avgColumn + " = " + avgColumn + " + (? / " + countColumn
-              + ") WHERE " + itemIDBColumn + "=?"},
-        // updateOneDiffSQL
-        "UPDATE " + diffsTable + " SET " + countColumn + "=?, " + avgColumn + "=? WHERE "
-            + itemIDAColumn + "=? AND " + itemIDBColumn + "=?",
-        // addDiffSQL
-        "INSERT INTO " + diffsTable + " (" + itemIDAColumn + ", " + itemIDBColumn + ", " + avgColumn
-            + ", " + stdevColumn + ", " + countColumn + ") VALUES (?,?,?,0,1)",
-        // removeDiffSQL
-        "DELETE FROM " + diffsTable + " WHERE " + itemIDAColumn + "=? AND " + itemIDBColumn + "=?",
-        // getRecommendableItemsSQL
-        "SELECT id FROM " + "(SELECT " + itemIDAColumn + " AS id FROM " + diffsTable + ", "
-            + dataModel.getPreferenceTable() + " WHERE " + itemIDBColumn + " = "
-            + dataModel.getItemIDColumn() + " AND " + dataModel.getUserIDColumn() + "=? UNION DISTINCT"
-            + " SELECT " + itemIDBColumn + " AS id FROM " + diffsTable + ", "
-            + dataModel.getPreferenceTable() + " WHERE " + itemIDAColumn + " = "
-            + dataModel.getItemIDColumn() + " AND " + dataModel.getUserIDColumn() + "=?) "
-            + "possible_item_ids WHERE id NOT IN (SELECT " + dataModel.getItemIDColumn() + " FROM "
-            + dataModel.getPreferenceTable() + " WHERE " + dataModel.getUserIDColumn() + "=?)",
-        // deleteDiffsSQL
-        "TRUNCATE " + diffsTable,
-        // createDiffsSQL
-        "INSERT INTO " + diffsTable + " (" + itemIDAColumn + ", " + itemIDBColumn + ", " + avgColumn
-            + ", " + stdevColumn + ", " + countColumn + ") SELECT prefsA." + dataModel.getItemIDColumn()
-            + ", prefsB." + dataModel.getItemIDColumn() + ", AVG(prefsB." + dataModel.getPreferenceColumn()
-            + " - prefsA." + dataModel.getPreferenceColumn() + "), STDDEV_POP(prefsB."
-            + dataModel.getPreferenceColumn() + " - prefsA." + dataModel.getPreferenceColumn()
-            + "), COUNT(1) AS count FROM " + dataModel.getPreferenceTable() + " prefsA, "
-            + dataModel.getPreferenceTable() + " prefsB WHERE prefsA." + dataModel.getUserIDColumn()
-            + " = prefsB." + dataModel.getUserIDColumn() + " AND prefsA." + dataModel.getItemIDColumn()
-            + " < prefsB." + dataModel.getItemIDColumn() + ' ' + " GROUP BY prefsA."
-            + dataModel.getItemIDColumn() + ", prefsB." + dataModel.getItemIDColumn()
-            + " HAVING count >= ?",
-        // diffsExistSQL
-        "SELECT COUNT(1) FROM " + diffsTable,
-        minDiffCount);
-  }
-  
-  /**
-   * @see org.apache.mahout.cf.taste.impl.model.jdbc.MySQLJDBCDataModel#getFetchSize()
-   */
-  @Override
-  protected int getFetchSize() {
-    return Integer.MIN_VALUE;
-  }
-  
-}
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java
--- mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputDriver.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,100 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.conversion.meanshift;
-
-import java.io.IOException;
-
-import org.apache.commons.cli2.CommandLine;
-import org.apache.commons.cli2.Group;
-import org.apache.commons.cli2.Option;
-import org.apache.commons.cli2.OptionException;
-import org.apache.commons.cli2.builder.GroupBuilder;
-import org.apache.commons.cli2.commandline.Parser;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.common.CommandLineUtil;
-import org.apache.mahout.common.commandline.DefaultOptionCreator;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * This class converts text files containing space-delimited floating point numbers into
- * Mahout sequence files of MeanShiftCanopy suitable for input to the MeanShift clustering job.
- *
- */
-public final class InputDriver {
-
-  private static final Logger log = LoggerFactory.getLogger(InputDriver.class);
-
-  private InputDriver() {
-  }
-
-  public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
-    GroupBuilder gbuilder = new GroupBuilder();
-
-    Option inputOpt = DefaultOptionCreator.inputOption().withRequired(false).create();
-    Option outputOpt = DefaultOptionCreator.outputOption().withRequired(false).create();
-    Option helpOpt = DefaultOptionCreator.helpOption();
-    Group group = gbuilder.withName("Options").withOption(inputOpt).withOption(outputOpt).withOption(helpOpt).create();
-
-    try {
-      Parser parser = new Parser();
-      parser.setGroup(group);
-      CommandLine cmdLine = parser.parse(args);
-      if (cmdLine.hasOption(helpOpt)) {
-        CommandLineUtil.printHelp(group);
-        return;
-      }
-
-      Path input = new Path(cmdLine.getValue(inputOpt, "testdata").toString());
-      Path output = new Path(cmdLine.getValue(outputOpt, "output").toString());
-      runJob(input, output);
-    } catch (OptionException e) {
-      log.error("Exception parsing command line: ", e);
-      CommandLineUtil.printHelp(group);
-    }
-  }
-
-  public static void runJob(Path input, Path output) throws IOException, InterruptedException, ClassNotFoundException {
-    Configuration conf = new Configuration();
-
-    Job job = new Job(conf, "Mean Shift Input Driver running over input: " + input);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(ClusterWritable.class);
-    job.setOutputFormatClass(SequenceFileOutputFormat.class);
-    job.setMapperClass(org.apache.mahout.clustering.conversion.meanshift.InputMapper.class);
-    job.setReducerClass(Reducer.class);
-    job.setNumReduceTasks(0);
-    job.setJarByClass(InputDriver.class);
-
-    FileInputFormat.setInputPaths(job, input);
-    FileOutputFormat.setOutputPath(job, output);
-
-    boolean succeeded = job.waitForCompletion(true);
-    if (!succeeded) {
-      throw new IllegalStateException("Job failed!");
-    }
-  }
-}
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputMapper.java mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputMapper.java
--- mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/clustering/conversion/meanshift/InputMapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,64 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.conversion.meanshift;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.mahout.clustering.iterator.ClusterWritable;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopy;
-import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
-import org.apache.mahout.math.DenseVector;
-import org.apache.mahout.math.Vector;
-
-import com.google.common.collect.Lists;
-
-public class InputMapper extends Mapper<LongWritable,Text,Text,ClusterWritable> {
-  
-  private static final Pattern SPACE = Pattern.compile(" ");
-  
-  private int nextCanopyId;
-  
-  private final ClusterWritable cw = new ClusterWritable();
-  
-  @Override
-  protected void map(LongWritable key, Text values, Context context) throws IOException, InterruptedException {
-    String[] numbers = SPACE.split(values.toString());
-    // sometimes there are multiple separator spaces
-    Collection<Double> doubles = Lists.newArrayList();
-    for (String value : numbers) {
-      if (!value.isEmpty()) {
-        doubles.add(Double.valueOf(value));
-      }
-    }
-    // ignore empty lines in input data
-    if (!doubles.isEmpty()) {
-      Vector point = new DenseVector(doubles.size());
-      int index = 0;
-      for (Double d : doubles) {
-        point.set(index++, d);
-      }
-      cw.setValue(new MeanShiftCanopy(point, nextCanopyId++, new EuclideanDistanceMeasure()));
-      context.write(new Text(), cw);
-    }
-  }
-}
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/LuceneIndexHelper.java mahout/integration/src/main/java/org/apache/mahout/text/LuceneIndexHelper.java
--- mahout/integration/src/main/java/org/apache/mahout/text/LuceneIndexHelper.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/integration/src/main/java/org/apache/mahout/text/LuceneIndexHelper.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,40 @@
+package org.apache.mahout.text;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.CollectionStatistics;
+import org.apache.lucene.search.IndexSearcher;
+
+import java.io.IOException;
+
+/**
+ * Utility for checking if a field exist in a Lucene index.
+ */
+public class LuceneIndexHelper {
+
+  private LuceneIndexHelper() {
+
+  }
+
+  public static void fieldShouldExistInIndex(IndexSearcher searcher, String field) throws IOException {
+    CollectionStatistics idFieldStatistics = searcher.collectionStatistics(field);
+    if (idFieldStatistics.docCount() == 0) {
+      throw new IllegalArgumentException("Field '" + field + "' does not exist in the index");
+    }
+  }
+
+}
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputFormat.java mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputFormat.java
--- mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputFormat.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputFormat.java	2014-03-29 01:03:14.000000000 -0700
@@ -27,7 +27,7 @@
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.lucene.index.SegmentInfoPerCommit;
+import org.apache.lucene.index.SegmentCommitInfo;
 import org.apache.lucene.index.SegmentInfos;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -57,7 +57,7 @@
       SegmentInfos segmentInfos = new SegmentInfos();
       segmentInfos.read(directory);
 
-      for (SegmentInfoPerCommit segmentInfo : segmentInfos) {
+      for (SegmentCommitInfo segmentInfo : segmentInfos) {
         LuceneSegmentInputSplit inputSplit = new LuceneSegmentInputSplit(indexPath, segmentInfo.info.name,
                                                                          segmentInfo.sizeInBytes());
         inputSplits.add(inputSplit);
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputSplit.java mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputSplit.java
--- mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputSplit.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentInputSplit.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,8 +21,8 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.lucene.index.SegmentCommitInfo;
 import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfoPerCommit;
 import org.apache.lucene.index.SegmentInfos;
 
 import java.io.DataInput;
@@ -87,14 +87,14 @@
    * @return the segment info or throws exception if not found
    * @throws IOException if an error occurs when accessing the directory
    */
-  public SegmentInfoPerCommit getSegment(Configuration configuration) throws IOException {
+  public SegmentCommitInfo getSegment(Configuration configuration) throws IOException {
     ReadOnlyFileSystemDirectory directory = new ReadOnlyFileSystemDirectory(FileSystem.get(configuration), indexPath,
                                                                             false, configuration);
 
     SegmentInfos segmentInfos = new SegmentInfos();
     segmentInfos.read(directory);
 
-    for (SegmentInfoPerCommit segmentInfo : segmentInfos) {
+    for (SegmentCommitInfo segmentInfo : segmentInfos) {
       if (segmentInfo.info.name.equals(segmentInfoName)) {
         return segmentInfo;
       }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentRecordReader.java mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentRecordReader.java
--- mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentRecordReader.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/LuceneSegmentRecordReader.java	2014-03-29 01:03:14.000000000 -0700
@@ -22,7 +22,7 @@
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.lucene.index.SegmentInfoPerCommit;
+import org.apache.lucene.index.SegmentCommitInfo;
 import org.apache.lucene.index.SegmentReader;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Scorer;
@@ -52,11 +52,17 @@
     Configuration configuration = context.getConfiguration();
     LuceneStorageConfiguration lucene2SeqConfiguration = new LuceneStorageConfiguration(configuration);
 
-    SegmentInfoPerCommit segmentInfo = inputSplit.getSegment(configuration);
+    SegmentCommitInfo segmentInfo = inputSplit.getSegment(configuration);
     segmentReader = new SegmentReader(segmentInfo, USE_TERM_INFO, IOContext.READ);
 
 
     IndexSearcher searcher = new IndexSearcher(segmentReader);
+    String idField = lucene2SeqConfiguration.getIdField();
+    LuceneIndexHelper.fieldShouldExistInIndex(searcher, idField);
+    for (String field : lucene2SeqConfiguration.getFields()) {
+        LuceneIndexHelper.fieldShouldExistInIndex(searcher, field);
+    }
+
     Weight weight = lucene2SeqConfiguration.getQuery().createWeight(searcher);
     scorer = weight.scorer(segmentReader.getContext(), false, false, null);
     if (scorer == null) {
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/LuceneSeqFileHelper.java mahout/integration/src/main/java/org/apache/mahout/text/LuceneSeqFileHelper.java
--- mahout/integration/src/main/java/org/apache/mahout/text/LuceneSeqFileHelper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/LuceneSeqFileHelper.java	2014-03-29 01:03:14.000000000 -0700
@@ -16,6 +16,7 @@
  */
 package org.apache.mahout.text;
 
+import com.google.common.base.Strings;
 import org.apache.hadoop.io.Text;
 import org.apache.lucene.document.Document;
 
@@ -47,14 +48,6 @@
         }
       }
     }
-    theValue.set(nullSafe(valueBuilder.toString()));
-  }
-
-  public static String nullSafe(String value) {
-    if (value == null) {
-      return "";
-    } else {
-      return value;
-    }
+    theValue.set(Strings.nullToEmpty(valueBuilder.toString()));
   }
 }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/LuceneStorageConfiguration.java mahout/integration/src/main/java/org/apache/mahout/text/LuceneStorageConfiguration.java
--- mahout/integration/src/main/java/org/apache/mahout/text/LuceneStorageConfiguration.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/LuceneStorageConfiguration.java	2014-03-29 01:03:14.000000000 -0700
@@ -44,7 +44,7 @@
 import java.util.List;
 import java.util.Set;
 
-import static org.apache.lucene.util.Version.LUCENE_43;
+import static org.apache.lucene.util.Version.LUCENE_46;
 
 /**
  * Holds all the configuration for {@link SequenceFilesFromLuceneStorage}, which generates a sequence file
@@ -212,7 +212,7 @@
       }
       idField = in.readUTF();
       fields = Arrays.asList(in.readUTF().split(SEPARATOR_FIELDS));
-      query = new QueryParser(LUCENE_43, "query", new StandardAnalyzer(LUCENE_43)).parse(in.readUTF());
+      query = new QueryParser(LUCENE_46, "query", new StandardAnalyzer(LUCENE_46)).parse(in.readUTF());
       maxHits = in.readInt();
     } catch (ParseException e) {
       throw new RuntimeException("Could not deserialize " + this.getClass().getName(), e);
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java mahout/integration/src/main/java/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java
--- mahout/integration/src/main/java/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/MailArchivesClusteringAnalyzer.java	2014-03-29 01:03:14.000000000 -0700
@@ -41,7 +41,7 @@
  * stop words, excluding non-alpha-numeric tokens, and porter stemming.
  */
 public final class MailArchivesClusteringAnalyzer extends StopwordAnalyzerBase {
-  private static final Version LUCENE_VERSION = Version.LUCENE_43;
+  private static final Version LUCENE_VERSION = Version.LUCENE_46;
   
   // extended set of stop words composed of common mail terms like "hi",
   // HTML tags, and Java keywords asmany of the messages in the archives
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java mahout/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java
--- mahout/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java	2014-03-29 01:03:14.000000000 -0700
@@ -24,13 +24,14 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.lucene.store.BaseDirectory;
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.BufferedIndexOutput;
-import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.Lock;
+import org.apache.lucene.store.LockFactory;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -45,7 +46,7 @@
  * <p/>
  * // TODO: Rename to FileSystemReadOnlyDirectory
  */
-public class ReadOnlyFileSystemDirectory extends Directory {
+public class ReadOnlyFileSystemDirectory extends BaseDirectory {
 
   private final FileSystem fs;
   private final Path directory;
@@ -56,10 +57,10 @@
       /**
        * Constructor
        *
-       * @param fs
-       * @param directory
-       * @param create
-       * @param conf
+       * @param fs - filesystem
+       * @param directory - directory path
+       * @param create - if true create the directory
+       * @param conf - MR Job Configuration
        * @throws IOException
        */
 
@@ -193,11 +194,26 @@
   }
 
   @Override
+  public void clearLock(String name) throws IOException {
+    // do nothing
+  }
+
+  @Override
   public void close() throws IOException {
     // do not close the file system
   }
 
   @Override
+  public void setLockFactory(LockFactory lockFactory) throws IOException {
+    // do nothing
+  }
+
+  @Override
+  public LockFactory getLockFactory() {
+    return null;
+  }
+
+  @Override
   public String toString() {
     return this.getClass().getName() + "@" + directory;
   }
@@ -271,7 +287,8 @@
     }
 
     @Override
-    protected void finalize() throws IOException {
+    protected void finalize() throws Throwable {
+      super.finalize();
       if (!isClone && isOpen) {
         close(); // close the file
       }
@@ -326,7 +343,8 @@
     }
 
     @Override
-    protected void finalize() throws IOException {
+    protected void finalize() throws Throwable {
+      super.finalize();
       if (isOpen) {
         close(); // close the file
       }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromDirectory.java mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromDirectory.java
--- mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromDirectory.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromDirectory.java	2014-03-29 01:03:14.000000000 -0700
@@ -23,10 +23,12 @@
 
 import com.google.common.collect.Maps;
 import com.google.common.io.Closeables;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
@@ -51,7 +53,7 @@
   private static final String PREFIX_ADDITION_FILTER = PrefixAdditionFilter.class.getName();
 
   private static final String[] CHUNK_SIZE_OPTION = {"chunkSize", "chunk"};
-  private static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass", "filter"};
+  public static final String[] FILE_FILTER_CLASS_OPTION = {"fileFilterClass", "filter"};
   private static final String[] CHARSET_OPTION = {"charset", "c"};
 
   private static final int MAX_JOB_SPLIT_LOCATIONS = 1000000;
@@ -107,8 +109,8 @@
         pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer, charset, fs);
       } else {
         pathFilter = ClassUtils.instantiateAs(fileFilterClassName, SequenceFilesFromDirectoryFilter.class,
-          new Class[]{Configuration.class, String.class, Map.class, ChunkedWriter.class, Charset.class, FileSystem.class},
-          new Object[]{conf, keyPrefix, options, writer, charset, fs});
+          new Class[] {Configuration.class, String.class, Map.class, ChunkedWriter.class, Charset.class, FileSystem.class},
+          new Object[] {conf, keyPrefix, options, writer, charset, fs});
       }
       fs.listStatus(input, pathFilter);
     } finally {
@@ -129,6 +131,24 @@
       keyPrefix = getOption(KEY_PREFIX_OPTION[0]);
     }
 
+    String fileFilterClassName = null;
+    if (hasOption(FILE_FILTER_CLASS_OPTION[0])) {
+      fileFilterClassName = getOption(FILE_FILTER_CLASS_OPTION[0]);
+    }
+
+    PathFilter pathFilter = null;
+    // Prefix Addition is presently handled in the Mapper and unlike runsequential()
+    // need not be done via a pathFilter
+    if (!StringUtils.isBlank(fileFilterClassName) && !PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
+      try {
+        pathFilter = (PathFilter) Class.forName(fileFilterClassName).newInstance();
+      } catch (InstantiationException e) {
+        throw new IllegalStateException(e);
+      } catch (IllegalAccessException e) {
+        throw new IllegalStateException(e);
+      }
+    }
+
     // Prepare Job for submission.
     Job job = prepareJob(input, output, MultipleTextFileInputFormat.class,
       SequenceFilesFromDirectoryMapper.class, Text.class, Text.class,
@@ -136,9 +156,18 @@
 
     Configuration jobConfig = job.getConfiguration();
     jobConfig.set(KEY_PREFIX_OPTION[0], keyPrefix);
+    jobConfig.set(FILE_FILTER_CLASS_OPTION[0], fileFilterClassName);
+
     FileSystem fs = FileSystem.get(jobConfig);
     FileStatus fsFileStatus = fs.getFileStatus(input);
-    String inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);
+
+    String inputDirList;
+    if (pathFilter != null) {
+      inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus, pathFilter);
+    } else {
+      inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);
+    }
+
     jobConfig.set(BASE_INPUT_PATH, input.toString());
 
     long chunkSizeInBytes = chunkSizeInMB * 1024 * 1024;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorage.java mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorage.java
--- mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorage.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorage.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.List;
 
+import com.google.common.base.Strings;
 import com.google.common.io.Closeables;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -62,6 +63,12 @@
       Directory directory = FSDirectory.open(new File(indexPath.toUri().getPath()));
       IndexReader reader = DirectoryReader.open(directory);
       IndexSearcher searcher = new IndexSearcher(reader);
+
+      LuceneIndexHelper.fieldShouldExistInIndex(searcher, lucene2seqConf.getIdField());
+      for (String field : lucene2seqConf.getFields()) {
+        LuceneIndexHelper.fieldShouldExistInIndex(searcher, field);
+      }
+
       Configuration configuration = lucene2seqConf.getConfiguration();
       FileSystem fileSystem = FileSystem.get(configuration);
       Path sequenceFilePath = new Path(lucene2seqConf.getSequenceFilesOutputPath(), indexPath.getName());
@@ -106,7 +113,7 @@
 
         Document doc = storedFieldVisitor.getDocument();
         List<String> fields = lucene2seqConf.getFields();
-        Text theKey = new Text(LuceneSeqFileHelper.nullSafe(doc.get(lucene2seqConf.getIdField())));
+        Text theKey = new Text(Strings.nullToEmpty(doc.get(lucene2seqConf.getIdField())));
         Text theValue = new Text();
         LuceneSeqFileHelper.populateValues(doc, theValue, fields);
         //if they are both empty, don't write
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriver.java mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriver.java
--- mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriver.java	2014-03-29 01:03:14.000000000 -0700
@@ -97,8 +97,8 @@
     if (hasOption(OPTION_QUERY)) {
       try {
         String queryString = COMPILE.matcher(getOption(OPTION_QUERY)).replaceAll("");
-        QueryParser queryParser = new QueryParser(Version.LUCENE_43, queryString,
-            new StandardAnalyzer(Version.LUCENE_43));
+        QueryParser queryParser = new QueryParser(Version.LUCENE_46, queryString,
+            new StandardAnalyzer(Version.LUCENE_46));
         query = queryParser.parse(queryString);
       } catch (ParseException e) {
         throw new IllegalArgumentException(e.getMessage(), e);
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMapper.java mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMapper.java
--- mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMapper.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,12 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.text;
 
+import com.google.common.base.Strings;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DocumentStoredFieldVisitor;
-import org.apache.lucene.index.SegmentInfoPerCommit;
+import org.apache.lucene.index.SegmentCommitInfo;
 import org.apache.lucene.index.SegmentReader;
 import org.apache.lucene.store.IOContext;
 
@@ -31,7 +49,7 @@
     Configuration configuration = context.getConfiguration();
     l2sConf = new LuceneStorageConfiguration(configuration);
     LuceneSegmentInputSplit inputSplit = (LuceneSegmentInputSplit) context.getInputSplit();
-    SegmentInfoPerCommit segmentInfo = inputSplit.getSegment(configuration);
+    SegmentCommitInfo segmentInfo = inputSplit.getSegment(configuration);
     segmentReader = new SegmentReader(segmentInfo, LuceneSeqFileHelper.USE_TERM_INFOS, IOContext.READ);
   }
 
@@ -42,7 +60,7 @@
     segmentReader.document(docId, storedFieldVisitor);
     Document document = storedFieldVisitor.getDocument();
     List<String> fields = l2sConf.getFields();
-    Text theKey = new Text(LuceneSeqFileHelper.nullSafe(document.get(l2sConf.getIdField())));
+    Text theKey = new Text(Strings.nullToEmpty(document.get(l2sConf.getIdField())));
     Text theValue = new Text();
     LuceneSeqFileHelper.populateValues(document, theValue, fields);
     //if they are both empty, don't write
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromMailArchives.java mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromMailArchives.java
--- mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromMailArchives.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/SequenceFilesFromMailArchives.java	2014-03-29 01:03:14.000000000 -0700
@@ -19,6 +19,11 @@
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.common.io.Closeables;
+
+import org.apache.commons.io.DirectoryWalker;
+import org.apache.commons.io.comparator.CompositeFileComparator;
+import org.apache.commons.io.comparator.DirectoryFileComparator;
+import org.apache.commons.io.comparator.PathFileComparator;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -38,9 +43,13 @@
 import org.slf4j.LoggerFactory;
 
 import java.io.File;
-import java.io.FileFilter;
 import java.io.IOException;
 import java.nio.charset.Charset;
+import java.util.ArrayDeque;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Deque;
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Pattern;
@@ -76,9 +85,9 @@
     MailProcessor processor = new MailProcessor(options, options.getPrefix(), writer);
     try {
       if (options.getInput().isDirectory()) {
-        PrefixAdditionFilter filter = new PrefixAdditionFilter(processor, writer);
-        options.getInput().listFiles(filter);
-        log.info("Parsed {} messages from {}", filter.getMessageCount(), options.getInput().getAbsolutePath());
+        PrefixAdditionDirectoryWalker walker = new PrefixAdditionDirectoryWalker(processor, writer);
+        walker.walk(options.getInput());
+        log.info("Parsed {} messages from {}", walker.getMessageCount(), options.getInput().getAbsolutePath());
       } else {
         long start = System.currentTimeMillis();
         long cnt = processor.parseMboxLineByLine(options.getInput());
@@ -90,40 +99,73 @@
     }
   }
 
-  public class PrefixAdditionFilter implements FileFilter {
-    private final MailProcessor processor;
+  private static class PrefixAdditionDirectoryWalker extends DirectoryWalker<Object> {
+
+    @SuppressWarnings("unchecked")
+    private static final Comparator<File> FILE_COMPARATOR = new CompositeFileComparator(
+        DirectoryFileComparator.DIRECTORY_REVERSE, PathFileComparator.PATH_COMPARATOR);
+
+    private final Deque<MailProcessor> processors = new ArrayDeque<MailProcessor>();
     private final ChunkedWriter writer;
-    private long messageCount;
+    private final Deque<Long> messageCounts = new ArrayDeque<Long>();
 
-    public PrefixAdditionFilter(MailProcessor processor, ChunkedWriter writer) {
-      this.processor = processor;
+    public PrefixAdditionDirectoryWalker(MailProcessor processor, ChunkedWriter writer) {
+      processors.addFirst(processor);
       this.writer = writer;
-      this.messageCount = 0;
+      messageCounts.addFirst(0L);
+    }
+
+    public void walk(File startDirectory) throws IOException {
+      super.walk(startDirectory, null);
     }
 
     public long getMessageCount() {
-      return messageCount;
+      return messageCounts.getFirst();
     }
 
     @Override
-    public boolean accept(File current) {
-      if (current.isDirectory()) {
+    protected void handleDirectoryStart(File current, int depth, Collection<Object> results) throws IOException {
+      if (depth > 0) {
         log.info("At {}", current.getAbsolutePath());
-        PrefixAdditionFilter nested = new PrefixAdditionFilter(
-          new MailProcessor(processor.getOptions(), processor.getPrefix()
-            + File.separator + current.getName(), writer), writer);
-        current.listFiles(nested);
-        long dirCount = nested.getMessageCount();
-        log.info("Parsed {} messages from directory {}", dirCount, current.getAbsolutePath());
-        messageCount += dirCount;
-      } else {
-        try {
-          messageCount += processor.parseMboxLineByLine(current);
-        } catch (IOException e) {
-          throw new IllegalStateException("Error processing " + current, e);
-        }
+        MailProcessor processor = processors.getFirst();
+        MailProcessor subDirProcessor = new MailProcessor(processor.getOptions(), processor.getPrefix()
+            + File.separator + current.getName(), writer);
+        processors.push(subDirProcessor);
+        messageCounts.push(0L);
+      }
+    }
+
+    @Override
+    protected File[] filterDirectoryContents(File directory, int depth, File[] files) throws IOException {
+      Arrays.sort(files, FILE_COMPARATOR);
+      return files;
+    }
+
+    @Override
+    protected void handleFile(File current, int depth, Collection<Object> results) throws IOException {
+      MailProcessor processor = processors.getFirst();
+      long currentDirMessageCount = messageCounts.pop();
+      try {
+        currentDirMessageCount += processor.parseMboxLineByLine(current);
+      } catch (IOException e) {
+        throw new IllegalStateException("Error processing " + current, e);
+      }
+      messageCounts.push(currentDirMessageCount);
+    }
+
+    @Override
+    protected void handleDirectoryEnd(File current, int depth, Collection<Object> results) throws IOException {
+      if (depth > 0) {
+        final long currentDirMessageCount = messageCounts.pop();
+        log.info("Parsed {} messages from directory {}", currentDirMessageCount, current.getAbsolutePath());
+
+        processors.pop();
+
+        // aggregate message counts
+        long parentDirMessageCount = messageCounts.pop();
+        parentDirMessageCount += currentDirMessageCount;
+        messageCounts.push(parentDirMessageCount);
       }
-      return false;
     }
   }
 
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/WholeFileRecordReader.java mahout/integration/src/main/java/org/apache/mahout/text/WholeFileRecordReader.java
--- mahout/integration/src/main/java/org/apache/mahout/text/WholeFileRecordReader.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/WholeFileRecordReader.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,10 +20,13 @@
 import java.io.IOException;
 
 import com.google.common.io.Closeables;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.IntWritable;
@@ -33,6 +36,8 @@
 import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
 import org.apache.hadoop.mapreduce.lib.input.FileSplit;
 
+import static org.apache.mahout.text.SequenceFilesFromDirectory.FILE_FILTER_CLASS_OPTION;
+
 /**
  * RecordReader used with the MultipleTextFileInputFormat class to read full files as
  * k/v pairs and groups of files as single input splits.
@@ -44,13 +49,16 @@
   private Configuration configuration;
   private BytesWritable value = new BytesWritable();
   private IntWritable index;
+  private String fileFilterClassName = null;
+  private PathFilter pathFilter = null;
 
   public WholeFileRecordReader(CombineFileSplit fileSplit, TaskAttemptContext taskAttemptContext, Integer idx)
-    throws IOException {
+      throws IOException {
     this.fileSplit = new FileSplit(fileSplit.getPath(idx), fileSplit.getOffset(idx),
-       fileSplit.getLength(idx), fileSplit.getLocations());
+      fileSplit.getLength(idx), fileSplit.getLocations());
     this.configuration = taskAttemptContext.getConfiguration();
     this.index = new IntWritable(idx);
+    this.fileFilterClassName = this.configuration.get(FILE_FILTER_CLASS_OPTION[0]);
   }
 
   @Override
@@ -71,7 +79,17 @@
   @Override
   public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
     throws IOException, InterruptedException {
-
+    if (!StringUtils.isBlank(fileFilterClassName) && !PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {
+      try {
+        pathFilter = (PathFilter) Class.forName(fileFilterClassName).newInstance();
+      } catch (ClassNotFoundException e) {
+        throw new IllegalStateException(e);
+      } catch (InstantiationException e) {
+        throw new IllegalStateException(e);
+      } catch (IllegalAccessException e) {
+        throw new IllegalStateException(e);
+      }
+    }
   }
 
   @Override
@@ -80,20 +98,31 @@
       byte[] contents = new byte[(int) fileSplit.getLength()];
       Path file = fileSplit.getPath();
       FileSystem fs = file.getFileSystem(this.configuration);
+
+      if (!fs.isFile(file)) {
+        return false;
+      }
+
+      FileStatus[] fileStatuses;
+      if (pathFilter != null) {
+        fileStatuses = fs.listStatus(file, pathFilter);
+      } else {
+        fileStatuses = fs.listStatus(file);
+      }
+
       FSDataInputStream in = null;
-      try {
-        if (!fs.isFile(file)) {
-          return false;
+      if (fileStatuses.length == 1) {
+        try {
+          in = fs.open(fileStatuses[0].getPath());
+          IOUtils.readFully(in, contents, 0, contents.length);
+          value.setCapacity(contents.length);
+          value.set(contents, 0, contents.length);
+        } finally {
+          Closeables.close(in, false);
         }
-        in = fs.open(file);
-        IOUtils.readFully(in, contents, 0, contents.length);
-        value.setCapacity(contents.length);
-        value.set(contents, 0, contents.length);
-      } finally {
-        Closeables.close(in, false);
+        processed = true;
+        return true;
       }
-      processed = true;
-      return true;
     }
     return false;
   }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaAnalyzer.java mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaAnalyzer.java
--- mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaAnalyzer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaAnalyzer.java	2014-03-29 01:03:14.000000000 -0700
@@ -33,19 +33,19 @@
 public class WikipediaAnalyzer extends StopwordAnalyzerBase {
   
   public WikipediaAnalyzer() {
-    super(Version.LUCENE_43, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    super(Version.LUCENE_46, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
   }
   
   public WikipediaAnalyzer(CharArraySet stopSet) {
-    super(Version.LUCENE_43, stopSet);
+    super(Version.LUCENE_46, stopSet);
   }
 
   @Override
   protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
     Tokenizer tokenizer = new WikipediaTokenizer(reader);
-    TokenStream result = new StandardFilter(Version.LUCENE_43, tokenizer);
-    result = new LowerCaseFilter(Version.LUCENE_43, result);
-    result = new StopFilter(Version.LUCENE_43, result, getStopwordSet());
+    TokenStream result = new StandardFilter(Version.LUCENE_46, tokenizer);
+    result = new LowerCaseFilter(Version.LUCENE_46, result);
+    result = new StopFilter(Version.LUCENE_46, result, getStopwordSet());
     return new TokenStreamComponents(tokenizer, result);
   }
 }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaXmlSplitter.java mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaXmlSplitter.java
--- mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaXmlSplitter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/text/wikipedia/WikipediaXmlSplitter.java	2014-03-29 01:03:14.000000000 -0700
@@ -71,20 +71,20 @@
  * </ol>
  */
 public final class WikipediaXmlSplitter {
-  
+
   private static final Logger log = LoggerFactory.getLogger(WikipediaXmlSplitter.class);
-  
+
   private WikipediaXmlSplitter() { }
-  
+
   public static void main(String[] args) throws IOException {
     DefaultOptionBuilder obuilder = new DefaultOptionBuilder();
     ArgumentBuilder abuilder = new ArgumentBuilder();
     GroupBuilder gbuilder = new GroupBuilder();
-    
+
     Option dumpFileOpt = obuilder.withLongName("dumpFile").withRequired(true).withArgument(
       abuilder.withName("dumpFile").withMinimum(1).withMaximum(1).create()).withDescription(
       "The path to the wikipedia dump file (.bz2 or uncompressed)").withShortName("d").create();
-    
+
     Option outputDirOpt = obuilder.withLongName("outputDir").withRequired(true).withArgument(
       abuilder.withName("outputDir").withMinimum(1).withMaximum(1).create()).withDescription(
       "The output directory to place the splits in:\n"
@@ -94,14 +94,14 @@
           + "AWS S3 (native files):\n\ts3n://bucket-name/wikipedia-xml-chunks\n")
 
     .withShortName("o").create();
-    
+
     Option s3IdOpt = obuilder.withLongName("s3ID").withRequired(false).withArgument(
       abuilder.withName("s3Id").withMinimum(1).withMaximum(1).create()).withDescription("Amazon S3 ID key")
         .withShortName("i").create();
     Option s3SecretOpt = obuilder.withLongName("s3Secret").withRequired(false).withArgument(
       abuilder.withName("s3Secret").withMinimum(1).withMaximum(1).create()).withDescription(
       "Amazon S3 secret key").withShortName("s").create();
-    
+
     Option chunkSizeOpt = obuilder.withLongName("chunkSize").withRequired(true).withArgument(
       abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription(
       "The Size of the chunk, in megabytes").withShortName("c").create();
@@ -114,7 +114,7 @@
         .withShortName("n").create();
     Group group = gbuilder.withName("Options").withOption(dumpFileOpt).withOption(outputDirOpt).withOption(
       chunkSizeOpt).withOption(numChunksOpt).withOption(s3IdOpt).withOption(s3SecretOpt).create();
-    
+
     Parser parser = new Parser();
     parser.setGroup(group);
     CommandLine cmdLine;
@@ -125,11 +125,11 @@
       CommandLineUtil.printHelp(group);
       return;
     }
-    
+
     Configuration conf = new Configuration();
     String dumpFilePath = (String) cmdLine.getValue(dumpFileOpt);
     String outputDirPath = (String) cmdLine.getValue(outputDirOpt);
-    
+
     if (cmdLine.hasOption(s3IdOpt)) {
       String id = (String) cmdLine.getValue(s3IdOpt);
       conf.set("fs.s3n.awsAccessKeyId", id);
@@ -143,14 +143,14 @@
     // do not compute crc file when using local FS
     conf.set("fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem");
     FileSystem fs = FileSystem.get(URI.create(outputDirPath), conf);
-    
+
     int chunkSize = 1024 * 1024 * Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));
-    
+
     int numChunks = Integer.MAX_VALUE;
     if (cmdLine.hasOption(numChunksOpt)) {
       numChunks = Integer.parseInt((String) cmdLine.getValue(numChunksOpt));
     }
-    
+
     String header = "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" "
                     + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" "
                     + "xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ "
@@ -178,11 +178,18 @@
                     + "      <namespace key=\"100\">Portal</namespace>\n"
                     + "      <namespace key=\"101\">Portal talk</namespace>\n" + "    </namespaces>\n"
                     + "  </siteinfo>\n";
-    
+
     StringBuilder content = new StringBuilder();
     content.append(header);
     NumberFormat decimalFormatter = new DecimalFormat("0000");
     File dumpFile = new File(dumpFilePath);
+
+    // If the specified path for the input file is incorrect, return immediately
+    if (!dumpFile.exists()) {
+      log.error("Input file path {} doesn't exist", dumpFilePath);
+      return;
+    }
+
     FileLineIterator it;
     if (dumpFilePath.endsWith(".bz2")) {
       // default compression format from http://download.wikimedia.org
@@ -192,7 +199,7 @@
       // assume the user has previously de-compressed the dump file
       it = new FileLineIterator(dumpFile);
     }
-    int filenumber = 0;
+    int fileNumber = 0;
     while (it.hasNext()) {
       String thisLine = it.next();
       if (thisLine.trim().startsWith("<page>")) {
@@ -207,11 +214,11 @@
           }
         }
         content.append(thisLine).append('\n');
-        
+
         if (content.length() > chunkSize || end) {
           content.append("</mediawiki>");
-          filenumber++;
-          String filename = outputDirPath + "/chunk-" + decimalFormatter.format(filenumber) + ".xml";
+          fileNumber++;
+          String filename = outputDirPath + "/chunk-" + decimalFormatter.format(fileNumber) + ".xml";
           BufferedWriter chunkWriter =
               new BufferedWriter(new OutputStreamWriter(fs.create(new Path(filename)), "UTF-8"));
           try {
@@ -219,7 +226,7 @@
           } finally {
             Closeables.close(chunkWriter, false);
           }
-          if (filenumber >= numChunks) {
+          if (fileNumber >= numChunks) {
             break;
           }
           content = new StringBuilder();
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/SplitInput.java mahout/integration/src/main/java/org/apache/mahout/utils/SplitInput.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/SplitInput.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/SplitInput.java	2014-03-29 01:03:14.000000000 -0700
@@ -615,16 +615,14 @@
    */
   public void validate() throws IOException {
     Preconditions.checkArgument(testSplitSize >= 1 || testSplitSize == -1,
-            "Invalid testSplitSize", testSplitSize);
+        "Invalid testSplitSize: " + testSplitSize + ". Must be: testSplitSize >= 1 or testSplitSize = -1");
     Preconditions.checkArgument(splitLocation >= 0 && splitLocation <= 100 || splitLocation == -1,
-            "Invalid splitLocation percentage", splitLocation);
+        "Invalid splitLocation percentage: " + splitLocation + ". Must be: 0 <= splitLocation <= 100 or splitLocation = -1");
     Preconditions.checkArgument(testSplitPct >= 0 && testSplitPct <= 100 || testSplitPct == -1,
-            "Invalid testSplitPct percentage", testSplitPct);
-    Preconditions.checkArgument(splitLocation >= 0 && splitLocation <= 100 || splitLocation == -1,
-            "Invalid splitLocation percentage", splitLocation);
+        "Invalid testSplitPct percentage: " + testSplitPct + ". Must be: 0 <= testSplitPct <= 100 or testSplitPct = -1");
     Preconditions.checkArgument(testRandomSelectionPct >= 0 && testRandomSelectionPct <= 100
-            || testRandomSelectionPct == -1,
-            "Invalid testRandomSelectionPct percentage", testRandomSelectionPct);
+            || testRandomSelectionPct == -1,"Invalid testRandomSelectionPct percentage: " + testRandomSelectionPct +
+        ". Must be: 0 <= testRandomSelectionPct <= 100 or testRandomSelectionPct = -1");
 
     Preconditions.checkArgument(trainingOutputDirectory != null || useMapRed,
         "No training output directory was specified");
@@ -653,10 +651,10 @@
       FileSystem fs = trainingOutputDirectory.getFileSystem(conf);
       FileStatus trainingOutputDirStatus = fs.getFileStatus(trainingOutputDirectory);
       Preconditions.checkArgument(trainingOutputDirStatus != null && trainingOutputDirStatus.isDir(),
-              "%s is not a directory", trainingOutputDirectory);
+          "%s is not a directory", trainingOutputDirectory);
       FileStatus testOutputDirStatus = fs.getFileStatus(testOutputDirectory);
       Preconditions.checkArgument(testOutputDirStatus != null && testOutputDirStatus.isDir(),
-              "%s is not a directory", testOutputDirectory);
+          "%s is not a directory", testOutputDirectory);
     }
   }
 
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/clustering/AbstractClusterWriter.java mahout/integration/src/main/java/org/apache/mahout/utils/clustering/AbstractClusterWriter.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/clustering/AbstractClusterWriter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/clustering/AbstractClusterWriter.java	2014-03-29 01:03:14.000000000 -0700
@@ -23,12 +23,11 @@
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.Iterator;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 
 import org.apache.commons.lang3.StringUtils;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.distance.DistanceMeasure;
@@ -46,7 +45,7 @@
   private static final Logger log = LoggerFactory.getLogger(AbstractClusterWriter.class);
 
   protected final Writer writer;
-  protected final Map<Integer, List<WeightedVectorWritable>> clusterIdToPoints;
+  protected final Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints;
   protected final DistanceMeasure measure;
 
   /**
@@ -57,52 +56,26 @@
    * @param measure The {@link org.apache.mahout.common.distance.DistanceMeasure} used to calculate the distance.
    *                Some writers may wish to use it for calculating weights for display.  May be null.
    */
-  protected AbstractClusterWriter(Writer writer, Map<Integer, List<WeightedVectorWritable>> clusterIdToPoints,
+  protected AbstractClusterWriter(Writer writer, Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints,
       DistanceMeasure measure) {
     this.writer = writer;
     this.clusterIdToPoints = clusterIdToPoints;
     this.measure = measure;
   }
-  
+
   protected Writer getWriter() {
     return writer;
   }
 
-  protected Map<Integer, List<WeightedVectorWritable>> getClusterIdToPoints() {
+  protected Map<Integer, List<WeightedPropertyVectorWritable>> getClusterIdToPoints() {
     return clusterIdToPoints;
   }
 
   public static String getTopFeatures(Vector vector, String[] dictionary, int numTerms) {
 
-    List<TermIndexWeight> vectorTerms = Lists.newArrayList();
-
-    for (Vector.Element elt : vector.nonZeroes()) {
-      vectorTerms.add(new TermIndexWeight(elt.index(), elt.get()));
-    }
-
-    // Sort results in reverse order (ie weight in descending order)
-    Collections.sort(vectorTerms, new Comparator<TermIndexWeight>() {
-      @Override
-      public int compare(TermIndexWeight one, TermIndexWeight two) {
-        return Double.compare(two.weight, one.weight);
-      }
-    });
-
-    Collection<Pair<String, Double>> topTerms = Lists.newLinkedList();
-
-    for (int i = 0; i < vectorTerms.size() && i < numTerms; i++) {
-      int index = vectorTerms.get(i).index;
-      String dictTerm = dictionary[index];
-      if (dictTerm == null) {
-        log.error("Dictionary entry missing for {}", index);
-        continue;
-      }
-      topTerms.add(new Pair<String, Double>(dictTerm, vectorTerms.get(i).weight));
-    }
-
     StringBuilder sb = new StringBuilder(100);
 
-    for (Pair<String, Double> item : topTerms) {
+    for (Pair<String, Double> item : getTopPairs(vector, dictionary, numTerms)) {
       String term = item.getFirst();
       sb.append("\n\t\t");
       sb.append(StringUtils.rightPad(term, 40));
@@ -114,35 +87,9 @@
 
   public static String getTopTerms(Vector vector, String[] dictionary, int numTerms) {
 
-    List<TermIndexWeight> vectorTerms = Lists.newArrayList();
-
-    for (Vector.Element elt : vector.nonZeroes()) {
-      vectorTerms.add(new TermIndexWeight(elt.index(), elt.get()));
-    }
-
-    // Sort results in reverse order (ie weight in descending order)
-    Collections.sort(vectorTerms, new Comparator<TermIndexWeight>() {
-      @Override
-      public int compare(TermIndexWeight one, TermIndexWeight two) {
-        return Double.compare(two.weight, one.weight);
-      }
-    });
-
-    Collection<Pair<String, Double>> topTerms = Lists.newLinkedList();
-
-    for (int i = 0; i < vectorTerms.size() && i < numTerms; i++) {
-      int index = vectorTerms.get(i).index;
-      String dictTerm = dictionary[index];
-      if (dictTerm == null) {
-        log.error("Dictionary entry missing for {}", index);
-        continue;
-      }
-      topTerms.add(new Pair<String, Double>(dictTerm, vectorTerms.get(i).weight));
-    }
-
     StringBuilder sb = new StringBuilder(100);
 
-    for (Pair<String, Double> item : topTerms) {
+    for (Pair<String, Double> item : getTopPairs(vector, dictionary, numTerms)) {
       String term = item.getFirst();
       sb.append(term).append('_');
     }
@@ -171,6 +118,36 @@
     return result;
   }
 
+  private static Collection<Pair<String, Double>> getTopPairs(Vector vector, String[] dictionary, int numTerms) {
+    List<TermIndexWeight> vectorTerms = Lists.newArrayList();
+
+    for (Vector.Element elt : vector.nonZeroes()) {
+      vectorTerms.add(new TermIndexWeight(elt.index(), elt.get()));
+    }
+
+    // Sort results in reverse order (ie weight in descending order)
+    Collections.sort(vectorTerms, new Comparator<TermIndexWeight>() {
+      @Override
+      public int compare(TermIndexWeight one, TermIndexWeight two) {
+        return Double.compare(two.weight, one.weight);
+      }
+    });
+
+    Collection<Pair<String, Double>> topTerms = Lists.newLinkedList();
+
+    for (int i = 0; i < vectorTerms.size() && i < numTerms; i++) {
+      int index = vectorTerms.get(i).index;
+      String dictTerm = dictionary[index];
+      if (dictTerm == null) {
+        log.error("Dictionary entry missing for {}", index);
+        continue;
+      }
+      topTerms.add(new Pair<String, Double>(dictTerm, vectorTerms.get(i).weight));
+    }
+
+    return topTerms;
+  }
+
   private static class TermIndexWeight {
     private final int index;
     private final double weight;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/clustering/CSVClusterWriter.java mahout/integration/src/main/java/org/apache/mahout/utils/clustering/CSVClusterWriter.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/clustering/CSVClusterWriter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/clustering/CSVClusterWriter.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,7 +18,7 @@
 package org.apache.mahout.utils.clustering;
 
 import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.math.NamedVector;
@@ -39,7 +39,7 @@
 
   private static final Pattern VEC_PATTERN = Pattern.compile("\\{|\\:|\\,|\\}");
 
-  public CSVClusterWriter(Writer writer, Map<Integer, List<WeightedVectorWritable>> clusterIdToPoints,
+  public CSVClusterWriter(Writer writer, Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints,
       DistanceMeasure measure) {
     super(writer, clusterIdToPoints, measure);
   }
@@ -49,9 +49,9 @@
     StringBuilder line = new StringBuilder();
     Cluster cluster = clusterWritable.getValue();
     line.append(cluster.getId());
-    List<WeightedVectorWritable> points = getClusterIdToPoints().get(cluster.getId());
+    List<WeightedPropertyVectorWritable> points = getClusterIdToPoints().get(cluster.getId());
     if (points != null) {
-      for (WeightedVectorWritable point : points) {
+      for (WeightedPropertyVectorWritable point : points) {
         Vector theVec = point.getVector();
         line.append(',');
         if (theVec instanceof NamedVector) {
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumper.java mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumper.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumper.java	2014-03-29 01:03:14.000000000 -0700
@@ -24,14 +24,14 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.TreeMap;
 
+import com.google.common.collect.Maps;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.mahout.clustering.cdbw.CDbwEvaluator;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
 import org.apache.mahout.clustering.evaluation.ClusterEvaluator;
 import org.apache.mahout.clustering.evaluation.RepresentativePointsDriver;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
@@ -63,6 +63,7 @@
     TEXT,
     CSV,
     GRAPH_ML,
+    JSON,
   }
 
   public static final String DICTIONARY_TYPE_OPTION = "dictionaryType";
@@ -82,7 +83,7 @@
   private String dictionaryFormat;
   private int subString = Integer.MAX_VALUE;
   private int numTopFeatures = 10;
-  private Map<Integer, List<WeightedVectorWritable>> clusterIdToPoints;
+  private Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints;
   private OUTPUT_FORMAT outputFormat = OUTPUT_FORMAT.TEXT;
   private boolean runEvaluation;
 
@@ -104,7 +105,7 @@
   public int run(String[] args) throws Exception {
     addInputOption();
     addOutputOption();
-    addOption(OUTPUT_FORMAT_OPT, "of", "The optional output format for the results.  Options: TEXT, CSV or GRAPH_ML",
+    addOption(OUTPUT_FORMAT_OPT, "of", "The optional output format for the results.  Options: TEXT, CSV, JSON or GRAPH_ML",
         "TEXT");
     addOption(SUBSTRING_OPTION, "b", "The number of chars of the asFormatString() to print");
     addOption(NUM_WORDS_OPTION, "n", "The number of top terms to print");
@@ -239,12 +240,22 @@
       case GRAPH_ML:
         result = new GraphMLClusterWriter(writer, clusterIdToPoints, measure, numTopFeatures, dictionary, subString);
         break;
+      case JSON:
+        result = new JsonClusterWriter(writer, clusterIdToPoints, measure, numTopFeatures, dictionary);
+        break;
       default:
         throw new IllegalStateException("Unknown outputformat: " + outputFormat);
     }
     return result;
   }
 
+  /**
+   * Convenience function to set the output format during testing.
+   */
+  public void setOutputFormat(OUTPUT_FORMAT of) {
+    outputFormat = of;
+  }
+
   private void init() {
     if (this.pointsDir != null) {
       Configuration conf = new Configuration();
@@ -264,7 +275,7 @@
     this.subString = subString;
   }
 
-  public Map<Integer, List<WeightedVectorWritable>> getClusterIdToPoints() {
+  public Map<Integer, List<WeightedPropertyVectorWritable>> getClusterIdToPoints() {
     return clusterIdToPoints;
   }
 
@@ -293,18 +304,17 @@
     this.maxPointsPerCluster = maxPointsPerCluster;
   }
 
-  public static Map<Integer, List<WeightedVectorWritable>> readPoints(Path pointsPathDir, long maxPointsPerCluster,
+  public static Map<Integer, List<WeightedPropertyVectorWritable>> readPoints(Path pointsPathDir, long maxPointsPerCluster,
       Configuration conf) {
-    Map<Integer, List<WeightedVectorWritable>> result = new TreeMap<Integer, List<WeightedVectorWritable>>();
-    for (Pair<IntWritable, WeightedVectorWritable> record
-        : new SequenceFileDirIterable<IntWritable, WeightedVectorWritable>(pointsPathDir, PathType.LIST,
+    Map<Integer, List<WeightedPropertyVectorWritable>> result = Maps.newTreeMap();
+    for (Pair<IntWritable, WeightedPropertyVectorWritable> record
+        : new SequenceFileDirIterable<IntWritable, WeightedPropertyVectorWritable>(pointsPathDir, PathType.LIST,
             PathFilters.logsCRCFilter(), conf)) {
       // value is the cluster id as an int, key is the name/id of the
-      // vector, but that doesn't matter because we only care about printing
-      // it
+      // vector, but that doesn't matter because we only care about printing it
       //String clusterId = value.toString();
       int keyValue = record.getFirst().get();
-      List<WeightedVectorWritable> pointList = result.get(keyValue);
+      List<WeightedPropertyVectorWritable> pointList = result.get(keyValue);
       if (pointList == null) {
         pointList = Lists.newArrayList();
         result.put(keyValue, pointList);
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumperWriter.java mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumperWriter.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumperWriter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/clustering/ClusterDumperWriter.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,7 +21,6 @@
 import org.apache.mahout.clustering.AbstractCluster;
 import org.apache.mahout.clustering.Cluster;
 import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.distance.DistanceMeasure;
 
@@ -40,7 +39,7 @@
   private final String[] dictionary;
   private final int numTopFeatures;
   
-  public ClusterDumperWriter(Writer writer, Map<Integer,List<WeightedVectorWritable>> clusterIdToPoints,
+  public ClusterDumperWriter(Writer writer, Map<Integer,List<WeightedPropertyVectorWritable>> clusterIdToPoints,
       DistanceMeasure measure, int numTopFeatures, String[] dictionary, int subString) {
     super(writer, clusterIdToPoints, measure);
     this.numTopFeatures = numTopFeatures;
@@ -69,27 +68,24 @@
       writer.write('\n');
     }
     
-    Map<Integer,List<WeightedVectorWritable>> clusterIdToPoints = getClusterIdToPoints();
-    List<WeightedVectorWritable> points = clusterIdToPoints.get(clusterWritable.getValue().getId());
+    Map<Integer,List<WeightedPropertyVectorWritable>> clusterIdToPoints = getClusterIdToPoints();
+    List<WeightedPropertyVectorWritable> points = clusterIdToPoints.get(clusterWritable.getValue().getId());
     if (points != null) {
       writer.write("\tWeight : [props - optional]:  Point:\n\t");
-      for (Iterator<WeightedVectorWritable> iterator = points.iterator(); iterator.hasNext();) {
-        WeightedVectorWritable point = iterator.next();
+      for (Iterator<WeightedPropertyVectorWritable> iterator = points.iterator(); iterator.hasNext();) {
+        WeightedPropertyVectorWritable point = iterator.next();
         writer.write(String.valueOf(point.getWeight()));
-        if (point instanceof WeightedPropertyVectorWritable) {
-          WeightedPropertyVectorWritable tmp = (WeightedPropertyVectorWritable) point;
-          Map<Text,Text> map = tmp.getProperties();
-          // map can be null since empty maps when written are returned as null
-          writer.write(" : [");
-          if (map != null) {
-            for (Map.Entry<Text,Text> entry : map.entrySet()) {
-              writer.write(entry.getKey().toString());
-              writer.write("=");
-              writer.write(entry.getValue().toString());
-            }
+        Map<Text,Text> map = point.getProperties();
+        // map can be null since empty maps when written are returned as null
+        writer.write(" : [");
+        if (map != null) {
+          for (Map.Entry<Text,Text> entry : map.entrySet()) {
+            writer.write(entry.getKey().toString());
+            writer.write("=");
+            writer.write(entry.getValue().toString());
           }
-          writer.write("]");
         }
+        writer.write("]");
         
         writer.write(": ");
         
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/clustering/GraphMLClusterWriter.java mahout/integration/src/main/java/org/apache/mahout/utils/clustering/GraphMLClusterWriter.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/clustering/GraphMLClusterWriter.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/clustering/GraphMLClusterWriter.java	2014-03-29 01:03:14.000000000 -0700
@@ -26,6 +26,7 @@
 import java.util.regex.Pattern;
 
 import org.apache.mahout.clustering.Cluster;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
 import org.apache.mahout.clustering.classify.WeightedVectorWritable;
 import org.apache.mahout.clustering.iterator.ClusterWritable;
 import org.apache.mahout.common.RandomUtils;
@@ -50,7 +51,7 @@
   private final int numTopFeatures;
   private final int subString;
 
-  public GraphMLClusterWriter(Writer writer, Map<Integer, List<WeightedVectorWritable>> clusterIdToPoints,
+  public GraphMLClusterWriter(Writer writer, Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints,
                               DistanceMeasure measure, int numTopFeatures, String[] dictionary, int subString)
     throws IOException {
     super(writer, clusterIdToPoints, measure);
@@ -115,7 +116,7 @@
     }
 
     line.append(createNode(clusterLabel, rgb, x, y));
-    List<WeightedVectorWritable> points = clusterIdToPoints.get(cluster.getId());
+    List<WeightedPropertyVectorWritable> points = clusterIdToPoints.get(cluster.getId());
     if (points != null) {
       for (WeightedVectorWritable point : points) {
         Vector theVec = point.getVector();
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/clustering/JsonClusterWriter.java mahout/integration/src/main/java/org/apache/mahout/utils/clustering/JsonClusterWriter.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/clustering/JsonClusterWriter.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/integration/src/main/java/org/apache/mahout/utils/clustering/JsonClusterWriter.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,177 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.utils.clustering;
+
+import java.io.IOException;
+import java.io.Writer;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Pattern;
+
+import com.google.common.collect.Maps;
+import org.apache.mahout.clustering.AbstractCluster;
+import org.apache.mahout.clustering.Cluster;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
+import org.apache.mahout.clustering.iterator.ClusterWritable;
+import org.apache.mahout.common.distance.DistanceMeasure;
+import org.apache.mahout.math.NamedVector;
+import org.apache.mahout.math.Vector;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Lists;
+
+/**
+ * Dump cluster info to JSON formatted lines. Heavily inspired by
+ * ClusterDumperWriter.java and CSVClusterWriter.java
+ *
+ */
+public class JsonClusterWriter extends AbstractClusterWriter {
+  private final String[] dictionary;
+  private final int numTopFeatures;
+  private final ObjectMapper jxn;
+
+  private static final Logger log = LoggerFactory
+      .getLogger(JsonClusterWriter.class);
+  private static final Pattern VEC_PATTERN = Pattern.compile("\\{|\\:|\\,|\\}");
+
+  public JsonClusterWriter(Writer writer,
+      Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints,
+      DistanceMeasure measure, int numTopFeatures, String[] dictionary) {
+    super(writer, clusterIdToPoints, measure);
+    this.numTopFeatures = numTopFeatures;
+    this.dictionary = dictionary;
+    jxn = new ObjectMapper();
+  }
+
+  /**
+   * Generate HashMap with cluster info and write as a single JSON formatted
+   * line
+   */
+  @Override
+  public void write(ClusterWritable clusterWritable) throws IOException {
+    Map<String, Object> res = Maps.newHashMap();
+
+    // get top terms
+    List<Object> topTerms = getTopFeaturesList(clusterWritable.getValue()
+        .getCenter(), dictionary, numTopFeatures);
+    res.put("top_terms", topTerms);
+
+    // get human-readable cluster representation
+    Cluster cluster = clusterWritable.getValue();
+    String fmtStr = cluster.asFormatString(dictionary);
+    res.put("cluster_id", cluster.getId());
+    res.put("cluster", fmtStr);
+
+    // get points
+    List<Object> points = getPoints(cluster, dictionary);
+    res.put("points", points);
+
+    // write JSON
+    Writer writer = getWriter();
+    writer.write(jxn.writeValueAsString(res) + "\n");
+  }
+
+  /**
+   * Create a List of HashMaps containing top terms information
+   *
+   * @return List<Object>
+   */
+  public List<Object> getTopFeaturesList(Vector vector, String[] dictionary,
+      int numTerms) {
+
+    List<TermIndexWeight> vectorTerms = Lists.newArrayList();
+
+    for (Vector.Element elt : vector.nonZeroes()) {
+      vectorTerms.add(new TermIndexWeight(elt.index(), elt.get()));
+    }
+
+    // Sort results in reverse order (i.e. weight in descending order)
+    Collections.sort(vectorTerms, new Comparator<TermIndexWeight>() {
+      @Override
+      public int compare(TermIndexWeight one, TermIndexWeight two) {
+        return Double.compare(two.weight, one.weight);
+      }
+    });
+
+    List<Object> topTerms = Lists.newLinkedList();
+
+    for (int i = 0; i < vectorTerms.size() && i < numTerms; i++) {
+      int index = vectorTerms.get(i).index;
+      String dictTerm = dictionary[index];
+      if (dictTerm == null) {
+        log.error("Dictionary entry missing for {}", index);
+        continue;
+      }
+      Map<String, Object> term_entry = Maps.newHashMap();
+      term_entry.put("term", dictTerm);
+      term_entry.put("weight", vectorTerms.get(i).weight);
+      topTerms.add(term_entry);
+    }
+
+    return topTerms;
+  }
+
+  /**
+   * Create a List of HashMaps containing Vector point information
+   *
+   * @return List<Object>
+   */
+  public List<Object> getPoints(Cluster cluster, String[] dictionary) {
+    List<Object> vectorObjs = Lists.newLinkedList();
+    List<WeightedPropertyVectorWritable> points = getClusterIdToPoints().get(
+        cluster.getId());
+
+    if (points != null) {
+      for (WeightedPropertyVectorWritable point : points) {
+        Map<String, Object> entry = Maps.newHashMap();
+        Vector theVec = point.getVector();
+        if (theVec instanceof NamedVector) {
+          entry.put("vector_name", ((NamedVector) theVec).getName());
+        } else {
+          String vecStr = theVec.asFormatString();
+          // do some basic manipulations for display
+          vecStr = VEC_PATTERN.matcher(vecStr).replaceAll("_");
+          entry.put("vector_name", vecStr);
+        }
+        entry.put("weight", String.valueOf(point.getWeight()));
+        entry.put("point",
+            AbstractCluster.formatVector(point.getVector(), dictionary));
+        vectorObjs.add(entry);
+      }
+    }
+    return vectorObjs;
+  }
+
+  /**
+   * Convenience class for sorting terms
+   *
+   */
+  private static class TermIndexWeight {
+    private final int index;
+    private final double weight;
+
+    TermIndexWeight(int index, double weight) {
+      this.index = index;
+      this.weight = weight;
+    }
+  }
+
+}
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/regex/AnalyzerTransformer.java mahout/integration/src/main/java/org/apache/mahout/utils/regex/AnalyzerTransformer.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/regex/AnalyzerTransformer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/regex/AnalyzerTransformer.java	2014-03-29 01:03:14.000000000 -0700
@@ -38,7 +38,7 @@
   private static final Logger log = LoggerFactory.getLogger(AnalyzerTransformer.class);
 
   public AnalyzerTransformer() {
-    this(new StandardAnalyzer(Version.LUCENE_43), "text");
+    this(new StandardAnalyzer(Version.LUCENE_46), "text");
   }
 
   public AnalyzerTransformer(Analyzer analyzer) {
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorDumper.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorDumper.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorDumper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorDumper.java	2014-03-29 01:03:14.000000000 -0700
@@ -148,6 +148,8 @@
     File output = getOutputFile();
     if (output != null) {
       shouldClose = true;
+      log.info("Output file: {}", output);
+      Files.createParentDirs(output);
       writer = Files.newWriter(output, Charsets.UTF_8);
     } else {
       shouldClose = false;
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorHelper.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorHelper.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorHelper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/VectorHelper.java	2014-03-29 01:03:14.000000000 -0700
@@ -193,12 +193,18 @@
    */
   public static String[] loadTermDictionary(Configuration conf, String filePattern) {
     OpenObjectIntHashMap<String> dict = new OpenObjectIntHashMap<String>();
+    int maxIndexValue = 0;
     for (Pair<Text, IntWritable> record
         : new SequenceFileDirIterable<Text, IntWritable>(new Path(filePattern), PathType.GLOB, null, null, true,
                                                          conf)) {
       dict.put(record.getFirst().toString(), record.getSecond().get());
+      if (record.getSecond().get() > maxIndexValue) {
+        maxIndexValue = record.getSecond().get();
+      }
     }
-    String[] dictionary = new String[dict.size()];
+    // Set dictionary size to greater of (maxIndexValue + 1, dict.size())
+    int maxDictionarySize = maxIndexValue + 1 > dict.size() ? maxIndexValue + 1 : dict.size();
+    String[] dictionary = new String[maxDictionarySize];
     for (String feature : dict.keys()) {
       dictionary[dict.get(feature)] = feature;
     }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFIterator.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFIterator.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFIterator.java	2014-03-29 01:03:14.000000000 -0700
@@ -19,9 +19,12 @@
 
 import java.io.BufferedReader;
 import java.io.IOException;
+import java.util.regex.Matcher;
 import java.util.regex.Pattern;
+import java.util.List;
 
 import com.google.common.collect.AbstractIterator;
+import com.google.common.collect.Lists;
 import com.google.common.io.Closeables;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.RandomAccessSparseVector;
@@ -31,8 +34,8 @@
 
   // This pattern will make sure a , inside a string is not a point for split.
   // Ex: "Arizona" , "0:08 PM, PDT" , 110 will be split considering "0:08 PM, PDT" as one string
-  private static final Pattern COMMA_PATTERN = Pattern.compile(",(?=([^\"]*\"[^\"]*\")*[^\"]*$)");
   private static final Pattern WORDS_WITHOUT_SPARSE = Pattern.compile("([\\w[^{]])*");
+  private static final Pattern DATA_PATTERN = Pattern.compile("^\\"+ARFFModel.ARFF_SPARSE+"(.*)\\"+ARFFModel.ARFF_SPARSE_END+"$");
 
   private final BufferedReader reader;
   private final ARFFModel model;
@@ -64,12 +67,12 @@
       return endOfData();
     }
     Vector result;
-    if (line.startsWith(ARFFModel.ARFF_SPARSE)) {
-      line = line.substring(1, line.indexOf(ARFFModel.ARFF_SPARSE_END));
-      String[] splits = COMMA_PATTERN.split(line);
+    Matcher contents = DATA_PATTERN.matcher(line);
+    if (contents.find()) {
+      line = contents.group(1);
+      String[] splits = splitCSV(line);
       result = new RandomAccessSparseVector(model.getLabelSize());
       for (String split : splits) {
-        split = split.trim();
         int idIndex = split.indexOf(' ');
         int idx = Integer.parseInt(split.substring(0, idIndex).trim());
         String data = split.substring(idIndex).trim();
@@ -79,7 +82,7 @@
       }
     } else {
       result = new DenseVector(model.getLabelSize());
-      String[] splits = COMMA_PATTERN.split(line);
+      String[] splits = splitCSV(line);
       for (int i = 0; i < splits.length; i++) {
         String split = splits[i];
         split = split.trim();
@@ -88,8 +91,54 @@
         }
       }
     }
-    //result.setLabelBindings(labelBindings);
     return result;
   }
 
+  /**
+   * Splits a string by comma, ignores commas inside quotes and escaped quotes.
+   * As quotes are both double and single possible, because there is no exact definition
+   * for ARFF files
+   * @param line -
+   * @return String[]
+   */
+  public static String[] splitCSV(String line) {
+    StringBuilder sb = new StringBuilder(128);
+    List<String> tokens = Lists.newArrayList();
+    char escapeChar = '\0';
+    for (int i = 0; i < line.length(); i++) {
+      char c = line.charAt(i);
+      if (c == '\\') {
+        i++;
+        sb.append(line.charAt(i));
+      }
+      else if (c == '"' || c == '\'') {
+        // token is closed
+        if (c == escapeChar) {
+          escapeChar = '\0';
+        }
+        else if (escapeChar == '\0') {
+          escapeChar = c;
+        }
+        sb.append(c);
+      }
+      else if (c == ',') {
+        if (escapeChar == '\0') {
+          tokens.add(sb.toString().trim());
+          sb.setLength(0); // start work on next token
+        }
+        else {
+          sb.append(c);
+        }
+      }
+      else {
+        sb.append(c);
+      }
+    }
+    if (sb.length() > 0) {
+      tokens.add(sb.toString().trim());
+    }
+
+    return tokens.toArray(new String[tokens.size()]);
+  }
+
 }
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterable.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterable.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterable.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterable.java	2014-03-29 01:03:14.000000000 -0700
@@ -31,7 +31,6 @@
 import java.text.SimpleDateFormat;
 import java.util.Iterator;
 import java.util.Locale;
-import java.util.regex.Pattern;
 
 /**
  * Read in ARFF (http://www.cs.waikato.ac.nz/~ml/weka/arff.html) and create {@link Vector}s
@@ -49,9 +48,6 @@
  */
 public class ARFFVectorIterable implements Iterable<Vector> {
 
-  private static final Pattern COMMA_PATTERN = Pattern.compile(",");
-  private static final Pattern SPACE_PATTERN = Pattern.compile(" ");
-
   private final BufferedReader buff;
   private final ARFFModel model;
 
@@ -80,61 +76,64 @@
     String line;
     while ((line = buff.readLine()) != null) {
       line = line.trim();
-      String lower = line.toLowerCase(Locale.ENGLISH);
-      Integer labelNumInt = labelNumber;
-      if (lower.startsWith(ARFFModel.ARFF_COMMENT)) {
-        continue;
-      } else if (lower.startsWith(ARFFModel.RELATION)) {
-        model.setRelation(ARFFType.removeQuotes(line.substring(ARFFModel.RELATION.length())));
-      } else if (lower.startsWith(ARFFModel.ATTRIBUTE)) {
-        String label;
-        ARFFType type;
-        if (lower.contains(ARFFType.NUMERIC.getIndicator())) {
-          label = ARFFType.NUMERIC.getLabel(lower);
-          type = ARFFType.NUMERIC;
-        } else if (lower.contains(ARFFType.INTEGER.getIndicator())) {
-          label = ARFFType.INTEGER.getLabel(lower);
-          type = ARFFType.INTEGER;
-        } else if (lower.contains(ARFFType.REAL.getIndicator())) {
-          label = ARFFType.REAL.getLabel(lower);
-          type = ARFFType.REAL;
-        } else if (lower.contains(ARFFType.STRING.getIndicator())) {
-          label = ARFFType.STRING.getLabel(lower);
-          type = ARFFType.STRING;
-        } else if (lower.contains(ARFFType.NOMINAL.getIndicator())) {
-          label = ARFFType.NOMINAL.getLabel(lower);
-          type = ARFFType.NOMINAL;
-          //@ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}
-          int classIdx = lower.indexOf(ARFFType.NOMINAL.getIndicator());
-          String[] classes = COMMA_PATTERN.split(line.substring(classIdx + 1, line.length() - 1));
-          for (int i = 0; i < classes.length; i++) {
-            model.addNominal(label, ARFFType.removeQuotes(classes[i]), i + 1);
-          }
-        } else if (lower.contains(ARFFType.DATE.getIndicator())) {
-          label = ARFFType.DATE.getLabel(lower);
-          type = ARFFType.DATE;
-          //TODO: DateFormatter map
-          DateFormat format = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss", Locale.ENGLISH);
-          int idx = lower.lastIndexOf(ARFFType.DATE.getIndicator());
-          String[] split = SPACE_PATTERN.split(line);
-          if (split.length >= 4) { //we have a date format
-            String formStr = line.substring(idx + ARFFType.DATE.getIndicator().length()).trim();
-            if (formStr.startsWith("\"")) {
-              formStr = formStr.substring(1, formStr.length() - 1);
+      if (!line.startsWith(ARFFModel.ARFF_COMMENT) && !line.isEmpty()) {
+        Integer labelNumInt = labelNumber;
+        String[] lineParts = line.split("[\\s\\t]+", 2);
+
+        // is it a relation name?
+        if (lineParts[0].equalsIgnoreCase(ARFFModel.RELATION)) {
+          model.setRelation(ARFFType.removeQuotes(lineParts[1]));
+        }
+        // or an attribute
+        else if (lineParts[0].equalsIgnoreCase(ARFFModel.ATTRIBUTE)) {
+          String label;
+          ARFFType type;
+
+          // split the name of the attribute and its description
+          String[] attrParts = lineParts[1].split("[\\s\\t]+", 2);
+          if (attrParts.length < 2)
+            throw new UnsupportedOperationException("No type for attribute found: " + lineParts[1]);
+
+          // label is attribute name
+          label = ARFFType.removeQuotes(attrParts[0].toLowerCase());
+          if (attrParts[1].equalsIgnoreCase(ARFFType.NUMERIC.getIndicator())) {
+            type = ARFFType.NUMERIC;
+          } else if (attrParts[1].equalsIgnoreCase(ARFFType.INTEGER.getIndicator())) {
+            type = ARFFType.INTEGER;
+          } else if (attrParts[1].equalsIgnoreCase(ARFFType.REAL.getIndicator())) {
+            type = ARFFType.REAL;
+          } else if (attrParts[1].equalsIgnoreCase(ARFFType.STRING.getIndicator())) {
+            type = ARFFType.STRING;
+          } else if (attrParts[1].toLowerCase().startsWith(ARFFType.NOMINAL.getIndicator())) {
+            type = ARFFType.NOMINAL;
+            // nominal example:
+            // @ATTRIBUTE class        {Iris-setosa,'Iris versicolor',Iris-virginica}
+            String[] classes = ARFFIterator.splitCSV(attrParts[1].substring(1, attrParts[1].length() - 1));
+            for (int i = 0; i < classes.length; i++) {
+              model.addNominal(label, ARFFType.removeQuotes(classes[i]), i + 1);
+            }
+          } else if (attrParts[1].toLowerCase().startsWith(ARFFType.DATE.getIndicator())) {
+            type = ARFFType.DATE;
+            //TODO: DateFormatter map
+            DateFormat format = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss", Locale.ENGLISH);
+            String formStr = attrParts[1].substring(ARFFType.DATE.getIndicator().length()).trim();
+            if (!formStr.isEmpty()) {
+              if (formStr.startsWith("\"")) {
+                formStr = formStr.substring(1, formStr.length() - 1);
+              }
+              format = new SimpleDateFormat(formStr, Locale.ENGLISH);
             }
-            format = new SimpleDateFormat(formStr, Locale.ENGLISH);
+            model.addDateFormat(labelNumInt, format);
+            //@attribute <name> date [<date-format>]
+          } else {
+            throw new UnsupportedOperationException("Invalid attribute: " + attrParts[1]);
           }
-          model.addDateFormat(labelNumInt, format);
-          //@attribute <name> date [<date-format>]
-        } else {
-          throw new UnsupportedOperationException("Invalid attribute: " + line);
+          model.addLabel(label, labelNumInt);
+          model.addType(labelNumInt, type);
+          labelNumber++;
+        } else if (lineParts[0].equalsIgnoreCase(ARFFModel.DATA)) {
+          break; //skip it
         }
-        model.addLabel(label, labelNumInt);
-        model.addType(labelNumInt, type);
-        labelNumber++;
-      } else if (lower.startsWith(ARFFModel.DATA)) {
-        //inData = true;
-        break; //skip it
       }
     }
 
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModel.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModel.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModel.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModel.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,7 +20,9 @@
 import com.google.common.collect.Maps;
 
 import java.text.DateFormat;
+import java.text.NumberFormat;
 import java.text.ParseException;
+import java.text.ParsePosition;
 import java.text.SimpleDateFormat;
 import java.util.Collections;
 import java.util.Date;
@@ -143,9 +145,19 @@
   }
   
   protected static double processNumeric(String data) {
-    return Double.parseDouble(data);
+    if (isNumeric(data)) {
+      return Double.parseDouble(data);
+    }
+    return Double.NaN;
   }
-  
+
+  public static boolean isNumeric(String str) {
+    NumberFormat formatter = NumberFormat.getInstance();
+    ParsePosition parsePosition = new ParsePosition(0);
+    formatter.parse(str, parsePosition);
+    return str.length() == parsePosition.getIndex();
+  }
+
   protected double processDate(String data, int idx) {
     DateFormat format = dateMap.get(idx);
     if (format == null) {
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfo.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfo.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfo.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfo.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,7 @@
 
 package org.apache.mahout.utils.vectors.lucene;
 
+import com.google.common.collect.Maps;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Terms;
@@ -27,7 +28,6 @@
 
 import java.io.IOException;
 import java.util.Iterator;
-import java.util.LinkedHashMap;
 import java.util.Map;
 
 
@@ -47,16 +47,15 @@
     int numDocs = reader.numDocs();
     double percent = numDocs * maxDfPercent / 100.0;
     //Should we use a linked hash map so that we know terms are in order?
-    termEntries = new LinkedHashMap<String, TermEntry>();
+    termEntries = Maps.newLinkedHashMap();
     int count = 0;
     BytesRef text;
     while ((text = te.next()) != null) {
       int df = te.docFreq();
-      if (df < minDf || df > percent) {
-        continue;
+      if (df >= minDf && df <= percent) {
+        TermEntry entry = new TermEntry(text.utf8ToString(), count++, df);
+        termEntries.put(entry.getTerm(), entry);
       }
-      TermEntry entry = new TermEntry(text.utf8ToString(), count++, df);
-      termEntries.put(entry.getTerm(), entry);
     }
   }
 
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/ClusterLabels.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/ClusterLabels.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/ClusterLabels.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/ClusterLabels.java	2014-03-29 01:03:14.000000000 -0700
@@ -24,7 +24,6 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.LinkedHashMap;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -57,7 +56,7 @@
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.OpenBitSet;
-import org.apache.mahout.clustering.classify.WeightedVectorWritable;
+import org.apache.mahout.clustering.classify.WeightedPropertyVectorWritable;
 import org.apache.mahout.common.CommandLineUtil;
 import org.apache.mahout.common.commandline.DefaultOptionCreator;
 import org.apache.mahout.math.NamedVector;
@@ -87,7 +86,7 @@
   private final String indexDir;
   private final String contentField;
   private String idField;
-  private final Map<Integer, List<WeightedVectorWritable>> clusterIdToPoints;
+  private final Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints;
   private String output;
   private final int minNumIds;
   private final int maxLabels;
@@ -115,15 +114,15 @@
       writer = Files.newWriter(new File(this.output), Charsets.UTF_8);
     }
     try {
-      for (Map.Entry<Integer, List<WeightedVectorWritable>> integerListEntry : clusterIdToPoints.entrySet()) {
-        List<WeightedVectorWritable> wvws = integerListEntry.getValue();
-        List<TermInfoClusterInOut> termInfos = getClusterLabels(integerListEntry.getKey(), wvws);
+      for (Map.Entry<Integer, List<WeightedPropertyVectorWritable>> integerListEntry : clusterIdToPoints.entrySet()) {
+        List<WeightedPropertyVectorWritable> wpvws = integerListEntry.getValue();
+        List<TermInfoClusterInOut> termInfos = getClusterLabels(integerListEntry.getKey(), wpvws);
         if (termInfos != null) {
           writer.write('\n');
           writer.write("Top labels for Cluster ");
           writer.write(String.valueOf(integerListEntry.getKey()));
           writer.write(" containing ");
-          writer.write(String.valueOf(wvws.size()));
+          writer.write(String.valueOf(wpvws.size()));
           writer.write(" vectors");
           writer.write('\n');
           writer.write("Term \t\t LLR \t\t In-ClusterDF \t\t Out-ClusterDF ");
@@ -149,14 +148,14 @@
    * Get the list of labels, sorted by best score.
    */
   protected List<TermInfoClusterInOut> getClusterLabels(Integer integer,
-                                                        Collection<WeightedVectorWritable> wvws) throws IOException {
+                                                        Collection<WeightedPropertyVectorWritable> wpvws) throws IOException {
 
-    if (wvws.size() < minNumIds) {
-      log.info("Skipping small cluster {} with size: {}", integer, wvws.size());
+    if (wpvws.size() < minNumIds) {
+      log.info("Skipping small cluster {} with size: {}", integer, wpvws.size());
       return null;
     }
 
-    log.info("Processing Cluster {} with {} documents", integer, wvws.size());
+    log.info("Processing Cluster {} with {} documents", integer, wpvws.size());
     Directory dir = FSDirectory.open(new File(this.indexDir));
     IndexReader reader = DirectoryReader.open(dir);
     
@@ -164,8 +163,8 @@
     log.info("# of documents in the index {}", reader.numDocs());
 
     Collection<String> idSet = Sets.newHashSet();
-    for (WeightedVectorWritable wvw : wvws) {
-      Vector vector = wvw.getVector();
+    for (WeightedPropertyVectorWritable wpvw : wpvws) {
+      Vector vector = wpvw.getVector();
       if (vector instanceof NamedVector) {
         idSet.add(((NamedVector) vector).getName());
       }
@@ -217,7 +216,7 @@
 
     List<TermInfoClusterInOut> clusteredTermInfo = Lists.newLinkedList();
 
-    int clusterSize = wvws.size();
+    int clusterSize = wpvws.size();
 
     for (TermEntry termEntry : termEntryMap.values()) {
         
diff -uNar -x .git mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/LuceneIterator.java mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/LuceneIterator.java
--- mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/LuceneIterator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/java/org/apache/mahout/utils/vectors/lucene/LuceneIterator.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,13 +18,13 @@
 package org.apache.mahout.utils.vectors.lucene;
 
 import com.google.common.base.Preconditions;
+import com.google.common.collect.Sets;
 import org.apache.lucene.index.IndexReader;
 import org.apache.mahout.utils.vectors.TermInfo;
 import org.apache.mahout.vectorizer.Weight;
 
 import java.io.IOException;
 import java.util.Set;
-import java.util.TreeSet;
 
 /**
  * An {@link java.util.Iterator} over {@link org.apache.mahout.math.Vector}s that uses a Lucene index as the source
@@ -42,22 +42,22 @@
    * @param indexReader {@link IndexReader} to read the documents from.
    * @param idField     field containing the id. May be null.
    * @param field       field to use for the Vector
-   * @param terminfo    terminfo
+   * @param termInfo    termInfo
    * @param weight      weight
-   * @param normPower   the normalization value. Must be nonnegative, or {@link LuceneIterable#NO_NORMALIZING}
+   * @param normPower   the normalization value. Must be non-negative, or {@link LuceneIterable#NO_NORMALIZING}
    */
-  public LuceneIterator(IndexReader indexReader, String idField, String field, TermInfo terminfo, Weight weight,
+  public LuceneIterator(IndexReader indexReader, String idField, String field, TermInfo termInfo, Weight weight,
                         double normPower) {
-    this(indexReader, idField, field, terminfo, weight, normPower, 0.0);
+    this(indexReader, idField, field, termInfo, weight, normPower, 0.0);
   }
 
   /**
    * @param indexReader {@link IndexReader} to read the documents from.
    * @param idField    field containing the id. May be null.
    * @param field      field to use for the Vector
-   * @param terminfo   terminfo
+   * @param termInfo   termInfo
    * @param weight     weight
-   * @param normPower  the normalization value. Must be nonnegative, or {@link LuceneIterable#NO_NORMALIZING}
+   * @param normPower  the normalization value. Must be non-negative, or {@link LuceneIterable#NO_NORMALIZING}
    * @param maxPercentErrorDocs most documents that will be tolerated without a term freq vector. In [0,1].
    * @see #LuceneIterator(org.apache.lucene.index.IndexReader, String, String, org.apache.mahout.utils.vectors.TermInfo,
    * org.apache.mahout.vectorizer.Weight, double)
@@ -65,18 +65,19 @@
   public LuceneIterator(IndexReader indexReader,
                         String idField,
                         String field,
-                        TermInfo terminfo,
+                        TermInfo termInfo,
                         Weight weight,
                         double normPower,
                         double maxPercentErrorDocs) {
-      super(terminfo, normPower, indexReader, weight, maxPercentErrorDocs, field);
+      super(termInfo, normPower, indexReader, weight, maxPercentErrorDocs, field);
       // term docs(null) is a better way of iterating all the docs in Lucene
     Preconditions.checkArgument(normPower == LuceneIterable.NO_NORMALIZING || normPower >= 0,
-            "If specified normPower must be nonnegative", normPower);
-    Preconditions.checkArgument(maxPercentErrorDocs >= 0.0 && maxPercentErrorDocs <= 1.0);
+        "normPower must be non-negative or -1, but normPower = " + normPower);
+    Preconditions.checkArgument(maxPercentErrorDocs >= 0.0 && maxPercentErrorDocs <= 1.0,
+        "Must be: 0.0 <= maxPercentErrorDocs <= 1.0");
     this.idField = idField;
     if (idField != null) {
-      idFieldSelector = new TreeSet<String>();
+      idFieldSelector = Sets.newTreeSet();
       idFieldSelector.add(idField);
     } else {
       /*The field in the index  containing the index. If null, then the Lucene internal doc id is used
diff -uNar -x .git mahout/integration/src/main/webapp/WEB-INF/web.xml mahout/integration/src/main/webapp/WEB-INF/web.xml
--- mahout/integration/src/main/webapp/WEB-INF/web.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/main/webapp/WEB-INF/web.xml	1969-12-31 16:00:00.000000000 -0800
@@ -1,47 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!DOCTYPE web-app PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"
-        "http://java.sun.com/dtd/web-app_2_3.dtd">
-
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<web-app>
-
-  <servlet>
-    <servlet-name>taste-recommender</servlet-name>
-    <display-name>Taste Recommender</display-name>
-    <description>Taste recommender servlet</description>
-    <servlet-class>org.apache.mahout.cf.taste.web.RecommenderServlet</servlet-class>
-    <init-param>
-      <param-name>recommender-class</param-name>
-      <param-value>org.apache.mahout.cf.taste.example.grouplens.GroupLensRecommender</param-value>
-    </init-param>
-    <load-on-startup>1</load-on-startup>
-  </servlet>
-
-  <servlet-mapping>
-    <servlet-name>taste-recommender</servlet-name>
-    <url-pattern>/RecommenderServlet</url-pattern>
-  </servlet-mapping>
-
-  <!-- The rest of the config is adapted from Axis's default web app -->
-
-  <session-config>
-    <session-timeout>5</session-timeout>
-  </session-config>
-
-</web-app>
\ No newline at end of file
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java
--- mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterDumper.java	2014-03-29 01:03:14.000000000 -0700
@@ -27,26 +27,18 @@
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Version;
 import org.apache.mahout.clustering.canopy.CanopyDriver;
-import org.apache.mahout.clustering.dirichlet.DirichletDriver;
-import org.apache.mahout.clustering.dirichlet.models.DistanceMeasureClusterDistribution;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
 import org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
 import org.apache.mahout.clustering.kmeans.KMeansDriver;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver;
 import org.apache.mahout.common.MahoutTestCase;
-import org.apache.mahout.common.distance.CosineDistanceMeasure;
 import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
-import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
 import org.apache.mahout.math.NamedVector;
-import org.apache.mahout.math.RandomAccessSparseVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
 import org.apache.mahout.utils.clustering.ClusterDumper;
@@ -92,7 +84,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(conf);
     // Create test data
     getSampleData(DOCS);
@@ -105,18 +97,23 @@
     RAMDirectory directory = new RAMDirectory();
     
     IndexWriter writer = new IndexWriter(directory, 
-           new IndexWriterConfig(Version.LUCENE_43,new StandardAnalyzer(
-        Version.LUCENE_43)));
+           new IndexWriterConfig(Version.LUCENE_46, new StandardAnalyzer(Version.LUCENE_46)));
             
     try {
       for (int i = 0; i < docs2.length; i++) {
         Document doc = new Document();
-        Field id = new Field("id", "doc_" + i, Field.Store.YES,
-            Field.Index.NOT_ANALYZED_NO_NORMS);
+        Field id = new StringField("id", "doc_" + i, Field.Store.YES);
         doc.add(id);
         // Store both position and offset information
-        Field text = new Field("content", docs2[i], Field.Store.NO,
-            Field.Index.ANALYZED, Field.TermVector.YES);
+        FieldType fieldType = new FieldType();
+        fieldType.setStored(false);
+        fieldType.setIndexed(true);
+        fieldType.setTokenized(true);
+        fieldType.setStoreTermVectors(true);
+        fieldType.setStoreTermVectorPositions(true);
+        fieldType.setStoreTermVectorOffsets(true);
+        fieldType.freeze();
+        Field text = new Field("content", docs2[i], fieldType);
         doc.add(text);
         writer.addDocument(doc);
       }
@@ -185,7 +182,7 @@
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     
     Path output = getTestTempDirPath("output");
-    CanopyDriver.run(new Configuration(), getTestTempDirPath("testdata"),
+    CanopyDriver.run(getConfiguration(), getTestTempDirPath("testdata"),
         output, measure, 8, 4, true, 0.0, true);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(new Path(output,
@@ -202,80 +199,50 @@
     CanopyDriver.run(conf, getTestTempDirPath("testdata"), output, measure, 8,
         4, false, 0.0, true);
     // now run the KMeans job
-    Path kmeansOutput = new Path(output, "kmeans");
-	KMeansDriver.run(conf, getTestTempDirPath("testdata"), new Path(output,
-        "clusters-0-final"), kmeansOutput, measure, 0.001, 10, true, 0.0, false);
+    Path kMeansOutput = new Path(output, "kmeans");
+    KMeansDriver.run(conf, getTestTempDirPath("testdata"), new Path(output,
+        "clusters-0-final"), kMeansOutput, 0.001, 10, true, 0.0, false);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-        output, 10), new Path(kmeansOutput, "clusteredPoints"));
+        output, 10), new Path(kMeansOutput, "clusteredPoints"));
     clusterDumper.printClusters(termDictionary);
   }
-  
+
   @Test
-  public void testFuzzyKmeans() throws Exception {
+  public void testJsonClusterDumper() throws Exception {
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     // now run the Canopy job to prime kMeans canopies
     Path output = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     CanopyDriver.run(conf, getTestTempDirPath("testdata"), output, measure, 8,
         4, false, 0.0, true);
-    // now run the Fuzzy KMeans job
+    // now run the KMeans job
     Path kmeansOutput = new Path(output, "kmeans");
-	FuzzyKMeansDriver.run(conf, getTestTempDirPath("testdata"), new Path(
-        output, "clusters-0-final"), kmeansOutput, measure, 0.001, 10, 1.1f, true,
-        true, 0, true);
+    KMeansDriver.run(conf, getTestTempDirPath("testdata"), new Path(output,
+        "clusters-0-final"), kmeansOutput, 0.001, 10, true, 0.0, false);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
         output, 10), new Path(kmeansOutput, "clusteredPoints"));
+    clusterDumper.setOutputFormat(ClusterDumper.OUTPUT_FORMAT.JSON);
     clusterDumper.printClusters(termDictionary);
   }
   
   @Test
-  public void testMeanShift() throws Exception {
-    DistanceMeasure measure = new CosineDistanceMeasure();
-    IKernelProfile kernelProfile = new TriangularKernelProfile();
-    Path output = getTestTempDirPath("output");
-    Configuration conf = new Configuration();
-    MeanShiftCanopyDriver.run(conf, getTestTempDirPath("testdata"), output,
-        measure, kernelProfile, 0.5, 0.01, 0.05, 10, false, true, true);
-    // run ClusterDumper
-    ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-        output, 10), new Path(output, "clusteredPoints"));
-    clusterDumper.printClusters(termDictionary);
-  }
-  
-  @Test
-  public void testDirichlet2() throws Exception {
-    Path output = getTestTempDirPath("output");
-    NamedVector prototype = (NamedVector) sampleData.get(0).get();
-    DistributionDescription description = new DistributionDescription(
-        GaussianClusterDistribution.class.getName(),
-        RandomAccessSparseVector.class.getName(), null, prototype.getDelegate()
-            .size());
-    Configuration conf = new Configuration();
-    DirichletDriver.run(conf, getTestTempDirPath("testdata"), output,
-        description, 15, 10, 1.0, true, true, 0, true);
-    // run ClusterDumper
-    ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-        output, 10), new Path(output, "clusteredPoints"));
-    clusterDumper.printClusters(termDictionary);
-  }
-  
-  @Test
-  public void testDirichlet3() throws Exception {
+  public void testFuzzyKmeans() throws Exception {
+    DistanceMeasure measure = new EuclideanDistanceMeasure();
+    // now run the Canopy job to prime kMeans canopies
     Path output = getTestTempDirPath("output");
-    NamedVector prototype = (NamedVector) sampleData.get(0).get();
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(),
-        RandomAccessSparseVector.class.getName(),
-        ManhattanDistanceMeasure.class.getName(), prototype.getDelegate()
-            .size());
-    Configuration conf = new Configuration();
-    DirichletDriver.run(conf, getTestTempDirPath("testdata"), output,
-        description, 15, 10, 1.0, true, true, 0, true);
+    Configuration conf = getConfiguration();
+    CanopyDriver.run(conf, getTestTempDirPath("testdata"), output, measure, 8,
+        4, false, 0.0, true);
+    // now run the Fuzzy KMeans job
+    Path kMeansOutput = new Path(output, "kmeans");
+    FuzzyKMeansDriver.run(conf, getTestTempDirPath("testdata"), new Path(
+        output, "clusters-0-final"), kMeansOutput, 0.001, 10, 1.1f, true,
+        true, 0, true);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-        output, 10), new Path(output, "clusteredPoints"));
+        output, 10), new Path(kMeansOutput, "clusteredPoints"));
     clusterDumper.printClusters(termDictionary);
   }
   
@@ -336,11 +303,11 @@
     CanopyDriver.run(conf, svdData, output, measure, 8, 4, false, 0.0, true);
     // now run the KMeans job
     Path kmeansOutput = new Path(output, "kmeans");
-	KMeansDriver.run(svdData, new Path(output, "clusters-0"), kmeansOutput, measure,
+    KMeansDriver.run(svdData, new Path(output, "clusters-0"), kmeansOutput, measure,
         0.001, 10, true, 0.0, true);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-    		kmeansOutput, 10), new Path(kmeansOutput, "clusteredPoints"));
+        kmeansOutput, 10), new Path(kmeansOutput, "clusteredPoints"));
     clusterDumper.printClusters(termDictionary);
   }
   
@@ -378,11 +345,11 @@
         0.0, true);
     // now run the KMeans job
     Path kmeansOutput = new Path(output, "kmeans");
-	KMeansDriver.run(sData.getRowPath(), new Path(output, "clusters-0"),
+    KMeansDriver.run(sData.getRowPath(), new Path(output, "clusters-0"),
         kmeansOutput, measure, 0.001, 10, true, 0.0, true);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-    		kmeansOutput, 10), new Path(kmeansOutput, "clusteredPoints"));
+        kmeansOutput, 10), new Path(kmeansOutput, "clusteredPoints"));
     clusterDumper.printClusters(termDictionary);
   }
   
@@ -423,11 +390,11 @@
         0.0, true);
     // now run the KMeans job
     Path kmeansOutput = new Path(output, "kmeans");
-	KMeansDriver.run(sData.getRowPath(), new Path(output, "clusters-0"),
+    KMeansDriver.run(sData.getRowPath(), new Path(output, "clusters-0"),
         kmeansOutput, measure, 0.001, 10, true, 0.0, true);
     // run ClusterDumper
     ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf,
-    		kmeansOutput, 10), new Path(kmeansOutput, "clusteredPoints"));
+        kmeansOutput, 10), new Path(kmeansOutput, "clusteredPoints"));
     clusterDumper.printClusters(termDictionary);
   }
    */
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java
--- mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/clustering/TestClusterEvaluator.java	2014-03-29 01:03:14.000000000 -0700
@@ -26,18 +26,11 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.mahout.clustering.canopy.Canopy;
 import org.apache.mahout.clustering.canopy.CanopyDriver;
-import org.apache.mahout.clustering.dirichlet.DirichletDriver;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
 import org.apache.mahout.clustering.evaluation.ClusterEvaluator;
 import org.apache.mahout.clustering.evaluation.RepresentativePointsDriver;
 import org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
-import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
 import org.apache.mahout.clustering.kmeans.KMeansDriver;
 import org.apache.mahout.clustering.kmeans.TestKmeansClustering;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver;
 import org.apache.mahout.common.HadoopUtil;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.distance.DistanceMeasure;
@@ -78,7 +71,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+    conf = getConfiguration();
     fs = FileSystem.get(conf);
     testdata = getTestTempDirPath("testdata");
     output = getTestTempDirPath("output");
@@ -269,7 +262,7 @@
   public void testCanopy() throws Exception {
     ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, true, 0.0, true);
     int numIterations = 10;
     Path clustersIn = new Path(output, "clusters-0-final");
@@ -287,11 +280,11 @@
     ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     // now run the Canopy job to prime kMeans canopies
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, false, 0.0, true);
     // now run the KMeans job
     Path kmeansOutput = new Path(output, "kmeans");
-    KMeansDriver.run(testdata, new Path(output, "clusters-0-final"), kmeansOutput, measure, 0.001, 10, true, 0.0, true);
+    KMeansDriver.run(testdata, new Path(output, "clusters-0-final"), kmeansOutput, 0.001, 10, true, 0.0, true);
     int numIterations = 10;
     Path clustersIn = new Path(kmeansOutput, "clusters-2");
     RepresentativePointsDriver.run(conf, clustersIn, new Path(kmeansOutput, "clusteredPoints"), kmeansOutput, measure,
@@ -308,11 +301,11 @@
     ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     // now run the Canopy job to prime kMeans canopies
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, false, 0.0, true);
     Path fuzzyKMeansOutput = new Path(output, "fuzzyk");
     // now run the KMeans job
-    FuzzyKMeansDriver.run(testdata, new Path(output, "clusters-0-final"), fuzzyKMeansOutput, measure, 0.001, 10, 2,
+    FuzzyKMeansDriver.run(testdata, new Path(output, "clusters-0-final"), fuzzyKMeansOutput, 0.001, 10, 2,
         true, true, 0, true);
     int numIterations = 10;
     Path clustersIn = new Path(fuzzyKMeansOutput, "clusters-4");
@@ -325,40 +318,4 @@
     System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());
   }
   
-  @Test
-  public void testMeanShift() throws Exception {
-    ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);
-    DistanceMeasure measure = new EuclideanDistanceMeasure();
-    IKernelProfile kernelProfile = new TriangularKernelProfile();
-    Configuration conf = new Configuration();
-    MeanShiftCanopyDriver.run(conf, testdata, output, measure, kernelProfile, 2.1, 1.0, 0.001, 10, false, true, true);
-    int numIterations = 10;
-    Path clustersIn = new Path(output, "clusters-8-final");
-    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure,
-        numIterations, true);
-    //printRepPoints(numIterations);
-    ClusterEvaluator evaluator = new ClusterEvaluator(conf, clustersIn);
-    // now print out the Results
-    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());
-    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());
-  }
-  
-  @Test
-  public void testDirichlet() throws Exception {
-    ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);
-    DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-        DenseVector.class.getName(), null, 2);
-    DirichletDriver.run(new Configuration(), testdata, output, description, 15, 5, 1.0, true, true, 0.0, true);
-    int numIterations = 10;
-    Configuration conf = new Configuration();
-    Path clustersIn = new Path(output, "clusters-5-final");
-    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output,
-        new EuclideanDistanceMeasure(), numIterations, true);
-    //printRepPoints(numIterations);
-    ClusterEvaluator evaluator = new ClusterEvaluator(conf, clustersIn);
-    // now print out the Results
-    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());
-    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());
-  }
-  
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/clustering/cdbw/TestCDbwEvaluator.java mahout/integration/src/test/java/org/apache/mahout/clustering/cdbw/TestCDbwEvaluator.java
--- mahout/integration/src/test/java/org/apache/mahout/clustering/cdbw/TestCDbwEvaluator.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/clustering/cdbw/TestCDbwEvaluator.java	2014-03-29 01:03:14.000000000 -0700
@@ -28,22 +28,16 @@
 import org.apache.mahout.clustering.Cluster;
 import org.apache.mahout.clustering.ClusteringTestUtils;
 import org.apache.mahout.clustering.TestClusterEvaluator;
-import org.apache.mahout.clustering.kernel.TriangularKernelProfile;
 import org.apache.mahout.clustering.canopy.Canopy;
 import org.apache.mahout.clustering.canopy.CanopyDriver;
-import org.apache.mahout.clustering.dirichlet.DirichletDriver;
-import org.apache.mahout.clustering.dirichlet.UncommonDistributions;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
+import org.apache.mahout.clustering.UncommonDistributions;
 import org.apache.mahout.clustering.evaluation.RepresentativePointsDriver;
 import org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver;
 import org.apache.mahout.clustering.kmeans.KMeansDriver;
 import org.apache.mahout.clustering.kmeans.TestKmeansClustering;
-import org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver;
 import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.distance.DistanceMeasure;
 import org.apache.mahout.common.distance.EuclideanDistanceMeasure;
-import org.apache.mahout.clustering.kernel.IKernelProfile;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
@@ -81,7 +75,7 @@
   @Before
   public void setUp() throws Exception {
     super.setUp();
-    conf = new Configuration();
+    conf = getConfiguration();
     fs = FileSystem.get(conf);
     testdata = getTestTempDirPath("testdata");
     output = getTestTempDirPath("output");
@@ -130,7 +124,6 @@
    *          double y-value of the sample mean
    * @param sd
    *          double standard deviation of the samples
-   * @throws Exception
    */
   private void generateSamples(int num, double mx, double my, double sd) {
     log.info("Generating {} samples m=[{}, {}] sd={}", num, mx, my, sd);
@@ -272,7 +265,7 @@
   public void testCanopy() throws Exception {
     ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
-    CanopyDriver.run(new Configuration(), testdata, output, measure, 3.1, 2.1, true, 0.0, true);
+    CanopyDriver.run(getConfiguration(), testdata, output, measure, 3.1, 2.1, true, 0.0, true);
     int numIterations = 10;
     Path clustersIn = new Path(output, "clusters-0-final");
     RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure,
@@ -291,10 +284,10 @@
     ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     // now run the Canopy job to prime kMeans canopies
-    CanopyDriver.run(new Configuration(), testdata, output, measure, 3.1, 2.1, false, 0.0, true);
+    CanopyDriver.run(getConfiguration(), testdata, output, measure, 3.1, 2.1, false, 0.0, true);
     // now run the KMeans job
     Path kmeansOutput = new Path(output, "kmeans");
-    KMeansDriver.run(testdata, new Path(output, "clusters-0-final"), kmeansOutput, measure, 0.001, 10, true, 0.0, true);
+    KMeansDriver.run(testdata, new Path(output, "clusters-0-final"), kmeansOutput, 0.001, 10, true, 0.0, true);
     int numIterations = 10;
     Path clustersIn = new Path(kmeansOutput, "clusters-10-final");
     RepresentativePointsDriver.run(conf, clustersIn, new Path(kmeansOutput, "clusteredPoints"), kmeansOutput, measure,
@@ -313,10 +306,10 @@
     ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);
     DistanceMeasure measure = new EuclideanDistanceMeasure();
     // now run the Canopy job to prime kMeans canopies
-    CanopyDriver.run(new Configuration(), testdata, output, measure, 3.1, 2.1, false, 0.0, true);
+    CanopyDriver.run(getConfiguration(), testdata, output, measure, 3.1, 2.1, false, 0.0, true);
     Path fuzzyKMeansOutput = new Path(output, "fuzzyk");
     // now run the KMeans job
-    FuzzyKMeansDriver.run(testdata, new Path(output, "clusters-0-final"), fuzzyKMeansOutput, measure, 0.001, 10, 2,
+    FuzzyKMeansDriver.run(testdata, new Path(output, "clusters-0-final"), fuzzyKMeansOutput, 0.001, 10, 2,
         true, true, 0, true);
     int numIterations = 10;
     Path clustersIn = new Path(fuzzyKMeansOutput, "clusters-4");
@@ -331,42 +324,4 @@
     System.out.println("Separation = " + evaluator.separation());
   }
   
-  @Test
-  public void testMeanShift() throws Exception {
-    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);
-    DistanceMeasure measure = new EuclideanDistanceMeasure();
-    IKernelProfile kernelProfile = new TriangularKernelProfile();
-    MeanShiftCanopyDriver.run(conf, testdata, output, measure, kernelProfile, 2.1, 1.0, 0.001, 10, false, true, true);
-    int numIterations = 10;
-    Path clustersIn = new Path(output, "clusters-2");
-    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure,
-        numIterations, true);
-    CDbwEvaluator evaluator = new CDbwEvaluator(conf, clustersIn);
-    // printRepPoints(numIterations);
-    // now print out the Results
-    System.out.println("Mean Shift CDbw = " + evaluator.getCDbw());
-    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());
-    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());
-    System.out.println("Separation = " + evaluator.separation());
-  }
-  
-  @Test
-  public void testDirichlet() throws Exception {
-    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);
-    DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-        DenseVector.class.getName(), null, 2);
-    DirichletDriver.run(new Configuration(), testdata, output, description, 15, 5, 1.0, true, true, 0.0, true);
-    int numIterations = 10;
-    Path clustersIn = new Path(output, "clusters-0");
-    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output,
-        new EuclideanDistanceMeasure(), numIterations, true);
-    CDbwEvaluator evaluator = new CDbwEvaluator(conf, clustersIn);
-    RepresentativePointsDriver.printRepresentativePoints(output, numIterations);
-    // now print out the Results
-    System.out.println("Dirichlet CDbw = " + evaluator.getCDbw());
-    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());
-    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());
-    System.out.println("Separation = " + evaluator.separation());
-  }
-  
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/clustering/dirichlet/TestL1ModelClustering.java mahout/integration/src/test/java/org/apache/mahout/clustering/dirichlet/TestL1ModelClustering.java
--- mahout/integration/src/test/java/org/apache/mahout/clustering/dirichlet/TestL1ModelClustering.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/clustering/dirichlet/TestL1ModelClustering.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,300 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.clustering.dirichlet;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
-import org.apache.mahout.clustering.Cluster;
-import org.apache.mahout.clustering.Model;
-import org.apache.mahout.clustering.ModelDistribution;
-import org.apache.mahout.clustering.classify.ClusterClassifier;
-import org.apache.mahout.clustering.dirichlet.models.DistanceMeasureClusterDistribution;
-import org.apache.mahout.clustering.dirichlet.models.DistributionDescription;
-import org.apache.mahout.clustering.dirichlet.models.GaussianClusterDistribution;
-import org.apache.mahout.clustering.iterator.ClusterIterator;
-import org.apache.mahout.clustering.iterator.DirichletClusteringPolicy;
-import org.apache.mahout.common.distance.CosineDistanceMeasure;
-import org.apache.mahout.common.distance.ManhattanDistanceMeasure;
-import org.apache.mahout.math.RandomAccessSparseVector;
-import org.apache.mahout.math.Vector;
-import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.utils.MahoutTestCase;
-import org.apache.mahout.utils.vectors.TermInfo;
-import org.apache.mahout.utils.vectors.lucene.CachedTermInfo;
-import org.apache.mahout.utils.vectors.lucene.LuceneIterable;
-import org.apache.mahout.vectorizer.TFIDF;
-import org.apache.mahout.vectorizer.Weight;
-import org.junit.Test;
-
-import com.google.common.collect.Lists;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-
-public final class TestL1ModelClustering extends MahoutTestCase {
-  
-  private class MapElement implements Comparable<MapElement> {
-    
-    private final Double pdf;
-    private final String doc;
-    
-    MapElement(double pdf, String doc) {
-      this.pdf = pdf;
-      this.doc = doc;
-    }
-    
-    @Override
-    // reverse compare to sort in reverse order
-    public int compareTo(MapElement e) {
-      if (e.pdf > pdf) {
-        return 1;
-      } else if (e.pdf < pdf) {
-        return -1;
-      } else {
-        return 0;
-      }
-    }
-    
-    @Override
-    public int hashCode() {
-      int hash = pdf == null ? 0 : pdf.hashCode();
-      hash ^= doc == null ? 0 : doc.hashCode();
-      return hash;
-    }
-    
-    @Override
-    public boolean equals(Object other) {
-      if (!(other instanceof MapElement)) {
-        return false;
-      }
-      MapElement otherElement = (MapElement) other;
-      if ((pdf == null && otherElement.pdf != null) || (pdf != null && !pdf.equals(otherElement.pdf))) {
-        return false;
-      }
-      return doc == null ? otherElement.doc == null : doc.equals(otherElement.doc);
-    }
-    
-    @Override
-    public String toString() {
-      return pdf.toString() + ' ' + doc;
-    }
-    
-  }
-  
-  private static final String[] DOCS = {"The quick red fox jumped over the lazy brown dogs.",
-      "The quick brown fox jumped over the lazy red dogs.", "The quick red cat jumped over the lazy brown dogs.",
-      "The quick brown cat jumped over the lazy red dogs.", "Mary had a little lamb whose fleece was white as snow.",
-      "Moby Dick is a story of a whale and a man obsessed.",
-      "The robber wore a black fleece jacket and a baseball cap.",
-      "The English Springer Spaniel is the best of all dogs."};
-  
-  private static final String[] DOCS2 = {"The quick red fox jumped over the lazy brown dogs.",
-      "The quick brown fox jumped over the lazy red dogs.", "The quick red cat jumped over the lazy brown dogs.",
-      "The quick brown cat jumped over the lazy red dogs.", "Mary had a little lamb whose fleece was white as snow.",
-      "Mary had a little goat whose fleece was white as snow.",
-      "Mary had a little lamb whose fleece was black as tar.",
-      "Dick had a little goat whose fleece was white as snow.", "Moby Dick is a story of a whale and a man obsessed.",
-      "Moby Bob is a story of a walrus and a man obsessed.", "Moby Dick is a story of a whale and a crazy man.",
-      "The robber wore a black fleece jacket and a baseball cap.",
-      "The robber wore a red fleece jacket and a baseball cap.",
-      "The robber wore a white fleece jacket and a baseball cap.",
-      "The English Springer Spaniel is the best of all dogs."};
-  
-  private List<Vector> sampleData;
-  
-  private void getSampleData(String[] docs2) throws IOException {
-    System.out.println();
-    sampleData = Lists.newArrayList();
-    RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter( directory, new IndexWriterConfig(Version.LUCENE_43,new StandardAnalyzer(Version.LUCENE_43)));
-
-    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);
-    customType.setStoreTermVectors(true);
-    
-    try {
-      for (int i = 0; i < docs2.length; i++) {
-        Document doc = new Document();
-        Field id = new Field("id", "doc_" + i, StringField.TYPE_STORED);
-        doc.add(id);
-        // Store both position and offset information
-        Field text = new Field("content", docs2[i], customType);
-        doc.add(text);
-        writer.addDocument(doc);
-        writer.commit();
-      }
-    } finally {
-      writer.close();
-    }
-    
-    IndexReader reader = DirectoryReader.open(directory);
-    System.out.println("Number of documents: \t"+reader.numDocs());
-    
-    
-    Weight weight = new TFIDF();
-    TermInfo termInfo = new CachedTermInfo(reader, "content", 1, 100);
-    Iterable<Vector> iterable = new LuceneIterable(reader, "id", "content", termInfo,weight);
-    
-    int i = 0;
-    for (Vector vector : iterable) {
-      assertNotNull(vector);
-      System.out.println(i + ". " + docs2[i++]);
-      System.out.println('\t' + formatVector(vector));
-      sampleData.add(vector);
-    }
-  }
-  
-  private static String formatVector(Vector v) {
-    StringBuilder buf = new StringBuilder();
-    int nzero = v.getNumNondefaultElements();
-    buf.append('(').append(nzero);
-    buf.append("nz) [");
-    // handle sparse Vectors gracefully, suppressing zero values
-    int nextIx = 0;
-    for (int i = 0; i < v.size(); i++) {
-      double elem = v.get(i);
-      if (elem == 0.0) {
-        continue;
-      }
-      if (i > nextIx) {
-        buf.append("..{").append(i).append("}=");
-      }
-      buf.append(String.format(Locale.ENGLISH, "%.2f", elem)).append(", ");
-      nextIx = i + 1;
-    }
-    buf.append(']');
-    return buf.toString();
-  }
-  
-  private void printClusters(List<Cluster> models, String[] docs) {
-    for (int m = 0; m < models.size(); m++) {
-      Cluster model = models.get(m);
-      long total = model.getTotalObservations();
-      if (total == 0) {
-        continue;
-      }
-      System.out.println();
-      System.out.println("Model[" + m + "] had " + total + " observations");
-      System.out.println("pdf           document");
-      MapElement[] map = new MapElement[sampleData.size()];
-      // sort the samples by pdf
-      double maxPdf = Double.MIN_NORMAL;
-      for (int i = 0; i < sampleData.size(); i++) {
-        VectorWritable sample = new VectorWritable(sampleData.get(i));
-        double pdf = Math.abs(model.pdf(sample));
-        maxPdf = Math.max(maxPdf, pdf);
-        map[i] = new MapElement(pdf, docs[i]);
-      }
-      Arrays.sort(map);
-      for (MapElement aMap : map) {
-        Double pdf = aMap.pdf;
-        double norm = pdf / maxPdf;
-        System.out.println(String.format(Locale.ENGLISH, "%.3f", norm) + ' ' + aMap.doc);
-      }
-    }
-  }
-  
-  @Test
-  public void testDocs() throws Exception {
-    getSampleData(DOCS);
-    DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-        RandomAccessSparseVector.class.getName(), ManhattanDistanceMeasure.class.getName(), sampleData.get(0).size());
-    
-    List<Cluster> models = Lists.newArrayList();
-    ModelDistribution<VectorWritable> modelDist = description.createModelDistribution(new Configuration());
-    for (Model<VectorWritable> cluster : modelDist.sampleFromPrior(15)) {
-      models.add((Cluster) cluster);
-    }
-    
-    ClusterClassifier classifier = new ClusterClassifier(models, new DirichletClusteringPolicy(15, 1.0));
-    ClusterClassifier posterior = ClusterIterator.iterate(sampleData, classifier, 10);
-    
-    printClusters(posterior.getModels(), DOCS);
-  }
-  
-  @Test
-  public void testDMDocs() throws Exception {
-    
-    getSampleData(DOCS);
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(), RandomAccessSparseVector.class.getName(),
-        CosineDistanceMeasure.class.getName(), sampleData.get(0).size());
-    
-    List<Cluster> models = Lists.newArrayList();
-    ModelDistribution<VectorWritable> modelDist = description.createModelDistribution(new Configuration());
-    for (Model<VectorWritable> cluster : modelDist.sampleFromPrior(15)) {
-      models.add((Cluster) cluster);
-    }
-    
-    ClusterClassifier classifier = new ClusterClassifier(models, new DirichletClusteringPolicy(15, 1.0));
-    ClusterClassifier posterior = ClusterIterator.iterate(sampleData, classifier, 10);
-    
-    printClusters(posterior.getModels(), DOCS);
-  }
-  
-  @Test
-  public void testDocs2() throws Exception {
-    getSampleData(DOCS2);
-    DistributionDescription description = new DistributionDescription(GaussianClusterDistribution.class.getName(),
-        RandomAccessSparseVector.class.getName(), ManhattanDistanceMeasure.class.getName(), sampleData.get(0).size());
-    
-    List<Cluster> models = Lists.newArrayList();
-    ModelDistribution<VectorWritable> modelDist = description.createModelDistribution(new Configuration());
-    for (Model<VectorWritable> cluster : modelDist.sampleFromPrior(15)) {
-      models.add((Cluster) cluster);
-    }
-    
-    ClusterClassifier classifier = new ClusterClassifier(models, new DirichletClusteringPolicy(15, 1.0));
-    ClusterClassifier posterior = ClusterIterator.iterate(sampleData, classifier, 10);
-    
-    printClusters(posterior.getModels(), DOCS2);
-  }
-  
-  @Test
-  public void testDMDocs2() throws Exception {
-    
-    getSampleData(DOCS);
-    DistributionDescription description = new DistributionDescription(
-        DistanceMeasureClusterDistribution.class.getName(), RandomAccessSparseVector.class.getName(),
-        CosineDistanceMeasure.class.getName(), sampleData.get(0).size());
-    
-    List<Cluster> models = Lists.newArrayList();
-    ModelDistribution<VectorWritable> modelDist = description.createModelDistribution(new Configuration());
-    for (Model<VectorWritable> cluster : modelDist.sampleFromPrior(15)) {
-      models.add((Cluster) cluster);
-    }
-    
-    ClusterClassifier classifier = new ClusterClassifier(models, new DirichletClusteringPolicy(15, 1.0));
-    ClusterClassifier posterior = ClusterIterator.iterate(sampleData, classifier, 10);
-    
-    printClusters(posterior.getModels(), DOCS2);
-  }
-  
-}
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/AbstractLuceneStorageTest.java mahout/integration/src/test/java/org/apache/mahout/text/AbstractLuceneStorageTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/AbstractLuceneStorageTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/AbstractLuceneStorageTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,7 @@
 package org.apache.mahout.text;
 
 import com.google.common.collect.Lists;
+
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Text;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
@@ -25,11 +26,11 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.Version;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.text.doc.MultipleFieldsDocument;
 import org.apache.mahout.text.doc.NumericFieldDocument;
 import org.apache.mahout.text.doc.SingleFieldDocument;
-import org.apache.mahout.utils.MahoutTestCase;
 
 import java.io.File;
 import java.io.IOException;
@@ -59,7 +60,7 @@
   }
 
   protected void commitDocuments(Directory directory, Iterable<SingleFieldDocument> theDocs) throws IOException{
-    IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_43, new StandardAnalyzer(Version.LUCENE_43)));
+    IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_46, new StandardAnalyzer(Version.LUCENE_46)));
 
     for (SingleFieldDocument singleFieldDocument : theDocs) {
       indexWriter.addDocument(singleFieldDocument.asLuceneDocument());
@@ -73,11 +74,6 @@
     commitDocuments(directory, Arrays.asList(documents));
   }
 
-  protected void assertSimpleDocumentEquals(SingleFieldDocument expected, Pair<Text, Text> actual) {
-    assertEquals(expected.getId(), actual.getFirst().toString());
-    assertEquals(expected.getField(), actual.getSecond().toString());
-  }
-
   protected void assertMultipleFieldsDocumentEquals(MultipleFieldsDocument expected, Pair<Text, Text> actual) {
     assertEquals(expected.getId(), actual.getFirst().toString());
     assertEquals(expected.getField() + " " + expected.getField1() + " " + expected.getField2(), actual.getSecond().toString());
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputFormatTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -28,6 +28,7 @@
 import org.junit.Test;
 
 import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
 import java.util.Collections;
 import java.util.List;
 
@@ -38,12 +39,13 @@
   private Configuration conf;
 
   @Before
-  public void before() throws IOException {
+  public void before() throws Exception {
     inputFormat = new LuceneSegmentInputFormat();
-    LuceneStorageConfiguration lucene2SeqConf = new LuceneStorageConfiguration(new Configuration(), Collections.singletonList(indexPath1), new Path("output"), "id", Collections.singletonList("field"));
+    LuceneStorageConfiguration lucene2SeqConf = new
+    LuceneStorageConfiguration(getConfiguration(), Collections.singletonList(indexPath1), new Path("output"), "id", Collections.singletonList("field"));
     conf = lucene2SeqConf.serialize();
 
-    jobContext = new JobContext(conf, new JobID());
+    jobContext = getJobContext(conf, new JobID());
   }
 
   @After
@@ -65,4 +67,19 @@
     List<LuceneSegmentInputSplit> splits = inputFormat.getSplits(jobContext);
     Assert.assertEquals(3, splits.size());
   }
+
+  // Use reflection to abstract this incompatibility between Hadoop 1 & 2 APIs.
+  private JobContext getJobContext(Configuration conf, JobID jobID) throws
+      ClassNotFoundException, NoSuchMethodException, IllegalAccessException,
+      InvocationTargetException, InstantiationException {
+    Class<? extends JobContext> clazz;
+    if (!JobContext.class.isInterface()) {
+      clazz = JobContext.class;
+    } else {
+      clazz = (Class<? extends JobContext>)
+          Class.forName("org.apache.hadoop.mapreduce.task.JobContextImpl");
+    }
+    return clazz.getConstructor(Configuration.class, JobID.class)
+        .newInstance(conf, jobID);
+  }
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputSplitTest.java mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputSplitTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputSplitTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentInputSplitTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,7 @@
 package org.apache.mahout.text;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.lucene.index.SegmentInfoPerCommit;
+import org.apache.lucene.index.SegmentCommitInfo;
 import org.apache.lucene.index.SegmentReader;
 import org.apache.lucene.store.IOContext;
 import org.apache.mahout.common.HadoopUtil;
@@ -37,7 +37,7 @@
 
   @Before
   public void before() throws IOException {
-    configuration = new Configuration();
+    configuration = getConfiguration();
   }
 
   @After
@@ -78,7 +78,7 @@
 
   private void assertSegmentContainsOneDoc(String segmentName) throws IOException {
     LuceneSegmentInputSplit inputSplit = new LuceneSegmentInputSplit(indexPath1, segmentName, 1000);
-    SegmentInfoPerCommit segment = inputSplit.getSegment(configuration);
+    SegmentCommitInfo segment = inputSplit.getSegment(configuration);
     SegmentReader segmentReader = new SegmentReader(segment, 1, IOContext.READ);//SegmentReader.get(true, segment, 1);
     assertEquals(segmentName, segment.info.name);
     assertEquals(1, segmentReader.numDocs());
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/LuceneSegmentRecordReaderTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -29,20 +29,27 @@
 import org.junit.Test;
 
 import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
 
 import static java.util.Arrays.asList;
+import static org.apache.mahout.text.doc.SingleFieldDocument.*;
 
 public class LuceneSegmentRecordReaderTest extends AbstractLuceneStorageTest {
   private Configuration configuration;
 
+  private LuceneSegmentRecordReader recordReader;
+
+  private SegmentInfos segmentInfos;
 
   @Before
   public void before() throws IOException, InterruptedException {
-    LuceneStorageConfiguration lucene2SeqConf = new LuceneStorageConfiguration(new Configuration(), asList(getIndexPath1()), new Path("output"), "id", asList("field"));
+    LuceneStorageConfiguration lucene2SeqConf = new LuceneStorageConfiguration(getConfiguration(), asList(getIndexPath1()), new Path("output"), ID_FIELD, asList(FIELD));
     configuration = lucene2SeqConf.serialize();
+    recordReader = new LuceneSegmentRecordReader();
     commitDocuments(getDirectory(getIndexPath1AsFile()), docs.subList(0, 500));
     commitDocuments(getDirectory(getIndexPath1AsFile()), docs.subList(500, 1000));
-
+    segmentInfos = new SegmentInfos();
+    segmentInfos.read(getDirectory(getIndexPath1AsFile()));
   }
 
   @After
@@ -52,13 +59,10 @@
 
   @Test
   public void testKey() throws Exception {
-    LuceneSegmentRecordReader recordReader = new LuceneSegmentRecordReader();
-    SegmentInfos segmentInfos = new SegmentInfos();
-    segmentInfos.read(getDirectory(getIndexPath1AsFile()));
-    for (SegmentInfoPerCommit segmentInfo : segmentInfos) {
+    for (SegmentCommitInfo segmentInfo : segmentInfos) {
       int docId = 0;
       LuceneSegmentInputSplit inputSplit = new LuceneSegmentInputSplit(getIndexPath1(), segmentInfo.info.name, segmentInfo.sizeInBytes());
-      TaskAttemptContext context = new TaskAttemptContext(configuration, new TaskAttemptID());
+      TaskAttemptContext context = getTaskAttemptContext(configuration, new TaskAttemptID());
       recordReader.initialize(inputSplit, context);
       for (int i = 0; i < 500; i++){
         recordReader.nextKeyValue();
@@ -69,4 +73,37 @@
       }
     }
   }
+
+  @Test(expected = IllegalArgumentException.class)
+  public void testNonExistingIdField() throws Exception {
+    configuration = new LuceneStorageConfiguration(getConfiguration(), asList(getIndexPath1()), new Path("output"), "nonExistingId", asList(FIELD)).serialize();
+    SegmentCommitInfo segmentInfo = segmentInfos.iterator().next();
+    LuceneSegmentInputSplit inputSplit = new LuceneSegmentInputSplit(getIndexPath1(), segmentInfo.info.name, segmentInfo.sizeInBytes());
+    TaskAttemptContext context = getTaskAttemptContext(configuration, new TaskAttemptID());
+    recordReader.initialize(inputSplit, context);
+  }
+
+  @Test(expected = IllegalArgumentException.class)
+  public void testNonExistingField() throws Exception {
+    configuration = new LuceneStorageConfiguration(getConfiguration(), asList(getIndexPath1()), new Path("output"), ID_FIELD, asList("nonExistingField")).serialize();
+    SegmentCommitInfo segmentInfo = segmentInfos.iterator().next();
+    LuceneSegmentInputSplit inputSplit = new LuceneSegmentInputSplit(getIndexPath1(), segmentInfo.info.name, segmentInfo.sizeInBytes());
+    TaskAttemptContext context = getTaskAttemptContext(configuration, new TaskAttemptID());
+    recordReader.initialize(inputSplit, context);
+  }
+
+  // Use reflection to abstract this incompatibility between Hadoop 1 & 2 APIs.
+  private TaskAttemptContext getTaskAttemptContext(Configuration conf, TaskAttemptID jobID) throws
+      ClassNotFoundException, NoSuchMethodException, IllegalAccessException,
+      InvocationTargetException, InstantiationException {
+    Class<? extends TaskAttemptContext> clazz;
+    if (!TaskAttemptContext.class.isInterface()) {
+      clazz = TaskAttemptContext.class;
+    } else {
+      clazz = (Class<? extends TaskAttemptContext>)
+          Class.forName("org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl");
+    }
+    return clazz.getConstructor(Configuration.class, TaskAttemptID.class)
+        .newInstance(conf, jobID);
+  }
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/LuceneStorageConfigurationTest.java mahout/integration/src/test/java/org/apache/mahout/text/LuceneStorageConfigurationTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/LuceneStorageConfigurationTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/LuceneStorageConfigurationTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,6 +18,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -25,11 +26,11 @@
 import static java.util.Arrays.asList;
 import static org.junit.Assert.assertEquals;
 
-public class LuceneStorageConfigurationTest {
+public class LuceneStorageConfigurationTest extends MahoutTestCase {
   
   @Test
   public void testSerialization() throws Exception {
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path indexPath = new Path("indexPath");
     Path outputPath = new Path("outputPath");
     LuceneStorageConfiguration luceneStorageConfiguration =
@@ -44,6 +45,6 @@
   
   @Test(expected = IllegalArgumentException.class)
   public void testSerializationNotSerialized() throws IOException {
-    new LuceneStorageConfiguration(new Configuration());
+    new LuceneStorageConfiguration(getConfiguration());
   }
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/MailArchivesClusteringAnalyzerTest.java mahout/integration/src/test/java/org/apache/mahout/text/MailArchivesClusteringAnalyzerTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/MailArchivesClusteringAnalyzerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/MailArchivesClusteringAnalyzerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -22,9 +22,8 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.mahout.utils.MahoutTestCase;
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 /**
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriverTest.java mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriverTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriverTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageDriverTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,7 @@
 
 package org.apache.mahout.text;
 
+import com.google.common.collect.Iterators;
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -27,6 +28,7 @@
 import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.PathType;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator;
+import org.apache.mahout.text.doc.MultipleFieldsDocument;
 import org.apache.mahout.text.doc.SingleFieldDocument;
 import org.junit.After;
 import org.junit.Before;
@@ -48,12 +50,12 @@
 
   @Before
   public void before() throws Exception {
-    conf = new Configuration();
+    conf = getConfiguration();
     conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,"
       + "org.apache.hadoop.io.serializer.WritableSerialization");
 
     seqFilesOutputPath = new Path(getTestTempDirPath(), "seqfiles");
-    idField = "id";
+    idField = SingleFieldDocument.ID_FIELD;
     fields = asList("field");
 
     driver = new SequenceFilesFromLuceneStorageDriver() {
@@ -90,6 +92,9 @@
 
   @Test
   public void testRun() throws Exception {
+    List<MultipleFieldsDocument> docs = asList(new MultipleFieldsDocument("123", "test 1", "test 2", "test 3"));
+    commitDocuments(getDirectory(getIndexPath1AsFile()), docs.get(0));
+
     String queryField = "queryfield";
     String queryTerm = "queryterm";
     String maxHits = "500";
@@ -150,15 +155,17 @@
       "-o", seqFilesOutputPath.toString(),
       "-id", idField,
       "-f", StringUtils.join(fields, SequenceFilesFromLuceneStorageDriver.SEPARATOR_FIELDS),
-      "-q", "invalid:query"
+      "-q", "invalid:query",
+      "-xm", "sequential"
     };
 
     driver.setConf(conf);
     driver.run(args);
     assertTrue(FileSystem.get(conf).exists(seqFilesOutputPath));
     //shouldn't be any real files in the seq files out path
-    SequenceFileDirIterator<Writable, Writable> iter = new SequenceFileDirIterator<Writable, Writable>(seqFilesOutputPath, PathType.LIST, PathFilters.logsCRCFilter(), null, false, conf);
-    assertFalse(iter.hasNext());
+    SequenceFileDirIterator<Writable, Writable> iter =
+        new SequenceFileDirIterator<Writable, Writable>(seqFilesOutputPath, PathType.LIST, PathFilters.logsCRCFilter(), null, false, conf);
+    assertFalse(Iterators.size(iter) > 0);
 
   }
 
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMRJobTest.java mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMRJobTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMRJobTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageMRJobTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -41,7 +41,7 @@
   @Before
   public void before() throws IOException {
     lucene2seq = new SequenceFilesFromLuceneStorageMRJob();
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     Path seqOutputPath = new Path(getTestTempDirPath(), "seqOutputPath");//don't make the output directory
     lucene2SeqConf = new LuceneStorageConfiguration(configuration, asList(getIndexPath1(), getIndexPath2()),
             seqOutputPath, SingleFieldDocument.ID_FIELD, asList(SingleFieldDocument.FIELD));
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageTest.java mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromLuceneStorageTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -16,6 +16,7 @@
  */
 package org.apache.mahout.text;
 
+import com.google.common.collect.Maps;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Text;
@@ -33,7 +34,6 @@
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
@@ -46,10 +46,9 @@
   private Path seqFilesOutputPath;
   private Configuration configuration;
 
-  @SuppressWarnings("unchecked")
   @Before
   public void before() throws IOException {
-    configuration = new Configuration();
+    configuration = getConfiguration();
     seqFilesOutputPath = new Path(getTestTempDirPath(), "seqfiles");
 
     lucene2Seq = new SequenceFilesFromLuceneStorage();
@@ -80,7 +79,7 @@
     lucene2Seq.run(lucene2SeqConf);
 
     Iterator<Pair<Text, Text>> iterator = lucene2SeqConf.getSequenceFileIterator();
-    Map<String, Text> map = new HashMap<String, Text>();
+    Map<String, Text> map = Maps.newHashMap();
     while (iterator.hasNext()) {
       Pair<Text, Text> next = iterator.next();
       map.put(next.getFirst().toString(), next.getSecond());
@@ -98,7 +97,6 @@
     }
   }
 
-  @SuppressWarnings("unchecked")
   @Test
   public void testRunSkipUnstoredFields() throws IOException {
     commitDocuments(getDirectory(getIndexPath1AsFile()), new UnstoredFieldsDocument("5", "This is test document 5"));
@@ -117,7 +115,6 @@
     assertFalse(iterator.hasNext());
   }
 
-  @SuppressWarnings("unchecked")
   @Test
   public void testRunMaxHits() throws IOException {
     commitDocuments(getDirectory(getIndexPath1AsFile()), docs.subList(0, 500));
@@ -139,7 +136,6 @@
     assertFalse(iterator.hasNext());
   }
 
-  @SuppressWarnings("unchecked")
   @Test
   public void testRunQuery() throws IOException {
     commitDocuments(getDirectory(getIndexPath1AsFile()), docs);
@@ -147,7 +143,7 @@
       asList(getIndexPath1()),
       seqFilesOutputPath,
       SingleFieldDocument.ID_FIELD,
-      asList(UnstoredFieldsDocument.FIELD, UnstoredFieldsDocument.UNSTORED_FIELD));
+      asList(SingleFieldDocument.FIELD));
 
     Query query = new TermQuery(new Term(lucene2SeqConf.getFields().get(0), "599"));
 
@@ -205,4 +201,30 @@
     assertNumericFieldEquals(doc2, iterator.next());
     assertNumericFieldEquals(doc3, iterator.next());
   }
+
+  @Test(expected = IllegalArgumentException.class)
+  public void testNonExistingIdField() throws IOException {
+    commitDocuments(getDirectory(getIndexPath1AsFile()), docs.subList(0, 500));
+
+    lucene2SeqConf = new LuceneStorageConfiguration(configuration,
+        asList(getIndexPath1()),
+        seqFilesOutputPath,
+        "nonExistingField",
+        asList(SingleFieldDocument.FIELD));
+
+    lucene2Seq.run(lucene2SeqConf);
+  }
+
+  @Test(expected = IllegalArgumentException.class)
+  public void testNonExistingField() throws IOException {
+    commitDocuments(getDirectory(getIndexPath1AsFile()), docs.subList(0, 500));
+
+    lucene2SeqConf = new LuceneStorageConfiguration(configuration,
+        asList(getIndexPath1()),
+        seqFilesOutputPath,
+        SingleFieldDocument.ID_FIELD,
+        asList(SingleFieldDocument.FIELD, "nonExistingField"));
+
+    lucene2Seq.run(lucene2SeqConf);
+  }
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java
--- mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/SequenceFilesFromMailArchivesTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,14 +21,15 @@
 import java.util.zip.GZIPOutputStream;
 
 import com.google.common.io.Closeables;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Text;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterator;
-import org.apache.mahout.utils.MahoutTestCase;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
@@ -97,7 +98,7 @@
     String expectedChunkPath = expectedChunkFile.getAbsolutePath();
     Assert.assertTrue("Expected chunk file " + expectedChunkPath + " not found!", expectedChunkFile.isFile());
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<Text, Text>(new Path(expectedChunkPath), true, conf);
     Assert.assertTrue("First key/value pair not found!", iterator.hasNext());
     Pair<Text, Text> record = iterator.next();
@@ -128,14 +129,15 @@
   @Test
   public void testMapReduce() throws Exception {
 
-    Path tmpDir = this.getTestTempDirPath();
+    Path tmpDir = getTestTempDirPath();
     Path mrOutputDir = new Path(tmpDir, "mail-archives-out-mr");
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     FileSystem fs = FileSystem.get(configuration);
 
     File expectedInputFile = new File(inputDir.toString());
 
     String[] args = {
+      "-Dhadoop.tmp.dir=" + configuration.get("hadoop.tmp.dir"),
       "--input", expectedInputFile.getAbsolutePath(),
       "--output", mrOutputDir.toString(),
       "--charset", "UTF-8",
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/TestPathFilter.java mahout/integration/src/test/java/org/apache/mahout/text/TestPathFilter.java
--- mahout/integration/src/test/java/org/apache/mahout/text/TestPathFilter.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/integration/src/test/java/org/apache/mahout/text/TestPathFilter.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,32 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.text;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+
+/**
+ * Dummy Path Filter for testing the MapReduce version of
+ * SequenceFilesFromDirectory
+ */
+public class TestPathFilter implements PathFilter {
+
+  @Override
+  public boolean accept(Path path) {
+    return path.getName().startsWith("t") || path.getName().startsWith("r") || path.getName().startsWith("f");
+  }
+}
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java mahout/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java
--- mahout/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/text/TestSequenceFilesFromDirectory.java	2014-03-29 01:03:14.000000000 -0700
@@ -25,16 +25,17 @@
 import com.google.common.base.Charsets;
 import com.google.common.collect.Maps;
 import com.google.common.io.Closeables;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.io.Text;
 import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
+import org.apache.mahout.common.iterator.sequencefile.PathFilters;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterator;
-import org.apache.mahout.utils.MahoutTestCase;
 import org.junit.Test;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -58,7 +59,7 @@
   @Test
   public void testSequenceFileFromDirectoryBasic() throws Exception {
     // parameters
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
 
     FileSystem fs = FileSystem.get(configuration);
 
@@ -107,7 +108,7 @@
   @Test
   public void testSequenceFileFromDirectoryMapReduce() throws Exception {
 
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
 
     FileSystem fs = FileSystem.get(conf);
 
@@ -125,12 +126,15 @@
     createFilesFromArrays(conf, inputDir, DATA1);
 
     SequenceFilesFromDirectory.main(new String[]{
+      "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
       "--input", inputDir.toString(),
       "--output", mrOutputDir.toString(),
       "--chunkSize", "64",
       "--charset", Charsets.UTF_8.name(),
       "--method", "mapreduce",
-      "--keyPrefix", "UID"});
+      "--keyPrefix", "UID",
+      "--fileFilterClass", "org.apache.mahout.text.TestPathFilter"
+    });
 
     checkMRResultFiles(conf, mrOutputDir, DATA1, "UID");
 
@@ -142,12 +146,15 @@
     logger.info("\n\n ---- recursive dirs: {}", dirs);
 
     SequenceFilesFromDirectory.main(new String[]{
+      "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"),
       "--input", inputDirRecur.toString(),
       "--output", mrOutputDirRecur.toString(),
       "--chunkSize", "64",
       "--charset", Charsets.UTF_8.name(),
       "--method", "mapreduce",
-      "--keyPrefix", "UID"});
+      "--keyPrefix", "UID",
+      "--fileFilterClass", "org.apache.mahout.text.TestPathFilter"
+    });
 
     checkMRResultFilesRecursive(conf, mrOutputDirRecur, DATA2, "UID");
   }
@@ -201,7 +208,7 @@
     FileSystem fs = FileSystem.get(configuration);
 
     // output exists?
-    FileStatus[] fileStatuses = fs.listStatus(outputDir, new ExcludeDotFiles());
+    FileStatus[] fileStatuses = fs.listStatus(outputDir, PathFilters.logsCRCFilter());
     assertEquals(1, fileStatuses.length); // only one
     assertEquals("chunk-0", fileStatuses[0].getPath().getName());
 
@@ -225,16 +232,6 @@
     }
   }
 
-  /**
-   * exclude hidden (starting with dot) files
-   */
-  private static class ExcludeDotFiles implements PathFilter {
-    @Override
-    public boolean accept(Path file) {
-      return !file.getName().startsWith(".") && !file.getName().startsWith("_");
-    }
-  }
-
   private static void checkRecursiveChunkFiles(Configuration configuration,
                                                Path outputDir,
                                                String[][] data,
@@ -244,7 +241,7 @@
     System.out.println(" ----------- check_Recursive_ChunkFiles ------------");
 
     // output exists?
-    FileStatus[] fileStatuses = fs.listStatus(outputDir, new ExcludeDotFiles());
+    FileStatus[] fileStatuses = fs.listStatus(outputDir, PathFilters.logsCRCFilter());
     assertEquals(1, fileStatuses.length); // only one
     assertEquals("chunk-0", fileStatuses[0].getPath().getName());
 
@@ -278,7 +275,7 @@
     FileSystem fs = FileSystem.get(conf);
 
     // output exists?
-    FileStatus[] fileStatuses = fs.listStatus(outputDir.suffix("/part-m-00000"), new ExcludeDotFiles());
+    FileStatus[] fileStatuses = fs.listStatus(outputDir.suffix("/part-m-00000"), PathFilters.logsCRCFilter());
     assertEquals(1, fileStatuses.length); // only one
     assertEquals("part-m-00000", fileStatuses[0].getPath().getName());
     Map<String, String> fileToData = Maps.newHashMap();
@@ -309,7 +306,7 @@
     FileSystem fs = FileSystem.get(configuration);
 
     // output exists?
-    FileStatus[] fileStatuses = fs.listStatus(outputDir.suffix("/part-m-00000"), new ExcludeDotFiles());
+    FileStatus[] fileStatuses = fs.listStatus(outputDir.suffix("/part-m-00000"), PathFilters.logsCRCFilter());
     assertEquals(1, fileStatuses.length); // only one
     assertEquals("part-m-00000", fileStatuses[0].getPath().getName());
     Map<String, String> fileToData = Maps.newHashMap();
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/Bump125Test.java mahout/integration/src/test/java/org/apache/mahout/utils/Bump125Test.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/Bump125Test.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/Bump125Test.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,6 +18,8 @@
 package org.apache.mahout.utils;
 
 import com.google.common.collect.Lists;
+
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 import java.util.Iterator;
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/MahoutTestCase.java mahout/integration/src/test/java/org/apache/mahout/utils/MahoutTestCase.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/MahoutTestCase.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/MahoutTestCase.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,30 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.mahout.utils;
-
-/**
- * This class should not exist. It's here to work around some bizarre problem in Maven
- * dependency management wherein it can see methods in {@link org.apache.mahout.common.MahoutTestCase}
- * but not constants. Duplicated here to make it jive.
- */
-public abstract class MahoutTestCase extends org.apache.mahout.common.MahoutTestCase {
-
-  /** "Close enough" value for floating-point comparisons. */
-  public static final double EPSILON = 0.000001;
-
-}
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java mahout/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/SplitInputTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -25,6 +25,7 @@
 
 import com.google.common.base.Charsets;
 import com.google.common.io.Closeables;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -35,6 +36,7 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.mahout.classifier.ClassifierData;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.common.Pair;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
 import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable;
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/TestConcatenateVectorsJob.java mahout/integration/src/test/java/org/apache/mahout/utils/TestConcatenateVectorsJob.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/TestConcatenateVectorsJob.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/TestConcatenateVectorsJob.java	2014-03-29 01:03:14.000000000 -0700
@@ -50,7 +50,7 @@
   @Test
   public void testConcatenateVectorsReducer() throws Exception {
     
-    Configuration configuration = new Configuration();
+    Configuration configuration = getConfiguration();
     configuration.set(ConcatenateVectorsJob.MATRIXA_DIMS, "5");
     configuration.set(ConcatenateVectorsJob.MATRIXB_DIMS, "3");
     
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/nlp/collocations/llr/BloomTokenFilterTest.java mahout/integration/src/test/java/org/apache/mahout/utils/nlp/collocations/llr/BloomTokenFilterTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/nlp/collocations/llr/BloomTokenFilterTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/nlp/collocations/llr/BloomTokenFilterTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -27,6 +27,7 @@
 import java.nio.charset.CharsetEncoder;
 
 import com.google.common.base.Charsets;
+
 import org.apache.hadoop.util.bloom.BloomFilter;
 import org.apache.hadoop.util.bloom.Filter;
 import org.apache.hadoop.util.bloom.Key;
@@ -37,7 +38,7 @@
 import org.apache.lucene.analysis.shingle.ShingleFilter;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.util.Version;
-import org.apache.mahout.utils.MahoutTestCase;
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 public final class BloomTokenFilterTest extends MahoutTestCase {
@@ -79,7 +80,7 @@
   @Test
   public void testAnalyzer() throws IOException {
     Reader reader = new StringReader(input);
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
     TokenStream ts = analyzer.tokenStream(null, reader);
     ts.reset();
     validateTokens(allTokens, ts);
@@ -91,7 +92,7 @@
   @Test
   public void testNonKeepdAnalyzer() throws IOException {
     Reader reader = new StringReader(input);
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
     TokenStream ts = analyzer.tokenStream(null, reader);
     ts.reset();
     TokenStream f = new BloomTokenFilter(getFilter(filterTokens), false /* toss matching tokens */, ts);
@@ -104,7 +105,7 @@
   @Test
   public void testKeepAnalyzer() throws IOException {
     Reader reader = new StringReader(input);
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
     TokenStream ts = analyzer.tokenStream(null, reader);
     ts.reset();
     TokenStream f = new BloomTokenFilter(getFilter(filterTokens), true /* keep matching tokens */, ts);
@@ -117,7 +118,7 @@
   @Test
   public void testShingleFilteredAnalyzer() throws IOException {
     Reader reader = new StringReader(input);
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_43);
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_46);
     TokenStream ts = analyzer.tokenStream(null, reader);
     ts.reset();
     ShingleFilter sf = new ShingleFilter(ts, 3);
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/regex/RegexMapperTest.java mahout/integration/src/test/java/org/apache/mahout/utils/regex/RegexMapperTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/regex/RegexMapperTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/regex/RegexMapperTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -32,7 +32,7 @@
   @Test
   public void testRegex() throws Exception {
     RegexMapper mapper = new RegexMapper();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(RegexMapper.REGEX, "(?<=(\\?|&)q=).*?(?=&|$)");
     conf.set(RegexMapper.TRANSFORMER_CLASS, URLDecodeTransformer.class.getName());
     DummyRecordWriter<LongWritable, Text> mapWriter = new DummyRecordWriter<LongWritable, Text>();
@@ -56,7 +56,7 @@
   @Test
   public void testGroups() throws Exception {
     RegexMapper mapper = new RegexMapper();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(RegexMapper.REGEX, "(\\d+)\\.(\\d+)\\.(\\d+)");
     conf.set(RegexMapper.TRANSFORMER_CLASS, URLDecodeTransformer.class.getName());
     conf.setStrings(RegexMapper.GROUP_MATCHERS, "1", "3");
@@ -79,7 +79,7 @@
   @Test
   public void testFPGFormatter() throws Exception {
     RegexMapper mapper = new RegexMapper();
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     conf.set(RegexMapper.REGEX, "(?<=(\\?|&)q=).*?(?=&|$)");
     conf.set(RegexMapper.TRANSFORMER_CLASS, URLDecodeTransformer.class.getName());
     conf.set(RegexMapper.FORMATTER_CLASS, FPGFormatter.class.getName());
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/VectorHelperTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/VectorHelperTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/VectorHelperTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/VectorHelperTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,13 +18,65 @@
 package org.apache.mahout.utils.vectors;
 
 import com.google.common.collect.Iterables;
+
+import com.google.common.io.Closeables;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.mahout.common.MahoutTestCase;
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.math.SequentialAccessSparseVector;
 import org.apache.mahout.math.Vector;
-import org.apache.mahout.utils.MahoutTestCase;
+import org.junit.Before;
 import org.junit.Test;
 
+import java.util.Random;
+
 public final class VectorHelperTest extends MahoutTestCase {
 
+  private static final int NUM_DOCS = 100;
+
+  private Path inputPathOne;
+  private Path inputPathTwo;
+
+  private Configuration conf;
+
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    conf = getConfiguration();
+
+    inputPathOne = getTestTempFilePath("documents/docs-one.file");
+    FileSystem fs = FileSystem.get(inputPathOne.toUri(), conf);
+    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPathOne, Text.class, IntWritable.class);
+    try {
+      Random rd = RandomUtils.getRandom();
+      for (int i = 0; i < NUM_DOCS; i++) {
+        // Make all indices higher than dictionary size
+        writer.append(new Text("Document::ID::" + i), new IntWritable(NUM_DOCS + rd.nextInt(NUM_DOCS)));
+      }
+    } finally {
+      Closeables.close(writer, false);
+    }
+
+    inputPathTwo = getTestTempFilePath("documents/docs-two.file");
+    fs = FileSystem.get(inputPathTwo.toUri(), conf);
+    writer = new SequenceFile.Writer(fs, conf, inputPathTwo, Text.class, IntWritable.class);
+    try {
+      Random rd = RandomUtils.getRandom();
+      for (int i = 0; i < NUM_DOCS; i++) {
+        // Keep indices within number of documents
+        writer.append(new Text("Document::ID::" + i), new IntWritable(rd.nextInt(NUM_DOCS)));
+      }
+    } finally {
+      Closeables.close(writer, false);
+    }
+  }
+
   @Test
   public void testJsonFormatting() throws Exception {
     Vector v = new SequentialAccessSparseVector(10);
@@ -84,4 +136,12 @@
     v.set(8, 0.0);
     assertEquals(0, VectorHelper.topEntries(v, 6).size());
   }
+
+  @Test
+  public void testLoadTermDictionary() throws Exception {
+    // With indices higher than dictionary size
+    VectorHelper.loadTermDictionary(conf, inputPathOne.toString());
+    // With dictionary size higher than indices
+    VectorHelper.loadTermDictionary(conf, inputPathTwo.toString());
+  }
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFTypeTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFTypeTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFTypeTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFTypeTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -16,7 +16,7 @@
 
 package org.apache.mahout.utils.vectors.arff;
 
-import org.apache.mahout.utils.MahoutTestCase;
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 public final class ARFFTypeTest extends MahoutTestCase {
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterableTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterableTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterableTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/ARFFVectorIterableTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -27,10 +27,11 @@
 
 import com.google.common.base.Charsets;
 import com.google.common.io.Resources;
+
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.RandomAccessSparseVector;
 import org.apache.mahout.math.Vector;
-import org.apache.mahout.utils.MahoutTestCase;
 import org.junit.Test;
 
 public final class ARFFVectorIterableTest extends MahoutTestCase {
@@ -123,7 +124,7 @@
     assertEquals(1, nominalMap.size());
     Map<String, Integer> noms = nominalMap.get("bar");
     assertNotNull("nominals for bar are null", noms);
-    assertEquals(2, noms.size());
+    assertEquals(5, noms.size());
     Map<Integer, ARFFType> integerARFFTypeMap = model.getTypeMap();
     assertNotNull("Type map null", integerARFFTypeMap);
     assertEquals(5, integerARFFTypeMap.size());
@@ -188,7 +189,7 @@
     assertEquals(1, nominalMap.size());
     Map<String,Integer> noms = nominalMap.get("bar");
     assertNotNull("nominals for bar are null", noms);
-    assertEquals(2, noms.size());
+    assertEquals(5, noms.size());
     Map<Integer,ARFFType> integerARFFTypeMap = model.getTypeMap();
     assertNotNull("Type map null", integerARFFTypeMap);
     assertEquals(5, integerARFFTypeMap.size());
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/DriverTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/DriverTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/DriverTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/DriverTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,7 +20,8 @@
 
 import com.google.common.base.Charsets;
 import com.google.common.io.Resources;
-import org.apache.mahout.utils.MahoutTestCase;
+
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 /**
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModelTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModelTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModelTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/arff/MapBackedARFFModelTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -16,7 +16,7 @@
 
 package org.apache.mahout.utils.vectors.arff;
 
-import org.apache.mahout.utils.MahoutTestCase;
+import org.apache.mahout.common.MahoutTestCase;
 import org.junit.Test;
 
 import java.util.Map;
@@ -39,4 +39,23 @@
     Map<String, Integer> windyValues = nominalMap.get(windy);
     assertEquals(77, windyValues.get(breezy).intValue());
   }
+
+  @Test
+  public void processBadNumeric() {
+    ARFFModel model = new MapBackedARFFModel();
+    model.addLabel("b1shkt70694difsmmmdv0ikmoh", 77);
+    model.addType(77, ARFFType.REAL);
+    assertTrue(Double.isNaN(model.getValue("b1shkt70694difsmmmdv0ikmoh", 77)));
+  }
+
+  @Test
+  public void processGoodNumeric() {
+    ARFFModel model = new MapBackedARFFModel();
+    model.addLabel("1234", 77);
+    model.addType(77, ARFFType.INTEGER);
+    assertTrue(1234 == model.getValue("1234", 77));
+    model.addLabel("131.34", 78);
+    model.addType(78, ARFFType.REAL);
+    assertTrue(131.34 == model.getValue("131.34", 78));
+  }
 }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/csv/CSVVectorIteratorTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/csv/CSVVectorIteratorTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/csv/CSVVectorIteratorTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/csv/CSVVectorIteratorTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -23,8 +23,9 @@
 import java.util.Iterator;
 
 import com.google.common.io.Closeables;
+
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.math.Vector;
-import org.apache.mahout.utils.MahoutTestCase;
 import org.apache.mahout.utils.vectors.RandomVectorIterable;
 import org.apache.mahout.utils.vectors.VectorHelper;
 import org.apache.mahout.utils.vectors.io.TextualVectorWriter;
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/io/VectorWriterTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/io/VectorWriterTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/io/VectorWriterTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/io/VectorWriterTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -22,16 +22,17 @@
 
 import com.google.common.collect.Lists;
 import com.google.common.io.Closeables;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.mahout.common.HadoopUtil;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorWritable;
-import org.apache.mahout.utils.MahoutTestCase;
 import org.apache.mahout.utils.vectors.RandomVectorIterable;
 import org.junit.Test;
 
@@ -40,7 +41,7 @@
   @Test
   public void testSFVW() throws Exception {
     Path path = getTestTempFilePath("sfvw");
-    Configuration conf = new Configuration();
+    Configuration conf = getConfiguration();
     FileSystem fs = FileSystem.get(conf);
     SequenceFile.Writer seqWriter = new SequenceFile.Writer(fs, conf, path, LongWritable.class, VectorWritable.class);
     SequenceFileVectorWriter writer = new SequenceFileVectorWriter(seqWriter);
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfoTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfoTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfoTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/CachedTermInfoTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,12 +1,31 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.utils.vectors.lucene;
 
 
 import java.io.IOException;
 
 import com.google.common.io.Closeables;
+
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
@@ -14,13 +33,10 @@
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Version;
-import org.apache.mahout.utils.MahoutTestCase;
+import org.apache.mahout.common.MahoutTestCase;
+import org.junit.Before;
 import org.junit.Test;
 
-/**
- *
- *
- **/
 public class CachedTermInfoTest extends MahoutTestCase {
   private RAMDirectory directory;
   private static final String[] DOCS = {
@@ -43,11 +59,20 @@
           "e"
   };
 
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
+  @Before
+  public void before() throws IOException {
     directory = new RAMDirectory();
-    directory = createTestIndex(Field.TermVector.NO, directory, true, 0);
+
+    FieldType fieldType = new FieldType();
+    fieldType.setStored(false);
+    fieldType.setIndexed(true);
+    fieldType.setTokenized(true);
+    fieldType.setStoreTermVectors(false);
+    fieldType.setStoreTermVectorPositions(false);
+    fieldType.setStoreTermVectorOffsets(false);
+    fieldType.freeze();
+
+    directory = createTestIndex(fieldType, directory, 0);
   }
 
   @Test
@@ -72,22 +97,19 @@
 
   }
 
-  static RAMDirectory createTestIndex(Field.TermVector termVector,
+  static RAMDirectory createTestIndex(FieldType fieldType,
                                       RAMDirectory directory,
-                                      boolean createNew,
                                       int startingId) throws IOException {
-    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_43, new WhitespaceAnalyzer(Version.LUCENE_43)));
+    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_46, new WhitespaceAnalyzer(Version.LUCENE_46)));
 
     try {
       for (int i = 0; i < DOCS.length; i++) {
         Document doc = new Document();
         Field id = new StringField("id", "doc_" + (i + startingId), Field.Store.YES);
         doc.add(id);
-        //Store both position and offset information
-        //Says it is deprecated, but doesn't seem to offer an alternative that supports term vectors...
-        Field text = new Field("content", DOCS[i], Field.Store.NO, Field.Index.ANALYZED, termVector);
+        Field text = new Field("content", DOCS[i], fieldType);
         doc.add(text);
-        Field text2 = new Field("content2", DOCS2[i], Field.Store.NO, Field.Index.ANALYZED, termVector);
+        Field text2 = new Field("content2", DOCS2[i], fieldType);
         doc.add(text2);
         writer.addDocument(doc);
       }
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/DriverTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/DriverTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/DriverTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/DriverTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -59,7 +59,7 @@
     outputDir = getTestTempDir("output");
     outputDir.delete();
 
-    conf = new Configuration();
+    conf = getConfiguration();
   }
 
   private Document asDocument(String line) {
@@ -91,8 +91,8 @@
   public void sequenceFileDictionary() throws IOException {
 
     Directory index = new SimpleFSDirectory(indexDir);
-    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_43);
-    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_43, analyzer);
+    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);
+    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_46, analyzer);
     final IndexWriter writer = new IndexWriter(index, config);
 
     try {
diff -uNar -x .git mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/LuceneIterableTest.java mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/LuceneIterableTest.java
--- mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/LuceneIterableTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/java/org/apache/mahout/utils/vectors/lucene/LuceneIterableTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -20,10 +20,14 @@
 import java.io.IOException;
 import java.util.Iterator;
 
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Iterators;
 import com.google.common.io.Closeables;
+
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
@@ -31,12 +35,13 @@
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Version;
+import org.apache.mahout.common.MahoutTestCase;
 import org.apache.mahout.math.NamedVector;
 import org.apache.mahout.math.Vector;
-import org.apache.mahout.utils.MahoutTestCase;
 import org.apache.mahout.utils.vectors.TermInfo;
 import org.apache.mahout.vectorizer.TFIDF;
 import org.apache.mahout.vectorizer.Weight;
+import org.junit.Before;
 import org.junit.Test;
 
 public final class LuceneIterableTest extends MahoutTestCase {
@@ -51,10 +56,29 @@
 
   private RAMDirectory directory;
 
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    directory = createTestIndex(Field.TermVector.YES);
+  private final FieldType TYPE_NO_TERM_VECTORS = new FieldType();
+
+  private final FieldType TYPE_TERM_VECTORS = new FieldType();
+
+  @Before
+  public void before() throws IOException {
+
+    TYPE_NO_TERM_VECTORS.setIndexed(true);
+    TYPE_NO_TERM_VECTORS.setTokenized(true);
+    TYPE_NO_TERM_VECTORS.setStoreTermVectors(false);
+    TYPE_NO_TERM_VECTORS.setStoreTermVectorPositions(false);
+    TYPE_NO_TERM_VECTORS.setStoreTermVectorOffsets(false);
+    TYPE_NO_TERM_VECTORS.freeze();
+
+    TYPE_TERM_VECTORS.setIndexed(true);
+    TYPE_TERM_VECTORS.setTokenized(true);
+    TYPE_TERM_VECTORS.setStored(true);
+    TYPE_TERM_VECTORS.setStoreTermVectors(true);
+    TYPE_TERM_VECTORS.setStoreTermVectorPositions(true);
+    TYPE_TERM_VECTORS.setStoreTermVectorOffsets(true);
+    TYPE_TERM_VECTORS.freeze();
+
+    directory = createTestIndex(TYPE_TERM_VECTORS);
   }
 
   @Test
@@ -86,25 +110,23 @@
 
   @Test(expected = IllegalStateException.class)
   public void testIterableNoTermVectors() throws IOException {
-    RAMDirectory directory = createTestIndex(Field.TermVector.NO);
+    RAMDirectory directory = createTestIndex(TYPE_NO_TERM_VECTORS);
     IndexReader reader = DirectoryReader.open(directory);
-    
-    
+
     Weight weight = new TFIDF();
     TermInfo termInfo = new CachedTermInfo(reader, "content", 1, 100);
     LuceneIterable iterable = new LuceneIterable(reader, "id", "content",  termInfo,weight);
 
     Iterator<Vector> iterator = iterable.iterator();
-    iterator.hasNext();
-    iterator.next();
+    Iterators.advance(iterator, 1);
   }
 
   @Test
   public void testIterableSomeNoiseTermVectors() throws IOException {
     //get noise vectors
-    RAMDirectory directory = createTestIndex(Field.TermVector.YES, new RAMDirectory(), true, 0);
+    RAMDirectory directory = createTestIndex(TYPE_TERM_VECTORS, new RAMDirectory(), 0);
     //get real vectors
-    createTestIndex(Field.TermVector.NO, directory, false, 5);
+    createTestIndex(TYPE_NO_TERM_VECTORS, directory, 5);
     IndexReader reader = DirectoryReader.open(directory);
 
     Weight weight = new TFIDF();
@@ -112,10 +134,9 @@
     
     boolean exceptionThrown;
     //0 percent tolerance
-    LuceneIterable iterable = new LuceneIterable(reader, "id", "content", termInfo,weight);
+    LuceneIterable iterable = new LuceneIterable(reader, "id", "content", termInfo, weight);
     try {
-      for (Object a : iterable) {
-      }
+      Iterables.skip(iterable, Iterables.size(iterable));
       exceptionThrown = false;
     }
     catch(IllegalStateException ise) {
@@ -126,8 +147,7 @@
     //100 percent tolerance
     iterable = new LuceneIterable(reader, "id", "content", termInfo,weight, -1, 1.0);
     try {
-      for (Object a : iterable) {
-      }
+      Iterables.skip(iterable, Iterables.size(iterable));
       exceptionThrown = false;
     }
     catch(IllegalStateException ise) {
@@ -138,44 +158,36 @@
     //50 percent tolerance
     iterable = new LuceneIterable(reader, "id", "content", termInfo,weight, -1, 0.5);
     Iterator<Vector> iterator = iterable.iterator();
-    iterator.next();
-    iterator.next();
-    iterator.next();
-    iterator.next();
-    iterator.next();
+    Iterators.advance(iterator, 5);
 
     try {
-        while (iterator.hasNext()) {
-            iterator.next();
-        }
-        exceptionThrown = false;
+      Iterators.advance(iterator, Iterators.size(iterator));
+      exceptionThrown = false;
     }
     catch(IllegalStateException ise) {
-        exceptionThrown = true;
+      exceptionThrown = true;
     }
     assertTrue(exceptionThrown);
   }
   
-  static RAMDirectory createTestIndex(Field.TermVector termVector) throws IOException {
-      return createTestIndex(termVector, new RAMDirectory(), true, 0);
+  static RAMDirectory createTestIndex(FieldType fieldType) throws IOException {
+      return createTestIndex(fieldType, new RAMDirectory(), 0);
   }
   
-  static RAMDirectory createTestIndex(Field.TermVector termVector,
+  static RAMDirectory createTestIndex(FieldType fieldType,
                                               RAMDirectory directory,
-                                              boolean createNew,
                                               int startingId) throws IOException {
-    IndexWriter writer = new IndexWriter( directory, new IndexWriterConfig(Version.LUCENE_43,new StandardAnalyzer(Version.LUCENE_43)));
-        
+    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(Version.LUCENE_46,new StandardAnalyzer(Version.LUCENE_46)));
+
     try {
       for (int i = 0; i < DOCS.length; i++) {
         Document doc = new Document();
         Field id = new StringField("id", "doc_" + (i + startingId), Field.Store.YES);
         doc.add(id);
         //Store both position and offset information
-        //Says it is deprecated, but doesn't seem to offer an alternative that supports term vectors...
-        Field text = new Field("content", DOCS[i], Field.Store.NO, Field.Index.ANALYZED, termVector);
+        Field text = new Field("content", DOCS[i], fieldType);
         doc.add(text);
-        Field text2 = new Field("content2", DOCS[i], Field.Store.NO, Field.Index.ANALYZED, termVector);
+        Field text2 = new Field("content2", DOCS[i], fieldType);
         doc.add(text2);
         writer.addDocument(doc);
       }
diff -uNar -x .git mahout/integration/src/test/resources/date.arff mahout/integration/src/test/resources/date.arff
--- mahout/integration/src/test/resources/date.arff	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/resources/date.arff	2014-03-29 01:03:14.000000000 -0700
@@ -4,7 +4,7 @@
    @RELATION MahoutDateTest
 
    @ATTRIBUTE junk  NUMERIC
-   @ATTRIBUTE date1
+   @ATTRIBUTE date1   date
    @ATTRIBUTE date2   date "yyyy.MM.dd G 'at' HH:mm:ss z"
    @ATTRIBUTE date3   date "EEE, MMM d, ''yy"
    @ATTRIBUTE date4   date "K:mm a, z"
diff -uNar -x .git mahout/integration/src/test/resources/non-numeric-1.arff mahout/integration/src/test/resources/non-numeric-1.arff
--- mahout/integration/src/test/resources/non-numeric-1.arff	2014-03-29 01:04:48.000000000 -0700
+++ mahout/integration/src/test/resources/non-numeric-1.arff	2014-03-29 01:03:14.000000000 -0700
@@ -5,7 +5,7 @@
 
    @ATTRIBUTE junk  NUMERIC
    @ATTRIBUTE foo  NUMERIC
-   @ATTRIBUTE bar   {c,d}
+   @ATTRIBUTE bar   {c,d,'xy, numeric','marc o\'polo', e}
    @ATTRIBUTE hockey  string
    @ATTRIBUTE football   date "yyyy-MM-dd"
 
diff -uNar -x .git mahout/math/pom.xml mahout/math/pom.xml
--- mahout/math/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -23,7 +23,7 @@
   <parent>
     <groupId>org.apache.mahout</groupId>
     <artifactId>mahout</artifactId>
-    <version>0.8</version>
+    <version>1.0-SNAPSHOT</version>
     <relativePath>../pom.xml</relativePath>
   </parent>
 
@@ -161,20 +161,30 @@
     <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
-      <scope>test</scope>
     </dependency>
 
     <dependency>
-      <groupId>org.easymock</groupId>
-      <artifactId>easymock</artifactId>
+      <groupId>org.apache.lucene</groupId>
+      <artifactId>lucene-test-framework</artifactId>
       <scope>test</scope>
     </dependency>
-    
+
     <dependency>
       <groupId>com.carrotsearch.randomizedtesting</groupId>
       <artifactId>randomizedtesting-runner</artifactId>
-      <version>2.0.10</version>
+    </dependency>
+
+      <dependency>
+          <groupId>com.tdunning</groupId>
+          <artifactId>t-digest</artifactId>
+          <version>2.0.2</version>
+      </dependency>
+
+    <dependency>
+      <groupId>org.easymock</groupId>
+      <artifactId>easymock</artifactId>
       <scope>test</scope>
     </dependency>
+    
   </dependencies>
 </project>
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/AbstractMatrix.java mahout/math/src/main/java/org/apache/mahout/math/AbstractMatrix.java
--- mahout/math/src/main/java/org/apache/mahout/math/AbstractMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/AbstractMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,19 +17,16 @@
 
 package org.apache.mahout.math;
 
-import java.util.Iterator;
-import java.util.Map;
-
-import org.apache.mahout.math.function.DoubleDoubleFunction;
-import org.apache.mahout.math.function.DoubleFunction;
-import org.apache.mahout.math.function.Functions;
-import org.apache.mahout.math.function.PlusMult;
-import org.apache.mahout.math.function.VectorFunction;
-
 import com.google.common.collect.AbstractIterator;
 import com.google.common.collect.Maps;
+import org.apache.mahout.math.function.*;
+
+import java.util.Iterator;
+import java.util.Map;
 
-/** A few universal implementations of convenience functions */
+/**
+ * A few universal implementations of convenience functions
+ */
 public abstract class AbstractMatrix implements Matrix {
 
   protected Map<String, Integer> columnLabelBindings;
@@ -61,6 +58,7 @@
   public Iterator<MatrixSlice> iterateAll() {
     return new AbstractIterator<MatrixSlice>() {
       private int slice;
+
       @Override
       protected MatrixSlice computeNext() {
         if (slice >= numSlices()) {
@@ -74,6 +72,7 @@
 
   /**
    * Abstracted out for the iterator
+   *
    * @return numRows() for row-based iterator, numColumns() for column-based.
    */
   @Override
@@ -228,7 +227,7 @@
     for (int row = 0; row < rows; row++) {
       for (int col = 0; col < columns; col++) {
         setQuick(row, col, function.apply(getQuick(row, col), other.getQuick(
-            row, col)));
+          row, col)));
       }
     }
     return this;
@@ -282,7 +281,8 @@
 
   /**
    * Returns a view of a row.  Changes to the view will affect the original.
-   * @param row  Which row to return.
+   *
+   * @param row Which row to return.
    * @return A vector that references the desired row.
    */
   @Override
@@ -293,6 +293,7 @@
 
   /**
    * Returns a view of a row.  Changes to the view will affect the original.
+   *
    * @param column Which column to return.
    * @return A vector that references the desired column.
    */
@@ -433,7 +434,7 @@
     for (int row = 0; row < rows; row++) {
       for (int col = 0; col < columns; col++) {
         result.setQuick(row, col, getQuick(row, col)
-            - other.getQuick(row, col));
+          - other.getQuick(row, col));
       }
     }
     return result;
@@ -466,7 +467,7 @@
     for (int row = 0; row < rows; row++) {
       for (int col = 0; col < columns; col++) {
         result.setQuick(row, col, getQuick(row, col)
-            + other.getQuick(row, col));
+          + other.getQuick(row, col));
       }
     }
     return result;
@@ -584,6 +585,26 @@
   }
 
   @Override
+  public Matrix viewPart(int[] offset, int[] size) {
+
+    if (offset[ROW] < 0) {
+      throw new IndexException(offset[ROW], 0);
+    }
+    if (offset[ROW] + size[ROW] > rowSize()) {
+      throw new IndexException(offset[ROW] + size[ROW], rowSize());
+    }
+    if (offset[COL] < 0) {
+      throw new IndexException(offset[COL], 0);
+    }
+    if (offset[COL] + size[COL] > columnSize()) {
+      throw new IndexException(offset[COL] + size[COL], columnSize());
+    }
+
+    return new MatrixView(this, offset, size);
+  }
+
+
+  @Override
   public double zSum() {
     double result = 0;
     for (int row = 0; row < rowSize(); row++) {
@@ -645,6 +666,7 @@
     public Iterator<Element> iterator() {
       return new AbstractIterator<Element>() {
         private int i;
+
         @Override
         protected Element computeNext() {
           if (i >= size()) {
@@ -658,6 +680,7 @@
     /**
      * Currently delegates to {@link #iterator()}.
      * TODO: This could be optimized to at least skip empty rows if there are many of them.
+     *
      * @return an iterator (currently dense).
      */
     @Override
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/AbstractVector.java mahout/math/src/main/java/org/apache/mahout/math/AbstractVector.java
--- mahout/math/src/main/java/org/apache/mahout/math/AbstractVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/AbstractVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -633,6 +633,30 @@
     return result.toString();
   }
 
+  /**
+   * toString() implementation for sparse vectors via {@link #nonZeroes()} method
+   * @return String representation of the vector
+   */
+  public String sparseVectorToString() {
+    Iterator<Element> it = iterateNonZero();
+    if (!it.hasNext()) {
+      return "{}";
+    }
+    else {
+      StringBuilder result = new StringBuilder();
+      result.append('{');
+      while (it.hasNext()) {
+        Vector.Element e = it.next();
+        result.append(e.index());
+        result.append(':');
+        result.append(e.get());
+        result.append(',');
+      }
+      result.setCharAt(result.length() - 1, '}');
+      return result.toString();
+    }
+  }
+
   protected final class LocalElement implements Element {
     int index;
 
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/CholeskyDecomposition.java mahout/math/src/main/java/org/apache/mahout/math/CholeskyDecomposition.java
--- mahout/math/src/main/java/org/apache/mahout/math/CholeskyDecomposition.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/CholeskyDecomposition.java	2014-03-29 01:03:14.000000000 -0700
@@ -29,7 +29,7 @@
  */
 public class CholeskyDecomposition {
   private final PivotedMatrix L;
-  private boolean isPositiveDefinite;
+  private boolean isPositiveDefinite = true;
 
   public CholeskyDecomposition(Matrix a) {
     this(a, true);
@@ -40,7 +40,7 @@
     L = new PivotedMatrix(new DenseMatrix(rows, rows));
 
     // must be square
-    Preconditions.checkArgument(rows == a.columnSize());
+    Preconditions.checkArgument(rows == a.columnSize(), "Must be a Square Matrix");
 
     if (pivot) {
       decomposeWithPivoting(a);
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/DenseSymmetricMatrix.java mahout/math/src/main/java/org/apache/mahout/math/DenseSymmetricMatrix.java
--- mahout/math/src/main/java/org/apache/mahout/math/DenseSymmetricMatrix.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/main/java/org/apache/mahout/math/DenseSymmetricMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,60 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+/**
+ * Economy packaging for a dense symmetric in-core matrix.
+ */
+public class DenseSymmetricMatrix extends UpperTriangular {
+  public DenseSymmetricMatrix(int n) {
+    super(n);
+  }
+
+  public DenseSymmetricMatrix(double[] data, boolean shallow) {
+    super(data, shallow);
+  }
+
+  public DenseSymmetricMatrix(Vector data) {
+    super(data);
+  }
+
+  public DenseSymmetricMatrix(UpperTriangular mx) {
+    super(mx);
+  }
+
+  @Override
+  public double getQuick(int row, int column) {
+    if (column < row) {
+      int swap = row;
+      row = column;
+      column = swap;
+    }
+    return super.getQuick(row, column);
+  }
+
+  @Override
+  public void setQuick(int row, int column, double value) {
+    if (column < row) {
+      int swap = row;
+      row = column;
+      column = swap;
+    }
+    super.setQuick(row, column, value);
+  }
+
+}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/DenseVector.java mahout/math/src/main/java/org/apache/mahout/math/DenseVector.java
--- mahout/math/src/main/java/org/apache/mahout/math/DenseVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/DenseVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -33,7 +33,9 @@
     super(0);
   }
 
-  /** Construct a new instance using provided values */
+  /** Construct a new instance using provided values
+   *  @param values - array of values
+   */
   public DenseVector(double[] values) {
     this(values, false);
   }
@@ -47,7 +49,9 @@
     this(values.values, shallowCopy);
   }
 
-  /** Construct a new instance of the given cardinality */
+  /** Construct a new instance of the given cardinality
+   * @param cardinality - number of values in the vector
+   */
   public DenseVector(int cardinality) {
     super(cardinality);
     this.values = new double[cardinality];
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/DiagonalMatrix.java mahout/math/src/main/java/org/apache/mahout/math/DiagonalMatrix.java
--- mahout/math/src/main/java/org/apache/mahout/math/DiagonalMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/DiagonalMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,7 @@
 
 package org.apache.mahout.math;
 
-public class DiagonalMatrix extends AbstractMatrix {
+public class DiagonalMatrix extends AbstractMatrix implements MatrixTimesOps {
   private final Vector diagonal;
 
   public DiagonalMatrix(Vector values) {
@@ -139,4 +139,30 @@
   public Matrix viewPart(int[] offset, int[] size) {
     return new MatrixView(this, offset, size);
   }
+
+  @Override
+  public Matrix times(Matrix other) {
+    return timesRight(other);
+  }
+
+  @Override
+  public Matrix timesRight(Matrix that) {
+    if (that.numRows() != diagonal.size())
+      throw new IllegalArgumentException("Incompatible number of rows in the right operand of matrix multiplication.");
+    Matrix m = that.like();
+    for (int row = 0; row < diagonal.size(); row++)
+      m.assignRow(row, that.viewRow(row).times(diagonal.getQuick(row)));
+    return m;
+  }
+
+  @Override
+  public Matrix timesLeft(Matrix that) {
+    if (that.numCols() != diagonal.size())
+      throw new IllegalArgumentException(
+          "Incompatible number of rows in the left operand of matrix-matrix multiplication.");
+    Matrix m = that.like();
+    for (int col = 0; col < diagonal.size(); col++)
+      m.assignColumn(col, that.viewColumn(col).times(diagonal.getQuick(col)));
+    return m;
+  }
 }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/FunctionalMatrixView.java mahout/math/src/main/java/org/apache/mahout/math/FunctionalMatrixView.java
--- mahout/math/src/main/java/org/apache/mahout/math/FunctionalMatrixView.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/main/java/org/apache/mahout/math/FunctionalMatrixView.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+import org.apache.mahout.math.function.IntIntFunction;
+
+/**
+ * Matrix View backed by an {@link IntIntFunction}
+ */
+class FunctionalMatrixView extends AbstractMatrix {
+
+  /**
+   * view generator function
+   */
+  private IntIntFunction gf;
+  private boolean denseLike;
+
+  public FunctionalMatrixView(int rows, int columns, IntIntFunction gf) {
+    this(rows, columns, gf, false);
+  }
+
+  /**
+   * @param gf        generator function
+   * @param denseLike whether like() should create Dense or Sparse matrix.
+   */
+  public FunctionalMatrixView(int rows, int columns, IntIntFunction gf, boolean denseLike) {
+    super(rows, columns);
+    this.gf = gf;
+    this.denseLike = denseLike;
+  }
+
+  @Override
+  public Matrix assignColumn(int column, Vector other) {
+    throw new UnsupportedOperationException("Assignment to a matrix not supported");
+  }
+
+  @Override
+  public Matrix assignRow(int row, Vector other) {
+    throw new UnsupportedOperationException("Assignment to a matrix view not supported");
+  }
+
+  @Override
+  public double getQuick(int row, int column) {
+    return gf.apply(row, column);
+  }
+
+  @Override
+  public Matrix like() {
+    return like(rows, columns);
+  }
+
+  @Override
+  public Matrix like(int rows, int columns) {
+    if (denseLike)
+      return new DenseMatrix(rows, columns);
+    else
+      return new SparseMatrix(rows, columns);
+  }
+
+  @Override
+  public void setQuick(int row, int column, double value) {
+    throw new UnsupportedOperationException("Assignment to a matrix view not supported");
+  }
+
+
+}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/Matrices.java mahout/math/src/main/java/org/apache/mahout/math/Matrices.java
--- mahout/math/src/main/java/org/apache/mahout/math/Matrices.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/main/java/org/apache/mahout/math/Matrices.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,172 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.function.DoubleFunction;
+import org.apache.mahout.math.function.Functions;
+import org.apache.mahout.math.function.IntIntFunction;
+
+import java.util.Random;
+
+/**
+ * @author dmitriy
+ */
+public final class Matrices {
+
+  /**
+   * Create a matrix view based on a function generator.
+   * <p/>
+   * The generator needs to be idempotent, i.e. returning same value
+   * for each combination of (row, column) argument sent to generator's
+   * {@link IntIntFunction#apply(int, int)} call.
+   *
+   * @param rows      Number of rows in a view
+   * @param columns   Number of columns in a view.
+   * @param gf        view generator
+   * @param denseLike type of matrix returne dby {@link org.apache.mahout.math.Matrix#like()}.
+   * @return new matrix view.
+   */
+  public static final Matrix functionalMatrixView(final int rows,
+                                                  final int columns,
+                                                  final IntIntFunction gf,
+                                                  final boolean denseLike) {
+    return new FunctionalMatrixView(rows, columns, gf, denseLike);
+  }
+
+  /**
+   * Shorter form of {@link Matrices#functionalMatrixView(int, int,
+   * org.apache.mahout.math.function.IntIntFunction, boolean)}.
+   */
+  public static final Matrix functionalMatrixView(final int rows,
+                                                  final int columns,
+                                                  final IntIntFunction gf) {
+    return new FunctionalMatrixView(rows, columns, gf);
+  }
+
+  /**
+   * A read-only transposed view of a matrix argument.
+   *
+   * @param m original matrix
+   * @return transposed view of original matrix
+   */
+  public static final Matrix transposedView(final Matrix m) {
+    IntIntFunction tf = new IntIntFunction() {
+      @Override
+      public double apply(int row, int col) {
+        return m.getQuick(col, row);
+      }
+    };
+
+    // TODO: Matrix api does not support denseLike() interrogation.
+    // so our guess has to be rough here.
+    return functionalMatrixView(m.numCols(), m.numRows(), tf, m instanceof DenseMatrix);
+  }
+
+  /**
+   * Random Gaussian matrix view.
+   *
+   * @param seed generator seed
+   */
+  public static final Matrix gaussianView(final int rows,
+                                          final int columns,
+                                          long seed) {
+    return functionalMatrixView(rows, columns, gaussianGenerator(seed), true);
+  }
+
+
+  /**
+   * Matrix view based on uniform [-1,1) distribution.
+   *
+   * @param seed generator seed
+   */
+  public static final Matrix symmetricUniformView(final int rows,
+                                                  final int columns,
+                                                  int seed) {
+    return functionalMatrixView(rows, columns, uniformSymmetricGenerator(seed), true);
+  }
+
+  /**
+   * Matrix view based on uniform [0,1) distribution.
+   *
+   * @param seed generator seed
+   */
+  public static final Matrix uniformView(final int rows,
+                                         final int columns,
+                                         int seed) {
+    return functionalMatrixView(rows, columns, uniformGenerator(seed), true);
+  }
+
+  /**
+   * Generator for a matrix populated by random Gauissian values (Gaussian matrix view)
+   *
+   * @param seed The seed for the matrix.
+   * @return Gaussian {@link IntIntFunction} generating matrix view with normal values
+   */
+  public static final IntIntFunction gaussianGenerator(final long seed) {
+    final Random rnd = RandomUtils.getRandom(seed);
+    IntIntFunction gaussianGF = new IntIntFunction() {
+      @Override
+      public double apply(int first, int second) {
+        rnd.setSeed(seed ^ (((long) first << 32) | (second & 0xffffffffl)));
+        return rnd.nextGaussian();
+      }
+    };
+    return gaussianGF;
+  }
+
+  private static final double UNIFORM_DIVISOR = Math.pow(2.0, 64);
+
+  /**
+   * Uniform [-1,1) matrix generator function.
+   * <p/>
+   * WARNING: to keep things performant, it is stateful and so not thread-safe.
+   * You'd need to create a copy per thread (with same seed) if shared between threads.
+   *
+   * @param seed
+   * @return Uniform {@link IntIntFunction} generator
+   */
+  public static final IntIntFunction uniformSymmetricGenerator(final int seed) {
+    return new IntIntFunction() {
+      private byte[] data = new byte[8];
+
+      @Override
+      public double apply(int row, int column) {
+        long d = ((long) row << Integer.SIZE) | (column & 0xffffffffl);
+        for (int i = 0; i < 8; i++, d >>>= 8) data[i] = (byte) d;
+        long hash = MurmurHash.hash64A(data, seed);
+        return hash / UNIFORM_DIVISOR;
+      }
+    };
+  }
+
+  /**
+   * Uniform [0,1) matrix generator function
+   *
+   * @param seed generator seed
+   */
+  public static final IntIntFunction uniformGenerator(final int seed) {
+    return Functions.chain(new DoubleFunction() {
+      @Override
+      public double apply(double x) {
+        return (x + 1.0) / 2.0;
+      }
+    }, uniformSymmetricGenerator(seed));
+  }
+
+}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/MatrixTimesOps.java mahout/math/src/main/java/org/apache/mahout/math/MatrixTimesOps.java
--- mahout/math/src/main/java/org/apache/mahout/math/MatrixTimesOps.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/main/java/org/apache/mahout/math/MatrixTimesOps.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+/**
+ * Optional interface for optimized matrix multiplications.
+ * Some concrete Matrix implementations may mix this in.
+ */
+public interface MatrixTimesOps {
+  /**
+   * computes matrix product of (this * that)
+   */
+  Matrix timesRight(Matrix that);
+
+  /**
+   * Computes matrix product of (that * this)
+   */
+  Matrix timesLeft(Matrix that);
+
+}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/MatrixVectorView.java mahout/math/src/main/java/org/apache/mahout/math/MatrixVectorView.java
--- mahout/math/src/main/java/org/apache/mahout/math/MatrixVectorView.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/MatrixVectorView.java	2014-03-29 01:03:14.000000000 -0700
@@ -38,10 +38,10 @@
 
   public MatrixVectorView(Matrix matrix, int row, int column, int rowStride, int columnStride) {
     super(viewSize(matrix, row, column, rowStride, columnStride));
-    if (row < 0 || row > matrix.rowSize()) {
+    if (row < 0 || row >= matrix.rowSize()) {
       throw new IndexException(row, matrix.rowSize());
     }
-    if (column < 0 || column > matrix.columnSize()) {
+    if (column < 0 || column >= matrix.columnSize()) {
       throw new IndexException(column, matrix.columnSize());
     }
 
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/QRDecomposition.java mahout/math/src/main/java/org/apache/mahout/math/QRDecomposition.java
--- mahout/math/src/main/java/org/apache/mahout/math/QRDecomposition.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/QRDecomposition.java	2014-03-29 01:03:14.000000000 -0700
@@ -33,9 +33,9 @@
  orthogonal matrix <tt>Q</tt> and an <tt>n x n</tt> upper triangular matrix <tt>R</tt> so that
  <tt>A = Q*R</tt>.
  <P>
- The QR decompostion always exists, even if the matrix does not have
+ The QR decomposition always exists, even if the matrix does not have
  full rank, so the constructor will never fail.  The primary use of the
- QR decomposition is in the least squares solution of nonsquare systems
+ QR decomposition is in the least squares solution of non-square systems
  of simultaneous linear equations.  This will fail if <tt>isFullRank()</tt>
  returns <tt>false</tt>.
  */
@@ -151,7 +151,7 @@
     Matrix x = B.like(columns, cols);
 
     // this can all be done a bit more efficiently if we don't actually
-    // form explicit versions of Q^T and R but this code isn't soo bad
+    // form explicit versions of Q^T and R but this code isn't so bad
     // and it is much easier to understand
     Matrix qt = getQ().transpose();
     Matrix y = qt.times(B);
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/RandomAccessSparseVector.java mahout/math/src/main/java/org/apache/mahout/math/RandomAccessSparseVector.java
--- mahout/math/src/main/java/org/apache/mahout/math/RandomAccessSparseVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/RandomAccessSparseVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -75,23 +75,7 @@
 
   @Override
   public String toString() {
-    StringBuilder result = new StringBuilder();
-    result.append('{');
-    Iterator<Element> it = iterateNonZero();
-    boolean first = true;
-    while (it.hasNext()) {
-      if (first) {
-        first = false;
-      } else {
-        result.append(',');
-      }
-      Element e = it.next();
-      result.append(e.index());
-      result.append(':');
-      result.append(e.get());
-    }
-    result.append('}');
-    return result.toString();
+    return sparseVectorToString();
   }
 
   @Override
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java mahout/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java
--- mahout/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -126,18 +126,7 @@
 
   @Override
   public String toString() {
-    StringBuilder result = new StringBuilder();
-    result.append('{');
-    Iterator<Element> it = iterateNonZero();
-    while (it.hasNext()) {
-      Element e = it.next();
-      result.append(e.index());
-      result.append(':');
-      result.append(e.get());
-      result.append(',');
-    }
-    result.append('}');
-    return result.toString();
+    return sparseVectorToString();
   }
 
   /**
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/SingularValueDecomposition.java mahout/math/src/main/java/org/apache/mahout/math/SingularValueDecomposition.java
--- mahout/math/src/main/java/org/apache/mahout/math/SingularValueDecomposition.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/SingularValueDecomposition.java	2014-03-29 01:03:14.000000000 -0700
@@ -268,8 +268,9 @@
     // Main iteration loop for the singular values.
     
     int pp = p - 1;
-    //int iter = 0;
+    int iter = 0;
     double eps = Math.pow(2.0, -52.0);
+    double tiny = Math.pow(2.0,-966.0);
     while (p > 0) {
       int k;
       
@@ -289,7 +290,7 @@
         if (k == -1) {
           break;
         }
-        if (Math.abs(e[k]) <= eps * (Math.abs(s[k]) + Math.abs(s[k + 1]))) {
+        if (Math.abs(e[k]) <= tiny +eps * (Math.abs(s[k]) + Math.abs(s[k + 1]))) {
           e[k] = 0.0;
           break;
         }
@@ -303,8 +304,10 @@
           if (ks == k) {
             break;
           }
-          double t = (ks == p ? 0.0 : Math.abs(e[ks])) + (ks == k + 1 ? 0.0 : Math.abs(e[ks - 1]));
-          if (Math.abs(s[ks]) <= eps * t) {
+          double t =
+            (ks != p ?  Math.abs(e[ks]) : 0.) +
+            (ks != k + 1 ?  Math.abs(e[ks-1]) : 0.);
+          if (Math.abs(s[ks]) <= tiny + eps * t) {
             s[ks] = 0.0;
             break;
           }
@@ -436,7 +439,7 @@
             }
           }
           e[p - 2] = f;
-          //iter += 1;
+          iter = iter + 1;
         }
           break;
         
@@ -480,7 +483,7 @@
             }
             k++;
           }
-          //iter = 0;
+          iter = 0;
           p--;
         }
           break;
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/SparseColumnMatrix.java mahout/math/src/main/java/org/apache/mahout/math/SparseColumnMatrix.java
--- mahout/math/src/main/java/org/apache/mahout/math/SparseColumnMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/SparseColumnMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -28,7 +28,7 @@
   /**
    * Construct a matrix of the given cardinality with the given data columns
    *
-   * @param columns     a RandomAccessSparseVector[] array of columns
+   * @param columns       a RandomAccessSparseVector[] array of columns
    * @param columnVectors
    */
   public SparseColumnMatrix(int rows, int columns, RandomAccessSparseVector[] columnVectors) {
@@ -65,7 +65,8 @@
 
   /**
    * Abstracted out for the iterator
-   * @return {@link #numCols()} 
+   *
+   * @return {@link #numCols()}
    */
   @Override
   public int numSlices() {
@@ -101,7 +102,7 @@
     result[COL] = columnVectors.length;
     for (int col = 0; col < columnSize(); col++) {
       result[ROW] = Math.max(result[ROW], columnVectors[col]
-          .getNumNondefaultElements());
+        .getNumNondefaultElements());
     }
     return result;
   }
@@ -156,4 +157,17 @@
     }
     return columnVectors[column];
   }
+
+  @Override
+  public Matrix transpose() {
+    SparseRowMatrix srm = new SparseRowMatrix(columns, rows);
+    for (int i = 0; i < columns; i++) {
+      Vector col = columnVectors[i];
+      if (col.getNumNonZeroElements() > 0)
+        // this should already be optimized
+        srm.assignRow(i, col);
+    }
+    return srm;
+  }
+
 }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/SparseMatrix.java mahout/math/src/main/java/org/apache/mahout/math/SparseMatrix.java
--- mahout/math/src/main/java/org/apache/mahout/math/SparseMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/SparseMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -55,10 +55,9 @@
 
   @Override
   public Matrix clone() {
-    SparseMatrix clone = (SparseMatrix) super.clone();
-    clone.rowVectors = rowVectors.clone();
-    for (int i = 0; i < numRows(); i++) {
-      clone.rowVectors.put(i, rowVectors.get(i).clone());
+    SparseMatrix clone = new SparseMatrix(numRows(), numCols());
+    for (MatrixSlice slice : this) {
+      clone.rowVectors.put(slice.index(), slice.clone());
     }
     return clone;
   }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/SparseRowMatrix.java mahout/math/src/main/java/org/apache/mahout/math/SparseRowMatrix.java
--- mahout/math/src/main/java/org/apache/mahout/math/SparseRowMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/SparseRowMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -166,4 +166,15 @@
     return rowVectors[row];
   }
 
+  @Override
+  public Matrix transpose() {
+    SparseColumnMatrix scm = new SparseColumnMatrix(columns, rows);
+    for (int i = 0; i < rows; i++) {
+      Vector row = rowVectors[i];
+      if ( row.getNumNonZeroElements() > 0)
+        scm.assignColumn(i, row);
+    }
+    return scm;
+  }
+
 }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/UpperTriangular.java mahout/math/src/main/java/org/apache/mahout/math/UpperTriangular.java
--- mahout/math/src/main/java/org/apache/mahout/math/UpperTriangular.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/main/java/org/apache/mahout/math/UpperTriangular.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,151 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+/**
+ * 
+ * Quick and dirty implementation of some {@link org.apache.mahout.math.Matrix} methods
+ * over packed upper triangular matrix.
+ *
+ */
+public class UpperTriangular extends AbstractMatrix {
+
+  private static final double EPSILON = 1.0e-12; // assume anything less than
+                                                 // that to be 0 during
+                                                 // non-upper assignments
+
+  private double[] values;
+
+  /**
+   * represents n x n upper triangular matrix
+   * 
+   * @param n
+   */
+
+  public UpperTriangular(int n) {
+    super(n, n);
+    values = new double[n * (n + 1) / 2];
+  }
+
+  public UpperTriangular(double[] data, boolean shallow) {
+    this(elementsToMatrixSize(data != null ? data.length : 0));
+    if (data == null) {
+      throw new IllegalArgumentException("data");
+    }
+    values = shallow ? data : data.clone();
+  }
+
+  public UpperTriangular(Vector data) {
+    this(elementsToMatrixSize(data.size()));
+
+    for (Vector.Element el:data.nonZeroes()) {
+      values[el.index()] = el.get();
+    }
+  }
+
+  private static int elementsToMatrixSize(int dataSize) {
+    return (int) Math.round((-1 + Math.sqrt(1 + 8 * dataSize)) / 2);
+  }
+
+  // copy-constructor
+  public UpperTriangular(UpperTriangular mx) {
+    this(mx.values, false);
+  }
+
+  @Override
+  public Matrix assignColumn(int column, Vector other) {
+    if (columnSize() != other.size()) {
+      throw new IndexException(columnSize(), other.size());
+    }
+    if (other.viewPart(column + 1, other.size() - column - 1).norm(1) > 1.0e-14) {
+      throw new IllegalArgumentException("Cannot set lower portion of triangular matrix to non-zero");
+    }
+    for (Vector.Element element : other.viewPart(0, column).all()) {
+      setQuick(element.index(), column, element.get());
+    }
+    return this;
+  }
+
+  @Override
+  public Matrix assignRow(int row, Vector other) {
+    if (columnSize() != other.size()) {
+      throw new IndexException(numCols(), other.size());
+    }
+    for (int i = 0; i < row; i++) {
+      if (Math.abs(other.getQuick(i)) > EPSILON) {
+        throw new IllegalArgumentException("non-triangular source");
+      }
+    }
+    for (int i = row; i < rows; i++) {
+      setQuick(row, i, other.get(i));
+    }
+    return this;
+  }
+
+  public Matrix assignNonZeroElementsInRow(int row, double[] other) {
+    System.arraycopy(other, row, values, getL(row, row), rows - row);
+    return this;
+  }
+
+  @Override
+  public double getQuick(int row, int column) {
+    if (row > column) {
+      return 0;
+    }
+    int i = getL(row, column);
+    return values[i];
+  }
+
+  private int getL(int row, int col) {
+    /*
+     * each row starts with some zero elements that we don't store. this
+     * accumulates an offset of (row+1)*row/2
+     */
+    return col + row * numCols() - (row + 1) * row / 2;
+  }
+
+  @Override
+  public Matrix like() {
+    return like(rowSize(), columnSize());
+  }
+
+  @Override
+  public Matrix like(int rows, int columns) {
+    return new DenseMatrix(rows, columns);
+  }
+
+  @Override
+  public void setQuick(int row, int column, double value) {
+    values[getL(row, column)] = value;
+  }
+
+  @Override
+  public int[] getNumNondefaultElements() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public Matrix viewPart(int[] offset, int[] size) {
+    return new MatrixView(this, offset, size);
+  }
+
+  public double[] getData() {
+    return values;
+  }
+
+}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/Vector.java mahout/math/src/main/java/org/apache/mahout/math/Vector.java
--- mahout/math/src/main/java/org/apache/mahout/math/Vector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/Vector.java	2014-03-29 01:03:14.000000000 -0700
@@ -366,7 +366,7 @@
 
   /**
    * Examples speak louder than words:  aggregate(plus, pow(2)) is another way to say
-   * getLengthSquared(), aggregate(max, abs) is norm(Double.POSITIVE_INFINITY).  To sum all of the postive values,
+   * getLengthSquared(), aggregate(max, abs) is norm(Double.POSITIVE_INFINITY).  To sum all of the positive values,
    * aggregate(plus, max(0)).
    * @param aggregator used to combine the current value of the aggregation with the result of map.apply(nextValue)
    * @param map a function to apply to each element of the vector in turn before passing to the aggregator
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAggregate.java mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAggregate.java
--- mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAggregate.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAggregate.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math;
 
 import org.apache.mahout.math.function.DoubleDoubleFunction;
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAssign.java mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAssign.java
--- mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAssign.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/VectorBinaryAssign.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math;
 
 import org.apache.mahout.math.Vector.Element;
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolver.java mahout/math/src/main/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolver.java
--- mahout/math/src/main/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolver.java	2014-03-29 01:03:14.000000000 -0700
@@ -36,10 +36,10 @@
   //TODO make feature vectors a simple array
   public static Vector solve(Iterable<Vector> featureVectors, Vector ratingVector, double lambda, int numFeatures) {
 
-    Preconditions.checkNotNull(featureVectors, "Feature vectors cannot be null");
+    Preconditions.checkNotNull(featureVectors, "Feature Vectors cannot be null");
     Preconditions.checkArgument(!Iterables.isEmpty(featureVectors));
-    Preconditions.checkNotNull(ratingVector, "rating vector cannot be null");
-    Preconditions.checkArgument(ratingVector.getNumNondefaultElements() > 0, "Rating vector cannot be empty");
+    Preconditions.checkNotNull(ratingVector, "Rating Vector cannot be null");
+    Preconditions.checkArgument(ratingVector.getNumNondefaultElements() > 0, "Rating Vector cannot be empty");
     Preconditions.checkArgument(Iterables.size(featureVectors) == ratingVector.getNumNondefaultElements());
 
     int nui = ratingVector.getNumNondefaultElements();
@@ -60,7 +60,7 @@
   }
 
   static Matrix addLambdaTimesNuiTimesE(Matrix matrix, double lambda, int nui) {
-    Preconditions.checkArgument(matrix.numCols() == matrix.numRows());
+    Preconditions.checkArgument(matrix.numCols() == matrix.numRows(), "Must be a Square Matrix");
     double lambdaTimesNui = lambda * nui;
     int numCols = matrix.numCols();
     for (int n = 0; n < numCols; n++) {
@@ -104,7 +104,7 @@
   }
 
   static Matrix createRiIiMaybeTransposed(Vector ratingVector) {
-    Preconditions.checkArgument(ratingVector.isSequentialAccess());
+    Preconditions.checkArgument(ratingVector.isSequentialAccess(), "Ratings should be iterable in Index or Sequential Order");
 
     double[][] RiIiMaybeTransposed = new double[ratingVector.getNumNondefaultElements()][1];
     int index = 0;
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/als/ImplicitFeedbackAlternatingLeastSquaresSolver.java mahout/math/src/main/java/org/apache/mahout/math/als/ImplicitFeedbackAlternatingLeastSquaresSolver.java
--- mahout/math/src/main/java/org/apache/mahout/math/als/ImplicitFeedbackAlternatingLeastSquaresSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/als/ImplicitFeedbackAlternatingLeastSquaresSolver.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,10 @@
 
 package org.apache.mahout.math.als;
 
-import com.google.common.base.Preconditions;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
 import org.apache.mahout.math.DenseMatrix;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.Matrix;
@@ -27,6 +30,10 @@
 import org.apache.mahout.math.function.Functions;
 import org.apache.mahout.math.list.IntArrayList;
 import org.apache.mahout.math.map.OpenIntObjectHashMap;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.base.Preconditions;
 
 /** see <a href="http://research.yahoo.com/pub/2433">Collaborative Filtering for Implicit Feedback Datasets</a> */
 public class ImplicitFeedbackAlternatingLeastSquaresSolver {
@@ -34,16 +41,20 @@
   private final int numFeatures;
   private final double alpha;
   private final double lambda;
+  private final int numTrainingThreads;
 
   private final OpenIntObjectHashMap<Vector> Y;
   private final Matrix YtransposeY;
-
+  
+  private static final Logger log = LoggerFactory.getLogger(ImplicitFeedbackAlternatingLeastSquaresSolver.class);
+  
   public ImplicitFeedbackAlternatingLeastSquaresSolver(int numFeatures, double lambda, double alpha,
-      OpenIntObjectHashMap<Vector> Y) {
+      OpenIntObjectHashMap<Vector> Y, int numTrainingThreads) {
     this.numFeatures = numFeatures;
     this.lambda = lambda;
     this.alpha = alpha;
     this.Y = Y;
+    this.numTrainingThreads = numTrainingThreads;
     YtransposeY = getYtransposeY(Y);
   }
 
@@ -60,28 +71,51 @@
   }
 
   /* Y' Y */
-  private Matrix getYtransposeY(OpenIntObjectHashMap<Vector> Y) {
-
-    IntArrayList indexes = Y.keys();
-    indexes.quickSort();
-    int numIndexes = indexes.size();
-
-    double[][] YtY = new double[numFeatures][numFeatures];
-
+  Matrix getYtransposeY(final OpenIntObjectHashMap<Vector> Y) {
+  
+    ExecutorService queue = Executors.newFixedThreadPool(numTrainingThreads);
+    if (log.isInfoEnabled()) {
+      log.info("Starting the computation of Y'Y");
+    }
+    long startTime = System.nanoTime();
+    final IntArrayList indexes = Y.keys();
+    final int numIndexes = indexes.size();
+  
+    final double[][] YtY = new double[numFeatures][numFeatures];
+  
     // Compute Y'Y by dot products between the 'columns' of Y
     for (int i = 0; i < numFeatures; i++) {
       for (int j = i; j < numFeatures; j++) {
-        double dot = 0;
-        for (int k = 0; k < numIndexes; k++) {
-          Vector row = Y.get(indexes.getQuick(k));
-          dot += row.getQuick(i) * row.getQuick(j);
-        }
-        YtY[i][j] = dot;
-        if (i != j) {
-          YtY[j][i] = dot;
-        }
+  
+        final int ii = i;
+        final int jj = j;
+        queue.execute(new Runnable() {
+          @Override
+          public void run() {
+            double dot = 0;
+            for (int k = 0; k < numIndexes; k++) {
+              Vector row = Y.get(indexes.getQuick(k));
+              dot += row.getQuick(ii) * row.getQuick(jj);
+            }
+            YtY[ii][jj] = dot;
+            if (ii != jj) {
+              YtY[jj][ii] = dot;
+            }
+          }
+        });
+  
       }
     }
+    queue.shutdown();
+    try {
+      queue.awaitTermination(1, TimeUnit.DAYS);
+    } catch (InterruptedException e) {
+      log.error("Error during Y'Y queue shutdown", e);
+      throw new RuntimeException("Error during Y'Y queue shutdown");
+    }
+    if (log.isInfoEnabled()) {
+      log.info("Computed Y'Y in " + (System.nanoTime() - startTime) / 1000000.0 + " ms" );
+    }
     return new DenseMatrix(YtY, true);
   }
 
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/decomposer/AsyncEigenVerifier.java mahout/math/src/main/java/org/apache/mahout/math/decomposer/AsyncEigenVerifier.java
--- mahout/math/src/main/java/org/apache/mahout/math/decomposer/AsyncEigenVerifier.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/decomposer/AsyncEigenVerifier.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,16 +17,16 @@
 
 package org.apache.mahout.math.decomposer;
 
-import java.util.concurrent.Executor;
+import java.io.Closeable;
+import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 
 import org.apache.mahout.math.Vector;
 import org.apache.mahout.math.VectorIterable;
 
+public class AsyncEigenVerifier extends SimpleEigenVerifier implements Closeable {
 
-public class AsyncEigenVerifier extends SimpleEigenVerifier {
-
-  private final Executor threadPool;
+  private final ExecutorService threadPool;
   private EigenStatus status;
   private boolean finished;
   private boolean started;
@@ -50,6 +50,10 @@
     return status;
   }
 
+  @Override
+  public void close() {
+	  this.threadPool.shutdownNow();
+  }
   protected EigenStatus innerVerify(VectorIterable corpus, Vector vector) {
     return super.verify(corpus, vector);
   }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/decomposer/lanczos/LanczosSolver.java mahout/math/src/main/java/org/apache/mahout/math/decomposer/lanczos/LanczosSolver.java
--- mahout/math/src/main/java/org/apache/mahout/math/decomposer/lanczos/LanczosSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/decomposer/lanczos/LanczosSolver.java	2014-03-29 01:03:14.000000000 -0700
@@ -39,7 +39,7 @@
  * See the SSVD code for a better option
  * {@link org.apache.mahout.math.ssvd.SequentialBigSvd}
  * See also the docs on
- * <a href=https://cwiki.apache.org/confluence/display/MAHOUT/Stochastic+Singular+Value+Decomposition>stochastic
+ * <a href=https://mahout.apache.org/users/dim-reduction/ssvd.html>stochastic
  * projection SVD</a>
  * <p>
  * To avoid floating point overflow problems which arise in power-methods like Lanczos, an initial pass is made
@@ -154,8 +154,8 @@
     startTime(TimingSection.FINAL_EIGEN_CREATE);
     for (int row = 0; row < i; row++) {
       Vector realEigen = null;
-      // the eigenvectors live as columns of V, in reverse order.  Weird but true.
-      Vector ejCol = eigenVects.viewColumn(i - row - 1);
+
+      Vector ejCol = eigenVects.viewColumn(row);
       int size = Math.min(ejCol.size(), state.getBasisSize());
       for (int j = 0; j < size; j++) {
         double d = ejCol.get(j);
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/function/Functions.java mahout/math/src/main/java/org/apache/mahout/math/function/Functions.java
--- mahout/math/src/main/java/org/apache/mahout/math/function/Functions.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/function/Functions.java	2014-03-29 01:03:14.000000000 -0700
@@ -1334,6 +1334,24 @@
   }
 
   /**
+   * Constructs the function <tt>g( h(a) )</tt>.
+   *
+   * @param g a unary function.
+   * @param h an {@link IntIntFunction} function.
+   * @return the unary function <tt>g( h(a) )</tt>.
+   */
+  public static IntIntFunction chain(final DoubleFunction g, final IntIntFunction h) {
+    return new IntIntFunction() {
+
+      @Override
+      public double apply(int first, int second) {
+        return g.apply(h.apply(first, second));
+      }
+    };
+  }
+
+
+  /**
    * Constructs a function that returns <tt>a < b ? -1 : a > b ? 1 : 0</tt>. <tt>a</tt> is a variable, <tt>b</tt> is
    * fixed.
    */
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/function/IntIntFunction.java mahout/math/src/main/java/org/apache/mahout/math/function/IntIntFunction.java
--- mahout/math/src/main/java/org/apache/mahout/math/function/IntIntFunction.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/main/java/org/apache/mahout/math/function/IntIntFunction.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,25 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.function;
+
+/**
+ * A function that takes to integer arguments and returns Double.
+ */
+public interface IntIntFunction {
+  double apply(int first, int second);
+}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/map/HashFunctions.java mahout/math/src/main/java/org/apache/mahout/math/map/HashFunctions.java
--- mahout/math/src/main/java/org/apache/mahout/math/map/HashFunctions.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/map/HashFunctions.java	2014-03-29 01:03:14.000000000 -0700
@@ -8,9 +8,9 @@
 */
 package org.apache.mahout.math.map;
 
+
 /**
  * Provides various hash functions.
- *
  */
 public final class HashFunctions {
 
@@ -54,22 +54,19 @@
 
   /**
    * Returns a hashcode for the specified value.
+   * The hashcode computation is similar to the last step
+   * of MurMurHash3.
    *
    * @return a hash code value for the specified value.
    */
   public static int hash(int value) {
-    return value;
-
-    //return value * 0x278DDE6D; // see org.apache.mahout.math.jet.random.engine.DRand
-
-    /*
-    value &= 0x7FFFFFFF; // make it >=0
-    int hashCode = 0;
-    do hashCode = 31*hashCode + value%10;
-    while ((value /= 10) > 0);
-
-    return 28629151*hashCode; // spread even further; h*31^5
-    */
+    int h = value;
+    h ^= h >>> 16;
+    h *= 0x85ebca6b;
+    h ^= h >>> 13;
+    h *= 0xc2b2ae35;
+    h ^= h >>> 16;
+    return h;
   }
 
   /**
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/random/ChineseRestaurant.java mahout/math/src/main/java/org/apache/mahout/math/random/ChineseRestaurant.java
--- mahout/math/src/main/java/org/apache/mahout/math/random/ChineseRestaurant.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/random/ChineseRestaurant.java	2014-03-29 01:03:14.000000000 -0700
@@ -54,8 +54,8 @@
    * @param discount  The discount parameter that drives the percentage of values that occur once in a large sample.
    */
   public ChineseRestaurant(double alpha, double discount) {
-    Preconditions.checkArgument(alpha > 0);
-    Preconditions.checkArgument(discount >= 0 && discount <= 1);
+    Preconditions.checkArgument(alpha > 0, "Strength Parameter, alpha must be greater then 0!");
+    Preconditions.checkArgument(discount >= 0 && discount <= 1, "Must be: 0 <= discount <= 1");
     this.alpha = alpha;
     this.discount = discount;
   }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/random/Empirical.java mahout/math/src/main/java/org/apache/mahout/math/random/Empirical.java
--- mahout/math/src/main/java/org/apache/mahout/math/random/Empirical.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/random/Empirical.java	2014-03-29 01:03:14.000000000 -0700
@@ -49,8 +49,8 @@
    *                       the 0-th (1.0-th) quantile if the left (right) tail is not allowed.
    */
   public Empirical(boolean exceedMinimum, boolean exceedMaximum, int samples, double... ecdf) {
-    Preconditions.checkArgument(ecdf.length % 2 == 0);
-    Preconditions.checkArgument(samples >= 3);
+    Preconditions.checkArgument(ecdf.length % 2 == 0, "ecdf must have an even count of values");
+    Preconditions.checkArgument(samples >= 3, "Sample size must be >= 3");
 
     // if we can't exceed the observed bounds, then we have to be given the bounds.
     Preconditions.checkArgument(exceedMinimum || ecdf[0] == 0);
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/solver/ConjugateGradientSolver.java mahout/math/src/main/java/org/apache/mahout/math/solver/ConjugateGradientSolver.java
--- mahout/math/src/main/java/org/apache/mahout/math/solver/ConjugateGradientSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/solver/ConjugateGradientSolver.java	2014-03-29 01:03:14.000000000 -0700
@@ -33,7 +33,7 @@
  * <p>Conjugate gradient requires the matrix A in the linear system Ax = b to be symmetric and positive
  * definite. For convenience, this implementation allows the input matrix to be be non-symmetric, in
  * which case the system A'Ax = b is solved. Because this requires only one pass through the matrix A, it
- * is faster than explictly computing A'A, then passing the results to the solver.
+ * is faster than explicitly computing A'A, then passing the results to the solver.
  * 
  * <p>For inputs that may be ill conditioned (often the case for highly sparse input), this solver
  * also accepts a parameter, lambda, which adds a scaled identity to the matrix A, solving the system
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/solver/EigenDecomposition.java mahout/math/src/main/java/org/apache/mahout/math/solver/EigenDecomposition.java
--- mahout/math/src/main/java/org/apache/mahout/math/solver/EigenDecomposition.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/solver/EigenDecomposition.java	2014-03-29 01:03:14.000000000 -0700
@@ -326,7 +326,7 @@
       int k = i;
       double p = d.getQuick(i);
       for (int j = i + 1; j < n; j++) {
-        if (d.getQuick(j) < p) {
+        if (d.getQuick(j) > p) {
           k = j;
           p = d.getQuick(j);
         }
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/ssvd/EigenSolverWrapper.java mahout/math/src/main/java/org/apache/mahout/math/ssvd/EigenSolverWrapper.java
--- mahout/math/src/main/java/org/apache/mahout/math/ssvd/EigenSolverWrapper.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/ssvd/EigenSolverWrapper.java	1969-12-31 16:00:00.000000000 -0800
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.mahout.math.ssvd;
-
-import org.apache.commons.math3.linear.Array2DRowRealMatrix;
-import org.apache.commons.math3.linear.EigenDecomposition;
-import org.apache.commons.math3.linear.RealMatrix;
-
-/**
- * wraps appropriate eigen solver for BBt matrix. Can be either colt or apache
- * commons math.
- * <P>
- * 
- * At the moment it is apache commons math which is only in mahout-math
- * dependencies.
- * <P>
- * 
- * I will be happy to switch this to Colt eigensolver if it is proven reliable
- * (i experience internal errors and unsorted singular values at some point).
- * 
- * But for now commons-math seems to be more reliable.
- * 
- * 
- */
-public class EigenSolverWrapper {
-
-  private final double[] eigenvalues;
-  private final double[][] uHat;
-
-  public EigenSolverWrapper(double[][] bbt) {
-    int dim = bbt.length;
-    EigenDecomposition evd2 = new EigenDecomposition(new Array2DRowRealMatrix(bbt));
-    eigenvalues = evd2.getRealEigenvalues();
-    RealMatrix uHatrm = evd2.getV();
-    uHat = new double[dim][];
-    for (int i = 0; i < dim; i++) {
-      uHat[i] = uHatrm.getRow(i);
-    }
-  }
-
-  public double[][] getUHat() {
-    return uHat;
-  }
-
-  public double[] getEigenValues() {
-    return eigenvalues;
-  }
-
-}
diff -uNar -x .git mahout/math/src/main/java/org/apache/mahout/math/stats/OnlineSummarizer.java mahout/math/src/main/java/org/apache/mahout/math/stats/OnlineSummarizer.java
--- mahout/math/src/main/java/org/apache/mahout/math/stats/OnlineSummarizer.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java/org/apache/mahout/math/stats/OnlineSummarizer.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,8 @@
 
 package org.apache.mahout.math.stats;
 
-import org.apache.mahout.math.list.DoubleArrayList;
+import com.tdunning.math.stats.TDigest;
+import com.tdunning.math.stats.TreeDigest;
 
 /**
  * Computes on-line estimates of mean, variance and all five quartiles (notably including the
@@ -40,13 +41,7 @@
  */
 public class OnlineSummarizer {
 
-  private boolean sorted = true;
-
-  // the first several samples are kept so we can boot-strap our estimates cleanly
-  private DoubleArrayList starter = new DoubleArrayList(100);
-
-  // quartile estimates
-  private final double[] q = new double[5];
+  private TDigest quantiles = new TreeDigest(100);
 
   // mean and variance estimates
   private double mean;
@@ -56,43 +51,13 @@
   private int n;
 
   public void add(double sample) {
-    sorted = false;
-
     n++;
     double oldMean = mean;
     mean += (sample - mean) / n;
     double diff = (sample - mean) * (sample - oldMean);
     variance += (diff - variance) / n;
 
-    if (n < 100) {
-      starter.add(sample);
-    } else if (n == 100 && starter != null) {
-      // when we first reach 100 elements, we switch to incremental operation
-      starter.add(sample);
-      for (int i = 0; i <= 4; i++) {
-        q[i] = getQuartile(i);
-      }
-      // this signals any invocations of getQuartile at exactly 100 elements that we have
-      // already switched to incremental operation
-      starter = null;
-    } else {
-      // n >= 100 && starter == null
-      q[0] = Math.min(sample, q[0]);
-      q[4] = Math.max(sample, q[4]);
-
-      double rate = 2 * (q[3] - q[1]) / n;
-      q[1] += (Math.signum(sample - q[1]) - 0.5) * rate;
-      q[2] += Math.signum(sample - q[2]) * rate;
-      q[3] += (Math.signum(sample - q[3]) + 0.5) * rate;
-
-      if (q[1] < q[0]) {
-        q[1] = q[0];
-      }
-
-      if (q[3] > q[4]) {
-        q[3] = q[4];
-      }
-    }
+    quantiles.add(sample);
   }
 
   public int getCount() {
@@ -111,48 +76,16 @@
     return getQuartile(0);
   }
 
-  private void sort() {
-    if (!sorted && starter != null) {
-      starter.sort();
-      sorted = true;
-    }
-  }
-
   public double getMax() {
     return getQuartile(4);
   }
 
   public double getQuartile(int i) {
-    if (n > 100 || starter == null) {
-      return q[i];
-    } else {
-      sort();
-      switch (i) {
-        case 0:
-          if (n == 0) {
-            throw new IllegalArgumentException("Must have at least one sample to estimate minimum value");
-          }
-          return starter.get(0);
-        case 1:
-        case 2:
-        case 3:
-          if (n >= 2) {
-            double x = i * (n - 1) / 4.0;
-            int k = (int) Math.floor(x);
-            double u = x - k;
-            return starter.get(k) * (1 - u) + starter.get(k + 1) * u;
-          } else {
-            throw new IllegalArgumentException("Must have at least two samples to estimate quartiles");
-          }
-        case 4:
-          if (n == 0) {
-            throw new IllegalArgumentException("Must have at least one sample to estimate maximum value");
-          }
-          return starter.get(starter.size() - 1);
-        default:
-          throw new IllegalArgumentException("Quartile number must be in the range [0..4] not " + i);
-      }
-    }
+    return quantiles.quantile(0.25 * i);
+  }
+
+  public double quantile(double q) {
+    return quantiles.quantile(q);
   }
 
   public double getMedian() {
diff -uNar -x .git mahout/math/src/main/java-templates/org/apache/mahout/math/map/AbstractObjectValueTypeMap.java.t mahout/math/src/main/java-templates/org/apache/mahout/math/map/AbstractObjectValueTypeMap.java.t
--- mahout/math/src/main/java-templates/org/apache/mahout/math/map/AbstractObjectValueTypeMap.java.t	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/main/java-templates/org/apache/mahout/math/map/AbstractObjectValueTypeMap.java.t	2014-03-29 01:03:14.000000000 -0700
@@ -155,7 +155,7 @@
   public int hashCode() {
     final int[] buf = new int[size()];
     forEachPair(
-      new Object${valueTypeCap}Procedure() {
+      new Object${valueTypeCap}Procedure<T>() {
         int i = 0;
 
         @Override
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/AbstractVectorTest.java mahout/math/src/test/java/org/apache/mahout/math/AbstractVectorTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/AbstractVectorTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/AbstractVectorTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,8 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math;
 
 import java.util.Iterator;
 import java.util.Random;
 
+import com.google.common.collect.Iterables;
 import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.math.Vector.Element;
 import org.apache.mahout.math.function.Functions;
@@ -593,11 +611,7 @@
       }
     }
 
-    int nonZeroIterated = 0;
-
-    for (Element ignored : v0.nonZeroes()) {
-      nonZeroIterated++;
-    }
+    int nonZeroIterated = Iterables.size(v0.nonZeroes());
     assertEquals(20, elements);
     assertEquals(v0.size(), elements);
     assertEquals(nonZeroIterated, nonZero);
@@ -623,4 +637,22 @@
       }
     }
   }
+
+
+  public void testToString() {
+    Vector w;
+
+    w = generateTestVector(20);
+    w.set(0, 1.1);
+    w.set(13, 100500.);
+    w.set(19, 3.141592);
+    assertEquals("{0:1.1,13:100500.0,19:3.141592}", w.toString());
+
+    w = generateTestVector(12);
+    w.set(10, 0.1);
+    assertEquals("{10:0.1}", w.toString());
+
+    w = generateTestVector(12);
+    assertEquals("{}", w.toString());
+  }
 }
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/DenseSymmetricTest.java mahout/math/src/test/java/org/apache/mahout/math/DenseSymmetricTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/DenseSymmetricTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/test/java/org/apache/mahout/math/DenseSymmetricTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+import org.apache.mahout.math.function.Functions;
+import org.apache.mahout.math.solver.EigenDecomposition;
+import org.junit.Test;
+
+public class DenseSymmetricTest extends MahoutTestCase {
+  @Test
+  public void testBasics() {
+    Matrix a = new DenseSymmetricMatrix(new double[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, false);
+    System.out.println(a.toString());
+
+    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[]{1, 5, 8, 10})).norm(1), 1.0e-10);
+    assertEquals(0, a.viewPart(0, 3, 1, 3).viewDiagonal().minus(
+        new DenseVector(new double[]{2, 6, 9})).norm(1), 1.0e-10);
+    assertEquals(4, a.get(0, 3), 1.0e-10);
+    System.out.println(a);
+
+    Matrix m = new DenseMatrix(4, 4).assign(a);
+    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+    System.out.println(m);
+
+    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(
+        Functions.PLUS, Functions.ABS), 1.0e-10);
+
+    System.out.println(a.plus(a));
+    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+  }
+
+  @Test
+  public void testEigen() {
+    Matrix a = new DenseSymmetricMatrix(new double[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, false);
+    Matrix b = new DenseMatrix(a.numRows(), a.numCols());
+    b.assign(a);
+
+    assertEquals(0, a.minus(b).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+
+    EigenDecomposition edA = new EigenDecomposition(a);
+    EigenDecomposition edB = new EigenDecomposition(b);
+
+    System.out.println(edA.getV());
+
+    assertEquals(0, edA.getV().minus(edB.getV()).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+    assertEquals(0, edA.getRealEigenvalues().minus(edA.getRealEigenvalues()).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+
+  }
+
+}
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/DiagonalMatrixTest.java mahout/math/src/test/java/org/apache/mahout/math/DiagonalMatrixTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/DiagonalMatrixTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/test/java/org/apache/mahout/math/DiagonalMatrixTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+import org.apache.mahout.math.function.Functions;
+import org.junit.Test;
+
+public class DiagonalMatrixTest extends MahoutTestCase {
+  @Test
+  public void testBasics() {
+    DiagonalMatrix a = new DiagonalMatrix(new double[]{1, 2, 3, 4});
+
+    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[]{1, 2, 3, 4})).norm(1), 1.0e-10);
+    assertEquals(0, a.viewPart(0, 3, 0, 3).viewDiagonal().minus(
+      new DenseVector(new double[]{1, 2, 3})).norm(1), 1.0e-10);
+
+    assertEquals(4, a.get(3, 3), 1.0e-10);
+
+    Matrix m = new DenseMatrix(4, 4);
+    m.assign(a);
+
+    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+
+    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(
+      Functions.PLUS, Functions.ABS), 1.0e-10);
+    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+
+    m = new DenseMatrix(new double[][]{{1, 2, 3, 4}, {5, 6, 7, 8}});
+
+    assertEquals(100, a.timesLeft(m).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+    assertEquals(100, a.times(m.transpose()).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+  }
+
+}
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/FileBasedMatrixTest.java mahout/math/src/test/java/org/apache/mahout/math/FileBasedMatrixTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/FileBasedMatrixTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/FileBasedMatrixTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -43,7 +43,7 @@
     for (int i = 0; i < 1000; i++) {
       m0.set(gen.nextInt(ROWS), gen.nextInt(COLUMNS), matrixValue(i));
     }
-    File f = File.createTempFile("foo", ".m");
+    File f = File.createTempFile("foo", ".m", getTestTempDir());
     f.deleteOnExit();
     System.out.printf("Starting to write to %s\n", f.getAbsolutePath());
     FileBasedMatrix.writeMatrix(f, m0);
@@ -66,7 +66,7 @@
 
   @Test
   public void testSetData() throws IOException {
-    File f = File.createTempFile("matrix", ".m");
+    File f = File.createTempFile("matrix", ".m", getTestTempDir());
     f.deleteOnExit();
 
     Matrix m0 = new DenseMatrix(100000, 30);
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/FileBasedSparseBinaryMatrixTest.java mahout/math/src/test/java/org/apache/mahout/math/FileBasedSparseBinaryMatrixTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/FileBasedSparseBinaryMatrixTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/FileBasedSparseBinaryMatrixTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -67,7 +67,7 @@
   @Test
   public void testSetData() throws IOException {
 
-    File f = File.createTempFile("matrix", ".m");
+    File f = File.createTempFile("matrix", ".m", getTestTempDir());
     f.deleteOnExit();
 
     Random gen = RandomUtils.getRandom();
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/FunctionTest.java mahout/math/src/test/java/org/apache/mahout/math/FunctionTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/FunctionTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/FunctionTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math;
 
 import java.lang.reflect.Field;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java mahout/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java
--- mahout/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/MahoutTestCase.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,15 +21,27 @@
 import java.io.FileFilter;
 import java.io.IOException;
 
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakAction;
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakZombies;
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+import org.apache.lucene.util.TimeUnits;
 import org.apache.mahout.common.RandomUtils;
 import org.junit.After;
-import org.junit.Assert;
 import org.junit.Before;
 
+import com.carrotsearch.randomizedtesting.RandomizedTest;
+
 /**
  * Superclass of all Mahout test cases.
  */
-public abstract class MahoutTestCase extends Assert {
+@ThreadLeakScope(ThreadLeakScope.Scope.SUITE)
+@ThreadLeakAction({ThreadLeakAction.Action.WARN, ThreadLeakAction.Action.INTERRUPT})
+@ThreadLeakLingering(linger = 20000) // Wait a bit longer for leaked threads to die.
+@ThreadLeakZombies(ThreadLeakZombies.Consequence.IGNORE_REMAINING_TESTS)
+@TimeoutSuite(millis = 2 * TimeUnits.HOUR)
+public abstract class MahoutTestCase extends RandomizedTest {
 
   /** "Close enough" value for floating-point comparisons. */
   public static final double EPSILON = 0.000001;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/MatricesTest.java mahout/math/src/test/java/org/apache/mahout/math/MatricesTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/MatricesTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/test/java/org/apache/mahout/math/MatricesTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,106 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+import org.apache.mahout.math.function.Functions;
+import org.apache.mahout.math.function.IntIntFunction;
+import org.junit.Test;
+
+public class MatricesTest extends MahoutTestCase {
+
+  @Test
+  public void testFunctionalView() {
+    Matrix m = Matrices.functionalMatrixView(5, 6, new IntIntFunction() {
+      @Override
+      public double apply(int row, int col) {
+        assertTrue(row < 5);
+        assertTrue(col < 6);
+        return row + col;
+      }
+    });
+
+    // row-wise sums are 15, 15+ 6, 15 +12, 15+18, 15+24
+    // so total sum is 1/2*(15+15+24)*5 =27*5 = 135
+    assertEquals(135, m.aggregate(Functions.PLUS, Functions.IDENTITY), 1e-10);
+  }
+
+  @Test
+  public void testTransposeView() {
+
+    Matrix m = Matrices.gaussianView(5, 6, 1234L);
+    Matrix controlM = new DenseMatrix(5, 6).assign(m);
+
+    System.out.printf("M=\n%s\n", m);
+    System.out.printf("controlM=\n%s\n", controlM);
+
+    Matrix mtm = Matrices.transposedView(m).times(m);
+    Matrix controlMtm = controlM.transpose().times(controlM);
+
+    System.out.printf("M'M=\n%s\n", mtm);
+
+    Matrix diff = mtm.minus(controlMtm);
+
+    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);
+
+  }
+
+  @Test
+  public void testUniformView() {
+    Matrix m1 = Matrices.uniformView(5, 6, 1234);
+    Matrix m2 = Matrices.uniformView(5, 6, 1234);
+
+    for (int row = 0; row < m1.numRows(); row++) {
+      for (int col = 0; col < m1.numCols(); col++) {
+        assertTrue(m1.getQuick(row, col) >= 0.0);
+        assertTrue(m1.getQuick(row, col) < 1.0);
+      }
+    }
+
+    Matrix diff = m1.minus(m2);
+
+    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);
+  }
+
+  @Test
+  public void testSymmetricUniformView() {
+    Matrix m1 = Matrices.symmetricUniformView(5, 6, 1234);
+    Matrix m2 = Matrices.symmetricUniformView(5, 6, 1234);
+
+    for (int row = 0; row < m1.numRows(); row++) {
+      for (int col = 0; col < m1.numCols(); col++) {
+        assertTrue(m1.getQuick(row, col) >= -1.0);
+        assertTrue(m1.getQuick(row, col) < 1.0);
+      }
+    }
+
+    Matrix diff = m1.minus(m2);
+
+    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);
+  }
+
+  @Test
+  public void testGaussianView() {
+    Matrix m1 = Matrices.gaussianView(5, 6, 1234);
+    Matrix m2 = Matrices.gaussianView(5, 6, 1234);
+
+    Matrix diff = m1.minus(m2);
+
+    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);
+  }
+
+}
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/MatrixVectorViewTest.java mahout/math/src/test/java/org/apache/mahout/math/MatrixVectorViewTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/MatrixVectorViewTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/MatrixVectorViewTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,7 @@
 
 package org.apache.mahout.math;
 
+import org.apache.mahout.math.function.Functions;
 import org.junit.Test;
 
 public class MatrixVectorViewTest extends MahoutTestCase {
@@ -34,4 +35,24 @@
     assertEquals(matrix.numRows(), outerProduct.numRows());
     assertEquals(matrix.numRows(), outerProduct.numCols());
   }
+
+  /**
+   * Test for out of range column or row access.
+   */
+  @Test
+  public void testIndexRange() {
+    Matrix m = new DenseMatrix(20, 30).assign(Functions.random());
+    try {
+      m.viewColumn(30);
+      fail("Should have thrown exception");
+    } catch (IllegalArgumentException e) {
+      assertTrue(e.getMessage().startsWith("Index 30 is outside allowable"));
+    }
+    try {
+      m.viewRow(20);
+      fail("Should have thrown exception");
+    } catch (IllegalArgumentException e) {
+      assertTrue(e.getMessage().startsWith("Index 20 is outside allowable"));
+    }
+  }
 }
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/MurmurHash3Test.java mahout/math/src/test/java/org/apache/mahout/math/MurmurHash3Test.java
--- mahout/math/src/test/java/org/apache/mahout/math/MurmurHash3Test.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/MurmurHash3Test.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,5 +1,12 @@
-/**
- * Public domain, just like the MurmurHash3 code.
+/*
+ *  This code is public domain.
+ *
+ *  The MurmurHash3 algorithm was created by Austin Appleby and put into the public domain.
+ *  See http://code.google.com/p/smhasher/
+ *
+ *  This java port was authored by
+ *  Yonik Seeley and was placed into the public domain per
+ *  https://github.com/yonik/java_util/blob/master/src/util/hash/MurmurHash3.java.
  */
 
 package org.apache.mahout.math;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/TestDenseVector.java mahout/math/src/test/java/org/apache/mahout/math/TestDenseVector.java
--- mahout/math/src/test/java/org/apache/mahout/math/TestDenseVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/TestDenseVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -18,6 +18,7 @@
 package org.apache.mahout.math;
 
 import org.apache.mahout.math.function.Functions;
+import org.junit.Test;
 
 public final class TestDenseVector extends AbstractVectorTest<DenseVector> {
 
@@ -37,4 +38,10 @@
     r.assign(Functions.random());
     return r;
   }
+
+  @Override
+  @Test
+  public void testToString() {
+    super.testToString();
+  }
 }
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/TestRandomAccessSparseVector.java mahout/math/src/test/java/org/apache/mahout/math/TestRandomAccessSparseVector.java
--- mahout/math/src/test/java/org/apache/mahout/math/TestRandomAccessSparseVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/TestRandomAccessSparseVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,7 +17,9 @@
 
 package org.apache.mahout.math;
 
+import com.google.common.base.Splitter;
 import org.apache.mahout.common.RandomUtils;
+import org.junit.Test;
 
 import java.util.Random;
 
@@ -39,4 +41,25 @@
     return r;
   }
 
+  @Override
+  @Test
+  public void testToString() {
+    Vector w;
+    w = generateTestVector(20);
+    w.set(0, 1.1);
+    w.set(13, 100500.);
+    w.set(19, 3.141592);
+
+    for (String token : Splitter.on(',').split(w.toString().substring(1, w.toString().length() - 2))) {
+      String[] tokens = token.split(":");
+      assertEquals(Double.parseDouble(tokens[1]), w.get(Integer.parseInt(tokens[0])), 0.0);
+    }
+
+    w = generateTestVector(12);
+    w.set(10, 0.1);
+    assertEquals("{10:0.1}", w.toString());
+
+    w = generateTestVector(12);
+    assertEquals("{}", w.toString());
+  }
 }
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/TestSequentialAccessSparseVector.java mahout/math/src/test/java/org/apache/mahout/math/TestSequentialAccessSparseVector.java
--- mahout/math/src/test/java/org/apache/mahout/math/TestSequentialAccessSparseVector.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/TestSequentialAccessSparseVector.java	2014-03-29 01:03:14.000000000 -0700
@@ -52,4 +52,11 @@
     }
     return r;
   }
+
+
+  @Override
+  @Test
+  public void testToString() {
+    super.testToString();
+  }
 }
\ No newline at end of file
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/TestSingularValueDecomposition.java mahout/math/src/test/java/org/apache/mahout/math/TestSingularValueDecomposition.java
--- mahout/math/src/test/java/org/apache/mahout/math/TestSingularValueDecomposition.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/TestSingularValueDecomposition.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,10 +17,19 @@
 
 package org.apache.mahout.math;
 
+import com.google.common.base.Charsets;
+import com.google.common.base.Splitter;
+import com.google.common.collect.Iterables;
+import com.google.common.io.Resources;
 import org.apache.mahout.common.RandomUtils;
+import org.apache.mahout.math.function.Functions;
 import org.junit.Test;
 
+import java.io.IOException;
+import java.util.List;
 import java.util.Random;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeoutException;
 
 //To launch this test only : mvn test -Dtest=org.apache.mahout.math.TestSingularValueDecomposition
 public final class TestSingularValueDecomposition extends MahoutTestCase {
@@ -225,6 +234,35 @@
     // replace 1.0e-15 with 1.5e-15
     assertEquals(3.0, svd.cond(), 1.5e-15);
   }
+
+  @Test
+  public void testSvdHang() throws IOException, InterruptedException, ExecutionException, TimeoutException {
+    System.out.printf("starting hanging-svd\n");
+    final Matrix m = readTsv("hanging-svd.tsv");
+    SingularValueDecomposition svd = new SingularValueDecomposition(m);
+    assertEquals(0, m.minus(svd.getU().times(svd.getS()).times(svd.getV().transpose())).aggregate(Functions.PLUS, Functions.ABS), 1e-10);
+    System.out.printf("No hang\n");
+  }
+
+  Matrix readTsv(String name) throws IOException {
+    Splitter onTab = Splitter.on("\t");
+    List<String> lines = Resources.readLines((Resources.getResource(name)), Charsets.UTF_8);
+    int rows = lines.size();
+    int columns = Iterables.size(onTab.split(lines.get(0)));
+    Matrix r = new DenseMatrix(rows, columns);
+    int row = 0;
+    for (String line : lines) {
+      Iterable<String> values = onTab.split(line);
+      int column = 0;
+      for (String value : values) {
+        r.set(row, column, Double.parseDouble(value));
+        column++;
+      }
+      row++;
+    }
+    return r;
+  }
+
   
   private static Matrix createTestMatrix(Random r, int rows, int columns, double[] singularValues) {
     Matrix u = createOrthogonalMatrix(r, rows);
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/TestSparseMatrix.java mahout/math/src/test/java/org/apache/mahout/math/TestSparseMatrix.java
--- mahout/math/src/test/java/org/apache/mahout/math/TestSparseMatrix.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/TestSparseMatrix.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,10 @@
 
 package org.apache.mahout.math;
 
+import java.util.Iterator;
+
+import org.junit.Test;
+
 public final class TestSparseMatrix extends MatrixTest {
 
   @Override
@@ -29,4 +33,39 @@
     }
     return matrix;
   }
+
+  /** Test copy method of sparse matrices which have empty non-initialized rows */
+  @Test
+  public void testSparseCopy() {
+    SparseMatrix matrix = createSparseMatrixWithEmptyRow();
+    Matrix copy = matrix.clone();
+
+    assertSame("wrong class", copy.getClass(), matrix.getClass());
+
+    SparseMatrix castedCopy = (SparseMatrix) copy;
+        
+    Iterator<MatrixSlice> originalSlices = matrix.iterator();
+    Iterator<MatrixSlice> copySlices = castedCopy.iterator();
+    
+    while (originalSlices.hasNext() && copySlices.hasNext()) {
+      MatrixSlice originalSlice = originalSlices.next();
+      MatrixSlice copySlice = copySlices.next();
+      assertEquals("Wrong row indices.", originalSlice.index(), copySlice.index());
+      assertEquals("Slices are not equal.", originalSlice, copySlice);
+    }
+    
+    assertSame("Number of rows of original and copy are not equal.", originalSlices.hasNext(), copySlices.hasNext());
+  }
+  
+  /**
+   * @return Sparse matrix whose last row is empty. Since no entry is set in the last row, there is no vector instance
+   * initialized in the underlying map.
+   */
+  private SparseMatrix createSparseMatrixWithEmptyRow() {
+    SparseMatrix result = new SparseMatrix(3, 3);
+    result.setQuick(0, 0, 1);
+    result.setQuick(1, 1 ,1);
+    result.setQuick(1, 2, 1);
+    return result;
+  }
 }
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/UpperTriangularTest.java mahout/math/src/test/java/org/apache/mahout/math/UpperTriangularTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/UpperTriangularTest.java	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/test/java/org/apache/mahout/math/UpperTriangularTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math;
+
+import org.apache.mahout.math.function.Functions;
+import org.junit.Test;
+
+public class UpperTriangularTest extends MahoutTestCase {
+  @Test
+  public void testBasics() {
+    Matrix a = new UpperTriangular(new double[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, false);
+    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[]{1, 5, 8, 10})).norm(1), 1.0e-10);
+    assertEquals(0, a.viewPart(0, 3, 1, 3).viewDiagonal().minus(
+        new DenseVector(new double[]{2, 6, 9})).norm(1), 1.0e-10);
+    assertEquals(4, a.get(0, 3), 1.0e-10);
+    print(a);
+    Matrix m = new DenseMatrix(4, 4).assign(a);
+    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+    print(m);
+
+    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(
+        Functions.PLUS, Functions.ABS), 1.0e-10);
+    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);
+  }
+
+  private static void print(Matrix m) {
+    for (int i = 0; i < m.rowSize(); i++) {
+      for (int j = 0; j < m.columnSize(); j++) {
+        if (Math.abs(m.get(i, j)) > 1.0e-10) {
+          System.out.printf("%10.3f ", m.get(i, j));
+        } else {
+          System.out.printf("%10s ", (i + j) % 3 == 0 ? "." : "");
+        }
+      }
+      System.out.printf("\n");
+    }
+    System.out.printf("\n");
+  }
+}
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAggregateTest.java mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAggregateTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAggregateTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAggregateTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math;
 
 import static org.junit.Assert.assertEquals;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAssignTest.java mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAssignTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAssignTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/VectorBinaryAssignTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math;
 
 import java.util.Collection;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolverTest.java mahout/math/src/test/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolverTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolverTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolverTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,9 @@
 
 package org.apache.mahout.math.als;
 
+import java.util.Arrays;
+
+import org.apache.mahout.math.DenseMatrix;
 import org.apache.mahout.math.DenseVector;
 import org.apache.mahout.math.MahoutTestCase;
 import org.apache.mahout.math.Matrix;
@@ -24,13 +27,71 @@
 import org.apache.mahout.math.SequentialAccessSparseVector;
 import org.apache.mahout.math.SparseMatrix;
 import org.apache.mahout.math.Vector;
+import org.apache.mahout.math.map.OpenIntObjectHashMap;
 import org.junit.Test;
 
-import java.util.Arrays;
-
 public class AlternatingLeastSquaresSolverTest extends MahoutTestCase {
 
   @Test
+  public void testYtY() {
+      
+      double[][] testMatrix = new double[][] {
+          new double[] { 1, 2, 3, 4, 5 },
+          new double[] { 1, 2, 3, 4, 5 },
+          new double[] { 1, 2, 3, 4, 5 },
+          new double[] { 1, 2, 3, 4, 5 },
+          new double[] { 1, 2, 3, 4, 5 }};
+      
+      double[][] testMatrix2 = new double[][] {
+          new double[] { 1, 2, 3, 4, 5, 6 },
+          new double[] { 5, 4, 3, 2, 1, 7 },
+          new double[] { 1, 2, 3, 4, 5, 8 },
+          new double[] { 1, 2, 3, 4, 5, 8 },
+          new double[] { 11, 12, 13, 20, 27, 8 }};
+      
+      double[][][] testData = new double[][][] {
+          testMatrix,
+          testMatrix2 };
+      
+    for (int i = 0; i < testData.length; i++) {
+      Matrix matrixToTest = new DenseMatrix(testData[i]);
+      
+      //test for race conditions by trying a few times
+      for (int j = 0; j < 100; j++) {
+        validateYtY(matrixToTest, 4);
+      }
+      
+      //one thread @ a time test
+      validateYtY(matrixToTest, 1);
+    }
+    
+  }
+
+  private void validateYtY(Matrix matrixToTest, int numThreads) {
+
+    OpenIntObjectHashMap<Vector> matrixToTestAsRowVectors = asRowVectors(matrixToTest);
+    ImplicitFeedbackAlternatingLeastSquaresSolver solver = new ImplicitFeedbackAlternatingLeastSquaresSolver(
+        matrixToTest.columnSize(), 1, 1, matrixToTestAsRowVectors, numThreads);
+
+    Matrix yTy = matrixToTest.transpose().times(matrixToTest);
+    Matrix shouldMatchyTy = solver.getYtransposeY(matrixToTestAsRowVectors);
+    
+    for (int row = 0; row < yTy.rowSize(); row++) {
+      for (int column = 0; column < yTy.columnSize(); column++) {
+        assertEquals(yTy.getQuick(row, column), shouldMatchyTy.getQuick(row, column), 0);
+      }
+    }
+  }
+
+  private OpenIntObjectHashMap<Vector> asRowVectors(Matrix matrix) {
+    OpenIntObjectHashMap<Vector> rows = new OpenIntObjectHashMap<Vector>();
+    for (int row = 0; row < matrix.numRows(); row++) {
+      rows.put(row, matrix.viewRow(row).clone());
+    }
+    return rows;
+  }
+  
+  @Test
   public void addLambdaTimesNuiTimesE() {
     int nui = 5;
     double lambda = 0.2;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/decomposer/hebbian/TestHebbianSolver.java mahout/math/src/test/java/org/apache/mahout/math/decomposer/hebbian/TestHebbianSolver.java
--- mahout/math/src/test/java/org/apache/mahout/math/decomposer/hebbian/TestHebbianSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/decomposer/hebbian/TestHebbianSolver.java	2014-03-29 01:03:14.000000000 -0700
@@ -21,7 +21,6 @@
 import org.apache.mahout.math.Matrix;
 
 import org.apache.mahout.math.decomposer.AsyncEigenVerifier;
-import org.apache.mahout.math.decomposer.SingularVectorVerifier;
 import org.apache.mahout.math.decomposer.SolverTest;
 import org.junit.Test;
 
@@ -50,7 +49,7 @@
                                 int desiredRank,
                                 TrainingState state) {
     HebbianUpdater updater = new HebbianUpdater();
-    SingularVectorVerifier verifier = new AsyncEigenVerifier();
+    AsyncEigenVerifier verifier = new AsyncEigenVerifier();
     HebbianSolver solver = new HebbianSolver(updater,
                                              verifier,
                                              convergence,
@@ -62,6 +61,7 @@
     state.setCurrentEigenValues(finalState.getCurrentEigenValues());
     long time = 0L;
     time += System.nanoTime() - start;
+    verifier.close();
     assertEquals(state.getCurrentEigens().numRows(), desiredRank);
     return time / 1000000L;
   }
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/decomposer/lanczos/TestLanczosSolver.java mahout/math/src/test/java/org/apache/mahout/math/decomposer/lanczos/TestLanczosSolver.java
--- mahout/math/src/test/java/org/apache/mahout/math/decomposer/lanczos/TestLanczosSolver.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/decomposer/lanczos/TestLanczosSolver.java	2014-03-29 01:03:14.000000000 -0700
@@ -49,12 +49,12 @@
 
     float fractionOfEigensExpectedGood = 0.6f;
     for (int i = 0; i < fractionOfEigensExpectedGood * desiredRank; i++) {
-      double s = state.getSingularValue(desiredRank - i - 1);
-      double e = eigenvalues.get(eigenvalues.size() - i - 1);
+      double s = state.getSingularValue(i);
+      double e = eigenvalues.get(i);
       log.info("{} : L = {}, E = {}", i, s, e);
       assertTrue("Singular value differs from eigenvalue", Math.abs((s-e)/e) < ERROR_TOLERANCE);
       Vector v = state.getRightSingularVector(i);
-      Vector v2 = decomposition.getV().viewColumn(eigenvalues.size() - i - 1);
+      Vector v2 = decomposition.getV().viewColumn(i);
       double error = 1 - Math.abs(v.dot(v2)/(v.norm(2) * v2.norm(2)));
       log.info("error: {}", error);
       assertTrue(i + ": 1 - cosAngle = " + error, error < ERROR_TOLERANCE);
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/randomized/RandomBlasting.java mahout/math/src/test/java/org/apache/mahout/math/randomized/RandomBlasting.java
--- mahout/math/src/test/java/org/apache/mahout/math/randomized/RandomBlasting.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/randomized/RandomBlasting.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,7 +1,23 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.randomized;
 
 import java.lang.reflect.Field;
-import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/set/HashUtilsTest.java mahout/math/src/test/java/org/apache/mahout/math/set/HashUtilsTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/set/HashUtilsTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/set/HashUtilsTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -1,3 +1,20 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.mahout.math.set;
 
 import com.google.common.collect.HashMultiset;
diff -uNar -x .git mahout/math/src/test/java/org/apache/mahout/math/stats/OnlineSummarizerTest.java mahout/math/src/test/java/org/apache/mahout/math/stats/OnlineSummarizerTest.java
--- mahout/math/src/test/java/org/apache/mahout/math/stats/OnlineSummarizerTest.java	2014-03-29 01:04:48.000000000 -0700
+++ mahout/math/src/test/java/org/apache/mahout/math/stats/OnlineSummarizerTest.java	2014-03-29 01:03:14.000000000 -0700
@@ -17,102 +17,92 @@
 
 package org.apache.mahout.math.stats;
 
+import org.apache.mahout.common.RandomUtils;
 import org.apache.mahout.math.MahoutTestCase;
+import org.apache.mahout.math.jet.random.AbstractContinousDistribution;
+import org.apache.mahout.math.jet.random.Gamma;
 import org.junit.Test;
 
+import java.util.Arrays;
 import java.util.Random;
 
 public final class OnlineSummarizerTest extends MahoutTestCase {
 
   @Test
-  public void testCount() {
-    OnlineSummarizer x = new OnlineSummarizer();
-    assertEquals(0, x.getCount());
-    x.add(1);
-    assertEquals(1, x.getCount());
-
-    for (int i = 2; i < 110; i++) {
-      x.add(i);
-      assertEquals(i, x.getCount());
+  public void testStats() {
+   /**
+     the reference limits here were derived using a numerical simulation where I took
+     10,000 samples from the distribution in question and computed the stats from that
+     sample to get min, 25%-ile, median and so on. I did this 1000 times to get 5% and
+     95% confidence limits for those values.
+   */
+
+    //symmetrical, well behaved
+    System.out.printf("normal\n");
+    check(normal(10000));
+
+    //asymmetrical, well behaved. The range for the maximum was fudged slightly to all this to pass.
+    System.out.printf("exp\n");
+    check(exp(10000));
+
+    //asymmetrical, wacko distribution where mean/median is about 200
+    System.out.printf("gamma\n");
+    check(gamma(10000, 0.1));
+  }
+
+  private static void check(double[] samples) {
+    OnlineSummarizer s = new OnlineSummarizer();
+    double mean = 0;
+    double sd = 0;
+    int n = 1;
+    for (double x : samples) {
+      s.add(x);
+      double old = mean;
+      mean += (x - mean) / n;
+      sd += (x - old) * (x - mean);
+      n++;
     }
-  }
+    sd = Math.sqrt(sd / samples.length);
 
-  @Test
-  public void testStats() {
-    // the reference limits here were derived using a numerical simulation where I took
-    // 10,000 samples from the distribution in question and computed the stats from that
-    // sample to get min, 25%-ile, median and so on.  I did this 1000 times to get 5% and
-    // 95% confidence limits for those values.
-
-    // symmetrical, well behaved
-    check(normal(10000),
-            -4.417246, -3.419809,
-            -0.6972919, -0.6519899,
-            -0.02056658, 0.02176474,
-            0.6503866, 0.6983311,
-            3.419809, 4.417246,
-            -0.01515753, 0.01592942,
-            0.988395, 1.011883);
-
-    // asymmetrical, well behaved.  The range for the maximum was fudged slightly to all this to pass.
-    check(exp(10000),
-            4.317969e-06, 3.278763e-04,
-            0.2783866, 0.298,
-            0.6765024, 0.7109463,
-            1.356929, 1.414761,
-            8, 13,
-            0.983805, 1.015920,
-            0.977162, 1.022093
-    );
-
-    // asymmetrical, wacko distribution where mean/median > 10^28
-    // TODO need more work here
-//    check(gamma(10000, 3),
-//            0, 0,                                             // minimum
-//            0, 6.26363334269806e-58,                          // 25th %-ile
-//            8.62261497075834e-30, 2.01422505081014e-28,       // median
-//            6.70225617733614e-12, 4.44299757853286e-11,       // 75th %-ile
-//            238.451174077827, 579.143886928158,               // maximum
-//            0.837031762527458, 1.17244066539313,              // mean
-//            8.10277696526878, 12.1426255901507);              // standard dev
-  }
+    Arrays.sort(samples);
 
-  private static void check(OnlineSummarizer x, double... values) {
     for (int i = 0; i < 5; i++) {
-      checkRange("quartile " + i, x.getQuartile(i), values[2 * i], values[2 * i + 1]);
+      int index = Math.abs(Arrays.binarySearch(samples, s.getQuartile(i)));
+      assertEquals("quartile " + i, i * (samples.length - 1) / 4.0, index, 10);
     }
-    assertEquals(x.getQuartile(2), x.getMedian(), 0);
+    assertEquals(s.getQuartile(2), s.getMedian(), 0);
 
-    checkRange("mean", x.getMean(), values[10], values[11]);
-    checkRange("sd", x.getSD(), values[12], values[13]);
+    assertEquals("mean", s.getMean(), mean, 0);
+    assertEquals("sd", s.getSD(), sd, 1e-8);
   }
 
-  private static void checkRange(String msg, double v, double low, double high) {
-    if (v < low || v > high) {
-      fail("Wanted " + msg + " to be in range [" + low + ',' + high + "] but got " + v);
+  private static double[] normal(int n) {
+    double[] r = new double[n];
+    Random gen = RandomUtils.getRandom(1L);
+    for (int i = 0; i < n; i++) {
+      r[i] = gen.nextGaussian();
     }
+    return r;
   }
 
-  private static OnlineSummarizer normal(int n) {
-    OnlineSummarizer x = new OnlineSummarizer();
-    // TODO use RandomUtils.getRandom() and rejigger constants to make test pass
-    Random gen = new Random(1L);
+  private static double[] exp(int n) {
+    double[] r = new double[n];
+    Random gen = RandomUtils.getRandom(1L);
     for (int i = 0; i < n; i++) {
-      x.add(gen.nextGaussian());
+      r[i] = -Math.log1p(-gen.nextDouble());
     }
-    return x;
+    return r;
   }
 
-  private static OnlineSummarizer exp(int n) {
-    OnlineSummarizer x = new OnlineSummarizer();
-    // TODO use RandomUtils.getRandom() and rejigger constants to make test pass
-    Random gen = new Random(1L);
+  private static double[] gamma(int n, double shape) {
+    double[] r = new double[n];
+    Random gen = RandomUtils.getRandom();
+    AbstractContinousDistribution gamma = new Gamma(shape, shape, gen);
     for (int i = 0; i < n; i++) {
-      x.add(-Math.log1p(-gen.nextDouble()));
+      r[i] = gamma.nextDouble();
     }
-    return x;
+    return r;
   }
-
 }
 
 
diff -uNar -x .git mahout/math/src/test/resources/hanging-svd.tsv mahout/math/src/test/resources/hanging-svd.tsv
--- mahout/math/src/test/resources/hanging-svd.tsv	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math/src/test/resources/hanging-svd.tsv	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,90 @@
+0.3349888963879321	4.054287083074458	5.366437690144569	9.480269148044338	12.118511660753464	14.964293435873882	16.823390522495433	17.578759210640822	20.267742181089872	18.79455567147264	18.736029749715897	19.67597852878228	19.466716649060928	17.741527323766324	15.908495828759298	14.606861668905168	12.706005635036263	9.353946476846014	5.696917772041925	4.958210795450044	1.1465806269525396	-0.02255856320666494	-1.1531065794167639	-2.7344813943358894	-5.064892190507581	-5.99531628393655	-5.285600220127952	-6.125969478226654	-4.86777274966846	-5.880994962171708	-5.268829019538682	-5.571000933463824	-4.9435602660145985	-2.243256133596091	-1.8080001085409496	-2.7506701668412137	-2.2977020624256492	-2.0555108260207398	-1.5442807289076441	-2.560077673521537	-3.0828476670642075	-3.4674407502570026	-5.4797439022071694	-7.047766455602743	-9.408170539957734	-11.393992673071907	-13.162249337148861	-14.74226227698836	-17.325537333987924	-19.061867899311686	-20.514998231264027	-21.056559574393653	-22.21128527462242	-23.711309605721908	-23.34056085249271	-22.738700839408356	-22.435065393713906	-20.286157814442245	-17.43949395080774	-15.785940272140728	-13.51753210778273	-10.536625803538966	-7.49757284548781	-3.507049160287889
+0.0	4.907189088387937	9.237947795446237	12.730186168400685	17.152513705860667	20.258914429786817	23.916013304811127	25.449577800211856	27.833919755519947	26.954945503466014	27.573788573380877	25.86590405793296	24.425521303230433	23.035711161151056	20.414525659033888	17.709779754892235	14.226617178740906	11.168260914343549	7.432168093323572	4.925905752888557	2.3981302522273857	0.08908483833396166	-0.7967830752174807	-1.2676997479998686	-2.6738935891629136	-1.7507943093585754	-1.1924800610313424	1.498053456225854	3.242519846074049	5.149410316387497	8.04551129374082	11.731832363544056	15.166180547280197	18.58643263533771	20.711555098115323	24.529455598049406	26.096973454608367	27.77165820640888	29.023388700957245	29.566191800580043	28.650812794479467	28.562237564565354	27.226194940531855	24.253805832003703	21.466050743914202	17.782170875441622	14.582678132271756	12.01161611583507	6.911301774679876	2.8902604042713205	-2.0387283401574345	-5.287179790246271	-9.410706107728323	-10.212033614948913	-12.629213889572336	-14.329228334533589	-14.921255815304855	-15.374391815725785	-14.688047226232133	-12.462470572141708	-11.96017242492212	-10.538197026737603	-7.2535575123466876	-2.8595429856724124
+0.0	0.0	1.1326674637851178E-13	1.1366536258596799E-13	9.674457795661973E-14	1.0618208600341125E-13	-3.092883068136469E-14	1.6161422386479353E-13	1.7504507626014908E-13	1.5386128553992963E-13	1.9766858645201795E-13	9.823783801632762E-14	7.877909892607784E-14	-6.178714449064822E-15	6.002552855663459E-14	1.5171639734747616E-13	6.920349533812898E-14	1.0504434850711103E-13	6.913722253705353E-14	5.0073508517255175E-15	1.5560804722542536E-14	2.5031857253621907E-16	-5.375054715083457E-15	8.649743174890393E-16	-1.3623957626077972E-14	-1.5894739655549516E-14	1.932358175209495E-15	5.642533209326764E-15	2.848285829781792E-14	1.3559806860504589E-15	2.5514049013173365E-14	1.5160964263835467E-13	6.10787110998386E-14	3.935363809299794E-14	8.390658963394061E-14	5.757506725183224E-14	5.1589621266988884E-14	2.1959802780109287E-13	8.392683058796424E-14	1.8759773242450333E-13	7.150672002576243E-14	3.4450756681391794E-14	5.490913772678414E-14	3.035764401751157E-13	1.7280812437359274E-13	1.2679521240698044E-13	1.0480961873384214E-13	4.6126196015741724E-14	-1.4350509935408637E-14	1.656305841368865E-14	-5.729748868476542E-14	-6.015856385928714E-14	-1.5075919610238206E-14	-6.130332039609612E-14	-1.6333372555575912E-13	-1.6987155370523368E-13	-1.6077650954183426E-13	-4.2791009139928325E-14	-4.093243250680342E-14	-4.89292416809468E-14	-6.182893226669624E-14	-9.033341900328917E-15	-3.1142340099608945E-14	-1.0180628556390702E-14
+0.0	0.0	0.0	3.829268436744954E-14	-1.7098043082509732E-13	-1.863933745570141E-13	-1.701692232602621E-13	1.5154832234012345E-14	-3.9232947681239326E-14	-6.954261214926364E-14	-6.969010443377969E-14	1.1216788237436951E-13	-7.024320050071462E-14	9.446880823245684E-14	-1.953904039124851E-13	-6.603967039200994E-14	-8.381249067618033E-14	-3.200582573995817E-14	-6.74777201660408E-14	-6.452787447572192E-17	-4.318758455982245E-15	-1.5527019795720657E-16	-5.519437834619788E-16	-1.8183033200480308E-15	1.0080175819886008E-14	-5.277458305335811E-15	2.9291045878086594E-15	9.264359121157197E-16	2.7313727438795116E-14	1.2389351899338623E-14	-2.868724933834942E-14	2.286130409996997E-15	-3.484505221688986E-14	-5.778010245911721E-14	-5.26547455721888E-14	-7.455734982280572E-14	-9.074462804842857E-14	-3.410759079431016E-14	-7.865026071812342E-14	-6.622403574765522E-14	-8.432871367198598E-14	-8.871660913633533E-14	-8.967530898568941E-14	-4.144533194897782E-14	-5.505149519557344E-14	2.6585484283997407E-14	-5.66001641829902E-15	4.7990302074372836E-14	-2.525805372335391E-15	-1.8989631631426797E-15	1.4793476136948357E-13	1.5990929121862004E-13	-3.379416968971366E-14	4.819310396558145E-14	1.985983611007074E-13	6.648214724555779E-14	9.439506209019887E-15	1.7330343430622408E-15	-4.673661765598726E-14	4.4210812283651966E-14	1.0422173554607346E-13	1.153574030270264E-13	-3.3517621656246285E-14	6.890655167228901E-15
+0.0	0.0	0.0	0.0	1.720512687361819E-27	2.380201696827328E-27	2.032019235222465E-27	-1.1290321967709165E-29	-9.644137685420738E-28	1.936032509488325E-27	6.510517232700756E-31	-1.0488443261881582E-27	8.33259398889305E-28	-1.0204150676053562E-27	2.2499913521732785E-27	9.401403901261556E-28	1.0897303744095217E-27	1.3078327017050124E-27	1.2354791535255945E-27	-3.056467432617711E-31	8.26157509674637E-30	9.608946983420194E-31	8.969085211643852E-30	2.9024699638035615E-29	-1.4496209161711786E-28	-4.8756992284147365E-29	-4.054288970888942E-29	-4.7326373940149286E-29	-3.493760564308542E-28	-1.4292212955087243E-28	3.4898542539688957E-28	7.45142939039094E-28	2.185580635017782E-28	7.442389265941775E-28	5.048906113959754E-28	4.588178511125595E-28	1.1623009398300257E-27	-1.110151696796083E-28	1.3133232379045823E-27	5.749871802680555E-28	9.85844221099711E-28	1.0367998693076507E-27	8.236021316608093E-28	-3.203174478488964E-29	6.436080319007244E-28	-3.873106701733881E-28	5.115855932836004E-28	-5.693230302756085E-28	9.073829939413646E-29	9.555608214633606E-29	-1.8760706457751544E-27	-2.284800917644141E-27	3.2589479094491193E-28	-1.0522731985973738E-27	-1.9117916836585797E-27	5.512888975410194E-28	1.141141758823748E-27	-1.3262398328226996E-28	1.9219046870933641E-28	-5.066593019108569E-28	-8.425477368079655E-28	-1.3129977120429445E-27	2.968036297767902E-28	-1.993574630963361E-29
+0.0	0.0	0.0	0.0	0.0	1.5752519836306054E-27	2.411900340081705E-27	-5.35160813940895E-29	7.206287601463632E-28	3.889107594440674E-28	5.55586799205811E-28	-1.1569278054050473E-27	7.124583660403937E-28	-6.47095213192652E-28	5.751957450601314E-28	1.5687156683458096E-28	4.215923358679426E-28	3.1537721249036014E-28	6.470952131926517E-28	6.319289191334492E-31	3.288583627652034E-29	1.8766373962144836E-30	4.08519705298392E-30	2.2723908607223162E-29	-9.395953221862967E-29	2.3694142907306692E-29	-1.766847725415536E-29	-3.727742310847804E-30	-4.518227940600213E-28	-1.0744068249347671E-28	1.184707145365337E-28	-2.5941001286447873E-29	2.8269563606648537E-28	-2.2386879850351838E-28	1.0458104455638877E-28	7.3206731189471475E-28	-2.6472076903336004E-28	2.6635484785455172E-28	1.2222909582527966E-27	5.457823262786507E-28	2.941341878148424E-28	5.948046909144571E-28	1.1079054407692316E-27	1.9445537972203367E-28	8.072349376696162E-28	-1.6667603976174372E-28	1.5115229096040327E-29	-5.702935085965583E-28	6.944834990072639E-30	1.925149111218674E-29	-1.0392741302791066E-27	-5.229052227819388E-29	1.160195963047435E-28	2.35307350251874E-28	-1.294190426385304E-27	-2.451118231790343E-28	-1.0335548544049235E-28	-6.58738024793661E-30	4.526398334706135E-28	-5.343437745302979E-28	-1.0817601796301522E-27	-9.444975586498802E-28	1.2092183276832387E-28	-7.659744474344791E-29
+0.0	0.0	0.0	0.0	0.0	0.0	1.689774356608911E-41	-3.938374043414623E-43	6.759291538736899E-42	1.261260295690439E-41	5.972997270127627E-42	-4.4370836863805186E-42	1.0380177621822102E-41	-6.986235593024432E-42	1.9249029218057264E-41	-2.737708579974634E-43	-2.6445290974284662E-42	-5.468659486013039E-42	-1.4609126066554754E-42	6.794573314615129E-45	4.091910319634905E-43	2.1033756911758458E-44	7.032648931954758E-45	2.194940185870694E-43	-1.1536099642026862E-42	1.8865254416919167E-43	-2.5824745183607816E-43	3.9586637990423983E-44	-1.9898016923220974E-42	-4.79856206808679E-43	1.051309155776058E-42	-2.5140415652558555E-42	2.5786062171847407E-42	-3.782392537882122E-42	-1.454504948434896E-42	7.007643725560301E-42	-2.2353258637219023E-42	1.739410768541473E-42	4.935729183039932E-42	4.828883748249627E-42	5.050718409779816E-42	9.7051465162387E-42	8.220957167250735E-42	2.248584626368554E-42	1.1095816128359472E-41	-3.2990138621582744E-42	2.7279402130899274E-43	-1.4475869087325376E-43	3.507921822763994E-43	-1.7971964806390283E-43	-1.345960194317605E-41	3.7437694889734395E-42	-9.334863356729482E-43	8.675815837332015E-43	-1.0952440766529738E-41	1.179622685953906E-43	-3.381044437746076E-42	-7.776238842907237E-43	7.541508333431788E-42	-3.5235064119513774E-42	-4.9864549665942444E-43	-1.0641515252247294E-41	1.0472860667735744E-42	-9.844031636616902E-43
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.5751444912939556E-43	-1.6884127796129255E-42	-5.785963794016864E-42	5.292040055503212E-43	-3.93122975551666E-43	-2.2201368042372784E-42	1.3356101092460454E-42	-6.383208314566492E-42	2.9440122832579025E-42	1.3771904239678507E-42	3.757348439407263E-42	5.609562458833381E-42	-1.225556151246543E-45	1.0253327607537394E-43	-2.116422269220957E-45	1.3131661893677954E-44	-2.001840151947782E-43	-1.5939120643360803E-43	1.866389126717651E-44	-7.875059606403582E-46	-7.674245586440284E-44	-8.416863707324148E-43	-6.013395515449806E-43	-2.967322459692868E-43	1.854655287904093E-42	1.6632125888724303E-42	1.359550290449507E-42	1.061558034943199E-42	2.5754594936782183E-42	3.201684233579432E-42	-3.13742374719115E-43	-7.963260273995223E-42	3.2760247962638937E-44	-2.7027204569176848E-42	-2.550259302937728E-42	8.089261227697743E-42	2.97992255506309E-43	3.207984281264578E-42	1.4855512441519742E-42	6.8355517383583E-44	1.4011306051713149E-42	-2.0398373145486823E-43	2.0435779678617257E-43	2.2176167851632375E-42	-3.6086673140383604E-42	1.0193477154528834E-42	3.528026703668793E-44	1.1088083925816207E-43	-2.318417548125205E-42	1.127393533252727E-42	6.107896230726597E-43	2.3108574909030742E-42	1.7463732183160398E-42	8.356383249546963E-42	1.2196892318397891E-42	-3.717028134222514E-44	-3.2634247008936337E-43
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	8.21012931318503E-57	4.039340042954185E-56	-2.56653651756919E-57	2.2441283662349545E-57	-7.340203967359862E-57	-1.7503569576558622E-56	8.674475297928424E-56	-1.201469085326865E-56	2.0195015734573415E-57	-1.6316309896768057E-56	-2.623811561184994E-56	-2.3065025283838222E-60	9.483182014643345E-59	4.127422225664389E-59	-8.199848380367192E-59	9.912079200393473E-58	3.3760549092179603E-57	-7.403095190214458E-59	6.493774902191693E-59	5.710235043426284E-58	8.595100960883849E-57	7.722010422429736E-57	4.403462172854675E-57	6.585312317597134E-57	-8.21170511724069E-57	-5.839169097246814E-58	-5.519824254658744E-57	-1.2598607762993305E-56	-1.2213839883060286E-56	-5.904646472662585E-58	-1.3204803281780678E-56	-1.5488872677212864E-56	1.3425252835359025E-56	-7.183873337425252E-57	-5.10386632203794E-56	-3.212718064268643E-57	-3.126808215748489E-56	-1.0847672104882435E-56	-6.1867459637889095E-58	-1.6291640412586453E-57	7.653483322785513E-58	-9.552598900198274E-58	1.7568204711875547E-56	2.379453256417447E-56	-1.1625956294161063E-56	-1.6145046485728478E-57	-2.118646647786729E-56	1.0096339598762779E-56	5.821699407457433E-57	-8.48108610365054E-58	-1.938238988449602E-57	3.9247845219155025E-57	-6.181890177929475E-56	-1.7657889697870554E-56	9.910593393767767E-58	1.0344474399456735E-57
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	5.827938285652229E-56	-3.496190160832525E-57	-1.468849932988738E-57	1.571137532779036E-57	2.893511622868076E-56	-1.0467703812140317E-55	1.7069754653005722E-56	-7.241962065153381E-57	1.8722722265616775E-56	2.582557319505542E-56	-1.3593063065883062E-59	7.160131985321145E-60	-8.813515141306615E-59	4.55819116565535E-59	-1.521016608881791E-57	-7.509444138605076E-57	-1.6206191591775985E-58	-1.6188211349625253E-58	-3.8536853220996116E-58	-8.649439438267887E-57	-1.6345558446490264E-57	-6.846109053964922E-57	-7.413805232801123E-57	1.1349832072731869E-56	-9.001308781546644E-58	7.36879868889335E-57	1.616962377485093E-56	1.8820918361415472E-57	-3.479824144866064E-57	-5.165114639011084E-56	3.9867103456269045E-56	-5.815663773677415E-56	4.093140593208713E-56	4.343540637495423E-56	1.0618475734231207E-56	-1.3223740900890215E-56	-3.6823535924508855E-57	7.43375131476021E-58	9.684589948145846E-57	-1.912778116078648E-58	1.8616343161835072E-58	-5.391783960146419E-56	-4.800152482959275E-56	2.634928570598169E-57	4.023866316250705E-57	5.34841401783534E-56	-1.3583793252152183E-56	-7.286968609061086E-57	-2.3812553231182414E-57	-5.223213995691988E-56	1.233997603870202E-56	3.724905233963659E-56	1.7728486795655178E-56	-1.339072540754782E-57	-3.0379417137719846E-58
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.3280978346628283E-71	5.843121557610399E-72	-1.4221258239233625E-72	-2.134868874956399E-70	-4.1246346340517876E-72	-1.708127013648594E-70	2.34004585532403E-71	4.87838465246903E-71	-3.153907440010257E-71	-8.689329635346688E-76	-3.313803509027279E-74	7.613108889832062E-73	-3.34084866627141E-73	4.4896945351239036E-72	7.74878643260603E-71	5.407437098837962E-73	-3.705070689701242E-73	-1.2028885483610484E-72	-1.8009501437959617E-71	-3.9623056182935853E-73	2.882716720013782E-71	5.595467167518448E-71	7.936708261455254E-72	9.266160124513738E-72	-9.836475556198948E-72	-3.869834611712894E-71	7.91690333503649E-72	-2.2244683866487522E-73	1.0961314114096947E-70	1.1326975572476651E-71	2.0856879638048792E-70	9.836182230304668E-72	-5.5142405827528087E-70	1.2287511145439432E-71	1.6126215598922887E-71	1.2741693721447136E-71	-2.7982226688043186E-72	-8.34941069161467E-71	3.853594814034264E-73	-1.5759157829237922E-72	3.753725696875273E-70	-2.154294306858889E-71	1.034215780479465E-71	2.6175387576073184E-71	-4.273275591431832E-70	1.1114247463496076E-70	-1.8497988422100197E-72	1.8541373511690664E-71	2.243602671438978E-70	-2.332402746776785E-71	-2.944248631357214E-70	-8.78629009456976E-71	-5.820386224539398E-72	2.489722783545022E-72
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.5403543593948737E-72	-5.605400060872148E-72	-4.345980797076783E-71	2.2066174781631765E-70	-5.6772300569061385E-71	-6.299756689200394E-72	-1.1777458979345515E-71	-1.3723253760798708E-70	1.704039872637171E-74	4.166571040923759E-74	-8.251304521261941E-74	1.5605773299217812E-73	3.2243687108576805E-72	6.572444637107148E-72	5.215157003715456E-73	4.149138341510112E-73	5.040570207799595E-72	-4.3352062976717025E-71	4.0728605390087644E-71	1.1487478624986199E-71	2.064447293420402E-72	-9.039938019384781E-72	2.482790511062613E-72	-4.14605397478226E-71	-9.699709834807764E-72	-2.7374211960774408E-71	4.3623420739511576E-72	-4.407701386261527E-71	-1.1475772996002875E-70	7.192576936200303E-71	-5.6931922782469674E-71	1.9062616799322988E-70	-9.623889283438568E-72	-1.9543079661683286E-71	8.614145765116763E-71	-4.55887692421075E-72	-8.706061556338064E-72	1.613057289408405E-72	-4.8738813859846547E-73	-1.063828844965147E-70	-2.290046688371511E-71	-5.371087953439176E-71	-8.373515278403093E-73	-2.3246315012767468E-70	5.776461866241958E-71	-3.2340790621733464E-71	3.009543815311638E-73	2.972165613671754E-71	-1.2591532267730445E-71	-2.6374910395579594E-70	-2.031724739671604E-71	6.015097075288112E-72	-1.8950981013820206E-73
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	5.4925127905614495E-86	4.233527847947939E-86	-1.233379608738095E-84	2.8464930523410394E-85	8.571235050423321E-86	5.425929983314222E-86	1.3529801118487372E-84	-1.755983351374802E-88	-4.735998152508561E-89	8.558088372988764E-88	-1.4975127225602883E-87	-5.061980902402982E-87	3.876941721870268E-85	-3.1205644753872857E-87	-1.8168836769027425E-87	-2.2507955543730173E-86	3.7908986948183276E-86	-3.0815781579143176E-85	7.6852744962477005E-87	-6.964358377049071E-87	5.110125995160978E-86	-5.751565105326284E-86	3.0017413310666446E-85	2.5550744546002585E-86	1.761230765405502E-85	-4.495317379519493E-86	3.5517300378292905E-85	1.111976606407325E-84	1.5422280806377508E-85	6.248102164753917E-85	-3.0192340705668216E-84	7.140495902913389E-86	2.1814308479420653E-85	-5.336545010995533E-85	3.1708616345168535E-86	-4.373281922740862E-86	-8.117588287948386E-87	1.0736375734834097E-86	-1.239104991520549E-86	3.0785200771603945E-86	2.845305835518004E-85	2.791409521028254E-86	1.4138848740233325E-84	4.03533812004617E-85	2.289011724238953E-85	-8.067106637103645E-88	1.3876634490200218E-85	1.1022084322417137E-86	3.177144906766895E-84	4.1739866598274566E-85	-2.7682248936797995E-87	7.571122353606183E-87
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.806881078171672E-85	2.2500354290596344E-85	-2.018796222147049E-85	-1.946615163528625E-86	8.052369503355606E-86	6.196691106590401E-86	-3.8788945923414604E-89	1.7708755483514906E-88	-1.1439408843546366E-87	-3.67337343977917E-88	3.483608908913272E-86	-5.020683847839655E-85	3.704455342481451E-89	-4.3869885528428065E-88	2.5961634975527547E-87	-2.4780268943021052E-86	-1.7277985685042296E-86	-1.695767716282666E-85	-1.8410635592497063E-86	-8.944686527221205E-86	3.8431948069519246E-86	-4.8651170218408784E-86	7.672789695660239E-87	4.053181604310704E-86	-9.929970156394139E-87	-3.827463558237305E-85	3.4900231987117696E-85	-3.530295195421239E-85	2.0480258971781297E-85	3.3950592322774036E-84	-2.187759982535073E-86	5.9084540333671566E-86	-2.760580419602627E-86	4.565106885063489E-87	1.5788084193874429E-86	1.0007104021059454E-87	-7.038718880532715E-87	5.983639253030454E-85	2.9002333114177937E-86	-2.6631481694990116E-87	-4.3974422213434446E-86	-6.86312769729911E-85	-3.9628943858813355E-85	7.16127038261626E-87	-2.36070222646901E-87	-3.805703689047486E-85	1.5946411600292725E-86	1.3623626757822485E-84	-3.725078502086722E-85	2.4843193937879637E-86	-9.252448109937883E-87
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.666515822828739E-99	8.351506639159741E-101	-3.727473303439878E-101	4.418059568047803E-101	-2.092083452710379E-99	-5.408931788987683E-103	-6.571992565350447E-103	2.610485941747302E-102	3.1528449360344275E-102	-8.048691110946728E-101	-1.5362269978762938E-99	1.2185375303501402E-101	9.881677849779078E-103	-2.0409331882678648E-101	2.1504343139667635E-101	-2.3176713774224824E-100	1.1000757506340405E-100	8.309163599812547E-101	1.8819028921178442E-100	-1.5455149555184034E-100	9.919667345877093E-100	1.6921142727837993E-101	1.1609947052655366E-101	-1.384686537980847E-100	5.80573306954754E-100	1.5963146414245206E-99	-9.427378677895713E-100	1.6719400273773008E-100	-7.516028235334724E-99	-3.7721233805594505E-100	1.8048672676511946E-99	-1.123014254908671E-99	1.9352280966093362E-100	-1.6547576047721366E-101	-7.340511440578859E-102	1.1388326395619858E-101	-1.0264168041357035E-99	6.679674263577346E-101	1.6718860986872991E-99	-1.4829303883928432E-100	-6.794954982396801E-99	1.023510205672039E-99	1.1014033813469158E-99	-2.9367166698394805E-102	1.4183243597843116E-100	5.972902850631398E-101	1.7177331871395773E-98	-5.67002004004651E-100	2.548312357776445E-101	-7.183477188126992E-102
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.3752972238533048E-100	-1.1314454076396294E-100	7.49118183326774E-101	-6.241525711397122E-100	-1.9252118832767323E-103	1.0913382010255332E-102	5.245600444000825E-103	-2.0196089435806188E-102	1.4891172544128678E-100	-1.6671088155549924E-99	5.776987948737385E-102	-2.7199018801187826E-102	-1.9253570080374928E-101	1.939331203184092E-100	-8.619249791929277E-101	5.6065653035276764E-101	-1.1442797137561365E-100	8.490906730764168E-100	-1.876510652192765E-100	3.0498364271599574E-100	5.758550507538934E-102	2.617522957972271E-100	-4.9285424211884346E-101	-1.6492758849510058E-99	3.603602961597526E-99	-3.101849141421595E-100	-6.908909629455538E-100	-1.4309575830112217E-98	-9.277852342644907E-101	9.362288567095705E-100	-5.445123242377901E-100	1.23905937570206E-100	-9.456857138480417E-102	-1.020095136645232E-101	5.676647369821816E-101	5.182357711887271E-100	-2.3939358356267637E-100	7.940065911503858E-100	2.928248263950899E-100	8.105877547268847E-102	-9.716245219993046E-100	5.663475318807489E-100	-1.0617855224677758E-102	-6.808937139705928E-100	-7.148370761997824E-101	1.0273929595245052E-98	7.916740404499419E-100	-1.7694455195892574E-100	2.5913477283925382E-101
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	6.053125127686442E-117	2.7844375587357715E-115	9.007050189997425E-114	8.866882511259435E-118	-1.8916016024020074E-117	-1.3454016397084267E-116	8.417627130688927E-117	-4.479312594487978E-115	-5.03620010623512E-114	1.1652265870796381E-115	6.998925928887465E-117	-3.9345313329961876E-116	2.4212500510745694E-115	-1.1440406491327356E-114	-8.020390794184535E-115	5.508343866194684E-115	2.808650059246497E-114	5.326750112364098E-115	4.794075101127636E-114	-1.891601602402006E-116	0.0	2.2699219228824155E-115	5.617300118492994E-114	-2.711800057203535E-114	7.021625148116242E-115	-1.5011750316662363E-114	-1.1622000245157915E-113	-2.0217437926472728E-114	-1.801410037999485E-113	-1.6948750357522E-114	1.906734415221239E-114	-1.0592968973451309E-116	3.177890692035386E-116	-6.658437640455123E-116	1.8401500388166794E-114	-2.663375056182049E-115	-1.467277530951192E-113	7.263750153223716E-116	-2.983434047308472E-113	-7.748000163438668E-115	-4.116125086826759E-115	1.1018579333991778E-116	3.389750071504419E-115	-4.2371875893805236E-116	-5.346120112772674E-113	-1.5011750316662363E-114	3.14762506639695E-115	-2.2699219228824053E-116
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	5.868454591335832E-116	-3.286334571148071E-114	-7.7940412541179116E-118	6.05184379731508E-117	1.2562160609578226E-116	-1.063657394679617E-116	7.746360060563302E-115	6.572669142296143E-114	-7.3355682391697895E-118	7.335568239169798E-117	-1.8192209233141063E-115	1.3145338284592235E-114	-3.9905491221083707E-115	7.3942527850831885E-115	-4.460025489415237E-115	9.201736799214583E-114	-7.981098244216741E-115	-2.018748379419533E-114	8.802681887003734E-116	6.572669142296118E-115	-1.9952745610541854E-115	7.887202970755353E-114	2.5915095475338918E-113	-6.572669142296118E-115	-6.0092975015278915E-114	4.0562758135313146E-113	2.2300127447076185E-115	4.3191825792231725E-114	-3.755810938454937E-114	3.5210727548014937E-115	-6.895434144819637E-116	-1.4377713748772786E-115	-1.7605363774007505E-116	3.0985440242253208E-114	-9.859003713444192E-115	9.013946252291842E-114	2.347381836534322E-114	1.1748646091854354E-113	-4.694763673068644E-114	1.3145338284592235E-114	-1.57714717142151E-116	-2.347381836534322E-114	-1.291060010093883E-115	1.7877660067045536E-112	-3.943601485377677E-114	-3.286334571148059E-115	1.789878650357437E-115
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	4.6154925565359686E-128	1.0742829723370809E-131	-7.840145229008246E-131	-1.1771728964297165E-130	1.1315496129523776E-130	-1.178381142783457E-128	-9.299216353450634E-128	-1.8177228882136135E-130	-8.647569435242434E-131	2.487445668721074E-129	-1.8527288774854003E-128	7.46197697925332E-129	-1.0740715799480424E-128	8.355191598805275E-129	-1.3898703665139686E-127	1.407106345377293E-128	3.538245253697029E-129	-1.1297976425373225E-129	-8.901165922915236E-129	2.750164409182927E-129	-1.3225722663458331E-127	-3.524343620111088E-127	5.91604915634739E-129	8.433381310806443E-128	-4.915458173572468E-127	-4.134951274165949E-129	-5.972089861470285E-128	5.58748663728193E-128	5.09714987072272E-129	9.608111340364374E-130	2.0277471750042743E-129	-1.1804850622950216E-130	-4.201790268293611E-128	1.2097122704707874E-128	-1.087726409496529E-127	-3.1170382638599943E-128	-3.3402855694621653E-127	5.982473828823614E-128	-1.5670105583996343E-128	2.75513843982727E-130	3.2172528981662184E-128	1.6386158822195516E-129	-2.6334372328823312E-126	5.55527710585945E-128	4.493227486694304E-129	-2.4785096907246118E-129
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.5961123026830954E-133	-6.344325331645738E-132	-5.487243990333357E-131	3.850308970220649E-131	-1.0414835138825122E-129	-8.952580348560119E-131	-2.385023358483633E-131	-2.1468318761195898E-132	4.735697406949369E-130	-5.1224923974949207E-129	3.9030763491848185E-130	2.7121344800388332E-129	5.916163513673553E-130	-1.8236406170017143E-128	6.687080154799607E-130	-6.601035910338449E-129	-2.744059132879044E-130	-2.5136856156457803E-129	6.646047494868642E-131	-2.1390698646160042E-128	-1.5928729375503518E-127	2.1220102253962627E-129	1.1065389310820515E-128	-1.829231006063824E-127	-2.9021281296582828E-130	-7.163059009998048E-129	2.621116579828478E-129	1.6413063972360468E-130	-2.20888600292797E-130	2.660594972337741E-130	-2.6002893963787195E-131	-3.4944905293880034E-129	2.0919196081135714E-129	-8.888917554970511E-129	-1.485599886937644E-128	-2.36411535312792E-128	2.9381871338399974E-128	-4.747603095399001E-129	-1.9047547252014967E-131	1.4987489893246062E-128	2.1500181243339337E-130	2.573727587850117E-127	9.30769936905314E-129	1.2624381705407399E-129	5.369294521374817E-130
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	8.027963857936006E-145	7.399349767567842E-144	-5.096936864564793E-144	1.152775619615496E-142	-1.7039988553476988E-143	2.6287499230067693E-144	6.856919222796316E-145	-8.139710065421492E-143	6.950840744984624E-142	1.8704356346842523E-143	-4.80132224280741E-142	-4.861359949038885E-143	2.395393109179435E-141	1.0804722256319084E-144	9.47068761690486E-142	4.315995561431725E-143	3.009681159552559E-142	-2.704781754489293E-146	2.6063203006891654E-141	2.308055268278898E-140	-2.9271896645077417E-142	-1.1061916168168637E-141	2.573844689243489E-140	2.905678413459291E-143	9.096896042349647E-142	-2.6037594173623628E-142	2.378178561389012E-143	3.386497111454366E-143	-3.180017455407587E-143	3.1266487049227196E-144	6.008507042132872E-142	-2.7064771182598338E-142	1.0360908288246379E-141	2.1068052581959298E-141	2.3772034773439746E-141	-3.611447443312412E-141	5.490490071791731E-142	2.7647709942807985E-144	-1.9279644251710127E-141	-1.6607206685031308E-143	-4.546273794660541E-140	-9.306047888494058E-142	-1.4818747499586204E-142	-8.325489510185E-143
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.8927471080992545E-145	1.8920587112605333E-146	-2.553126195496849E-145	1.1125318990148643E-144	3.101055659287181E-146	-5.720792853892049E-147	1.955322380740169E-144	4.286674650670466E-143	-2.2510851985338892E-144	-3.829689293245213E-144	-3.523462843502798E-144	1.264024362368979E-142	-2.2008597651798473E-144	-1.26942579932443E-143	-2.633131676094498E-144	-2.3643732894906584E-143	-8.213813400406924E-145	-2.906730869268309E-143	-1.4228495187057863E-141	1.9395167893228529E-143	1.0119103098907878E-143	3.2516474242175006E-142	7.410591968971998E-145	9.822596812528295E-144	-4.213208940040777E-144	-1.9928193565460249E-144	3.959796295765971E-145	-1.5323025233381136E-144	-1.5253325053459291E-145	1.7507528689672495E-143	-1.1774780103683876E-143	3.2201551963565544E-144	7.892089761030873E-143	1.7700852550688078E-142	8.248602223049155E-143	4.068700675653734E-144	-3.4671106782840023E-146	6.016390112246038E-143	2.992323378609155E-145	2.1904492223952993E-141	-2.974579261693931E-143	1.1303806522495628E-143	1.6234600327232726E-144
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	3.18312712044497E-160	3.578217181485619E-157	-3.073832968398786E-157	-2.2894933683425796E-158	2.8180851406826683E-159	-1.6526678279590194E-157	-6.24253681461222E-156	5.111297645296562E-157	2.1170342750956064E-156	5.837265316290092E-157	-1.9830400021143208E-155	3.7142236876213166E-157	-2.993178897800882E-156	2.6796165612060213E-157	3.601240826628721E-156	1.3553709785411415E-157	1.967356667429041E-156	2.689716299860563E-154	-5.126343748738485E-156	4.5204053253499853E-156	-1.5620752231539605E-154	-1.487186266321604E-157	4.475381357240778E-157	8.559705044602738E-157	2.7826813219188956E-157	4.025851671920956E-158	4.597127631093567E-157	2.6650011818858793E-158	-4.574424368070294E-156	3.1068040233333917E-156	4.066726183041799E-156	-1.23218782227871E-155	-2.6220563267185493E-155	-2.3541417401899816E-155	-3.1802103486610827E-156	1.8094010402322214E-158	-1.5172410796734143E-155	-2.169939882134126E-158	-4.186759250180215E-154	9.630082168663782E-156	-8.748987763302339E-157	-7.03988648379519E-157
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.1527319864375612E-157	-1.8429636729025602E-157	-1.3616214192373328E-158	1.7240377600380285E-159	-8.523059392497059E-158	-3.9854592858899426E-156	3.225909942464624E-157	1.2424515090094615E-156	3.65654703463918E-157	-1.2424164112709511E-155	2.3200581312480782E-157	-1.6523523028210965E-156	1.642225101168863E-157	2.147796950051829E-156	8.734671459789762E-158	1.1575962872182714E-156	1.608177748000034E-154	-3.136276981326342E-156	2.6633284031748084E-156	-9.374620036145417E-155	-9.052532659702922E-158	3.116759978598453E-157	5.4807842092862474E-157	1.6855157767096212E-157	2.379776685512038E-158	2.7944634539375424E-157	1.6093735844395092E-158	-2.5958912224728457E-156	1.8434010783647826E-156	2.428883399336746E-156	-7.787796310697368E-156	-1.6947546428860447E-155	-1.423953058015431E-155	-1.9433119272131458E-156	1.0628150739664782E-158	-9.565202496880898E-156	-1.4201245184955043E-158	-2.6751775815362427E-154	5.714060029448772E-156	-5.535321485714388E-157	-4.209716917890147E-157
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-1.15E-321	0.0	-0.0	0.0	-2.233E-321	0.0	0.0	0.0	-0.0	0.0	-0.0	-3.384E-321	-0.0	-0.0	1.15E-321	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	-0.0	-1.15E-321	-4.536E-321	-0.0	-0.0	0.0	-2.233E-321	0.0	-6.193E-320	-0.0	-0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	-0.0	0.0	-0.0	0.0	0.0	0.0	-0.0	0.0	-0.0	-1.15E-321	-0.0	-0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	-0.0	-0.0	-1.15E-321	-0.0	-0.0	0.0	-0.0	0.0	-1.577E-320	-0.0	-0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	-0.0	0.0	-0.0	0.0	0.0	0.0	-0.0	0.0	-0.0	-0.0	-0.0	-0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	-0.0	-0.0	-0.0	-0.0	-0.0	0.0	-0.0	0.0	-3.384E-321	-0.0	-0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	3.3928939783171892E-161	-5.971070967985776E-160	3.919228586895629E-161	-8.1068020579006E-161	3.76020340136455E-161	-1.282916911223899E-159	0.0	3.8559419405747664E-160	0.0	-4.881127912932609E-161	0.0	-6.757289008632457E-161	-2.6016087040577666E-159	-1.3551935340053494E-160	-1.4613357638231904E-160	6.017085931886934E-160	0.0	1.1023512143830742E-160	8.602189719518531E-161	0.0	0.0	0.0	0.0	4.0578788178297766E-160	-6.682230373046679E-161	-4.610784773535117E-161	-9.735219343041831E-160	-3.046896329020949E-159	-1.9878907033405172E-160	-7.802228664423545E-161	0.0	-1.1358180642183033E-159	0.0	-4.06186241549076E-158	-2.0682564792492065E-160	-7.060436627342906E-161	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	4.725649922711621E-161	0.0	0.0	0.0	1.13392491010474E-160	0.0	-3.4081280003173884E-161	0.0	0.0	0.0	0.0	2.2994603610486572E-160	0.0	0.0	-5.3182381111822905E-161	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-3.586612279542216E-161	0.0	0.0	8.604615415462173E-161	2.693043316446475E-160	0.0	0.0	0.0	1.0039093646667553E-160	0.0	3.5901359626104684E-159	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-3.384E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	5.618E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-9.0E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-3.384E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-2.233E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-5.618E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-2.233E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0153E-320	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-4.536E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-5.618E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-2.233E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	3.384E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	3.384E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-3.384E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-2.233E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-2.233E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0	0.0	0.0	-1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	3.384E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	3.3928939783171892E-161	0.0	0.0	0.0	0.0	0.0	3.481062746862698E-160	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	-0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.15E-321	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	9.487821703229626E-161	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
+0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0
diff -uNar -x .git mahout/math-scala/pom.xml mahout/math-scala/pom.xml
--- mahout/math-scala/pom.xml	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,192 @@
+<?xml version="1.0" encoding="UTF-8"?>
+
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.mahout</groupId>
+    <artifactId>mahout</artifactId>
+    <version>1.0-SNAPSHOT</version>
+    <relativePath>../pom.xml</relativePath>
+  </parent>
+
+  <artifactId>mahout-math-scala</artifactId>
+  <name>Mahout Math/Scala wrappers</name>
+  <description>High performance scientific and technical computing data structures and methods,
+    mostly based on CERN's
+    Colt Java API
+  </description>
+
+  <packaging>jar</packaging>
+
+  <!-- this is needed for scalatest plugin until they publish it to central -->
+  <pluginRepositories>
+    <pluginRepository>
+      <id>sonatype</id>
+      <url>https://oss.sonatype.org/content/groups/public</url>
+      <releases>
+        <enabled>true</enabled>
+      </releases>
+    </pluginRepository>
+  </pluginRepositories>
+
+  <build>
+    <defaultGoal>install</defaultGoal>
+
+    <plugins>
+
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>build-helper-maven-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>add-source</id>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>add-source</goal>
+            </goals>
+            <configuration>
+              <sources>
+                <source>${project.build.directory}/generated-sources/mahout</source>
+              </sources>
+            </configuration>
+          </execution>
+          <execution>
+            <id>add-test-source</id>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>add-test-source</goal>
+            </goals>
+            <configuration>
+              <sources>
+                <source>${project.build.directory}/generated-test-sources/mahout</source>
+              </sources>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+
+      <!-- create test jar so other modules can reuse the math test utility classes. -->
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <executions>
+          <execution>
+            <goals>
+              <goal>test-jar</goal>
+            </goals>
+            <phase>package</phase>
+          </execution>
+        </executions>
+      </plugin>
+
+      <plugin>
+        <artifactId>maven-javadoc-plugin</artifactId>
+      </plugin>
+
+      <plugin>
+        <artifactId>maven-source-plugin</artifactId>
+      </plugin>
+
+      <plugin>
+        <groupId>org.scala-tools</groupId>
+        <artifactId>maven-scala-plugin</artifactId>
+        <version>2.15.2</version>
+        <executions>
+          <execution>
+            <goals>
+              <goal>compile</goal>
+              <goal>testCompile</goal>
+            </goals>
+          </execution>
+        </executions>
+        <configuration>
+          <sourceDir>src/main/scala</sourceDir>
+          <jvmArgs>
+            <jvmArg>-Xms64m</jvmArg>
+            <jvmArg>-Xmx1024m</jvmArg>
+          </jvmArgs>
+        </configuration>
+      </plugin>
+
+      <!--this is what scalatest recommends to do to enable scala tests -->
+
+      <!-- disable surefire -->
+      <!--<plugin>-->
+        <!--<groupId>org.apache.maven.plugins</groupId>-->
+        <!--<artifactId>maven-surefire-plugin</artifactId>-->
+        <!--<version>2.7</version>-->
+        <!--<configuration>-->
+          <!--<skipTests>true</skipTests>-->
+        <!--</configuration>-->
+      <!--</plugin>-->
+      <!-- enable scalatest -->
+      <plugin>
+        <groupId>org.scalatest</groupId>
+        <artifactId>scalatest-maven-plugin</artifactId>
+        <version>1.0-M2</version>
+        <configuration>
+          <reportsDirectory>${project.build.directory}/scalatest-reports</reportsDirectory>
+          <junitxml>.</junitxml>
+          <filereports>WDF TestSuite.txt</filereports>
+        </configuration>
+        <executions>
+          <execution>
+            <id>test</id>
+            <goals>
+              <goal>test</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+
+    </plugins>
+  </build>
+
+  <dependencies>
+
+    <dependency>
+      <groupId>org.apache.mahout</groupId>
+      <artifactId>mahout-math</artifactId>
+    </dependency>
+
+
+    <!--  3rd-party -->
+    <dependency>
+      <groupId>log4j</groupId>
+      <artifactId>log4j</artifactId>
+    </dependency>
+
+    <!-- scala stuff -->
+    <dependency>
+      <groupId>org.scala-lang</groupId>
+      <artifactId>scala-library</artifactId>
+      <version>2.10.3</version>
+    </dependency>
+
+    <dependency>
+      <groupId>org.scalatest</groupId>
+      <artifactId>scalatest_2.10</artifactId>
+      <version>2.0</version>
+      <scope>test</scope>
+    </dependency>
+
+  </dependencies>
+</project>
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Vector, Matrix}
+import scala.collection.JavaConversions._
+import RLikeOps._
+
+class MatlabLikeMatrixOps(_m: Matrix) extends MatrixOps(_m) {
+
+  /**
+   * matrix-matrix multiplication
+   * @param that
+   * @return
+   */
+  def *(that: Matrix) = m.times(that)
+
+  /**
+   * matrix-vector multiplication
+   * @param that
+   * @return
+   */
+  def *(that: Vector) = m.times(that)
+
+  /**
+   * Hadamard product
+   *
+   * @param that
+   * @return
+   */
+
+  private[math] def *@(that: Matrix) = cloned *= that
+
+  private[math] def *@(that: Double) = cloned *= that
+
+  /**
+   * in-place Hadamard product. We probably don't want to use assign
+   * to optimize for sparse operations, in case of Hadamard product
+   * it really can be done
+   * @param that
+   */
+  private[math] def *@=(that: Matrix) = {
+    m.zip(that).foreach(t => t._1.vector *= t._2.vector)
+    m
+  }
+
+  private[math] def *@=(that: Double) = {
+    m.foreach(_.vector() *= that)
+    m
+  }
+}
+
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Vector, MatrixTimesOps, Matrix}
+
+/**
+ * Matlab-like operators. Declare <code>import MatlabLikeOps._</code> to enable.
+ *
+ * (This option is mutually exclusive to other translations such as RLikeOps).
+ */
+object MatlabLikeOps {
+
+  implicit def v2vOps(v: Vector) = new MatlabLikeVectorOps(v)
+
+  implicit def times2timesOps(m: MatrixTimesOps) = new MatlabLikeTimesOps(m)
+
+  implicit def m2mOps(m: Matrix) = new MatlabLikeMatrixOps(m)
+
+}
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeTimesOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeTimesOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeTimesOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeTimesOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,28 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Matrix, MatrixTimesOps}
+
+class MatlabLikeTimesOps(m: MatrixTimesOps) {
+
+  def :*(that: Matrix) = m.timesRight(that)
+
+  def *:(that: Matrix) = m.timesLeft(that)
+
+}
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeVectorOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeVectorOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeVectorOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatlabLikeVectorOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.Vector
+import org.apache.mahout.math.function.Functions
+import RLikeOps._
+
+/**
+ * R-like operators.
+ *
+ * For now, all element-wise operators are declared private to math package
+ * since we are still discussing what is the best approach to have to replace
+ * Matlab syntax for elementwise '.*' since it is not directly available for
+ * Scala DSL.
+ *
+ * @param _v
+ */
+class MatlabLikeVectorOps(_v: Vector) extends VectorOps(_v) {
+
+  /** Elementwise *= */
+  private[math] def *@=(that: Vector) = v.assign(that, Functions.MULT)
+
+  /** Elementwise /= */
+  private[math] def /@=(that: Vector) = v.assign(that, Functions.DIV)
+
+  /** Elementwise *= */
+  private[math] def *@=(that: Double) = v.assign(Functions.MULT, that)
+
+  /** Elementwise /= */
+  private[math] def /@=(that: Double) = v.assign(Functions.DIV, that)
+
+  /** Elementwise right-associative /= */
+  private[math] def /@=:(that: Double) = v.assign(Functions.INV).assign(Functions.MULT, that)
+
+  /** Elementwise right-associative /= */
+  private[math] def /@=:(that: Vector) = v.assign(Functions.INV).assign(that, Functions.MULT)
+
+  /** Elementwise * */
+  private[math] def *@(that: Vector) = cloned *= that
+
+  /** Elementwise * */
+  private[math] def *@(that: Double) = cloned *= that
+
+  /** Elementwise / */
+  private[math] def /@(that: Vector) = cloned /= that
+
+  /** Elementwise / */
+  private[math] def /@(that: Double) = cloned /= that
+
+  /** Elementwise right-associative / */
+  private[math] def /@:(that: Double) = that /=: v.cloned
+
+  /** Elementwise right-associative / */
+  private[math] def /@:(that: Vector) = that.cloned /= v
+
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/MatrixOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Matrices, QRDecomposition, Vector, Matrix}
+import scala.collection.JavaConversions._
+import org.apache.mahout.math.function.{VectorFunction, DoubleFunction, Functions}
+import scala.math._
+
+class MatrixOps(val m: Matrix) {
+
+  import MatrixOps._
+
+  def nrow = m.rowSize()
+
+  def ncol = m.columnSize()
+
+
+  def unary_- = m.assign(Functions.NEGATE)
+
+  def +=(that: Matrix) = m.assign(that, Functions.PLUS)
+
+  def -=(that: Matrix) = m.assign(that, Functions.MINUS)
+
+  def +=(that: Double) = m.assign(new DoubleFunction {
+    def apply(x: Double): Double = x + that
+  })
+
+  def -=(that: Double) = +=(-that)
+
+  def -=:(that: Double) = m.assign(new DoubleFunction {
+    def apply(x: Double): Double = that - x
+  })
+
+  def +(that: Matrix) = cloned += that
+
+  def -(that: Matrix) = cloned -= that
+
+  // m.plus(that)?
+
+  def +(that: Double) = cloned += that
+
+  def -(that: Double) = cloned -= that
+
+  def -:(that: Double) = that -=: cloned
+
+
+  def norm = sqrt(m.aggregate(Functions.PLUS, Functions.SQUARE))
+
+  def pnorm(p: Int) = pow(m.aggregate(Functions.PLUS, Functions.chain(Functions.ABS, Functions.pow(p))), 1.0 / p)
+
+  def apply(row: Int, col: Int) = m.get(row, col)
+
+  def update(row: Int, col: Int, v: Double): Matrix = {
+    m.setQuick(row, col, v);
+    m
+  }
+
+  def update(rowRange: Range, colRange: Range, that: Matrix) = apply(rowRange, colRange) := that
+
+  def update(row: Int, colRange: Range, that: Vector) = apply(row, colRange) := that
+
+  def update(rowRange: Range, col: Int, that: Vector) = apply(rowRange, col) := that
+
+  def apply(rowRange: Range, colRange: Range): Matrix = {
+
+    if (rowRange == :: &&
+        colRange == ::) return m
+
+    val rr = if (rowRange == ::) (0 until m.nrow)
+    else rowRange
+    val cr = if (colRange == ::) (0 until m.ncol)
+    else colRange
+
+    return m.viewPart(rr.start, rr.length, cr.start, cr.length)
+
+  }
+
+  def apply(row: Int, colRange: Range): Vector = {
+    var r = m.viewRow(row)
+    if (colRange != ::) r = r.viewPart(colRange.start, colRange.length)
+    r
+  }
+
+  def apply(rowRange: Range, col: Int): Vector = {
+    var c = m.viewColumn(col)
+    if (rowRange != ::) c = c.viewPart(rowRange.start, rowRange.length)
+    c
+  }
+
+  /**
+   * Warning: This provides read-only view only.
+   * In most cases that's what one wants. To get a copy,
+   * use <code>m.t cloned</code>
+   * @return transposed view
+   */
+  def t = Matrices.transposedView(m)
+
+  def det = m.determinant()
+
+  def sum = m.zSum()
+
+  def :=(that: Matrix) = m.assign(that)
+
+  /**
+   * Assigning from a row-wise collection of vectors
+   * @param that
+   */
+  def :=(that: TraversableOnce[Vector]) = {
+    var row = 0
+    that.foreach(v => {
+      m.assignRow(row, v)
+      row += 1
+    })
+  }
+
+  def :=(f: (Int, Int, Double) => Double): Matrix = {
+    for (r <- 0 until nrow; c <- 0 until ncol) m(r, c) = f(r, c, m(r, c))
+    m
+  }
+
+  def cloned: Matrix = m.like := m
+
+  /**
+   * Ideally, we would probably want to override equals(). But that is not
+   * possible without modifying AbstractMatrix implementation in Mahout
+   * which would require discussion at Mahout team.
+   * @param that
+   * @return
+   */
+  def equiv(that: Matrix) =
+    that != null &&
+        nrow == that.nrow &&
+        m.view.zip(that).forall(t => {
+          t._1.equiv(t._2)
+        })
+
+  def nequiv(that: Matrix) = !equiv(that)
+
+  def ===(that: Matrix) = equiv(that)
+
+  def !==(that: Matrix) = nequiv(that)
+
+  /**
+   * test if rank == min(nrow,ncol).
+   * @return
+   */
+  def isFullRank: Boolean =
+    new QRDecomposition(if (nrow < ncol) m t else m cloned).hasFullRank
+
+  // We need this for some functions below (but it would screw some functions above)
+  import RLikeOps.v2vOps
+
+  def colSums() = m.aggregateColumns(vectorSumFunc)
+
+  def rowSums() = m.aggregateRows(vectorSumFunc)
+
+  def colMeans() = if (m.nrow == 0) colSums() else colSums() /= m.nrow
+
+  def rowMeans() = if (m.ncol == 0) rowSums() else rowSums() /= m.ncol
+
+
+}
+
+object MatrixOps {
+  implicit def m2ops(m: Matrix): MatrixOps = new MatrixOps(m)
+
+  implicit def v2ops(v: Vector): VectorOps = new VectorOps(v)
+
+  private def vectorSumFunc = new VectorFunction {
+    def apply(f: Vector): Double = f.sum
+  }
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Vector, Matrix}
+import scala.collection.JavaConversions._
+import RLikeOps._
+
+class RLikeMatrixOps(_m: Matrix) extends MatrixOps(_m) {
+
+  /**
+   * matrix-matrix multiplication
+   * @param that
+   * @return
+   */
+  def %*%(that: Matrix) = m.times(that)
+
+  /**
+   * matrix-vector multiplication
+   * @param that
+   * @return
+   */
+  def %*%(that: Vector) = m.times(that)
+
+  /**
+   * Hadamard product
+   *
+   * @param that
+   * @return
+   */
+
+  def *(that: Matrix) = cloned *= that
+
+  def *(that: Double) = cloned *= that
+
+  def /(that:Matrix) = cloned /= that
+
+  def /(that:Double) = cloned /= that
+
+  /** 1.0 /: A is eqivalent to R's 1.0/A */
+  def /:(that:Double) = that /=: cloned
+
+  /**
+   * in-place Hadamard product. We probably don't want to use assign
+   * to optimize for sparse operations, in case of Hadamard product
+   * it really can be done
+   * @param that
+   */
+  def *=(that: Matrix) = {
+    m.zip(that).foreach(t => t._1.vector *= t._2.vector)
+    m
+  }
+
+  /** Elementwise deletion */
+  def /=(that: Matrix) = {
+    m.zip(that).foreach(t => t._1.vector() /= t._2.vector)
+    m
+  }
+
+  def *=(that: Double) = {
+    m.foreach(_.vector() *= that)
+    m
+  }
+
+  def /=(that: Double) = {
+    m.foreach(_.vector() /= that)
+    m
+  }
+
+  /** 1.0 /=: A is equivalent to A = 1.0/A in R */
+  def /=:(that:Double) = {
+    m.foreach(that /=: _.vector())
+    m
+  }
+}
+
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Vector, MatrixTimesOps, Matrix}
+
+/**
+ * R-like operators. Declare <code>import RLikeOps._</code> to enable.
+ */
+object RLikeOps {
+
+  implicit def v2vOps(v: Vector) = new RLikeVectorOps(v)
+
+  implicit def times2timesOps(m: MatrixTimesOps) = new RLikeTimesOps(m)
+
+  implicit def m2mOps(m: Matrix) = new RLikeMatrixOps(m)
+
+}
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeTimesOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeTimesOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeTimesOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeTimesOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,28 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.{Matrix, MatrixTimesOps}
+
+class RLikeTimesOps(m: MatrixTimesOps) {
+
+  def :%*%(that: Matrix) = m.timesRight(that)
+
+  def %*%:(that: Matrix) = m.timesLeft(that)
+
+}
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeVectorOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeVectorOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeVectorOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/RLikeVectorOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,68 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math.Vector
+import org.apache.mahout.math.function.Functions
+import RLikeOps._
+
+/**
+ * R-like operators
+ *
+ * @param _v
+ */
+class RLikeVectorOps(_v: Vector) extends VectorOps(_v) {
+
+  /** Elementwise *= */
+  def *=(that: Vector) = v.assign(that, Functions.MULT)
+
+  /** Elementwise /= */
+  def /=(that: Vector) = v.assign(that, Functions.DIV)
+
+  /** Elementwise *= */
+  def *=(that: Double) = v.assign(Functions.MULT, that)
+
+  /** Elementwise /= */
+  def /=(that: Double) = v.assign(Functions.DIV, that)
+
+  /** Elementwise right-associative /= */
+  def /=:(that: Double) = v.assign(Functions.INV).assign(Functions.MULT, that)
+
+  /** Elementwise right-associative /= */
+  def /=:(that: Vector) = v.assign(Functions.INV).assign(that, Functions.MULT)
+
+  /** Elementwise * */
+  def *(that: Vector) = cloned *= that
+
+  /** Elementwise * */
+  def *(that: Double) = cloned *= that
+
+  /** Elementwise / */
+  def /(that: Vector) = cloned /= that
+
+  /** Elementwise / */
+  def /(that: Double) = cloned /= that
+
+  /** Elementwise right-associative / */
+  def /:(that: Double) = that /=: v.cloned
+
+  /** Elementwise right-associative / */
+  def /:(that: Vector) = that.cloned /= v
+
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/SSVD.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/SSVD.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/SSVD.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/SSVD.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,165 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import scala.math._
+import org.apache.mahout.math.{Matrices, Matrix}
+import RLikeOps._
+import org.apache.mahout.common.RandomUtils
+import scala._
+import org.apache.log4j.Logger
+
+private[math] object SSVD {
+
+  private val log = Logger.getLogger(SSVD.getClass)
+
+  /**
+   * In-core SSVD algorithm.
+   *
+   * @param a input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations
+   * @return (U,V,s)
+   */
+  def ssvd(a: Matrix, k: Int, p: Int = 15, q: Int = 0) = {
+    val m = a.nrow
+    val n = a.ncol
+    if (k > min(m, n))
+      throw new IllegalArgumentException(
+        "k cannot be greater than smaller of m,n")
+    val pfxed = min(p, min(m, n) - k)
+
+    // Actual decomposition rank
+    val r = k + pfxed
+
+    val rnd = RandomUtils.getRandom
+    val omega = Matrices.symmetricUniformView(n, r, rnd.nextInt)
+
+    var y = a %*% omega
+    var yty = y.t %*% y
+    val at = a.t
+    var ch = chol(yty)
+    assert(ch.isPositiveDefinite, "Rank-deficiency detected during s-SVD")
+    var bt = ch.solveRight(at %*% y)
+
+    // Power iterations
+    for (i <- 0 until q) {
+      y = a %*% bt
+      yty = y.t %*% y
+      ch = chol(yty)
+      bt = ch.solveRight(at %*% y)
+    }
+
+    val bbt = bt.t %*% bt
+    val (uhat, d) = eigen(bbt)
+
+    val s = d.sqrt
+    val u = ch.solveRight(y) %*% uhat
+    val v = bt %*% (uhat %*%: diagv(1 /: s))
+
+    (u(::, 0 until k), v(::, 0 until k), s(0 until k))
+  }
+
+  /**
+   * PCA based on SSVD that runs without forming an always-dense A-(colMeans(A)) input for SVD. This
+   * follows the solution outlined in MAHOUT-817. For in-core version it, for most part, is supposed
+   * to save some memory for sparse inputs by removing direct mean subtraction.<P>
+   *
+   * Hint: Usually one wants to use AV which is approsimately USigma, i.e.<code>u %*%: diagv(s)</code>.
+   * If retaining distances and orignal scaled variances not that important, the normalized PCA space
+   * is just U.
+   *
+   * Important: data points are considered to be rows.
+   *
+   * @param a input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations
+   * @return (U,V,s)
+   */
+  def spca(a:Matrix, k: Int, p: Int = 15, q: Int = 0) = {
+    val m = a.nrow
+    val n = a.ncol
+    if (k > min(m, n))
+      throw new IllegalArgumentException(
+        "k cannot be greater than smaller of m,n")
+    val pfxed = min(p, min(m, n) - k)
+
+    // Actual decomposition rank
+    val r = k + pfxed
+
+    val rnd = RandomUtils.getRandom
+    val omega = Matrices.symmetricUniformView(n, r, rnd.nextInt)
+
+    // Dataset mean
+    val xi = a.colMeans()
+
+    if (log.isDebugEnabled) log.debug("xi=%s".format(xi))
+
+    var y = a %*% omega
+
+    // Fixing y
+    val s_o = omega.t %*% xi
+    y := ((r,c,v) => v - s_o(c))
+
+    var yty = y.t %*% y
+    var ch = chol(yty)
+//    assert(ch.isPositiveDefinite, "Rank-deficiency detected during s-SVD")
+
+    // This is implicit Q of QR(Y)
+    var qm = ch.solveRight(y)
+    var bt = a.t %*% qm
+    var s_q = qm.colSums()
+    var s_b = bt.t %*% xi
+
+    // Power iterations
+    for (i <- 0 until q) {
+
+      // Fix bt
+      bt -= xi cross s_q
+
+      y = a %*% bt
+
+      // Fix Y again.
+      y := ((r,c,v) => v - s_b(c))
+
+      yty = y.t %*% y
+      ch = chol(yty)
+      qm = ch.solveRight(y)
+      bt = a.t %*% qm
+      s_q = qm.colSums()
+      s_b = bt.t %*% xi
+    }
+
+    val c = s_q cross s_b
+
+    // BB' computation becomes
+    val bbt = bt.t %*% bt -c - c.t +  (s_q cross s_q) * (xi dot xi)
+
+    val (uhat, d) = eigen(bbt)
+
+    val s = d.sqrt
+    val u = qm %*% uhat
+    val v = bt %*% (uhat %*%: diagv(1 /: s))
+
+    (u(::, 0 until k), v(::, 0 until k), s(0 until k))
+
+  }
+
+}
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/VectorOps.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/VectorOps.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/VectorOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/VectorOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.apache.mahout.math._
+import scala.collection.JavaConversions._
+import org.apache.mahout.math.function.Functions
+
+/**
+ * Syntactic sugar for mahout vectors
+ * @param v Mahout vector
+ */
+class VectorOps(val v: Vector) {
+
+  import RLikeOps._
+
+  def apply(i: Int) = v.get(i)
+
+  def update(i: Int, that: Double) = v.setQuick(i, that)
+
+  /** Warning: we only support consecutive views, step is not supported directly */
+  def apply(r: Range) = if (r == ::) v else v.viewPart(r.start, r.length * r.step)
+
+  def update(r: Range, that: Vector) = apply(r) := that
+
+  def sum = v.zSum()
+
+  def :=(that: Vector): Vector = {
+
+    // assign op in Mahout requires same
+    // cardinality between vectors .
+    // we want to relax it here and require
+    // v to have _at least_ as large cardinality
+    // as "that".
+    if (that.length == v.size())
+      v.assign(that)
+    else if (that.length < v.size) {
+      v.assign(0.0)
+      that.nonZeroes().foreach(t => v.setQuick(t.index, t.get))
+      v
+    } else throw new IllegalArgumentException("Assigner's cardinality less than assignee's")
+  }
+
+  def :=(that: Double): Vector = v.assign(that)
+
+  def :=(f: (Int, Double) => Double): Vector = {
+    for (i <- 0 until length) v(i) = f(i, v(i))
+    v
+  }
+
+  def equiv(that: Vector) =
+    length == that.length &&
+        v.all.view.zip(that.all).forall(t => t._1.get == t._2.get)
+
+  def ===(that: Vector) = equiv(that)
+
+  def !==(that: Vector) = nequiv(that)
+
+  def nequiv(that: Vector) = !equiv(that)
+
+  def unary_- = v.assign(Functions.NEGATE)
+
+  def +=(that: Vector) = v.assign(that, Functions.PLUS)
+
+  def -=(that: Vector) = v.assign(that, Functions.MINUS)
+
+  def +=(that: Double) = v.assign(Functions.PLUS, that)
+
+  def -=(that: Double) = +=(-that)
+
+  def -=:(that: Vector) = v.assign(Functions.NEGATE).assign(that, Functions.PLUS)
+
+  def -=:(that: Double) = v.assign(Functions.NEGATE).assign(Functions.PLUS, that)
+
+  def +(that: Vector) = cloned += that
+
+  def -(that: Vector) = cloned -= that
+
+  def -:(that: Vector) = that.cloned -= v
+
+  def +(that: Double) = cloned += that
+
+  def -(that: Double) = cloned -= that
+
+  def -:(that: Double) = that -=: v.cloned
+
+  def length = v.size()
+
+  def cloned: Vector = v.like := v
+
+  def sqrt = v.cloned.assign(Functions.SQRT)
+
+  /** Convert to a single column matrix */
+  def toColMatrix: Matrix = {
+    import RLikeOps._
+    v match {
+
+      case vd: Vector if (vd.isDense) => dense(vd).t
+      case srsv: RandomAccessSparseVector => new SparseColumnMatrix(srsv.length, 1, Array(srsv))
+      case _ => sparse(v).t
+    }
+  }
+
+}
+
+object VectorOps {
+}
diff -uNar -x .git mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/package.scala mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/package.scala
--- mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/package.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/package.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,261 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math
+
+import org.apache.mahout.math._
+import org.apache.mahout.math.solver.EigenDecomposition
+
+/**
+ * Mahout matrices and vectors' scala syntactic sugar
+ */
+package object scalabindings {
+
+  // Reserved "ALL" range
+  final val `::`: Range = null
+
+  implicit def seq2Vector(s: TraversableOnce[AnyVal]) =
+    new DenseVector(s.map(_.asInstanceOf[Number].doubleValue()).toArray)
+
+  implicit def tuple2TravOnce2svec[V <: AnyVal](sdata: TraversableOnce[(Int, V)]) = svec(sdata)
+
+  implicit def t1vec(s: Tuple1[AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t2vec(s: Tuple2[AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t3vec(s: Tuple3[AnyVal, AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t4vec(s: Tuple4[AnyVal, AnyVal, AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t5vec(s: Tuple5[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t6vec(s: Tuple6[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t7vec(s: Tuple7[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t8vec(s: Tuple8[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal]): Vector = prod2Vec(s)
+
+  implicit def t9vec(s: Tuple9[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal]): Vector =
+    prod2Vec(s)
+
+  implicit def t10vec(s: Tuple10[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t11vec(s: Tuple11[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t12vec(s: Tuple12[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t13vec(s: Tuple13[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t14vec(s: Tuple14[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t15vec(s: Tuple15[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t16vec(s: Tuple16[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t17vec(s: Tuple17[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t18vec(s: Tuple18[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t19vec(s: Tuple19[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t20vec(s: Tuple20[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t21vec(s: Tuple21[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+  implicit def t22vec(s: Tuple22[AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal
+      , AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal, AnyVal])
+  : Vector = prod2Vec(s)
+
+
+  def prod2Vec(s: Product) = new DenseVector(s.productIterator.
+      map(_.asInstanceOf[Number].doubleValue()).toArray)
+
+  def diagv(v: Vector): DiagonalMatrix = new DiagonalMatrix(v)
+
+  def diag(v: Double, size: Int): DiagonalMatrix =
+    new DiagonalMatrix(new DenseVector(Array.fill(size)(v)))
+
+  def eye(size: Int) = new DiagonalMatrix(1.0, size)
+
+  /**
+   * Create dense matrix out of inline arguments -- rows -- which can be tuples,
+   * iterables of Double, or just single Number (for columnar vectors)
+   * @param rows
+   * @tparam R
+   * @return
+   */
+  def dense[R](rows: R*): DenseMatrix = {
+    import MatrixOps._
+    val data = for (r <- rows) yield {
+      r match {
+        case n: Number => Array(n.doubleValue())
+        case t: Product => t.productIterator.map(_.asInstanceOf[Number].doubleValue()).toArray
+        case t: Vector => Array.tabulate(t.length)(t(_))
+        case t: Array[Double] => t
+        case t: Iterable[Double] => t.toArray
+        case t: Array[Array[Double]] => if (rows.size == 1)
+          return new DenseMatrix(t)
+        else
+          throw new IllegalArgumentException(
+            "double[][] data parameter can be the only argumentn for dense()")
+        case t:Array[Vector] =>
+          val m = new DenseMatrix(t.size,t.head.length)
+          t.view.zipWithIndex.foreach({case(v,idx) => m(idx,::) := v})
+          return m
+        case _ => throw new IllegalArgumentException("unsupported type in the inline Matrix initializer")
+      }
+    }
+    new DenseMatrix(data.toArray)
+  }
+
+  /**
+   * Default initializes are always row-wise.
+   * create a sparse,
+   * e.g.
+   * m = sparse(
+   * (0,5)::(9,3)::Nil,
+   * (2,3.5)::(7,8)::Nil
+   * )
+   *
+   * @param rows
+   * @return
+   */
+
+  def sparse(rows: Vector*): SparseRowMatrix = {
+    import MatrixOps._
+    val nrow = rows.size
+    val ncol = rows.map(_.size()).max
+    val m = new SparseRowMatrix(nrow, ncol)
+    m := rows
+    m
+
+  }
+
+  /**
+   * create a sparse vector out of list of tuple2's
+   * @param sdata
+   * @return
+   */
+  def svec(sdata: TraversableOnce[(Int, AnyVal)]) = {
+    val cardinality = if (sdata.size > 0) sdata.map(_._1).max + 1 else 0
+    val initialCapacity = sdata.size
+    val sv = new RandomAccessSparseVector(cardinality, initialCapacity)
+    sdata.foreach(t => sv.setQuick(t._1, t._2.asInstanceOf[Number].doubleValue()))
+    sv
+  }
+
+  def dvec(fromV: Vector) = new DenseVector(fromV)
+
+  def dvec(ddata: TraversableOnce[Double]) = new DenseVector(ddata.toArray)
+
+  def dvec(numbers: Number*) = new DenseVector(numbers.map(_.doubleValue()).toArray)
+
+  def chol(m: Matrix, pivoting: Boolean = false) = new CholeskyDecomposition(m, pivoting)
+
+  /**
+   * computes SVD
+   * @param m svd input
+   * @return (U,V, singular-values-vector)
+   */
+  def svd(m: Matrix) = {
+    val svdObj = new SingularValueDecomposition(m)
+    (svdObj.getU, svdObj.getV, new DenseVector(svdObj.getSingularValues))
+  }
+
+  /**
+   * Computes Eigendecomposition of a symmetric matrix
+   * @param m symmetric input matrix
+   * @return (V, eigen-values-vector)
+   */
+  def eigen(m: Matrix) = {
+    val ed = new EigenDecomposition(m, true)
+    (ed.getV, ed.getRealEigenvalues)
+  }
+
+
+  /**
+   * More general version of eigen decomposition
+   * @param m
+   * @param symmetric
+   * @return (V, eigenvalues-real-vector, eigenvalues-imaginary-vector)
+   */
+  def eigenFull(m: Matrix, symmetric: Boolean = true) {
+    val ed = new EigenDecomposition(m, symmetric)
+    (ed.getV, ed.getRealEigenvalues, ed.getImagEigenvalues)
+  }
+
+  /**
+   * QR.
+   *
+   * Right now Mahout's QR seems to be using argument for in-place transformations,
+   * so the matrix context gets messed after this. Hence we force cloning of the
+   * argument before passing it to Mahout's QR so to keep expected semantics.
+   * @param m
+   * @return (Q,R)
+   */
+  def qr(m: Matrix) = {
+    import MatrixOps._
+    val qrdec = new QRDecomposition(m cloned)
+    (qrdec.getQ, qrdec.getR)
+  }
+
+  def ssvd(a: Matrix, k: Int, p: Int = 15, q: Int = 0) = SSVD.ssvd(a, k, p, q)
+
+  /**
+   * PCA based on SSVD that runs without forming an always-dense A-(colMeans(A)) input for SVD. This
+   * follows the solution outlined in MAHOUT-817. For in-core version it, for most part, is supposed
+   * to save some memory for sparse inputs by removing direct mean subtraction.<P>
+   *
+   * Hint: Usually one wants to use AV which is approsimately USigma, i.e.<code>u %*%: diagv(s)</code>.
+   * If retaining distances and orignal scaled variances not that important, the normalized PCA space
+   * is just U.
+   *
+   * Important: data points are considered to be rows.
+   *
+   * @param a input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations
+   * @return (U,V,s)
+   */
+  def spca(a: Matrix, k: Int, p: Int = 15, q: Int = 0) =
+    SSVD.spca(a = a, k = k, p = p, q = q)
+
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MathSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MathSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MathSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MathSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,243 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.scalatest.{Matchers, FunSuite}
+import org.apache.mahout.math._
+import scala.math._
+import RLikeOps._
+import scala._
+import scala.util.Random
+import org.apache.mahout.test.MahoutSuite
+import org.apache.mahout.common.RandomUtils
+
+class MathSuite extends FunSuite with MahoutSuite {
+
+  test("chol") {
+
+    // try to solve Ax=b with cholesky:
+    // this requires
+    // (LL')x = B
+    // L'x= (L^-1)B
+    // x=(L'^-1)(L^-1)B
+
+    val a = dense((1, 2, 3), (2, 3, 4), (3, 4, 5.5))
+
+    // make sure it is symmetric for a valid solution
+    a := a.t %*% a
+
+    printf("A= \n%s\n", a)
+
+    val b = dense((9, 8, 7)).t
+
+    printf("b = \n%s\n", b)
+
+    // fails if chol(a,true)
+    val ch = chol(a)
+
+    printf("L = \n%s\n", ch.getL)
+
+    printf("(L^-1)b =\n%s\n", ch.solveLeft(b))
+
+    val x = ch.solveRight(eye(3)) %*% ch.solveLeft(b)
+
+    printf("x = \n%s\n", x.toString)
+
+    val axmb = (a %*% x) - b
+
+    printf("AX - B = \n%s\n", axmb.toString)
+
+    axmb.norm should be < 1e-10
+
+  }
+
+  test("chol2") {
+
+    val vtv = new DenseSymmetricMatrix(
+      Array(
+        0.0021401286568947376, 0.001309251254596442, 0.0016003218703045058,
+        0.001545407014131058, 0.0012772546647977234,
+        0.001747768702674435
+      ), true)
+
+    printf("V'V=\n%s\n", vtv cloned)
+
+    val vblock = dense(
+      (0.0012356809018514347, 0.006141139195280868, 8.037742467936037E-4),
+      (0.007910767859830255, 0.007989899899005457, 0.006877961936587515),
+      (0.007011211118759952, 0.007458865101641882, 0.0048344749320346795),
+      (0.006578789899685284, 0.0010812485516549452, 0.0062146270886981655)
+    )
+
+    val d = diag(15.0, 4)
+
+
+    val b = dense(
+      (0.36378319648203084),
+      (0.3627384439613304),
+      (0.2996934112658234))
+
+    printf("B=\n%s\n", b)
+
+
+    val cholArg = vtv + (vblock.t %*% d %*% vblock) + diag(4e-6, 3)
+
+    printf("cholArg=\n%s\n", cholArg)
+
+    printf("V'DV=\n%s\n", (vblock.t %*% d %*% vblock))
+
+    printf("V'V+V'DV=\n%s\n", vtv + (vblock.t %*% d %*% vblock))
+
+    val ch = chol(cholArg)
+
+    printf("L=\n%s\n", ch.getL)
+
+    val x = ch.solveRight(eye(cholArg.nrow)) %*% ch.solveLeft(b)
+
+    printf("X=\n%s\n", x)
+
+    assert((cholArg %*% x - b).norm < 1e-10)
+
+  }
+
+  test("qr") {
+    val a = dense((1, 2, 3), (2, 3, 6), (3, 4, 5), (4, 7, 8))
+    val (q, r) = qr(a)
+
+    printf("Q=\n%s\n", q)
+    printf("R=\n%s\n", r)
+
+    for (i <- 0 until q.ncol; j <- i + 1 until q.ncol)
+      assert(abs(q(::, i) dot q(::, j)) < 1e-10)
+  }
+
+  test("svd") {
+
+    val a = dense((1, 2, 3), (3, 4, 5))
+
+    val (u, v, s) = svd(a)
+
+    printf("U:\n%s\n", u.toString)
+    printf("V:\n%s\n", v.toString)
+    printf("Sigma:\n%s\n", s.toString)
+
+    val aBar = u %*% diagv(s) %*% v.t
+
+    val amab = a - aBar
+
+    printf("A-USV'=\n%s\n", amab.toString)
+
+    assert(amab.norm < 1e-10)
+
+  }
+
+  test("ssvd") {
+    val a = dense(
+      (1, 2, 3),
+      (3, 4, 5),
+      (-2, 6, 7),
+      (-3, 8, 9)
+    )
+
+    val rank = 2
+    val (u, v, s) = ssvd(a, k = rank, q = 1)
+
+    val (uControl, vControl, sControl) = svd(a)
+
+    printf("U:\n%s\n", u)
+    printf("U-control:\n%s\n", uControl)
+    printf("V:\n%s\n", v)
+    printf("V-control:\n%s\n", vControl)
+    printf("Sigma:\n%s\n", s)
+    printf("Sigma-control:\n%s\n", sControl)
+
+    (s - sControl(0 until rank)).norm(2) should be < 1E-7
+
+    // Singular vectors may be equivalent down to a sign only.
+    (u.norm - uControl(::, 0 until rank).norm).abs should be < 1E-7
+    (v.norm - vControl(::, 0 until rank).norm).abs should be < 1E-7
+
+  }
+
+  test("spca") {
+
+    import math._
+
+    val rnd = RandomUtils.getRandom
+
+    // Number of points
+    val m =  500
+    // Length of actual spectrum
+    val spectrumLen = 40
+
+    val spectrum = dvec((0 until spectrumLen).map(x => 300.0 * exp(-x) max 1e-3))
+    printf("spectrum:%s\n", spectrum)
+
+    val (u, _) = qr(new SparseRowMatrix(m, spectrumLen) :=
+        ((r, c, v) => if (rnd.nextDouble() < 0.2) 0 else rnd.nextDouble() + 5.0))
+
+    // PCA Rotation matrix -- should also be orthonormal.
+    val (tr, _) = qr(Matrices.symmetricUniformView(spectrumLen, spectrumLen, rnd.nextInt) - 10.0)
+
+    val input = (u %*%: diagv(spectrum)) %*% tr.t
+
+    // Calculate just first 10 principal factors and reduce dimensionality.
+    // Since we assert just validity of the s-pca, not stochastic error, we bump p parameter to
+    // ensure to zero stochastic error and assert only functional correctness of the method's pca-
+    // specific additions.
+    val k = 10
+    var (pca, _, s) = spca(a = input, k = k, p=spectrumLen, q = 1)
+    printf("Svs:%s\n",s)
+    // Un-normalized pca data:
+    pca = pca %*%: diagv(s)
+
+    // Of course, once we calculated the pca, the spectrum is going to be different since our originally
+    // generated input was not centered. So here, we'd just brute-solve pca to verify
+    val xi = input.colMeans()
+    for (r <- 0 until input.nrow) input(r, ::) -= xi
+    var (pcaControl, _, sControl) = svd(m = input)
+
+    printf("Svs-control:%s\n",sControl)
+    pcaControl = (pcaControl %*%: diagv(sControl))(::,0 until k)
+
+    printf("pca:\n%s\n", pca(0 until 10, 0 until 10))
+    printf("pcaControl:\n%s\n", pcaControl(0 until 10, 0 until 10))
+
+    (pca(0 until 10, 0 until 10).norm - pcaControl(0 until 10, 0 until 10).norm).abs should be < 1E-5
+
+  }
+
+  test("random uniform") {
+    val omega1 = Matrices.symmetricUniformView(2, 3, 1234)
+    val omega2 = Matrices.symmetricUniformView(2, 3, 1234)
+
+    val a = sparse(
+      0 -> 1 :: 1 -> 2 :: Nil,
+      0 -> 3 :: 1 -> 4 :: Nil,
+      0 -> 2 :: 1 -> 0.0 :: Nil
+    )
+
+    val block = a(0 to 0, ::).cloned
+    val block2 = a(1 to 1, ::).cloned
+
+    (block %*% omega1 - (a %*% omega2)(0 to 0, ::)).norm should be < 1e-7
+    (block2 %*% omega1 - (a %*% omega2)(1 to 1, ::)).norm should be < 1e-7
+
+  }
+
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOpsSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOpsSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatlabLikeMatrixOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,67 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.scalatest.FunSuite
+import MatlabLikeOps._
+import scala.Predef._
+import org.apache.mahout.test.MahoutSuite
+
+class MatlabLikeMatrixOpsSuite extends FunSuite with MahoutSuite {
+
+  test("multiplication") {
+
+    val a = dense((1, 2, 3), (3, 4, 5))
+    val b = dense(1, 4, 5)
+    val m = a * b
+
+    assert(m(0, 0) == 24)
+    assert(m(1, 0) == 44)
+    println(m.toString)
+  }
+
+  test("Hadamard") {
+    val a = dense(
+      (1, 2, 3),
+      (3, 4, 5)
+    )
+    val b = dense(
+      (1, 1, 2),
+      (2, 1, 1)
+    )
+
+    val c = a *@ b
+
+    printf("C=\n%s\n", c)
+
+    assert(c(0, 0) == 1)
+    assert(c(1, 2) == 5)
+    println(c.toString)
+
+    val d = a *@ 5.0
+    assert(d(0, 0) == 5)
+    assert(d(1, 1) == 20)
+
+    a *@= b
+    assert(a(0, 0) == 1)
+    assert(a(1, 2) == 5)
+    println(a.toString)
+
+  }
+
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/MatrixOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,125 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.scalatest.{Matchers, FunSuite}
+import MatrixOps._
+import scala._
+import org.apache.mahout.test.MahoutSuite
+
+
+class MatrixOpsSuite extends FunSuite with MahoutSuite {
+
+
+  test("equivalence") {
+    val a = dense((1, 2, 3), (3, 4, 5))
+    val b = dense((1, 2, 3), (3, 4, 5))
+    val c = dense((1, 4, 3), (3, 4, 5))
+    assert(a === b)
+    assert(a !== c)
+
+  }
+  test("elementwise plus, minus") {
+    val a = dense((1, 2, 3), (3, 4, 5))
+    val b = dense((1, 1, 2), (2, 1, 1))
+
+    val c = a + b
+    assert(c(0, 0) == 2)
+    assert(c(1, 2) == 6)
+    println(c.toString)
+
+  }
+
+  test("matrix, vector slicing") {
+
+    val a = dense((1, 2, 3), (3, 4, 5))
+
+    assert(a(::, 0).sum == 4)
+    assert(a(1, ::).sum == 12)
+
+    assert(a(0 to 1, 1 to 2).sum == 14)
+
+    // assign to slice-vector
+    a(0, 0 to 1) :=(3, 5)
+    // or
+    a(0, 0 to 1) = (3, 5)
+
+    assert(a(0, ::).sum == 11)
+
+    println(a.toString)
+
+    // assign to a slice-matrix
+    a(0 to 1, 0 to 1) := dense((1, 1), (2, 2.5))
+
+    // or
+    a(0 to 1, 0 to 1) = dense((1, 1), (2, 2.5))
+
+    println(a)
+    println(a.sum)
+
+    val b = dense((1, 2, 3), (3, 4, 5))
+    b(0, ::) -= dvec(1, 2, 3)
+    println(b)
+    b(0, ::) should equal(dvec(0, 0, 0))
+
+  }
+
+  test("assignments") {
+
+    val a = dense((1, 2, 3), (3, 4, 5))
+
+    val b = a cloned
+
+    b(0, 0) = 2.0
+
+    printf("B=\n%s\n", b)
+
+    assert((b - a).norm - 1 < 1e-10)
+
+    val e = eye(5)
+
+    printf("I(5)=\n%s\n", e)
+
+    a(0 to 1, 1 to 2) = dense((3, 2), (2, 3))
+    a(0 to 1, 1 to 2) := dense((3, 2), (2, 3))
+
+
+  }
+
+  test("sparse") {
+
+    val a = sparse((1, 3) :: Nil,
+      (0, 2) ::(1, 2.5) :: Nil
+    )
+    println(a.toString)
+  }
+
+  test("colSums, rowSums, colMeans, rowMeans") {
+    val a = dense(
+      (2, 3, 4),
+      (3, 4, 5)
+    )
+
+    a.colSums() should equal(dvec(5, 7, 9))
+    a.rowSums() should equal(dvec(9, 12))
+    a.colMeans() should equal(dvec(2.5, 3.5, 4.5))
+    a.rowMeans() should equal(dvec(3, 4))
+
+  }
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOpsSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOpsSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeMatrixOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.scalatest.FunSuite
+import RLikeOps._
+import org.apache.mahout.test.MahoutSuite
+
+class RLikeMatrixOpsSuite extends FunSuite with MahoutSuite {
+
+  test("multiplication") {
+
+    val a = dense((1, 2, 3), (3, 4, 5))
+    val b = dense(1, 4, 5)
+    val m = a %*% b
+
+    assert(m(0, 0) == 24)
+    assert(m(1, 0) == 44)
+    println(m.toString)
+  }
+
+  test("Hadamard") {
+    val a = dense(
+      (1, 2, 3),
+      (3, 4, 5)
+    )
+    val b = dense(
+      (1, 1, 2),
+      (2, 1, 1)
+    )
+
+    val c = a * b
+
+    printf("C=\n%s\n", c)
+
+    assert(c(0, 0) == 1)
+    assert(c(1, 2) == 5)
+    println(c.toString)
+
+    val d = a * 5.0
+    assert(d(0, 0) == 5)
+    assert(d(1, 1) == 20)
+
+    a *= b
+    assert(a(0, 0) == 1)
+    assert(a(1, 2) == 5)
+    println(a.toString)
+
+  }
+
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeVectorOpsSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeVectorOpsSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeVectorOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/RLikeVectorOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.scalatest.FunSuite
+import org.apache.mahout.math.Vector
+import RLikeOps._
+import org.apache.mahout.test.MahoutSuite
+
+/**
+ *
+ * @author dmitriy
+ */
+class RLikeVectorOpsSuite extends FunSuite with MahoutSuite {
+
+  test("Hadamard") {
+    val a: Vector = (1, 2, 3)
+    val b = (3, 4, 5)
+
+    val c = a * b
+    println(c)
+    assert(c ===(3, 8, 15))
+  }
+
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/VectorOpsSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/VectorOpsSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/VectorOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/math/scalabindings/VectorOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.math.scalabindings
+
+import org.scalatest.FunSuite
+import org.apache.mahout.math.{RandomAccessSparseVector, Vector}
+import MatrixOps._
+import org.apache.mahout.test.MahoutSuite
+
+/** VectorOps Suite */
+class VectorOpsSuite extends FunSuite with MahoutSuite {
+
+  test("inline create") {
+
+    val sparseVec = svec((5 -> 1) :: (10 -> 2.0) :: Nil)
+    println(sparseVec)
+
+    val sparseVec2: Vector = (5 -> 1.0) :: (10 -> 2.0) :: Nil
+    println(sparseVec2)
+
+    val sparseVec3: Vector = new RandomAccessSparseVector(100) := (5 -> 1.0) :: Nil
+    println(sparseVec3)
+
+    val denseVec1: Vector = (1.0, 1.1, 1.2)
+    println(denseVec1)
+
+    val denseVec2 = dvec(1, 0, 1.1, 1.2)
+    println(denseVec2)
+  }
+
+  test("plus minus") {
+
+    val a: Vector = (1, 2, 3)
+    val b: Vector = (0 -> 3) :: (1 -> 4) :: (2 -> 5) :: Nil
+
+    val c = a + b
+    val d = b - a
+    val e = -b - a
+
+    assert(c ===(4, 6, 8))
+    assert(d ===(2, 2, 2))
+    assert(e ===(-4, -6, -8))
+
+  }
+
+  test("dot") {
+
+    val a: Vector = (1, 2, 3)
+    val b = (3, 4, 5)
+
+    val c = a dot b
+    println(c)
+    assert(c == 26)
+
+  }
+
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/test/LoggerConfiguration.scala mahout/math-scala/src/test/scala/org/apache/mahout/test/LoggerConfiguration.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/test/LoggerConfiguration.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/test/LoggerConfiguration.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,19 @@
+package org.apache.mahout.test
+
+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfterEach, BeforeAndAfter, Suite}
+import org.apache.log4j.{Level, Logger, BasicConfigurator}
+
+/**
+ * @author dmitriy
+ */
+trait LoggerConfiguration extends BeforeAndAfterAll {
+  this: Suite =>
+
+  override protected def beforeAll(): Unit = {
+    super.beforeAll()
+    BasicConfigurator.resetConfiguration()
+    BasicConfigurator.configure()
+    Logger.getRootLogger.setLevel(Level.ERROR)
+    Logger.getLogger("org.apache.mahout.math.scalabindings").setLevel(Level.DEBUG)
+  }
+}
diff -uNar -x .git mahout/math-scala/src/test/scala/org/apache/mahout/test/MahoutSuite.scala mahout/math-scala/src/test/scala/org/apache/mahout/test/MahoutSuite.scala
--- mahout/math-scala/src/test/scala/org/apache/mahout/test/MahoutSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/math-scala/src/test/scala/org/apache/mahout/test/MahoutSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,16 @@
+package org.apache.mahout.test
+
+import org.scalatest.{Matchers, BeforeAndAfterEach, Suite}
+import org.apache.mahout.common.RandomUtils
+
+/**
+ * @author dmitriy
+ */
+trait MahoutSuite extends BeforeAndAfterEach with LoggerConfiguration with Matchers {
+  this:Suite =>
+
+  override protected def beforeEach() {
+    super.beforeEach()
+    RandomUtils.useTestSeed()
+  }
+}
diff -uNar -x .git mahout/pom.xml mahout/pom.xml
--- mahout/pom.xml	2014-03-29 01:04:48.000000000 -0700
+++ mahout/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -20,7 +20,7 @@
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.mahout</groupId>
   <artifactId>mahout</artifactId>
-  <version>0.8</version>
+  <version>1.0-SNAPSHOT</version>
 
   <parent>
     <groupId>org.apache</groupId>
@@ -99,13 +99,14 @@
     <maven.compiler.target>1.6</maven.compiler.target>
     <maven.clover.multiproject>true</maven.clover.multiproject>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
-    <msurefire.version>2.15</msurefire.version>
+    <msurefire.version>2.16</msurefire.version>
     <mpmd.version>3.0.1</mpmd.version>
     <mcheckstyle.version>2.10</mcheckstyle.version>
     <mfindbugs.version>2.5.2</mfindbugs.version>
     <mjavadoc.version>2.9.1</mjavadoc.version>
-    <hadoop.version>1.1.2</hadoop.version>
-    <lucene.version>4.3.0</lucene.version>
+    <hadoop.1.version>1.2.1</hadoop.1.version>
+    <lucene.version>4.6.1</lucene.version>
+    <slf4j.version>1.7.5</slf4j.version>
   </properties>
   <issueManagement>
     <system>Jira</system>
@@ -161,18 +162,21 @@
         <artifactId>mahout-integration</artifactId>
         <groupId>${project.groupId}</groupId>
         <version>${project.version}</version>
+        <optional>true</optional>
       </dependency>
 
       <dependency>
         <artifactId>mahout-buildtools</artifactId>
         <groupId>${project.groupId}</groupId>
         <version>${project.version}</version>
+        <optional>true</optional>
       </dependency>
 
       <dependency>
         <artifactId>mahout-examples</artifactId>
         <groupId>${project.groupId}</groupId>
         <version>${project.version}</version>
+        <optional>true</optional>
       </dependency>
 
       <!-- 3rd party  -->
@@ -191,7 +195,7 @@
         <artifactId>lucene-benchmark</artifactId>
         <version>${lucene.version}</version>
       </dependency>
-      
+
       <dependency>
         <groupId>junit</groupId>
         <artifactId>junit</artifactId>
@@ -199,148 +203,29 @@
         <scope>test</scope>
       </dependency>
       <dependency>
+        <groupId>org.hamcrest</groupId>
+        <artifactId>hamcrest-all</artifactId>
+        <version>1.3</version>
+        <scope>test</scope>
+      </dependency>
+      <dependency>
         <groupId>org.easymock</groupId>
         <artifactId>easymock</artifactId>
-        <version>3.1</version>
+        <version>3.2</version>
         <scope>test</scope>
       </dependency>
-
       <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-core</artifactId>
-        <version>${hadoop.version}</version>
-        <exclusions>
-          <exclusion>
-            <groupId>net.sf.kosmosfs</groupId>
-            <artifactId>kfs</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jetty</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jetty-util</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>hsqldb</groupId>
-            <artifactId>hsqldb</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>junit</groupId>
-            <artifactId>junit</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>oro</groupId>
-            <artifactId>oro</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jsp-2.1</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jsp-api-2.1</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>servlet-api-2.5</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>tomcat</groupId>
-            <artifactId>jasper-runtime</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>tomcat</groupId>
-            <artifactId>jasper-compiler</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>xmlenc</groupId>
-            <artifactId>xmlenc</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>net.java.dev.jets3t</groupId>
-            <artifactId>jets3t</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.eclipse.jdt</groupId>
-            <artifactId>core</artifactId>
-          </exclusion>
-        </exclusions>
-      </dependency>
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-common</artifactId>
-        <version>${hadoop.version}</version>
-        <exclusions>
-          <exclusion>
-            <groupId>net.sf.kosmosfs</groupId>
-            <artifactId>kfs</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jetty</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jetty-util</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>hsqldb</groupId>
-            <artifactId>hsqldb</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>junit</groupId>
-            <artifactId>junit</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>oro</groupId>
-            <artifactId>oro</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jsp-2.1</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>jsp-api-2.1</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.mortbay.jetty</groupId>
-            <artifactId>servlet-api-2.5</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>tomcat</groupId>
-            <artifactId>jasper-runtime</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>tomcat</groupId>
-            <artifactId>jasper-compiler</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>xmlenc</groupId>
-            <artifactId>xmlenc</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>net.java.dev.jets3t</groupId>
-            <artifactId>jets3t</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>org.eclipse.jdt</groupId>
-            <artifactId>core</artifactId>
-          </exclusion>
-        </exclusions>
-      </dependency>
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-mapreduce-client-core</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-mapreduce-client-common</artifactId>
-        <version>${hadoop.version}</version>
+        <groupId>com.carrotsearch.randomizedtesting</groupId>
+        <artifactId>randomizedtesting-runner</artifactId>
+        <version>2.0.15</version>
+        <scope>test</scope>
       </dependency>
+      <dependency>
+        <groupId>org.apache.lucene</groupId>
+        <artifactId>lucene-test-framework</artifactId>
+        <version>${lucene.version}</version>
+        <scope>test</scope>
+       </dependency>
 
       <dependency>
         <groupId>org.codehaus.jackson</groupId>
@@ -366,21 +251,44 @@
       </dependency>
 
       <dependency>
+        <groupId>commons-io</groupId>
+        <artifactId>commons-io</artifactId>
+        <version>2.4</version>
+      </dependency>
+
+      <dependency>
         <groupId>org.slf4j</groupId>
         <artifactId>slf4j-api</artifactId>
-        <version>1.7.5</version>
+        <version>${slf4j.version}</version>
       </dependency>
       <dependency>
         <groupId>org.slf4j</groupId>
         <artifactId>slf4j-jcl</artifactId>
-        <version>1.7.5</version>
+        <version>${slf4j.version}</version>
+        <scope>test</scope>
+      </dependency>
+      <dependency>
+        <groupId>org.slf4j</groupId>
+        <artifactId>jcl-over-slf4j</artifactId>
+        <version>${slf4j.version}</version>
         <scope>test</scope>
       </dependency>
+      <dependency>
+        <groupId>org.slf4j</groupId>
+        <artifactId>slf4j-log4j12</artifactId>
+        <version>${slf4j.version}</version>
+      </dependency>
 
       <dependency>
-        <groupId>commons-io</groupId>
-        <artifactId>commons-io</artifactId>
-        <version>2.4</version>
+        <groupId>commons-logging</groupId>
+        <artifactId>commons-logging</artifactId>
+        <version>1.1.3</version>
+      </dependency>
+
+      <dependency>
+        <groupId>log4j</groupId>
+        <artifactId>log4j</artifactId>
+        <version>1.2.17</version>
       </dependency>
 
       <dependency>
@@ -404,7 +312,7 @@
       <dependency>
         <groupId>com.google.guava</groupId>
         <artifactId>guava</artifactId>
-        <version>14.0.1</version>
+        <version>16.0</version>
       </dependency>
 
       <dependency>
@@ -436,7 +344,7 @@
                   <pluginExecutionFilter>
                     <groupId>org.apache.maven.plugins</groupId>
                     <artifactId>maven-dependency-plugin</artifactId>
-                    <versionRange>2.7</versionRange>
+                    <versionRange>2.8</versionRange>
                     <goals>
                       <goal>copy-dependencies</goal>
                     </goals>
@@ -502,12 +410,12 @@
         <plugin>
           <groupId>org.apache.maven.plugins</groupId>
           <artifactId>maven-dependency-plugin</artifactId>
-          <version>2.7</version>
+          <version>2.8</version>
         </plugin>
         <plugin>
           <groupId>org.apache.maven.plugins</groupId>
           <artifactId>maven-install-plugin</artifactId>
-          <version>2.4</version>
+          <version>2.5.1</version>
         </plugin>
         <plugin>
           <groupId>org.apache.maven.plugins</groupId>
@@ -621,12 +529,11 @@
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-surefire-plugin</artifactId>
         <configuration>
-          <forkCount>1</forkCount>
+          <forkCount>2</forkCount>
           <reuseForks>false</reuseForks>
-          <threadCount>1</threadCount>
-          <perCoreThreadCount>false</perCoreThreadCount>
-          <parallel>classes</parallel>
-          <argLine>-Xmx512m</argLine>
+          <argLine>-Xmx512m -Djava.security.manager
+          -Djava.security.policy=${project.build.directory}/../../buildtools/src/test/resources/java.policy</argLine>
+          <argLine>-Djava.security.auth.login.config=${project.build.directory}/../../buildtools/src/test/resources/jaas.config</argLine>
           <testFailureIgnore>false</testFailureIgnore>
           <redirectTestOutputToFile>true</redirectTestOutputToFile>
           <systemPropertyVariables>
@@ -650,7 +557,7 @@
       <plugin>
         <groupId>com.atlassian.maven.plugins</groupId>
         <artifactId>maven-clover2-plugin</artifactId>
-        <version>3.1.11.1</version>
+        <version>3.1.12</version>
       </plugin>
       <plugin>
         <groupId>org.apache.maven.plugins</groupId>
@@ -704,13 +611,51 @@
           <effort>Default</effort>
           <!--visitors>FindDeadLocalStores,UnreadFields</visitors-->
           <!--omitVisitors>FindDeadLocalStores,UnreadFields</omitVisitors-->
-          <debug>true</debug>
           <relaxed>true</relaxed>
           <!-- classpath -->
           <excludeFilterFile>findbugs-exclude.xml</excludeFilterFile>
           <failOnError>false</failOnError>
         </configuration>
       </plugin>
+      <plugin>
+        <groupId>org.apache.rat</groupId>
+        <artifactId>apache-rat-plugin</artifactId>
+        <version>0.10</version>
+        <configuration>
+          <licenses>
+            <license implementation="org.apache.rat.analysis.license.SimplePatternBasedLicense">
+              <patterns>
+                <note>CERN license for Colt -- basically only requires attribution</note>
+                <pattern>1999 CERN - European Organization for Nuclear Research</pattern>
+              </patterns>
+            </license>
+            <license implementation="org.apache.rat.analysis.license.SimplePatternBasedLicense">
+              <patterns>
+                <pattern>public domain</pattern>
+              </patterns>
+            </license>
+          </licenses>
+          <excludes>
+            <exclude>**/*.conf</exclude>
+            <exclude>**/*.iml</exclude>
+            <exclude>**/*.md</exclude>
+            <exclude>**/*.props</exclude>
+            <exclude>**/gen/**</exclude>
+            <exclude>**/resources/**</exclude>
+            <exclude>**/images/**</exclude>
+            <exclude>**/target/**</exclude>
+            <exclude>**/testdata/**</exclude>
+            <exclude>**/.idea/**</exclude>
+            <exclude>**/package-info.java</exclude>
+            <exclude>**/.git/**</exclude>
+            <exclude>**/.classpath</exclude>
+            <exclude>**/.project</exclude>
+            <exclude>**/.settings/**</exclude>
+            <exclude>**/*.patch</exclude>
+          </excludes>
+          <excludeSubProjects>false</excludeSubProjects>
+        </configuration>
+      </plugin>
     </plugins>
     <resources>
       <resource>
@@ -726,9 +671,196 @@
     <module>integration</module>
     <module>examples</module>
     <module>distribution</module>
+    <module>math-scala</module>
+    <module>spark</module>
   </modules>
   <profiles>
     <profile>
+      <id>hadoop1</id>
+      <activation>
+        <property>
+          <name>!hadoop2.version</name>
+        </property>
+      </activation>
+      <dependencyManagement>
+        <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-core</artifactId>
+          <version>${hadoop.1.version}</version>
+          <exclusions>
+            <exclusion>
+              <groupId>net.sf.kosmosfs</groupId>
+              <artifactId>kfs</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jetty</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jetty-util</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>hsqldb</groupId>
+              <artifactId>hsqldb</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>junit</groupId>
+              <artifactId>junit</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>oro</groupId>
+              <artifactId>oro</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jsp-2.1</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jsp-api-2.1</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>servlet-api-2.5</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>tomcat</groupId>
+              <artifactId>jasper-runtime</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>tomcat</groupId>
+              <artifactId>jasper-compiler</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>xmlenc</groupId>
+              <artifactId>xmlenc</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>net.java.dev.jets3t</groupId>
+              <artifactId>jets3t</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.eclipse.jdt</groupId>
+              <artifactId>core</artifactId>
+            </exclusion>
+          </exclusions>
+        </dependency>
+        </dependencies>
+      </dependencyManagement>
+    </profile>
+    <profile>
+      <id>hadoop2</id>
+      <activation>
+        <property>
+          <name>hadoop2.version</name>
+        </property>
+      </activation>
+      <dependencyManagement>
+        <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-auth</artifactId>
+          <version>${hadoop2.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>log4j</groupId>
+          <artifactId>log4j</artifactId>
+          <version>1.2.17</version>
+        </dependency>
+        <dependency>
+          <groupId>commons-lang</groupId>
+          <artifactId>commons-lang</artifactId>
+          <version>2.6</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-yarn-common</artifactId>
+          <version>${hadoop2.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-yarn-api</artifactId>
+          <version>${hadoop2.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+          <version>${hadoop2.version}</version>
+          <exclusions>
+            <exclusion>
+              <groupId>net.sf.kosmosfs</groupId>
+              <artifactId>kfs</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jetty</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jetty-util</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>hsqldb</groupId>
+              <artifactId>hsqldb</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>junit</groupId>
+              <artifactId>junit</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>oro</groupId>
+              <artifactId>oro</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jsp-2.1</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>jsp-api-2.1</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.mortbay.jetty</groupId>
+              <artifactId>servlet-api-2.5</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>tomcat</groupId>
+              <artifactId>jasper-runtime</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>tomcat</groupId>
+              <artifactId>jasper-compiler</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>xmlenc</groupId>
+              <artifactId>xmlenc</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>net.java.dev.jets3t</groupId>
+              <artifactId>jets3t</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.eclipse.jdt</groupId>
+              <artifactId>core</artifactId>
+            </exclusion>
+          </exclusions>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-core</artifactId>
+          <version>${hadoop2.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-common</artifactId>
+          <version>${hadoop2.version}</version>
+        </dependency>
+        </dependencies>
+      </dependencyManagement>
+    </profile>
+    <profile>
       <id>fastinstall</id>
       <properties>
         <skipTests>true</skipTests>
@@ -900,11 +1032,40 @@
         </plugins>
       </build>
     </profile>
+    <profile>
+      <id>ci</id>
+      <activation>
+        <activeByDefault>false</activeByDefault>
+        <property>
+          <name>env.JENKINS_URL</name>
+        </property>
+      </activation>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.codehaus.mojo</groupId>
+            <artifactId>build-helper-maven-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>remove-old-mahout-artifacts</id>
+                <phase>prepare-package</phase>
+                <goals>
+                  <goal>remove-project-artifact</goal>
+                </goals>
+                <configuration>
+                  <failOnError>false</failOnError>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
   </profiles>
   <scm>
-    <connection>scm:svn:https://svn.apache.org/repos/asf/mahout/tags/mahout-0.8</connection>
-    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/mahout/tags/mahout-0.8</developerConnection>
-    <url>https://svn.apache.org/repos/asf/mahout/tags/mahout-0.8</url>
+    <connection>scm:svn:https://svn.apache.org/repos/asf/mahout</connection>
+    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/mahout</developerConnection>
+    <url>https://svn.apache.org/repos/asf/mahout</url>
   </scm>
   <distributionManagement>
     <site>
@@ -918,7 +1079,7 @@
       <plugin>
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-surefire-report-plugin</artifactId>
-        <version>${surefire.version}</version>
+        <version>${msurefire.version}</version>
       </plugin>
       <!-- checkstyle -->
       <plugin>
@@ -935,7 +1096,7 @@
       <plugin>
         <groupId>com.atlassian.maven.plugins</groupId>
         <artifactId>maven-clover2-plugin</artifactId>
-        <version>3.1.10.1</version>
+        <version>3.1.12</version>
         <configuration>
           <generateHistorical>true</generateHistorical>
           <licenseLocation>buildtools/clover.license</licenseLocation>
@@ -1015,7 +1176,7 @@
       <plugin>
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-project-info-reports-plugin</artifactId>
-        <version>2.6</version>
+        <version>2.7</version>
         <reportSets>
           <reportSet>
             <reports>
diff -uNar -x .git mahout/spark/pom.xml mahout/spark/pom.xml
--- mahout/spark/pom.xml	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/pom.xml	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,272 @@
+<?xml version="1.0" encoding="UTF-8"?>
+
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.mahout</groupId>
+    <artifactId>mahout</artifactId>
+    <version>1.0-SNAPSHOT</version>
+    <relativePath>../pom.xml</relativePath>
+  </parent>
+
+  <artifactId>mahout-spark</artifactId>
+  <name>Mahout Spark bindings</name>
+  <description>
+    Mahout Bindings for Apache Spark
+  </description>
+
+  <packaging>jar</packaging>
+
+  <!-- this is needed for scalatest plugin until they publish it to central -->
+  <pluginRepositories>
+    <pluginRepository>
+      <id>sonatype</id>
+      <url>https://oss.sonatype.org/content/groups/public</url>
+      <releases>
+        <enabled>true</enabled>
+      </releases>
+    </pluginRepository>
+  </pluginRepositories>
+
+  <build>
+    <defaultGoal>install</defaultGoal>
+
+    <plugins>
+
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>build-helper-maven-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>add-source</id>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>add-source</goal>
+            </goals>
+            <configuration>
+              <sources>
+                <source>${project.build.directory}/generated-sources/mahout</source>
+              </sources>
+            </configuration>
+          </execution>
+          <execution>
+            <id>add-test-source</id>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>add-test-source</goal>
+            </goals>
+            <configuration>
+              <sources>
+                <source>${project.build.directory}/generated-test-sources/mahout</source>
+              </sources>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+
+      <!-- create test jar so other modules can reuse the math test utility classes. -->
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <executions>
+          <execution>
+            <goals>
+              <goal>test-jar</goal>
+            </goals>
+            <phase>package</phase>
+          </execution>
+        </executions>
+      </plugin>
+
+      <plugin>
+        <artifactId>maven-javadoc-plugin</artifactId>
+      </plugin>
+
+      <plugin>
+        <artifactId>maven-source-plugin</artifactId>
+      </plugin>
+
+      <plugin>
+        <groupId>org.scala-tools</groupId>
+        <artifactId>maven-scala-plugin</artifactId>
+        <executions>
+          <execution>
+            <goals>
+              <goal>compile</goal>
+              <goal>testCompile</goal>
+            </goals>
+          </execution>
+        </executions>
+        <configuration>
+          <sourceDir>src/main/scala</sourceDir>
+          <jvmArgs>
+            <jvmArg>-Xms64m</jvmArg>
+            <jvmArg>-Xmx1024m</jvmArg>
+          </jvmArgs>
+        </configuration>
+      </plugin>
+
+      <!--this is what scalatest recommends to do to enable scala tests -->
+
+      <!-- disable surefire -->
+      <!--<plugin>-->
+      <!--<groupId>org.apache.maven.plugins</groupId>-->
+      <!--<artifactId>maven-surefire-plugin</artifactId>-->
+      <!--<version>2.7</version>-->
+      <!--<configuration>-->
+      <!--<skipTests>true</skipTests>-->
+      <!--</configuration>-->
+      <!--</plugin>-->
+      <!-- enable scalatest -->
+      <plugin>
+        <groupId>org.scalatest</groupId>
+        <artifactId>scalatest-maven-plugin</artifactId>
+        <version>1.0-M2</version>
+        <configuration>
+          <reportsDirectory>${project.build.directory}/scalatest-reports</reportsDirectory>
+          <junitxml>.</junitxml>
+          <filereports>WDF TestSuite.txt</filereports>
+        </configuration>
+        <executions>
+          <execution>
+            <id>test</id>
+            <goals>
+              <goal>test</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+
+    </plugins>
+  </build>
+
+  <profiles>
+    <profile>
+      <id>2.0-CDH</id>
+      <properties>
+        <cdh.version>2.0.0-cdh4.3.0</cdh.version>
+      </properties>
+
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-client</artifactId>
+          <version>${cdh.version}</version>
+          <exclusions>
+            <exclusion>
+              <groupId>asm</groupId>
+              <artifactId>asm</artifactId>
+            </exclusion>
+          </exclusions>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.mahout</groupId>
+          <artifactId>mahout-core</artifactId>
+          <version>${pom.version}</version>
+          <exclusions>
+            <exclusion>
+              <groupId>asm</groupId>
+              <artifactId>asm</artifactId>
+            </exclusion>
+            <exclusion>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-core</artifactId>
+            </exclusion>
+          </exclusions>
+        </dependency>
+
+
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+          <version>${cdh.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-core</artifactId>
+          <version>${cdh.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-common</artifactId>
+          <version>${cdh.version}</version>
+        </dependency>
+      </dependencies>
+    </profile>
+  </profiles>
+  <dependencies>
+
+    <!-- spark stuff -->
+    <dependency>
+      <groupId>org.apache.spark</groupId>
+      <artifactId>spark-core_2.10</artifactId>
+      <version>0.9.0-incubating</version>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.spark</groupId>
+      <artifactId>spark-bagel_2.10</artifactId>
+      <version>0.9.0-incubating</version>
+    </dependency>
+
+    <dependency>
+
+      <groupId>org.apache.mahout</groupId>
+      <artifactId>mahout-core</artifactId>
+      <version>${pom.version}</version>
+      <exclusions>
+        <exclusion>
+          <groupId>asm</groupId>
+          <artifactId>asm</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.mahout</groupId>
+      <artifactId>mahout-math-scala</artifactId>
+      <version>${pom.version}</version>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.mahout</groupId>
+      <artifactId>mahout-math-scala</artifactId>
+      <classifier>tests</classifier>
+      <version>${pom.version}</version>
+      <scope>test</scope>
+    </dependency>
+
+
+    <!--  3rd-party -->
+
+
+    <!-- scala stuff -->
+
+    <dependency>
+      <groupId>org.scalatest</groupId>
+      <artifactId>scalatest_2.10</artifactId>
+      <version>2.0</version>
+      <scope>test</scope>
+    </dependency>
+
+  </dependencies>
+</project>
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/ABt.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/ABt.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/ABt.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/ABt.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,157 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm.plan.OpABt
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm._
+import org.apache.mahout.math.{Matrix, SparseRowMatrix}
+import org.apache.spark.SparkContext._
+
+/** Contains RDD plans for ABt operator */
+object ABt {
+
+  /**
+   * General entry point for AB' operator.
+   *
+   * @param operator the AB' operator
+   * @param srcA A source RDD
+   * @param srcB B source RDD 
+   * @tparam K
+   */
+  def abt[K: ClassTag](
+      operator: OpABt[K],
+      srcA: DrmRddInput[K],
+      srcB: DrmRddInput[Int]): DrmRddInput[K] =
+    abt_nograph(operator, srcA, srcB)
+
+  /**
+   * Computes AB' without GraphX.
+   *
+   * General idea here is that we split both A and B vertically into blocks (one block per split),
+   * then compute cartesian join of the blocks of both data sets. This creates tuples of the form of
+   * (A-block, B-block). We enumerate A-blocks and transform this into (A-block-id, A-block, B-block)
+   * and then compute A-block %*% B-block', thus producing tuples (A-block-id, AB'-block).
+   *
+   * The next step is to group the above tuples by A-block-id and stitch al AB'-blocks in the group
+   * horizontally, forming single vertical block of the final product AB'.
+   *
+   * This logic is complicated a little by the fact that we have to keep block row and column keys
+   * so that the stitching of AB'-blocks happens according to integer row indices of the B input.
+   */
+  private[blas] def abt_nograph[K: ClassTag](
+      operator: OpABt[K],
+      srcA: DrmRddInput[K],
+      srcB: DrmRddInput[Int]): DrmRddInput[K] = {
+
+    // Blockify everything.
+    val blocksA = srcA.toBlockifiedDrmRdd()
+
+        // mark row-blocks with group id
+        .mapPartitionsWithIndex((part, iter) => {
+      val rowBlockId = part
+      val (blockKeys, block) = iter.next()
+
+      // Each partition must have exactly one block due to implementation guarantees of blockify()
+      iter.ensuring(!_.hasNext)
+
+      // the output is (row block id, array of row keys, and the matrix representing the block).
+      Iterator((rowBlockId, blockKeys, block))
+    })
+
+    val blocksB = srcB.toBlockifiedDrmRdd()
+
+    // Final product's geometry. We want to extract that into local variables since we want to use
+    // them as closure attributes.
+    val prodNCol = operator.ncol
+    val prodNRow = operator.nrow
+    
+    // Approximate number of final partitions.
+    val numProductPartitions =
+      if (blocksA.partitions.size > blocksB.partitions.size) {
+        ((prodNCol.toDouble / operator.A.ncol) * blocksA.partitions.size).ceil.toInt
+      } else {
+        ((prodNRow.toDouble / operator.B.ncol) * blocksB.partitions.size).ceil.toInt
+      }
+
+    //srcA.partitions.size.max(that = srcB.partitions.size)
+
+
+    // The plan.
+    val blockifiedRdd :BlockifiedDrmRdd[K] = blocksA
+
+        // Build Cartesian. It may require a bit more memory there at tasks.
+        .cartesian(blocksB)
+
+        // Multiply blocks
+        .map({
+
+      // Our structure here after the Cartesian (full-join):
+      case ((blockId, rowKeys, blA), (colKeys, blB)) =>
+
+        // Compute block-product -- even though columns will require re-keying later, the direct
+        // multiplication still works.
+        val blockProd = blA %*% blB.t
+
+        // Output block in the form (blockIds, row keys, colKeys, block matrix).
+        blockId ->(rowKeys, colKeys, blockProd)
+
+    })
+
+        // Combine -- this is probably the most efficient
+        .combineByKey[(Array[K],Matrix)](
+
+          createCombiner = (t:(Array[K],Array[Int],Matrix)) => t match {
+            case (rowKeys, colKeys, blockProd) =>
+
+              // Accumulator is a row-wise block of sparse vectors.
+              val acc:Matrix = new SparseRowMatrix(rowKeys.size, prodNCol)
+
+              // Update accumulator using colKeys as column index indirection
+              colKeys.view.zipWithIndex.foreach({
+                case (col, srcCol) =>
+                  acc(::, col) := blockProd(::, srcCol)
+              })
+              rowKeys -> acc
+          },
+
+          mergeValue = (a: (Array[K], Matrix), v: (Array[K], Array[Int], Matrix)) => {
+            val (_, acc) = a
+            val (_, colKeys, blockProd) = v
+
+            // Note the assignment rather than +=. We really assume that B' operand matrix is keyed
+            // uniquely!
+            colKeys.view.zipWithIndex.foreach({
+              case (col, srcCol) => acc(::, col) := blockProd(::, srcCol)
+            })
+            a
+          },
+
+          mergeCombiners = (c1: (Array[K], Matrix), c2: (Array[K], Matrix)) => {
+            c1._2 += c2._2
+            c1
+          })
+
+        // Combine leaves residual block key -- we don't need that.
+        .map(_._2)
+
+    new DrmRddInput(blockifiedSrc = Some(blockifiedRdd))
+  }
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AewB.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AewB.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AewB.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AewB.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,97 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.sparkbindings.drm.plan.{OpAewScalar, OpAewB}
+import org.apache.mahout.sparkbindings.drm.DrmRddInput
+import scala.reflect.ClassTag
+import org.apache.spark.SparkContext._
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.math.{Matrix, Vector}
+
+/** Elementwise drm-drm operators */
+object AewB {
+
+  @inline
+  def a_plus_b[K: ClassTag](op: OpAewB[K], srcA: DrmRddInput[K], srcB: DrmRddInput[K]): DrmRddInput[K] =
+    a_ew_b(op, srcA, srcB, reduceFunc = (a, b) => a += b)
+
+  @inline
+  def a_minus_b[K: ClassTag](op: OpAewB[K], srcA: DrmRddInput[K], srcB: DrmRddInput[K]): DrmRddInput[K] =
+    a_ew_b(op, srcA, srcB, reduceFunc = (a, b) => a -= b)
+
+  @inline
+  def a_hadamard_b[K: ClassTag](op: OpAewB[K], srcA: DrmRddInput[K], srcB: DrmRddInput[K]): DrmRddInput[K] =
+    a_ew_b(op, srcA, srcB, reduceFunc = (a, b) => a *= b)
+
+  @inline
+  def a_eldiv_b[K: ClassTag](op: OpAewB[K], srcA: DrmRddInput[K], srcB: DrmRddInput[K]): DrmRddInput[K] =
+    a_ew_b(op, srcA, srcB, reduceFunc = (a, b) => a /= b)
+
+  @inline
+  def a_plus_scalar[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double): DrmRddInput[K] =
+    a_ew_scalar(op, srcA, scalar, reduceFunc = (A, s) => A += s)
+
+  @inline
+  def a_minus_scalar[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double): DrmRddInput[K] =
+    a_ew_scalar(op, srcA, scalar, reduceFunc = (A, s) => A -= s)
+
+  @inline
+  def scalar_minus_a[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double): DrmRddInput[K] =
+    a_ew_scalar(op, srcA, scalar, reduceFunc = (A, s) => s -=: A)
+
+  @inline
+  def a_times_scalar[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double): DrmRddInput[K] =
+    a_ew_scalar(op, srcA, scalar, reduceFunc = (A, s) => A *= s)
+
+  @inline
+  def a_div_scalar[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double): DrmRddInput[K] =
+    a_ew_scalar(op, srcA, scalar, reduceFunc = (A, s) => A /= s)
+
+  @inline
+  def scalar_div_a[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double): DrmRddInput[K] =
+    a_ew_scalar(op, srcA, scalar, reduceFunc = (A, s) => s /=: A)
+
+  /** Parallel way of this operation (this assumes different partitioning of the sources */
+  private[blas] def a_ew_b[K: ClassTag](op: OpAewB[K], srcA: DrmRddInput[K], srcB: DrmRddInput[K],
+      reduceFunc: (Vector, Vector) => Vector): DrmRddInput[K] = {
+    val a = srcA.toDrmRdd()
+    val b = srcB.toDrmRdd()
+    val rdd = a
+        .cogroup(b, numPartitions = a.partitions.size max b.partitions.size)
+        .map({
+      case (key, (vectorSeqA, vectorSeqB)) =>
+        key -> reduceFunc(vectorSeqA.reduce(reduceFunc), vectorSeqB.reduce(reduceFunc))
+    })
+
+    new DrmRddInput(rowWiseSrc = Some(op.ncol -> rdd))
+  }
+
+  private[blas] def a_ew_scalar[K: ClassTag](op: OpAewScalar[K], srcA: DrmRddInput[K], scalar: Double,
+      reduceFunc: (Matrix, Double) => Matrix): DrmRddInput[K] = {
+    val a = srcA.toBlockifiedDrmRdd()
+    val rdd = a
+        .map({
+      case (keys, block) => keys -> reduceFunc(block, scalar)
+    })
+    new DrmRddInput[K](blockifiedSrc = Some(rdd))
+  }
+
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AinCoreB.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AinCoreB.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AinCoreB.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AinCoreB.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,51 @@
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+
+import org.apache.mahout.sparkbindings.drm._
+import org.apache.mahout.sparkbindings.drm.plan.OpTimesRightMatrix
+import org.apache.mahout.sparkbindings.drm.DrmRddInput
+import scala.reflect.ClassTag
+import org.apache.mahout.math.DiagonalMatrix
+
+/** Matrix product with one of operands an in-core matrix */
+object AinCoreB {
+
+  def rightMultiply[K: ClassTag](op: OpTimesRightMatrix[K], srcA: DrmRddInput[K]): DrmRddInput[K] = {
+    if ( op.right.isInstanceOf[DiagonalMatrix])
+      rightMultiply_diag(op, srcA)
+    else
+      rightMultiply_common(op,srcA)
+  }
+
+  private def rightMultiply_diag[K: ClassTag](op: OpTimesRightMatrix[K], srcA: DrmRddInput[K]): DrmRddInput[K] = {
+    val rddA = srcA.toBlockifiedDrmRdd()
+    implicit val sc = rddA.sparkContext
+    val dg = drmBroadcast(x = op.right.viewDiagonal())
+
+    val rdd = rddA
+        // Just multiply the blocks
+        .map {
+      case (keys, blockA) => keys -> (blockA %*%: diagv(dg))
+    }
+    new DrmRddInput(blockifiedSrc = Some(rdd))
+  }
+
+  private def rightMultiply_common[K: ClassTag](op: OpTimesRightMatrix[K], srcA: DrmRddInput[K]): DrmRddInput[K] = {
+
+    val rddA = srcA.toBlockifiedDrmRdd()
+    implicit val sc = rddA.sparkContext
+
+    val bcastB = drmBroadcast(m = op.right)
+
+    val rdd = rddA
+        // Just multiply the blocks
+        .map {
+      case (keys, blockA) => keys -> (blockA %*% bcastB)
+    }
+
+    new DrmRddInput(blockifiedSrc = Some(rdd))
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/At.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/At.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/At.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/At.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.sparkbindings.drm.plan.OpAt
+import org.apache.mahout.sparkbindings.drm.DrmRddInput
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.spark.SparkContext._
+import org.apache.mahout.math.{DenseVector, Vector, SequentialAccessSparseVector}
+
+/** A' algorithms */
+object At {
+
+  def at(
+      operator: OpAt,
+      srcA: DrmRddInput[Int]): DrmRddInput[Int] = at_nograph(operator = operator, srcA = srcA)
+
+  /**
+   * Non-GraphX spark implementation of transposition.
+   *
+   * The idea here is simple: compile vertical column vectors of every partition block as sparse
+   * vectors of the <code>A.nrow</code> length; then group them by their column index and sum the
+   * groups into final rows of the transposed matrix.
+   */
+  private[blas] def at_nograph(operator: OpAt, srcA: DrmRddInput[Int]): DrmRddInput[Int] = {
+    val drmRdd = srcA.toBlockifiedDrmRdd()
+    val numPartitions = drmRdd.partitions.size
+    val ncol = operator.ncol
+
+    // Validity of this conversion must be checked at logical operator level.
+    val nrow = operator.nrow.toInt
+    val atRdd = drmRdd
+        // Split
+        .flatMap({
+      case (keys, blockA) =>
+        (0 until blockA.ncol).view.map(blockCol => {
+          // Compute sparse vector. This should be quick if we assign values siquentially.
+          val colV: Vector = new SequentialAccessSparseVector(ncol)
+          keys.view.zipWithIndex.foreach({
+            case (row, blockRow) => colV(row) = blockA(blockRow, blockCol)
+          })
+
+          blockCol -> colV
+        })
+    })
+
+        // Regroup
+        .groupByKey(numPartitions = numPartitions)
+
+        // Reduce
+        .map({
+      case (key, vSeq) =>
+        var v: Vector = vSeq.reduce(_ + _)
+        key -> v
+    }).densify()
+
+    new DrmRddInput(rowWiseSrc = Some(ncol -> atRdd))
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtA.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtA.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtA.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtA.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.math._
+import org.apache.mahout.sparkbindings.drm._
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import collection._
+import JavaConversions._
+import org.apache.mahout.sparkbindings.drm.plan.OpAtA
+import org.apache.spark.SparkContext._
+import org.apache.log4j.Logger
+
+/**
+ * Collection of algorithms to compute X' times X
+ */
+object AtA {
+
+  final val log = Logger.getLogger(AtA.getClass)
+
+  final val PROPERTY_ATA_MAXINMEMNCOL = "mahout.math.AtA.maxInMemNCol"
+
+  /** Materialize A'A operator */
+  def at_a(operator: OpAtA[_], srcRdd: DrmRddInput[_]): DrmRddInput[Int] = {
+
+    val maxInMemNCol = System.getProperty(PROPERTY_ATA_MAXINMEMNCOL, "2000").toInt
+    maxInMemNCol.ensuring(_ > 0, "Invalid A'A in-memory setting for optimizer")
+
+    if (operator.ncol <= maxInMemNCol) {
+      // If we can comfortably fit upper-triangular operator into a map memory, we will run slim
+      // algorithm with upper-triangular accumulators in maps. 
+      val inCoreA = at_a_slim(srcRdd = srcRdd, operator = operator)
+      val drmRdd = parallelizeInCore(inCoreA, numPartitions = 1)(sc = srcRdd.sparkContext)
+      new DrmRddInput(rowWiseSrc = Some(inCoreA.ncol, drmRdd))
+    } else {
+      // Otherwise, we need to run a distributed, big version
+      new DrmRddInput(rowWiseSrc = Some(operator.ncol, at_a_nongraph(srcRdd = srcRdd, op = operator)))
+
+    }
+  }
+
+
+  /**
+   * Computes A' * A for tall but skinny A matrices. Comes up a lot in SSVD and ALS flavors alike.
+   * @return
+   */
+  def at_a_slim(operator: OpAtA[_], srcRdd: DrmRdd[_]): Matrix = {
+
+    log.debug("Applying slim A'A.")
+
+    val ncol = operator.ncol
+    // Compute backing vector of tiny-upper-triangular accumulator accross all the data.
+    val resSym = srcRdd.mapPartitions(pIter => {
+
+      val ut = new UpperTriangular(ncol)
+
+      // Strategy is to add to an outer product of each row to the upper triangular accumulator.
+      pIter.foreach({
+        case (k, v) =>
+
+          // Use slightly various traversal strategies over dense vs. sparse source.
+          if (v.isDense) {
+
+            // Update upper-triangular pattern only (due to symmetry).
+            // Note: Scala for-comprehensions are said to be fairly inefficient this way, but this is
+            // such spectacular case they were deesigned for.. Yes I do observe some 20% difference
+            // compared to while loops with no other payload, but the other payload is usually much
+            // heavier than this overhead, so... I am keeping this as is for the time being.
+
+            for (row <- 0 until v.length; col <- row until v.length)
+              ut(row, col) = ut(row, col) + v(row) * v(col)
+
+          } else {
+
+            // Sparse source.
+            v.nonZeroes().view
+
+                // Outer iterator iterates over rows of outer product.
+                .foreach(elrow => {
+
+              // Inner loop for columns of outer product.
+              v.nonZeroes().view
+
+                  // Filter out non-upper nonzero elements from the double loop.
+                  .filter(_.index >= elrow.index)
+
+                  // Incrementally update outer product value in the uppper triangular accumulator.
+                  .foreach(elcol => {
+
+                val row = elrow.index
+                val col = elcol.index
+                ut(row, col) = ut(row, col) + elrow.get() * elcol.get()
+
+              })
+            })
+
+          }
+      })
+
+      Iterator(dvec(ddata = ut.getData): Vector)
+    })
+
+        .collect()
+        .reduce(_ += _)
+
+    new DenseSymmetricMatrix(resSym)
+  }
+
+  /** The version of A'A that does not use GraphX */
+  def at_a_nongraph(op: OpAtA[_], srcRdd: DrmRdd[_]): DrmRdd[Int] = {
+
+    log.debug("Applying non-slim non-graph A'A.")
+
+    // Determine how many partitions the new matrix would need approximately. We base that on 
+    // geometry only, but it may eventually not be that adequate. Indeed, A'A tends to be much more
+    // dense in reality than the source.
+
+    val m = op.A.nrow
+    val n = op.A.ncol
+    val numParts = (srcRdd.partitions.size.toDouble * n / m).ceil.round.toInt max 1
+    val blockHeight = (n - 1) / numParts + 1
+
+    val rddAtA = srcRdd
+
+        // Remove key, key is irrelevant
+        .map(_._2)
+
+        // Form partial outer blocks for each partition
+        .flatMap {
+      v =>
+        for (blockKey <- Stream.range(0, numParts)) yield {
+          val blockStart = blockKey * blockHeight
+          val blockEnd = n min (blockStart + blockHeight)
+          blockKey -> (v(blockStart until blockEnd) cross v)
+        }
+    }
+        // Combine outer blocks
+        .reduceByKey(_ += _)
+
+        // Restore proper block keys
+        .map {
+      case (blockKey, block) =>
+        val blockStart = blockKey * blockHeight
+        val rowKeys = Array.tabulate(block.nrow)(blockStart + _)
+        rowKeys -> block
+    }
+
+    new DrmRddInput[Int](blockifiedSrc = Some(rddAtA))
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtB.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtB.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtB.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/AtB.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm._
+import org.apache.spark.rdd.RDD
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.spark.SparkContext._
+import org.apache.mahout.sparkbindings.drm.plan.{OpAtB}
+import org.apache.log4j.Logger
+
+object AtB {
+
+  private val log = Logger.getLogger(AtB.getClass)
+
+  /**
+   * The logic for computing A'B is pretty much map-side generation of partial outer product blocks
+   * over co-grouped rows of A and B. If A and B are identically partitioned, we can just directly
+   * zip all the rows. Otherwise, we need to inner-join them first.
+   */
+  def atb_nograph[A: ClassTag](
+      operator: OpAtB[A],
+      srcA: DrmRddInput[A],
+      srcB: DrmRddInput[A],
+      zippable:Boolean = false
+      ): DrmRddInput[Int] = {
+
+    val rddA = srcA.toDrmRdd()
+    val zipped = if ( zippable ) {
+
+      log.debug("A and B for A'B are identically distributed, performing row-wise zip.")
+
+      rddA.zip(other = srcB.toDrmRdd())
+
+    } else {
+
+      log.debug("A and B for A'B are not identically partitioned, performing inner join.")
+
+      rddA.join(other=srcB.toDrmRdd()).map({
+        case (key,(v1,v2) ) => (key -> v1) -> (key -> v2)
+      })
+    }
+
+    val blockHeight = safeToNonNegInt(
+      (operator.B.ncol.toDouble/rddA.partitions.size).ceil.round max 1L
+    )
+
+    computeAtBZipped(
+      zipped,
+      nrow = operator.nrow,
+      ancol = operator.A.ncol,
+      bncol = operator.B.ncol,
+      blockHeight = blockHeight
+    )
+  }
+
+
+//  private[sparkbindings] def atb_nograph()
+
+  /** Given already zipped, joined rdd of rows of A' and B, compute their product A'B */
+  private[sparkbindings] def computeAtBZipped[A: ClassTag](zipped:RDD[(DrmTuple[A], DrmTuple[A])],
+      nrow:Long, ancol:Int, bncol:Int, blockHeight: Int) = {
+
+    // Since Q and A are partitioned same way,we can just zip their rows and proceed from there by
+    // forming outer products. Our optimizer lacks this primitive, so we will implement it using RDDs
+    // directly. We try to compile B' = A'Q now by collecting outer products of rows of A and Q. At
+    // this point we need to split n-range  of B' into sutiable number of partitions.
+
+    val btNumParts = safeToNonNegInt((nrow - 1) / blockHeight + 1)
+
+    val rddBt = zipped
+
+        // Produce outer product blocks
+        .flatMap {
+      case ((aKey, aRow), (qKey, qRow)) =>
+        for (blockKey <- Stream.range(0, btNumParts)) yield {
+          val blockStart = blockKey * blockHeight
+          val blockEnd = ancol min (blockStart + blockHeight)
+
+          // Create block by cross product of proper slice of aRow and qRow
+          blockKey -> (aRow(blockStart until blockEnd) cross qRow)
+        }
+    }
+        // Combine blocks by just summing them up
+        .reduceByKey {
+      case (block1, block2) => block1 += block2
+    }
+
+        // Throw away block key, generate row keys instead.
+        .map {
+      case (blockKey, block) =>
+        val blockStart = blockKey * blockHeight
+        val rowKeys = Array.tabulate(block.nrow)(blockStart + _)
+        rowKeys -> block
+    }
+
+    new DrmRddInput[Int](blockifiedSrc = Some(rddBt))
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Ax.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Ax.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Ax.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Ax.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,61 @@
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+
+import org.apache.mahout.sparkbindings.drm._
+import org.apache.mahout.sparkbindings.drm.plan.{OpAtx, OpAx, OpTimesRightMatrix}
+import org.apache.mahout.sparkbindings.drm.DrmRddInput
+import scala.reflect.ClassTag
+
+
+/** Matrix product with one of operands an in-core matrix */
+object Ax {
+
+  def ax_with_broadcast[K: ClassTag](op: OpAx[K], srcA: DrmRddInput[K]): DrmRddInput[K] = {
+
+    val rddA = srcA.toBlockifiedDrmRdd()
+    implicit val sc = rddA.sparkContext
+
+    val bcastX = drmBroadcast(x = op.x)
+
+    val rdd = rddA
+        // Just multiply the blocks
+        .map({
+      case (keys, blockA) => keys -> (blockA %*% bcastX).toColMatrix
+    })
+
+    new DrmRddInput(blockifiedSrc = Some(rdd))
+  }
+
+  def atx_with_broadcast(op: OpAtx, srcA: DrmRddInput[Int]): DrmRddInput[Int] = {
+    val rddA = srcA.toBlockifiedDrmRdd()
+    implicit val sc = rddA.sparkContext
+
+    val bcastX = drmBroadcast(x = op.x)
+
+    val inCoreM = rddA
+        // Just multiply the blocks
+        .map {
+      case (keys, blockA) =>
+        keys.zipWithIndex.map {
+          case (key, idx) => blockA(idx, ::) * bcastX.value(key)
+        }
+            .reduce(_ += _)
+    }
+        // All-reduce
+        .reduce(_ += _)
+        // Convert back to mtx
+        .toColMatrix
+
+    // It is ridiculous, but in this scheme we will have to re-parallelize it again in order to plug
+    // it back as drm blockified rdd
+
+    val rdd = sc.parallelize(Seq(inCoreM), numSlices = 1)
+        .map(block => Array.tabulate(block.nrow)(i => i) -> block)
+
+    new DrmRddInput(blockifiedSrc = Some(rdd))
+
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/DrmRddOps.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/DrmRddOps.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/DrmRddOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/DrmRddOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.sparkbindings.drm.DrmRdd
+import scala.reflect.ClassTag
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.math.{SequentialAccessSparseVector, DenseVector}
+
+/**
+ *
+ * @author dmitriy
+ */
+class DrmRddOps[K: ClassTag](private[blas] val rdd: DrmRdd[K]) {
+
+  def densify(threshold: Double = 0.80): DrmRdd[K] = rdd.map({
+    case (key, v) =>
+      val vd = if (!v.isDense && v.getNumNonZeroElements > threshold * v.length) new DenseVector(v) else v
+      key -> vd
+  })
+
+  def sparsify(threshold: Double = 0.80): DrmRdd[K] = rdd.map({
+    case (key, v) =>
+      val vs = if (v.isDense() && v.getNumNonZeroElements <= threshold * v.length)
+        new SequentialAccessSparseVector(v)
+      else v
+      key -> vs
+  })
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Slicing.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Slicing.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Slicing.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/Slicing.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,31 @@
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.sparkbindings.drm.plan.OpRowRange
+import org.apache.mahout.sparkbindings.drm.DrmRddInput
+
+/**
+ *
+ * @author dmitriy
+ */
+object Slicing {
+
+  def rowRange(op: OpRowRange, srcA: DrmRddInput[Int]): DrmRddInput[Int] = {
+    val rowRange = op.rowRange
+    val ncol = op.ncol
+    val rdd = srcA.toDrmRdd()
+
+        // Filter the rows in the range only
+        .filter({
+      case (key, vector) => rowRange.contains(key)
+    })
+
+        // Now we need to adjust the row index
+        .map({
+      case (key, vector) => (key - rowRange.head) -> vector
+    })
+
+    // TODO: we probably need to re-shuffle result or at least cut down the partitions of 0 size
+
+    new DrmRddInput(rowWiseSrc = Some(ncol -> rdd))
+  }
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/package.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/package.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/package.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/blas/package.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,31 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings
+
+import org.apache.mahout.sparkbindings.drm.DrmRdd
+import scala.reflect.ClassTag
+
+/**
+ * This package contains distributed algorithms that distributed matrix expression optimizer picks
+ * from.
+ */
+package object blas {
+
+  implicit def drmRdd2ops[K:ClassTag](rdd:DrmRdd[K]):DrmRddOps[K] = new DrmRddOps[K](rdd)
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DQR.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DQR.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DQR.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DQR.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,56 @@
+package org.apache.mahout.sparkbindings.drm.decompositions
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.Matrix
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm._
+import RLikeDrmOps._
+import org.apache.log4j.Logger
+
+object DQR {
+
+  private val log = Logger.getLogger(DQR.getClass)
+
+  /**
+   * Distributed _thin_ QR. A'A must fit in a memory, i.e. if A is m x n, then n should be pretty
+   * controlled (<5000 or so). <P>
+   *
+   * It is recommended to checkpoint A since it does two passes over it. <P>
+   *
+   * It also guarantees that Q is partitioned exactly the same way (and in same key-order) as A, so
+   * their RDD should be able to zip successfully.
+   */
+  def dqrThin[K: ClassTag](A: DrmLike[K], checkRankDeficiency: Boolean = true): (DrmLike[K], Matrix) = {
+
+    if (A.ncol > 5000)
+      log.warn("A is too fat. A'A must fit in memory and easily broadcasted.")
+
+    val AtA = (A.t %*% A).checkpoint()
+    val inCoreAtA = AtA.collect
+    implicit val sc = AtA.rdd.sparkContext
+
+    if (log.isDebugEnabled) log.debug("A'A=\n%s\n".format(inCoreAtA))
+
+    val ch = chol(inCoreAtA)
+    val inCoreR = (ch.getL cloned) t
+
+    if (log.isDebugEnabled) log.debug("R=\n%s\n".format(inCoreR))
+
+    if (checkRankDeficiency && !ch.isPositiveDefinite)
+      throw new IllegalArgumentException("R is rank-deficient.")
+
+    val bcastAtA = sc.broadcast(inCoreAtA)
+
+    // Unfortunately, I don't think Cholesky decomposition is serializable to backend. So we re-
+    // decompose A'A in the backend again.
+
+    // Compute Q = A*inv(L') -- we can do it blockwise.
+    val Q = A.mapBlock() {
+      case (keys, block) => keys -> chol(bcastAtA).solveRight(block)
+    }
+
+    Q -> inCoreR
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSPCA.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSPCA.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSPCA.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSPCA.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,154 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.decompositions
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.{Matrices, Vector}
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm._
+import RLikeDrmOps._
+import org.apache.spark.storage.StorageLevel
+import org.apache.mahout.common.RandomUtils
+
+object DSPCA {
+
+  /**
+   * Distributed Stochastic PCA decomposition algorithm. A logical reflow of the "SSVD-PCA options.pdf"
+   * document of the MAHOUT-817.
+   *
+   * @param A input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations (hint: use either 0 or 1)
+   * @return (U,V,s). Note that U, V are non-checkpointed matrices (i.e. one needs to actually use them
+   *         e.g. save them to hdfs in order to trigger their computation.
+   */
+  def dspca[K: ClassTag](A: DrmLike[K], k: Int, p: Int = 15, q: Int = 0):
+  (DrmLike[K], DrmLike[Int], Vector) = {
+
+    val drmA = A.checkpoint()
+    implicit val sc = drmA.rdd.sparkContext
+
+    val m = drmA.nrow
+    val n = drmA.ncol
+    assert(k <= (m min n), "k cannot be greater than smaller of m, n.")
+    val pfxed = safeToNonNegInt((m min n) - k min p)
+
+    // Actual decomposition rank
+    val r = k + pfxed
+
+    // Dataset mean
+    val xi = drmA.colMeans
+
+    // We represent Omega by its seed.
+    val omegaSeed = RandomUtils.getRandom().nextInt()
+    val omega = Matrices.symmetricUniformView(n, r, omegaSeed)
+
+    // This done in front in a single-threaded fashion for now. Even though it doesn't require any
+    // memory beyond that is required to keep xi around, it still might be parallelized to backs
+    // for significantly big n and r. TODO
+    val s_o = omega.t %*% xi
+
+    val bcastS_o = drmBroadcast(s_o)
+    val bcastXi = drmBroadcast(xi)
+
+    var drmY = drmA.mapBlock(ncol = r) {
+      case (keys, blockA) =>
+        val s_o:Vector = bcastS_o
+        val blockY = blockA %*% Matrices.symmetricUniformView(n, r, omegaSeed)
+        for (row <- 0 until blockY.nrow) blockY(row, ::) -= s_o
+        keys -> blockY
+    }
+        // Checkpoint Y
+        .checkpoint()
+
+    var drmQ = dqrThin(drmY, checkRankDeficiency = false)._1.checkpoint()
+
+    var s_q = drmQ.colSums()
+    var bcastVarS_q = drmBroadcast(s_q)
+
+    // This actually should be optimized as identically partitioned map-side A'B since A and Q should
+    // still be identically partitioned.
+    var drmBt = (drmA.t %*% drmQ).checkpoint()
+
+    var s_b = (drmBt.t %*% xi).collect(::, 0)
+    var bcastVarS_b = drmBroadcast(s_b)
+
+    for (i <- 0 until q) {
+
+      // These closures don't seem to live well with outside-scope vars. This doesn't record closure
+      // attributes correctly. So we create additional set of vals for broadcast vars to properly 
+      // create readonly closure attributes in this very scope.
+      val bcastS_q = bcastVarS_q
+      val bcastS_b = bcastVarS_b
+      val bcastXib = bcastXi
+
+      // Fix Bt as B' -= xi cross s_q
+      drmBt = drmBt.mapBlock() {
+        case (keys, block) =>
+          val s_q: Vector = bcastS_q
+          val xi: Vector = bcastXib
+          keys.zipWithIndex.foreach {
+            case (key, idx) => block(idx, ::) -= s_q * xi(key)
+          }
+          keys -> block
+      }
+
+      drmY.uncache()
+      drmQ.uncache()
+
+      drmY = (drmA %*% drmBt)
+          // Fix Y by subtracting s_b from each row of the AB'
+          .mapBlock() {
+        case (keys, block) =>
+          val s_b: Vector = bcastS_b
+          for (row <- 0 until block.nrow) block(row, ::) -= s_b
+          keys -> block
+      }
+          // Checkpoint Y
+          .checkpoint()
+
+      drmQ = dqrThin(drmY, checkRankDeficiency = false)._1.checkpoint()
+
+      s_q = drmQ.colSums()
+      bcastVarS_q = drmBroadcast(s_q)
+
+      // This on the other hand should be inner-join-and-map A'B optimization since A and Q_i are not
+      // identically partitioned anymore.
+      drmBt = (drmA.t %*% drmQ).checkpoint()
+
+      s_b = (drmBt.t %*% xi).collect(::, 0)
+      bcastVarS_b = drmBroadcast(s_b)
+    }
+
+    val c = s_q cross s_b
+    val inCoreBBt = (drmBt.t %*% drmBt).checkpoint(StorageLevel.NONE).collect -
+        c - c.t + (s_q cross s_q) * (xi dot xi)
+    val (inCoreUHat, d) = eigen(inCoreBBt)
+    val s = d.sqrt
+
+    // Since neither drmU nor drmV are actually computed until actually used, we don't need the flags
+    // instructing compute (or not compute) either of the U,V outputs anymore. Neat, isn't it?
+    val drmU = drmQ %*% inCoreUHat
+    val drmV = drmBt %*% (inCoreUHat %*%: diagv(1 /: s))
+
+    (drmU(::, 0 until k), drmV(::, 0 until k), s(0 until k))
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSSVD.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSSVD.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSSVD.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/decompositions/DSSVD.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,86 @@
+package org.apache.mahout.sparkbindings.drm.decompositions
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.{Matrices, Vector}
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm._
+import RLikeDrmOps._
+import scala.util.Random
+import org.apache.spark.SparkContext._
+import org.apache.spark.storage.StorageLevel
+import org.apache.mahout.common.RandomUtils
+
+object DSSVD {
+
+  /**
+   * Distributed Stochastic Singular Value decomposition algorithm.
+   *
+   * @param A input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations
+   * @return (U,V,s). Note that U, V are non-checkpointed matrices (i.e. one needs to actually use them
+   *         e.g. save them to hdfs in order to trigger their computation.
+   */
+  def dssvd[K: ClassTag](A: DrmLike[K], k: Int, p: Int = 15, q: Int = 0):
+  (DrmLike[K], DrmLike[Int], Vector) = {
+
+    val drmA = A.checkpoint()
+
+    val m = drmA.nrow
+    val n = drmA.ncol
+    assert(k <= (m min n), "k cannot be greater than smaller of m, n.")
+    val pfxed = safeToNonNegInt((m min n) - k min p)
+
+    // Actual decomposition rank
+    val r = k + pfxed
+
+    // We represent Omega by its seed.
+    val omegaSeed = RandomUtils.getRandom().nextInt()
+
+    // Compute Y = A*Omega. Instead of redistributing view, we redistribute the Omega seed only and
+    // instantiate the Omega random matrix view in the backend instead. That way serialized closure
+    // is much more compact.
+    var drmY = drmA.mapBlock(ncol = r) {
+      case (keys, blockA) =>
+        val blockY = blockA %*% Matrices.symmetricUniformView(n, r, omegaSeed)
+        keys -> blockY
+    }
+
+    var drmQ = dqrThin(drmY.checkpoint())._1
+    // Checkpoint Q if last iteration
+    if (q == 0) drmQ = drmQ.checkpoint()
+
+    // This actually should be optimized as identically partitioned map-side A'B since A and Q should
+    // still be identically partitioned.
+    var drmBt = drmA.t %*% drmQ
+    // Checkpoint B' if last iteration
+    if (q == 0) drmBt = drmBt.checkpoint()
+
+    for (i <- 0  until q) {
+      drmY = drmA %*% drmBt
+      drmQ = dqrThin(drmY.checkpoint())._1
+      // Checkpoint Q if last iteration
+      if (i == q - 1) drmQ = drmQ.checkpoint()
+
+      // This on the other hand should be inner-join-and-map A'B optimization since A and Q_i are not
+      // identically partitioned anymore.
+      drmBt = drmA.t %*% drmQ
+      // Checkpoint B' if last iteration
+      if (i == q - 1) drmBt = drmBt.checkpoint()
+    }
+
+    val inCoreBBt = (drmBt.t %*% drmBt).checkpoint(StorageLevel.NONE).collect
+    val (inCoreUHat, d) = eigen(inCoreBBt)
+    val s = d.sqrt
+
+    // Since neither drmU nor drmV are actually computed until actually used, we don't need the flags
+    // instructing compute (or not compute) either of the U,V outputs anymore. Neat, isn't it?
+    val drmU = drmQ %*% inCoreUHat
+    val drmV = drmBt %*% (inCoreUHat %*%: diagv(1 /: s))
+
+    (drmU(::, 0 until k), drmV(::, 0 until k), s(0 until k))
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrm.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrm.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrm.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrm.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import org.apache.mahout.math.Matrix
+import org.apache.hadoop.io.Writable
+
+/**
+ * Checkpointed DRM API. This is a matrix that has optimized RDD lineage behind it and can be
+ * therefore collected or saved.
+ * @tparam K matrix key type (e.g. the keys of sequence files once persisted)
+ */
+trait CheckpointedDrm[K] extends DrmLike[K] {
+
+  def rdd: DrmRdd[K]
+
+  def collect: Matrix
+
+  def writeDRM(path: String)(implicit ev: K => Writable)
+
+  /** If this checkpoint is already declared cached, uncache. */
+  def uncache()
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmBase.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmBase.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmBase.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmBase.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import org.apache.mahout.math.{SparseMatrix, DenseMatrix, Matrix, Vector}
+import math._
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import scala.collection.JavaConversions._
+import org.apache.spark.storage.StorageLevel
+import org.apache.spark.rdd.RDD
+import org.apache.hadoop.io.Writable
+import org.apache.spark.SparkContext._
+import reflect._
+import scala.util.Random
+
+/**
+ *
+ * @author dmitriy
+ */
+class CheckpointedDrmBase[K : ClassTag](
+    val rdd: DrmRdd[K],
+    private var _nrow: Long = -1L,
+    private var _ncol: Int = -1,
+    private val _cacheStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,
+    private[sparkbindings] val partitioningTag: Long = Random.nextLong()
+
+    ) extends CheckpointedDrm[K] {
+
+
+  lazy val nrow = if (_nrow >= 0) _nrow else computeNRow
+  lazy val ncol = if (_ncol >= 0) _ncol else computeNCol
+
+  private var cached: Boolean = false
+
+
+  /**
+   * Action operator -- does not necessary means Spark action; but does mean running BLAS optimizer
+   * and writing down Spark graph lineage since last checkpointed DRM.
+   */
+  def checkpoint(sLevel:StorageLevel): CheckpointedDrm[K] =
+  // We are already checkpointed in a sense that we already have Spark lineage. So just return self.
+    this
+
+  def cache() = {
+    if (!cached) {
+      rdd.persist(_cacheStorageLevel)
+      cached = true
+    }
+    this
+  }
+
+
+  /**
+   * if matrix was previously persisted into cache,
+   * delete cached representation
+   */
+  def uncache() = {
+    if (cached) {
+      rdd.unpersist(blocking = false)
+      cached = false
+    }
+    this
+  }
+
+  def mapRows(mapfun: (K, Vector) => Vector): CheckpointedDrmBase[K] =
+    new CheckpointedDrmBase[K](rdd.map(t => (t._1, mapfun(t._1, t._2))))
+
+
+  /**
+   * Collecting DRM to fron-end in-core Matrix.
+   *
+   * If key in DRM is Int, then matrix is collected using key as row index.
+   * Otherwise, order of rows in result is undefined but key.toString is applied
+   * as rowLabelBindings of the in-core matrix .
+   *
+   * Note that this pre-allocates target matrix and then assigns collected RDD to it
+   * thus this likely would require about 2 times the RDD memory
+   * @return
+   */
+  def collect: Matrix = {
+
+    val intRowIndices = implicitly[ClassManifest[K]] == classManifest[Int]
+
+    val cols = rdd.map(_._2.length).fold(0)(max(_, _))
+    val rows = if (intRowIndices) rdd.map(_._1.asInstanceOf[Int]).fold(-1)(max(_, _)) + 1 else rdd.count().toInt
+
+    // since currently spark #collect() requires Serializeable support,
+    // we serialize DRM vectors into byte arrays on backend and restore Vector
+    // instances on the front end:
+    val data = rdd.map(t => (t._1, t._2)).collect()
+
+
+    val m = if (data.forall(_._2.isDense))
+      new DenseMatrix(rows, cols)
+
+    else
+      new SparseMatrix(rows, cols)
+
+    if (intRowIndices)
+      data.foreach(t => m(t._1.asInstanceOf[Int], ::) := t._2)
+    else {
+
+      // assign all rows sequentially
+      val d = data.zipWithIndex
+      d.foreach(t => m(t._2, ::) := t._1._2)
+
+      // row bindings
+      val rowBindings = d.map(t => (t._1._1.toString, t._2: java.lang.Integer)).toMap
+
+      m.setRowLabelBindings(rowBindings)
+    }
+
+    m
+  }
+
+
+  /**
+   * Dump matrix as computed Mahout's DRM into specified (HD)FS path
+   * @param path
+   */
+  def writeDRM(path: String)(implicit ev: K => Writable) = rdd.saveAsSequenceFile(path)
+
+  protected def computeNRow = {
+
+    val intRowIndex = classTag[K] == classTag[Int]
+
+    if (intRowIndex)
+      cache().rdd.map(_._1.asInstanceOf[Int]).fold(-1)(max(_, _)) + 1L
+    else
+      cache().rdd.count()
+  }
+
+  protected def computeNCol =
+    cache().rdd.map(_._2.length).fold(-1)(max(_, _))
+
+  protected def computeNNonZero =
+    cache().rdd.map(_._2.getNumNonZeroElements.toLong).sum().toLong
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedOps.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedOps.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.{DenseVector, Vector}
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import RLikeDrmOps._
+import org.apache.spark.SparkContext._
+
+
+/**
+ * Additional experimental operations over CheckpointedDRM implementation. I will possibly move them up to
+ * the DRMBase once they stabilize.
+ *
+ */
+class CheckpointedOps[K: ClassTag](val drm: CheckpointedDrm[K]) {
+
+  /**
+   * Reorganize every partition into a single in-core matrix
+   * @return
+   */
+  def blockify(): BlockifiedDrmRdd[K] =
+    org.apache.mahout.sparkbindings.drm.blockify(rdd = drm.rdd, blockncol = drm.ncol)
+
+  /** Column sums. At this point this runs on checkpoint and collects in-core vector. */
+  def colSums(): Vector = {
+    val n = drm.ncol
+
+    drm.rdd
+        // Throw away keys
+        .map(_._2)
+        // Fold() doesn't work with kryo still. So work around it.
+        .mapPartitions(iter => {
+      val acc = ((new DenseVector(n): Vector) /: iter)((acc, v) => acc += v)
+      Iterator(acc)
+    })
+        // Since we preallocated new accumulator vector per partition, this must not cause any side
+        // effects now.
+        .reduce(_ += _)
+
+  }
+
+  def colMeans(): Vector = if (drm.nrow == 0) drm.colSums() else drm.colSums() /= drm.nrow
+
+}
+
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLike.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLike.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLike.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLike.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import org.apache.spark.storage.StorageLevel
+
+/**
+ *
+ * Basic spark DRM trait.
+ *
+ * Since we already call the package "sparkbindings", I will not use stem "spark" with classes in
+ * this package. Spark backing is already implied.
+ *
+ */
+trait DrmLike[K] {
+
+  private[sparkbindings] def partitioningTag:Long
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long
+
+  /** R-like syntax for number of columns */
+  def ncol: Int
+
+  /**
+   * Action operator -- does not necessary means Spark action; but does mean running BLAS optimizer
+   * and writing down Spark graph lineage since last checkpointed DRM.
+   */
+  def checkpoint(sLevel: StorageLevel = StorageLevel.MEMORY_ONLY): CheckpointedDrm[K]
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOps.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOps.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import scala.reflect.ClassTag
+import org.apache.hadoop.io.Writable
+import org.apache.mahout.sparkbindings.drm.plan.{OpRowRange, OpMapBlock}
+import RLikeDrmOps._
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+
+/** Common Drm ops */
+class DrmLikeOps[K : ClassTag](protected[drm] val drm: DrmLike[K]) {
+
+  /**
+   * Map matrix block-wise vertically. Blocks of the new matrix can be modified original block
+   * matrices; or they could be completely new matrices with new keyset. In the latter case, output
+   * matrix width must be specified with <code>ncol</code> parameter.<P>
+   *
+   * New block heights must be of the same height as the original geometry.<P>
+   *
+   * @param ncol new matrix' width (only needed if width changes).
+   * @param bmf
+   * @tparam R
+   * @return
+   */
+  def mapBlock[R : ClassTag](ncol: Int = -1)
+      (bmf: BlockMapFunc[K, R]): DrmLike[R] =
+    new OpMapBlock[K, R](A = drm, bmf = bmf, _ncol = ncol)
+
+
+  /**
+   * Slicing the DRM. Should eventually work just like in-core drm (e.g. A(0 until 5, 5 until 15)).<P>
+   *
+   * The all-range is denoted by '::', e.g.: A(::, 0 until 5).<P>
+   *
+   * Row range is currently unsupported except for the all-range. When it will be fully supported,
+   * the input must be Int-keyed, i.e. of DrmLike[Int] type for non-all-range specifications.
+   *
+   * @param rowRange Row range. This must be '::' (all-range) unless matrix rows are keyed by Int key.
+   * @param colRange col range. Must be a sub-range of <code>0 until ncol</code>. '::' denotes all-range.
+   */
+  def apply(rowRange: Range, colRange: Range): DrmLike[K] = {
+
+
+    val rowSrc: DrmLike[K] = if (rowRange != ::) {
+
+      if (implicitly[ClassTag[Int]] == implicitly[ClassTag[K]]) {
+
+        assert(rowRange.head >= 0 && rowRange.last < drm.nrow, "rows range out of range")
+        val intKeyed = drm.asInstanceOf[DrmLike[Int]]
+
+        new OpRowRange(A = intKeyed, rowRange = rowRange).asInstanceOf[DrmLike[K]]
+
+      } else throw new IllegalArgumentException("non-all row range is only supported for Int-keyed DRMs.")
+
+    } else drm
+
+    if (colRange != ::) {
+
+      assert(colRange.head >= 0 && colRange.last < drm.ncol, "col range out of range")
+
+      // Use mapBlock operator to do in-core subranging.
+      rowSrc.mapBlock(ncol = colRange.length)({
+        case (keys, block) => keys -> block(::, colRange)
+      })
+
+    } else rowSrc
+  }
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmRddInput.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmRddInput.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmRddInput.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/DrmRddInput.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import scala.reflect.ClassTag
+import org.apache.spark.SparkContext
+import org.apache.spark.storage.StorageLevel
+
+/** Encapsulates either DrmRdd[K] or BlockifiedDrmRdd[K] */
+class DrmRddInput[K: ClassTag](
+    private val rowWiseSrc: Option[( /*ncol*/ Int, /*rdd*/ DrmRdd[K])] = None,
+    private val blockifiedSrc: Option[BlockifiedDrmRdd[K]] = None
+    ) {
+
+  assert(rowWiseSrc.isDefined || blockifiedSrc.isDefined, "Undefined input")
+
+  private lazy val backingRdd = rowWiseSrc.map(_._2).getOrElse(blockifiedSrc.get)
+
+  def toDrmRdd(): DrmRdd[K] = rowWiseSrc.map(_._2).getOrElse(deblockify(rdd = blockifiedSrc.get))
+
+  def toBlockifiedDrmRdd() = blockifiedSrc.getOrElse(blockify(rdd = rowWiseSrc.get._2, blockncol = rowWiseSrc.get._1))
+
+  def sparkContext: SparkContext = backingRdd.sparkContext
+
+  def persist(sl: StorageLevel) = backingRdd.persist(newLevel = sl)
+
+  def unpersist(blocking: Boolean = true) = backingRdd.unpersist(blocking)
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOps.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOps.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOps.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOps.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.plan._
+import org.apache.mahout.math.{Matrices, SparseColumnMatrix, Vector, Matrix}
+import org.apache.mahout.sparkbindings.drm.plan.OpTimesLeftMatrix
+import org.apache.mahout.sparkbindings.drm.plan.OpAt
+import org.apache.mahout.sparkbindings.drm.plan.OpAB
+import org.apache.mahout.sparkbindings.drm.plan.OpTimesRightMatrix
+import org.apache.hadoop.io.Writable
+import org.apache.spark.SparkContext._
+
+/**
+ * @author dmitriy
+ */
+class RLikeDrmOps[K: ClassTag](drm: DrmLike[K]) extends DrmLikeOps[K](drm) {
+
+  import RLikeDrmOps._
+
+  def +(that: DrmLike[K]): DrmLike[K] = OpAewB[K](A = this, B = that, op = '+')
+
+  def -(that: DrmLike[K]): DrmLike[K] = OpAewB[K](A = this, B = that, op = '-')
+
+  def *(that: DrmLike[K]): DrmLike[K] = OpAewB[K](A = this, B = that, op = '*')
+
+  def /(that: DrmLike[K]): DrmLike[K] = OpAewB[K](A = this, B = that, op = '/')
+
+  def +(that: Double): DrmLike[K] = OpAewScalar[K](A = this, scalar = that, op = "+")
+
+  def -(that: Double): DrmLike[K] = OpAewScalar[K](A = this, scalar = that, op = "-")
+
+  def -:(that: Double): DrmLike[K] = OpAewScalar[K](A = this, scalar = that, op = "-:")
+
+  def *(that: Double): DrmLike[K] = OpAewScalar[K](A = this, scalar = that, op = "*")
+
+  def /(that: Double): DrmLike[K] = OpAewScalar[K](A = this, scalar = that, op = "/")
+
+  def /:(that: Double): DrmLike[K] = OpAewScalar[K](A = this, scalar = that, op = "/:")
+
+  def :%*%(that: DrmLike[Int]): DrmLike[K] = OpAB[K](A = this.drm, B = that)
+
+  def %*%[B:ClassTag](that:DrmLike[B]):DrmLike[K] = OpABAnyKey[B,K](A=this.drm, B=that)
+
+  def %*%(that: DrmLike[Int]): DrmLike[K] = this :%*% that
+
+  def :%*%(that: Matrix): DrmLike[K] = OpTimesRightMatrix[K](A = this.drm, right = that)
+
+  def %*%(that: Matrix): DrmLike[K] = this :%*% that
+
+  def :%*%(that: Vector): DrmLike[K] = OpAx(A = this.drm, x = that)
+
+  def %*%(that: Vector): DrmLike[K] = :%*%(that)
+
+  def t: DrmLike[Int] = OpAtAnyKey(A = drm)
+}
+
+class RLikeDrmIntOps(drm: DrmLike[Int]) extends RLikeDrmOps[Int](drm) {
+
+  override def t: DrmLike[Int] = OpAt(A = drm)
+
+  def %*%:[K: ClassTag](that: DrmLike[K]): DrmLike[K] = OpAB[K](A = that, B = this.drm)
+
+  def %*%:(that: Matrix): DrmLike[Int] = OpTimesLeftMatrix(left = that, A = this.drm)
+
+
+}
+
+object RLikeDrmOps {
+  implicit def drmInt2RLikeOps(drm: DrmLike[Int]): RLikeDrmIntOps = new RLikeDrmIntOps(drm)
+
+  implicit def drm2RLikeOps[K: ClassTag](drm: DrmLike[K]): RLikeDrmOps[K] = new RLikeDrmOps[K](drm)
+
+  implicit def rlikeOps2Drm[K: ClassTag](ops: RLikeDrmOps[K]): DrmLike[K] = ops.drm
+
+  implicit def ops2Drm[K: ClassTag](ops: DrmLikeOps[K]): DrmLike[K] = ops.drm
+
+  implicit def cp2cpops[K:ClassTag](cp:CheckpointedDrm[K]):CheckpointedOps[K] = new CheckpointedOps(cp)
+
+  /**
+   * This is probably dangerous since it triggers implicit checkpointing with default storage level
+   * setting.
+   */
+  implicit def drm2cpops[K:ClassTag](drm:DrmLike[K]):CheckpointedOps[K] = new CheckpointedOps(drm.checkpoint())
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/package.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/package.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/package.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/package.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,282 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings
+
+import org.apache.mahout.math._
+import org.apache.spark.SparkContext
+import scala.collection.JavaConversions._
+import org.apache.hadoop.io.{LongWritable, Text, IntWritable, Writable}
+import org.apache.log4j.Logger
+import java.lang.Math
+import org.apache.spark.rdd.RDD
+import scala.reflect.ClassTag
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import SparkContext._
+import org.apache.spark.broadcast.Broadcast
+import org.apache.mahout.sparkbindings.drm.decompositions.{DSPCA, DSSVD, DQR}
+
+
+package object drm {
+
+  private[drm] final val log = Logger.getLogger("org.apache.mahout.sparkbindings");
+
+  /** Drm row-wise tuple */
+  type DrmTuple[K] = (K, Vector)
+
+  /** Row-wise organized DRM rdd type */
+  type DrmRdd[K] = RDD[DrmTuple[K]]
+
+  /** Drm block-wise tuple: Array of row keys and the matrix block. */
+  type BlockifiedDrmTuple[K] = (Array[K], _ <: Matrix)
+
+  /**
+   * Blockifed DRM rdd (keys of original DRM are grouped into array corresponding to rows of Matrix
+   * object value
+   */
+  type BlockifiedDrmRdd[K] = RDD[BlockifiedDrmTuple[K]]
+
+  /** Block-map func */
+  type BlockMapFunc[S, R] = BlockifiedDrmTuple[S] => BlockifiedDrmTuple[R]
+
+  implicit def input2drmRdd[K](input: DrmRddInput[K]): DrmRdd[K] = input.toDrmRdd()
+
+  implicit def input2blockifiedDrmRdd[K](input: DrmRddInput[K]): BlockifiedDrmRdd[K] = input.toBlockifiedDrmRdd()
+
+  implicit def cpDrm2DrmRddInput[K: ClassTag](cp: CheckpointedDrm[K]): DrmRddInput[K] =
+    new DrmRddInput(rowWiseSrc = Some(cp.ncol -> cp.rdd))
+
+  implicit def drm2drmOps[K <% Writable : ClassTag](drm: CheckpointedDrmBase[K]): CheckpointedOps[K] =
+    new CheckpointedOps[K](drm)
+
+  implicit def v2Writable(v: Vector): VectorWritable = new VectorWritable(v)
+
+  implicit def m2Writable(m: Matrix): MatrixWritable = new MatrixWritable(m)
+
+  implicit def vw2v(vw: VectorWritable): Vector = vw.get()
+
+  implicit def mw2m(mw: MatrixWritable): Matrix = mw.get()
+
+  implicit def drmLike2Checkpointed[K](drm: DrmLike[K]): CheckpointedDrm[K] = drm.checkpoint()
+
+  implicit def bcast2Matrix(bcast: Broadcast[_ <: Matrix]): Matrix = bcast.value
+
+  implicit def bcast2Vector(bcast: Broadcast[_ <: Vector]): Vector = bcast.value
+
+
+  /**
+   * Load DRM from hdfs (as in Mahout DRM format)
+   *
+   * @param path
+   * @param sc spark context (wanted to make that implicit, doesn't work in current version of
+   *           scala with the type bounds, sorry)
+   *
+   * @return DRM[Any] where Any is automatically translated to value type
+   */
+  def drmFromHDFS(path: String)(implicit sc: SparkContext): CheckpointedDrmBase[Any] = {
+    val rdd = sc.sequenceFile(path, classOf[Writable], classOf[VectorWritable]).map(t => (t._1, t._2.get()))
+
+    val key = rdd.map(_._1).take(1)(0)
+    val keyWClass = key.getClass.asSubclass(classOf[Writable])
+
+    val key2val = key match {
+      case xx: IntWritable => (v: AnyRef) => v.asInstanceOf[IntWritable].get
+      case xx: Text => (v: AnyRef) => v.asInstanceOf[Text].toString
+      case xx: LongWritable => (v: AnyRef) => v.asInstanceOf[LongWritable].get
+      case xx: Writable => (v: AnyRef) => v
+    }
+
+    val val2key = key match {
+      case xx: IntWritable => (x: Any) => new IntWritable(x.asInstanceOf[Int])
+      case xx: Text => (x: Any) => new Text(x.toString)
+      case xx: LongWritable => (x: Any) => new LongWritable(x.asInstanceOf[Int])
+      case xx: Writable => (x: Any) => x.asInstanceOf[Writable]
+    }
+
+    {
+      implicit val km: ClassManifest[AnyRef] = ClassManifest.classType(key.getClass)
+      implicit def getWritable(x: Any): Writable = val2key()
+      new CheckpointedDrmBase[Any](rdd.map(t => (key2val(t._1), t._2)))
+    }
+  }
+
+  /** Shortcut to parallelizing matrices with indices, ignore row labels. */
+  def drmParallelize(m: Matrix, numPartitions: Int = 1)
+      (implicit sc: SparkContext) =
+    drmParallelizeWithRowIndices(m, numPartitions)(sc)
+
+  /** Parallelize in-core matrix as spark distributed matrix, using row ordinal indices as data set keys. */
+  def drmParallelizeWithRowIndices(m: Matrix, numPartitions: Int = 1)
+      (implicit sc: SparkContext)
+  : CheckpointedDrm[Int] = {
+
+    new CheckpointedDrmBase(parallelizeInCore(m, numPartitions))
+  }
+
+  private[sparkbindings] def parallelizeInCore(m: Matrix, numPartitions: Int = 1)
+      (implicit sc: SparkContext): DrmRdd[Int] = {
+
+    val p = (0 until m.nrow).map(i => i -> m(i, ::))
+    sc.parallelize(p, numPartitions)
+
+  }
+
+  /** Parallelize in-core matrix as spark distributed matrix, using row labels as a data set keys. */
+  def drmParallelizeWithRowLabels(m: Matrix, numPartitions: Int = 1)
+      (implicit sc: SparkContext)
+  : CheckpointedDrmBase[String] = {
+
+
+    // In spark 0.8, I have patched ability to parallelize kryo objects directly, so no need to
+    // wrap that into byte array anymore
+    val rb = m.getRowLabelBindings
+    val p = for (i: String <- rb.keySet().toIndexedSeq) yield i -> m(rb(i), ::)
+
+
+    new CheckpointedDrmBase(sc.parallelize(p, numPartitions))
+  }
+
+  /** This creates an empty DRM with specified number of partitions and cardinality. */
+  def drmParallelizeEmpty(nrow: Int, ncol: Int, numPartitions: Int = 10)
+      (implicit sc: SparkContext): CheckpointedDrm[Int] = {
+    val rdd = sc.parallelize(0 to numPartitions, numPartitions).flatMap(part => {
+      val partNRow = (nrow - 1) / numPartitions + 1
+      val partStart = partNRow * part
+      val partEnd = Math.min(partStart + partNRow, nrow)
+
+      for (i <- partStart until partEnd) yield (i, new RandomAccessSparseVector(ncol): Vector)
+    })
+    new CheckpointedDrmBase[Int](rdd, nrow, ncol)
+  }
+
+  def drmParallelizeEmptyLong(nrow: Long, ncol: Int, numPartitions: Int = 10)
+      (implicit sc: SparkContext): CheckpointedDrmBase[Long] = {
+    val rdd = sc.parallelize(0 to numPartitions, numPartitions).flatMap(part => {
+      val partNRow = (nrow - 1) / numPartitions + 1
+      val partStart = partNRow * part
+      val partEnd = Math.min(partStart + partNRow, nrow)
+
+      for (i <- partStart until partEnd) yield (i, new RandomAccessSparseVector(ncol): Vector)
+    })
+    new CheckpointedDrmBase[Long](rdd, nrow, ncol)
+  }
+
+  def drmWrap[K : ClassTag](
+      rdd: DrmRdd[K],
+      nrow: Int = -1,
+      ncol: Int = -1
+      ): CheckpointedDrm[K] =
+    new CheckpointedDrmBase[K](
+      rdd = rdd,
+      _nrow = nrow,
+      _ncol = ncol
+    )
+
+
+  /** Broadcast vector (Mahout vectors are not closure-friendly, use this instead. */
+  def drmBroadcast(x: Vector)(implicit sc: SparkContext): Broadcast[Vector] = sc.broadcast(x)
+
+  /** Broadcast in-core Mahout matrix. Use this instead of closure. */
+  def drmBroadcast(m: Matrix)(implicit sc: SparkContext): Broadcast[Matrix] = sc.broadcast(m)
+
+  def safeToNonNegInt(x: Long): Int = {
+    assert(x == x << -31 >>> -31, "transformation from long to Int is losing signficant bits, or is a negative number")
+    x.toInt
+  }
+
+  def blockify[K: ClassTag](rdd: DrmRdd[K], blockncol: Int): BlockifiedDrmRdd[K] = {
+
+    rdd.mapPartitions(iter => {
+
+      if (!iter.hasNext) Iterator.empty
+      else {
+
+        val data = iter.toIterable
+        val keys = data.map(t => t._1).toArray[K]
+        val vectors = data.map(t => t._2).toArray
+
+        val block = new SparseRowMatrix(vectors.size, blockncol, vectors)
+
+        Iterator(keys -> block)
+      }
+    })
+  }
+
+  def deblockify[K: ClassTag](rdd: BlockifiedDrmRdd[K]): DrmRdd[K] =
+
+  // Just flat-map rows, connect with the keys
+    rdd.flatMap({
+      case (blockKeys: Array[K], block: Matrix) =>
+
+        blockKeys.ensuring(blockKeys.size == block.nrow)
+        blockKeys.view.zipWithIndex.map({
+          case (key, idx) =>
+            var v = block(idx, ::)
+
+            // If a view rather than a concrete vector, clone into a concrete vector in order not to
+            // attempt to serialize outer matrix when we save it (Although maybe most often this
+            // copying is excessive?)
+            // if (v.isInstanceOf[MatrixVectorView]) v = v.cloned
+            key -> v
+        })
+
+    })
+
+  // ============== Decompositions ===================
+
+  /**
+   * Distributed _thin_ QR. A'A must fit in a memory, i.e. if A is m x n, then n should be pretty
+   * controlled (<5000 or so). <P>
+   *
+   * It is recommended to checkpoint A since it does two passes over it. <P>
+   *
+   * It also guarantees that Q is partitioned exactly the same way (and in same key-order) as A, so
+   * their RDD should be able to zip successfully.
+   */
+  def dqrThin[K: ClassTag](A: DrmLike[K], checkRankDeficiency: Boolean = true): (DrmLike[K], Matrix) =
+    DQR.dqrThin(A, checkRankDeficiency)
+
+  /**
+   * Distributed Stochastic Singular Value decomposition algorithm.
+   *
+   * @param A input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations
+   * @return (U,V,s). Note that U, V are non-checkpointed matrices (i.e. one needs to actually use them
+   *         e.g. save them to hdfs in order to trigger their computation.
+   */
+  def dssvd[K: ClassTag](A: DrmLike[K], k: Int, p: Int = 15, q: Int = 0):
+  (DrmLike[K], DrmLike[Int], Vector) = DSSVD.dssvd(A, k, p, q)
+
+  /**
+   * Distributed Stochastic PCA decomposition algorithm. A logical reflow of the "SSVD-PCA options.pdf"
+   * document of the MAHOUT-817.
+   *
+   * @param A input matrix A
+   * @param k request SSVD rank
+   * @param p oversampling parameter
+   * @param q number of power iterations (hint: use either 0 or 1)
+   * @return (U,V,s). Note that U, V are non-checkpointed matrices (i.e. one needs to actually use them
+   *         e.g. save them to hdfs in order to trigger their computation.
+   */
+  def dspca[K: ClassTag](A: DrmLike[K], k: Int, p: Int = 15, q: Int = 0):
+  (DrmLike[K], DrmLike[Int], Vector) = DSPCA.dspca(A, k, p, q)
+
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractBinaryOp.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractBinaryOp.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractBinaryOp.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractBinaryOp.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+import scala.util.Random
+
+/**
+ * @author dmitriy
+ */
+abstract class AbstractBinaryOp[A : ClassTag, B : ClassTag, K : ClassTag]
+    extends CheckpointAction[K] with DrmLike[K] {
+
+  protected[plan] var A: DrmLike[A]
+  protected[plan] var B: DrmLike[B]
+
+  // These are explicit evidence export. Sometimes scala falls over to figure that on its own.
+  def classTagA: ClassTag[A] = implicitly[ClassTag[A]]
+
+  def classTagB: ClassTag[B] = implicitly[ClassTag[B]]
+
+  def classTagK: ClassTag[K] = implicitly[ClassTag[K]]
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractUnaryOp.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractUnaryOp.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractUnaryOp.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/AbstractUnaryOp.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/** Abstract unary operator */
+abstract class AbstractUnaryOp[A: ClassTag, K: ClassTag]
+    extends CheckpointAction[K] with DrmLike[K] {
+
+  protected[plan] var A: DrmLike[A]
+
+  def classTagA: ClassTag[A] = implicitly[ClassTag[A]]
+
+  def classTagK: ClassTag[K] = implicitly[ClassTag[K]]
+
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/CheckpointAction.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/CheckpointAction.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/CheckpointAction.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/CheckpointAction.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,193 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.blas._
+import org.apache.mahout.sparkbindings.drm._
+import CheckpointAction._
+import org.apache.spark.SparkContext._
+import org.apache.hadoop.io.Writable
+import org.apache.spark.storage.StorageLevel
+import scala.util.Random
+
+/** Implementation of distributed expression checkpoint and optimizer. */
+abstract class CheckpointAction[K: ClassTag] extends DrmLike[K] {
+
+  private[sparkbindings] lazy val partitioningTag: Long = Random.nextLong()
+
+  private var cp:Option[CheckpointedDrm[K]] = None
+
+
+  def isIdenticallyPartitioned(other:DrmLike[_]) =
+    partitioningTag!= 0L && partitioningTag == other.partitioningTag
+
+  /**
+   * Action operator -- does not necessary means Spark action; but does mean running BLAS optimizer
+   * and writing down Spark graph lineage since last checkpointed DRM.
+   */
+  def checkpoint(sLevel: StorageLevel): CheckpointedDrm[K] = cp.getOrElse({
+    // Non-zero count is sparsely supported by logical operators now. So assume we have no knowledge
+    // if it is unsupported, instead of failing.
+    val plan = optimize(this)
+    val rdd = exec(plan)
+    val newcp = new CheckpointedDrmBase(
+      rdd = rdd,
+      _nrow = nrow,
+      _ncol = ncol,
+      _cacheStorageLevel = sLevel,
+      partitioningTag = plan.partitioningTag
+    )
+    cp = Some(newcp)
+    newcp.cache()
+  })
+
+}
+
+object CheckpointAction {
+
+  /** Perform expression optimization. Return physical plan that we can pass to exec() */
+  def optimize[K: ClassTag](action: DrmLike[K]): DrmLike[K] = pass3(pass2(pass1(action)))
+
+
+  /** This is mostly multiplication operations rewrites */
+  private def pass1[K: ClassTag](action: DrmLike[K]): DrmLike[K] = {
+
+    action match {
+      case OpAB(OpAt(a), b) if (a == b) => OpAtA(pass1(a))
+      case OpABAnyKey(OpAtAnyKey(a), b) if (a == b) => OpAtA(pass1(a))
+
+      // For now, rewrite left-multiply via transpositions, i.e.
+      // inCoreA %*% B = (B' %*% inCoreA')'
+      case op@OpTimesLeftMatrix(a, b) =>
+        OpAt(OpTimesRightMatrix(A = OpAt(pass1(b)), right = a.t))
+
+      // Stop at checkpoints
+      case cd: CheckpointedDrm[_] => action
+
+      // For everything else we just pass-thru the operator arguments to optimizer
+      case uop: AbstractUnaryOp[_, K] =>
+        uop.A = pass1(uop.A)(uop.classTagA)
+        uop
+      case bop: AbstractBinaryOp[_, _, K] =>
+        bop.A = pass1(bop.A)(bop.classTagA)
+        bop.B = pass1(bop.B)(bop.classTagB)
+        bop
+    }
+  }
+
+  /** This would remove stuff like A.t.t that previous step may have created */
+  private def pass2[K: ClassTag](action: DrmLike[K]): DrmLike[K] = {
+    action match {
+      // A.t.t => A
+      case OpAt(top@OpAt(a)) => pass2(a)(top.classTagA)
+
+      // A.t.t => A
+//      case OpAt(top@OpAtAnyKey(a)) => pass2(a)(top.classTagA)
+
+
+      // Stop at checkpoints
+      case cd: CheckpointedDrm[_] => action
+
+      // For everything else we just pass-thru the operator arguments to optimizer
+      case uop: AbstractUnaryOp[_, K] =>
+        uop.A = pass2(uop.A)(uop.classTagA)
+        uop
+      case bop: AbstractBinaryOp[_, _, K] =>
+        bop.A = pass2(bop.A)(bop.classTagA)
+        bop.B = pass2(bop.B)(bop.classTagB)
+        bop
+    }
+  }
+
+ /** Some further rewrites that are conditioned on A.t.t removal */
+  private def pass3[K: ClassTag](action: DrmLike[K]): DrmLike[K] = {
+    action match {
+
+      // matrix products.
+      case OpAB(a, OpAt(b)) => OpABt(pass3(a), pass3(b))
+
+      // AtB cases that make sense.
+      case OpAB(OpAt(a), b) if (a.partitioningTag == b.partitioningTag) => OpAtB(pass3(a), pass3(b))
+      case OpABAnyKey(OpAtAnyKey(a), b) => OpAtB(pass3(a), pass3(b))
+
+      // Need some cost to choose between the following.
+
+      case OpAB(OpAt(a), b) => OpAtB(pass3(a), pass3(b))
+      //      case OpAB(OpAt(a), b) => OpAt(OpABt(OpAt(pass1(b)), pass1(a)))
+      case OpAB(a, b) => OpABt(pass3(a), OpAt(pass3(b)))
+      // Rewrite A'x
+      case op@OpAx(op1@OpAt(a), x) => OpAtx(pass3(a)(op1.classTagA), x)
+
+      // Stop at checkpoints
+      case cd: CheckpointedDrm[_] => action
+
+      // For everything else we just pass-thru the operator arguments to optimizer
+      case uop: AbstractUnaryOp[_, K] =>
+        uop.A = pass3(uop.A)(uop.classTagA)
+        uop
+      case bop: AbstractBinaryOp[_, _, K] =>
+        bop.A = pass3(bop.A)(bop.classTagA)
+        bop.B = pass3(bop.B)(bop.classTagB)
+        bop
+    }
+  }
+
+
+  /** Execute previously optimized physical plan */
+  def exec[K: ClassTag](oper: DrmLike[K]): DrmRddInput[K] = {
+    // I do explicit evidence propagation here since matching via case classes seems to be loosing
+    // it and subsequently may cause something like DrmRddInput[Any] instead of [Int] or [String].
+    // Hence you see explicit evidence attached to all recursive exec() calls.
+    oper match {
+      // If there are any such cases, they must go away in pass1. If they were not, then it wasn't
+      // the A'A case but actual transposition intent which should be removed from consideration
+      // (we cannot do actual flip for non-int-keyed arguments)
+      case OpAtAnyKey(_) =>
+        throw new IllegalArgumentException("\"A\" must be Int-keyed in this A.t expression.")
+      case op@OpAt(a) => At.at(op, exec(a)(op.classTagA))
+      case op@OpABt(a, b) => ABt.abt(op, exec(a)(op.classTagA), exec(b)(op.classTagB))
+      case op@OpAtB(a, b) => AtB.atb_nograph(op, exec(a)(op.classTagA), exec(b)(op.classTagB),
+        zippable = a.partitioningTag == b.partitioningTag)
+      case op@OpAtA(a) => AtA.at_a(op, exec(a)(op.classTagA))
+      case op@OpAx(a, x) => Ax.ax_with_broadcast(op, exec(a)(op.classTagA))
+      case op@OpAtx(a, x) => Ax.atx_with_broadcast(op, exec(a)(op.classTagA))
+      case op@OpAewB(a, b, '+') => AewB.a_plus_b(op, exec(a)(op.classTagA), exec(b)(op.classTagB))
+      case op@OpAewB(a, b, '-') => AewB.a_minus_b(op, exec(a)(op.classTagA), exec(b)(op.classTagB))
+      case op@OpAewB(a, b, '*') => AewB.a_hadamard_b(op, exec(a)(op.classTagA), exec(b)(op.classTagB))
+      case op@OpAewB(a, b, '/') => AewB.a_eldiv_b(op, exec(a)(op.classTagA), exec(b)(op.classTagB))
+      case op@OpAewScalar(a, s, "+") => AewB.a_plus_scalar(op, exec(a)(op.classTagA), s)
+      case op@OpAewScalar(a, s, "-") => AewB.a_minus_scalar(op, exec(a)(op.classTagA), s)
+      case op@OpAewScalar(a, s, "-:") => AewB.scalar_minus_a(op, exec(a)(op.classTagA), s)
+      case op@OpAewScalar(a, s, "*") => AewB.a_times_scalar(op, exec(a)(op.classTagA), s)
+      case op@OpAewScalar(a, s, "/") => AewB.a_div_scalar(op, exec(a)(op.classTagA), s)
+      case op@OpAewScalar(a, s, "/:") => AewB.scalar_div_a(op, exec(a)(op.classTagA), s)
+      case op@OpRowRange(a, _) => Slicing.rowRange(op, exec(a)(op.classTagA))
+      case op@OpTimesRightMatrix(a, _) => AinCoreB.rightMultiply(op, exec(a)(op.classTagA))
+      // Custom operators, we just execute them
+      case blockOp: OpMapBlock[K, _] => blockOp.exec(src = exec(blockOp.A)(blockOp.classTagA))
+      case cp: CheckpointedDrm[K] => new DrmRddInput[K](rowWiseSrc = Some((cp.ncol, cp.rdd)))
+      case _ => throw new IllegalArgumentException("Internal:Optimizer has no exec policy for operator %s."
+          .format(oper))
+
+    }
+  }
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAB.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAB.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAB.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAB.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+import org.apache.hadoop.io.Writable
+import org.apache.spark.SparkContext._
+
+/** Logical AB */
+case class OpAB[K: ClassTag ](
+    override var A: DrmLike[K],
+    override var B: DrmLike[Int])
+    extends AbstractBinaryOp[K, Int, K] {
+
+  assert(A.ncol == B.nrow, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = B.ncol
+
+  /** Non-zero element count */
+  def nNonZero: Long =
+  // TODO: for purposes of cost calculation, approximate based on operands
+    throw new UnsupportedOperationException
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABAnyKey.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABAnyKey.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABAnyKey.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABAnyKey.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/** Logical AB */
+case class OpABAnyKey[B:ClassTag, K: ClassTag ](
+    override var A: DrmLike[K],
+    override var B: DrmLike[B])
+    extends AbstractBinaryOp[K, B, K] {
+
+  assert(A.ncol == B.nrow, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = B.ncol
+
+  /** Non-zero element count */
+  def nNonZero: Long =
+  // TODO: for purposes of cost calculation, approximate based on operands
+    throw new UnsupportedOperationException
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABt.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABt.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABt.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpABt.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.sparkbindings.drm._
+import scala.reflect.ClassTag
+import org.apache.spark.SparkContext._
+
+/** Logical AB' */
+case class OpABt[K: ClassTag](
+    override var A: DrmLike[K],
+    override var B: DrmLike[Int])
+    extends AbstractBinaryOp[K,Int,K]  {
+
+  assert(A.ncol == B.ncol, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = safeToNonNegInt(B.nrow)
+
+  /** Non-zero element count */
+  def nNonZero: Long =
+  // TODO: for purposes of cost calculation, approximate based on operands
+    throw new UnsupportedOperationException
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewB.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewB.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewB.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewB.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/** DRM elementwise operator */
+case class OpAewB[K: ClassTag](
+    override var A: DrmLike[K],
+    override var B: DrmLike[K],
+    val op: Char
+    ) extends AbstractBinaryOp[K, K, K] {
+
+  assert(A.ncol == B.ncol, "arguments must have same number of columns")
+  assert(A.nrow == B.nrow, "arguments must have same number of rows")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = A.ncol
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewScalar.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewScalar.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewScalar.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAewScalar.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,38 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/** Operator denoting expressions like 5.0 - A or A * 5.6 */
+case class OpAewScalar[K: ClassTag](
+    override var A: DrmLike[K],
+    val scalar: Double,
+    val op: String
+    ) extends AbstractUnaryOp[K,K] {
+
+  override private[sparkbindings] lazy val partitioningTag: Long = A.partitioningTag
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = A.ncol
+
+}
\ No newline at end of file
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAt.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAt.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAt.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAt.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.sparkbindings.drm._
+import scala.reflect.ClassTag
+
+/** Logical A' */
+case class OpAt(
+    override var A: DrmLike[Int])
+    extends AbstractUnaryOp[Int, Int] {
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.ncol
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = safeToNonNegInt(A.nrow)
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtA.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtA.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtA.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtA.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,37 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.DrmLike
+import org.apache.spark.SparkContext._
+
+/** A'A */
+case class OpAtA[K: ClassTag](
+    override var A: DrmLike[K]
+    ) extends AbstractUnaryOp[K, Int] {
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.ncol
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = A.ncol
+
+  /** Non-zero element count */
+  def nNonZero: Long = throw new UnsupportedOperationException
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtAnyKey.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtAnyKey.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtAnyKey.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtAnyKey.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.sparkbindings.drm._
+import scala.reflect.ClassTag
+
+/** Logical A' for any row key to support A'A optimizations */
+case class OpAtAnyKey[A:ClassTag](
+    override var A: DrmLike[A])
+    extends AbstractUnaryOp[A, Int] {
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.ncol
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = safeToNonNegInt(A.nrow)
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtB.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtB.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtB.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtB.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.sparkbindings.drm._
+import scala.reflect.ClassTag
+
+/** Logical A'B */
+case class OpAtB[A:ClassTag](
+    override var A: DrmLike[A],
+    override var B: DrmLike[A])
+    extends AbstractBinaryOp[A,A,Int]  {
+
+  assert(A.nrow == B.nrow, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.ncol
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = B.ncol
+
+  /** Non-zero element count */
+  def nNonZero: Long =
+  // TODO: for purposes of cost calculation, approximate based on operands
+    throw new UnsupportedOperationException
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtx.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtx.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtx.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAtx.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.Vector
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm._
+
+/** Logical A'x. */
+case class OpAtx(
+    override var A: DrmLike[Int],
+    val x: Vector
+    ) extends AbstractUnaryOp[Int, Int] {
+
+  override private[sparkbindings] lazy val partitioningTag: Long = A.partitioningTag
+
+  assert(A.nrow == x.length, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = safeToNonNegInt(A.ncol)
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = 1
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAx.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAx.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAx.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpAx.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.{Vector, Matrix}
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/** Logical Ax. */
+case class OpAx[K: ClassTag](
+    override var A: DrmLike[K],
+    val x: Vector
+    ) extends AbstractUnaryOp[K, K] {
+
+  override private[sparkbindings] lazy val partitioningTag: Long = A.partitioningTag
+
+  assert(A.ncol == x.length, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = 1
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpMapBlock.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpMapBlock.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpMapBlock.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpMapBlock.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.sparkbindings.drm.{DrmRddInput, BlockMapFunc, DrmLike}
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+
+/**
+ * @author dmitriy
+ */
+class OpMapBlock[S: ClassTag, R: ClassTag](
+    override var A: DrmLike[S],
+    val bmf: BlockMapFunc[S, R],
+    val _ncol: Int = -1,
+    val _nrow: Long = -1
+    ) extends AbstractUnaryOp[S, R] {
+
+
+  override private[sparkbindings] lazy val partitioningTag: Long = A.partitioningTag
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = if (_nrow >= 0) _nrow else A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = if (_ncol >= 0) _ncol else A.ncol
+
+  def exec(src: DrmRddInput[S]): DrmRddInput[R] = {
+
+    // We can't use attributes to avoid putting the whole this into closure.
+    val bmf = this.bmf
+    val ncol = this.ncol
+
+    val rdd = src.toBlockifiedDrmRdd()
+        .map(blockTuple => {
+      val out = bmf(blockTuple)
+
+      assert(out._2.nrow == blockTuple._2.nrow, "block mapping must return same number of rows.")
+      assert(out._2.ncol == ncol, "block map must return %d number of columns.".format(ncol))
+
+      out
+    })
+    new DrmRddInput(blockifiedSrc = Some(rdd))
+  }
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpRowRange.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpRowRange.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpRowRange.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpRowRange.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,37 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.sparkbindings.drm.DrmLike
+import org.apache.spark.SparkContext._
+
+/** Logical row-range slicing */
+case class OpRowRange(
+  override var A:DrmLike[Int],
+  val rowRange:Range
+    ) extends AbstractUnaryOp[Int, Int] {
+
+  assert(rowRange.head>=0 && rowRange.last< A.nrow, "row range out of range")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = rowRange.length
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = A.ncol
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesLeftMatrix.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesLeftMatrix.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesLeftMatrix.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesLeftMatrix.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,44 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import org.apache.mahout.math.Matrix
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm.DrmLike
+import org.apache.spark.SparkContext._
+
+/** Logical Times-left over in-core matrix operand */
+case class OpTimesLeftMatrix(
+    val left: Matrix,
+    override var A: DrmLike[Int]
+    ) extends AbstractUnaryOp[Int, Int] {
+
+  assert(left.ncol == A.nrow, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = left.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = A.ncol
+
+  /** Non-zero element count */
+  // TODO
+  def nNonZero: Long = throw new UnsupportedOperationException
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesRightMatrix.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesRightMatrix.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesRightMatrix.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/OpTimesRightMatrix.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.plan
+
+import scala.reflect.ClassTag
+import org.apache.mahout.math.Matrix
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/** Logical times-right over in-core matrix operand. */
+case class OpTimesRightMatrix[K: ClassTag](
+    override var A: DrmLike[K],
+    val right: Matrix
+    ) extends AbstractUnaryOp[K, K] {
+
+  override private[sparkbindings] lazy val partitioningTag: Long = A.partitioningTag
+
+  assert(A.ncol == right.nrow, "Incompatible operand geometry")
+
+  /** R-like syntax for number of rows. */
+  def nrow: Long = A.nrow
+
+  /** R-like syntax for number of columns */
+  def ncol: Int = right.ncol
+
+  /** Non-zero element count */
+  // TODO
+  def nNonZero: Long = throw new UnsupportedOperationException
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/package.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/package.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/package.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/plan/package.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,24 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+
+/** This package contains logical distributed expression operators. */
+package object plan {
+
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/MahoutKryoRegistrator.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/MahoutKryoRegistrator.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/MahoutKryoRegistrator.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/MahoutKryoRegistrator.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.io
+
+import com.esotericsoftware.kryo.Kryo
+import org.apache.mahout.math._
+import org.apache.spark.serializer.KryoRegistrator
+import org.apache.mahout.sparkbindings.drm._
+
+
+/**
+ *
+ */
+class MahoutKryoRegistrator extends KryoRegistrator {
+
+  override def registerClasses(kryo: Kryo) = {
+
+    kryo.addDefaultSerializer(classOf[Vector], new WritableKryoSerializer[Vector, VectorWritable])
+    kryo.addDefaultSerializer(classOf[DenseVector], new WritableKryoSerializer[Vector, VectorWritable])
+    kryo.addDefaultSerializer(classOf[Matrix], new WritableKryoSerializer[Matrix, MatrixWritable])
+
+  }
+}
+
+object MahoutKryoRegistrator {
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/WritableKryoSerializer.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/WritableKryoSerializer.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/WritableKryoSerializer.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/io/WritableKryoSerializer.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.io
+
+import com.esotericsoftware.kryo.{Kryo, Serializer}
+import com.esotericsoftware.kryo.io.{Input, Output}
+import org.apache.hadoop.io.{DataInputBuffer, DataOutputBuffer, Writable}
+import scala.reflect.ClassTag
+
+/**
+ *
+ * @author dmitriy
+ */
+class WritableKryoSerializer[V <% Writable, W <: Writable <% V : ClassTag] extends Serializer[V] {
+
+  def write(kryo: Kryo, out: Output, v: V) = {
+    val dob = new DataOutputBuffer()
+    v.write(dob)
+    dob.close()
+
+    out.writeInt(dob.getLength)
+    out.write(dob.getData, 0, dob.getLength)
+  }
+
+  def read(kryo: Kryo, in: Input, vClazz: Class[V]): V = {
+    val dib = new DataInputBuffer()
+    val len = in.readInt()
+    val data = new Array[Byte](len)
+    in.read(data)
+    dib.reset(data, len)
+    val w: W = implicitly[ClassTag[W]].runtimeClass.newInstance().asInstanceOf[W]
+    w.readFields(dib)
+    w
+
+  }
+}
diff -uNar -x .git mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala
--- mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/package.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,133 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout
+
+import org.apache.spark.rdd.RDD
+import org.apache.mahout.math.{Matrix, Vector}
+import scala.reflect.ClassTag
+import org.apache.spark.{SparkConf, SparkContext}
+import java.io._
+import scala.collection.mutable.ArrayBuffer
+import org.apache.mahout.common.IOUtils
+import org.apache.log4j.Logger
+import org.apache.mahout.sparkbindings.drm.DrmLike
+
+/**
+ * @author dmitriy
+ */
+package object sparkbindings {
+
+  private[sparkbindings] val log = Logger.getLogger("org.apache.mahout.sparkbindings")
+
+  /**
+   * Create proper spark context that includes local Mahout jars
+   * @param masterUrl
+   * @param appName
+   * @param customJars
+   * @return
+   */
+  def mahoutSparkContext(masterUrl: String, appName: String,
+      customJars: TraversableOnce[String] = Nil): SparkContext = {
+    val closeables = new java.util.ArrayDeque[Closeable]()
+
+    try {
+
+      val conf = if (!masterUrl.startsWith("local")
+          || System.getProperties.contains("mahout.home")
+          || System.getenv("MAHOUT_HOME") != null
+      ) {
+        var mhome = System.getenv("MAHOUT_HOME")
+        if (mhome == null) mhome = System.getProperty("mahout.home")
+
+        if (mhome == null)
+          throw new IllegalArgumentException("MAHOUT_HOME is required to spawn mahout-based spark jobs.")
+
+        // Figure Mahout classpath using $MAHOUT_HOME/mahout classpath command.
+
+        val fmhome = new File(mhome)
+        val bin = new File(fmhome, "bin")
+        val exec = new File(bin, "mahout")
+        if (!exec.canExecute)
+          throw new IllegalArgumentException("Cannot execute %s.".format(exec.getAbsolutePath))
+
+        val p = Runtime.getRuntime.exec(Array(exec.getAbsolutePath, "classpath"))
+
+        closeables.addFirst(new Closeable {
+          def close() {
+            p.destroy()
+          }
+        })
+
+        val r = new BufferedReader(new InputStreamReader(p.getInputStream))
+        closeables.addFirst(r)
+
+        val w = new StringWriter()
+        closeables.addFirst(w)
+
+        var continue = true;
+        val jars = new ArrayBuffer[String]()
+        do {
+          val cp = r.readLine()
+          if (cp == null)
+            throw new IllegalArgumentException("Unable to read output from \"mahout classpath\"")
+
+          val j = cp.split(File.pathSeparatorChar)
+          if (j.size > 10) {
+            // assume this is a valid classpath line
+            jars ++= j
+            continue = false
+          }
+        } while (continue)
+
+        //      if (s_log.isDebugEnabled) {
+        //        s_log.debug("Mahout jars:")
+        //        jars.foreach(j => s_log.debug(j))
+        //      }
+
+        // context specific jars
+        val mcjars = jars.filter(j =>
+          j.matches(".*mahout-math-.*\\.jar") ||
+              j.matches(".*mahout-math-scala-.*\\.jar") ||
+              j.matches(".*mahout-core-.*\\.jar") ||
+              j.matches(".*mahout-spark-.*\\.jar")
+        ).filter(!_.matches(".*-tests.jar")) ++
+            SparkContext.jarOfClass(classOf[DrmLike[_]]) ++ customJars
+
+        if (log.isDebugEnabled) {
+          log.debug("Mahout jars:")
+          mcjars.foreach(j => log.debug(j))
+        }
+
+        new SparkConf().setJars(jars = mcjars.toSeq)
+
+      } else new SparkConf()
+
+      conf.setAppName(appName).setMaster(masterUrl)
+          .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
+          .set("spark.kryo.registrator", "org.apache.mahout.sparkbindings.io.MahoutKryoRegistrator")
+
+      new SparkContext(config = conf)
+
+    } finally {
+      IOUtils.close(closeables)
+    }
+
+  }
+
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/ABtSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/ABtSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/ABtSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/ABtSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.scalatest.FunSuite
+import org.apache.mahout.math.scalabindings._
+import org.apache.mahout.sparkbindings._
+import drm._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm.plan.OpABt
+import org.apache.spark.SparkContext._
+
+/** Tests for AB' operator algorithms */
+class ABtSuite extends FunSuite with MahoutLocalContext {
+
+  test("ABt") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5))
+    val inCoreB = dense((3, 4, 5), (5, 6, 7))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = drmParallelize(m = inCoreB)
+
+    val op = new OpABt(A, B)
+
+    val drm = new CheckpointedDrmBase(ABt.abt(op, srcA = A, srcB = B), op.nrow, op.ncol)
+
+    val inCoreMControl = inCoreA %*% inCoreB.t
+    val inCoreM = drm.collect
+
+    assert((inCoreM - inCoreMControl).norm < 1E-5)
+
+    println(inCoreM)
+
+  }
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AewBSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AewBSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AewBSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AewBSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.scalatest.FunSuite
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm._
+import org.apache.mahout.sparkbindings.drm.plan.OpAewB
+import org.apache.spark.SparkContext._
+
+/** Elementwise matrix operation tests */
+class AewBSuite extends FunSuite with MahoutLocalContext {
+
+  test("A * B Hadamard") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (7, 8, 9))
+    val inCoreB = dense((3, 4, 5), (5, 6, 7), (0, 0, 0), (9, 8, 7))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = drmParallelize(m = inCoreB)
+
+    val op = new OpAewB(A, B, '*')
+
+    val M = new CheckpointedDrmBase(AewB.a_hadamard_b(op, srcA = A, srcB = B), op.nrow, op.ncol)
+
+    val inCoreM = M.collect
+    val inCoreMControl = inCoreA * inCoreB
+
+    assert((inCoreM - inCoreMControl).norm < 1E-10)
+
+  }
+
+  test("A + B Elementwise") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (7, 8, 9))
+    val inCoreB = dense((3, 4, 5), (5, 6, 7), (0, 0, 0), (9, 8, 7))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = drmParallelize(m = inCoreB)
+
+    val op = new OpAewB(A, B, '+')
+
+    val M = new CheckpointedDrmBase(AewB.a_plus_b(op, srcA = A, srcB = B), op.nrow, op.ncol)
+
+    val inCoreM = M.collect
+    val inCoreMControl = inCoreA + inCoreB
+
+    assert((inCoreM - inCoreMControl).norm < 1E-10)
+
+  }
+
+  test("A - B Elementwise") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (7, 8, 9))
+    val inCoreB = dense((3, 4, 5), (5, 6, 7), (0, 0, 0), (9, 8, 7))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = drmParallelize(m = inCoreB)
+
+    val op = new OpAewB(A, B, '-')
+
+    val M = new CheckpointedDrmBase(AewB.a_minus_b(op, srcA = A, srcB = B), op.nrow, op.ncol)
+
+    val inCoreM = M.collect
+    val inCoreMControl = inCoreA - inCoreB
+
+    assert((inCoreM - inCoreMControl).norm < 1E-10)
+
+  }
+
+  test("A / B Elementwise") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 0), (7, 8, 9))
+    val inCoreB = dense((3, 4, 5), (5, 6, 7), (10, 20, 30), (9, 8, 7))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = drmParallelize(m = inCoreB)
+
+    val op = new OpAewB(A, B, '/')
+
+    val M = new CheckpointedDrmBase(AewB.a_eldiv_b(op, srcA = A, srcB = B), op.nrow, op.ncol)
+
+    val inCoreM = M.collect
+    val inCoreMControl = inCoreA / inCoreB
+
+    assert((inCoreM - inCoreMControl).norm < 1E-10)
+
+  }
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtASuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtASuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtASuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtASuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,49 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.scalatest.FunSuite
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings._
+import drm._
+import org.apache.mahout.sparkbindings.drm.plan.OpAtA
+import org.apache.spark.SparkContext._
+
+/** Tests for {@link XtX} */
+class AtASuite extends FunSuite with MahoutLocalContext {
+
+  test("AtA slim") {
+
+    val inCoreA = dense((1, 2), (2, 3))
+    val drmA = drmParallelize(inCoreA)
+
+    val operator = new OpAtA[Int](A = drmA)
+    val inCoreAtA = AtA.at_a_slim(operator = operator, srcRdd = drmA.rdd)
+    println(inCoreAtA)
+
+    val expectedAtA = inCoreA.t %*% inCoreA
+    println(expectedAtA)
+
+    assert(expectedAtA === inCoreAtA)
+
+  }
+
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/blas/AtSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.blas
+
+import org.scalatest.FunSuite
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings._
+import drm._
+import org.apache.mahout.sparkbindings.drm.plan.OpAt
+import org.apache.spark.SparkContext._
+
+/** Tests for A' algorithms */
+class AtSuite extends FunSuite with MahoutLocalContext {
+
+  test("At") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5))
+    val A = drmParallelize(m=inCoreA, numPartitions = 2)
+
+    val op = new OpAt(A)
+    val AtDrm = new CheckpointedDrmBase(rdd= At.at(op,srcA=A),_nrow=op.nrow,_ncol=op.ncol)
+    val inCoreAt = AtDrm.collect
+    val inCoreControlAt = inCoreA.t
+
+    println(inCoreAt)
+    assert((inCoreAt - inCoreControlAt).norm < 1E-5)
+
+
+
+  }
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/decompositions/MathSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/decompositions/MathSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/decompositions/MathSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/decompositions/MathSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,181 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm.decompositions
+
+import org.scalatest.{Matchers, FunSuite}
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.drm._
+import RLikeDrmOps._
+import scala.util.Random
+import org.apache.mahout.math.{Matrices, SparseRowMatrix}
+import org.apache.spark.storage.StorageLevel
+import org.apache.mahout.common.RandomUtils
+
+/**
+ *
+ * @author dmitriy
+ */
+class MathSuite extends FunSuite with Matchers with MahoutLocalContext {
+
+  test("thin distributed qr") {
+
+    val inCoreA = dense(
+      (1, 2, 3, 4),
+      (2, 3, 4, 5),
+      (3, -4, 5, 6),
+      (4, 5, 6, 7),
+      (8, 6, 7, 8)
+    )
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val (drmQ, inCoreR) = dqrThin(A, checkRankDeficiency = false)
+
+    // Assert optimizer still knows Q and A are identically partitioned
+    drmQ.partitioningTag should equal (A.partitioningTag)
+
+    drmQ.rdd.partitions.size should be(A.rdd.partitions.size)
+
+    // Should also be zippable
+    drmQ.rdd.zip(other = A.rdd)
+
+    val inCoreQ = drmQ.collect
+
+    printf("A=\n%s\n", inCoreA)
+    printf("Q=\n%s\n", inCoreQ)
+    printf("R=\n%s\n", inCoreR)
+
+    val (qControl, rControl) = qr(inCoreA)
+    printf("qControl=\n%s\n", qControl)
+    printf("rControl=\n%s\n", rControl)
+
+    // Validate with Cholesky
+    val ch = chol(inCoreA.t %*% inCoreA)
+    printf("A'A=\n%s\n", inCoreA.t %*% inCoreA)
+    printf("L:\n%s\n", ch.getL)
+
+    val rControl2 = (ch.getL cloned).t
+    val qControl2 = ch.solveRight(inCoreA)
+    printf("qControl2=\n%s\n", qControl2)
+    printf("rControl2=\n%s\n", rControl2)
+
+    // Housholder approach seems to be a little bit more stable
+    (rControl - inCoreR).norm should be < 1E-5
+    (qControl - inCoreQ).norm should be < 1E-5
+
+    // Assert identicity with in-core Cholesky-based -- this should be tighter.
+    (rControl2 - inCoreR).norm should be < 1E-10
+    (qControl2 - inCoreQ).norm should be < 1E-10
+
+    // Assert orhtogonality:
+    // (a) Q[,j] dot Q[,j] == 1.0 for all j
+    // (b) Q[,i] dot Q[,j] == 0.0 for all i != j
+    for (col <- 0 until inCoreQ.ncol)
+      ((inCoreQ(::, col) dot inCoreQ(::, col)) - 1.0).abs should be < 1e-10
+    for (col1 <- 0 until inCoreQ.ncol - 1; col2 <- col1 + 1 until inCoreQ.ncol)
+      (inCoreQ(::, col1) dot inCoreQ(::, col2)).abs should be < 1e-10
+
+
+  }
+
+  test("dssvd - the naive-est - q=0") {
+    dssvdNaive(q = 0)
+  }
+
+  test("ddsvd - naive - q=1") {
+    dssvdNaive(q = 1)
+  }
+
+  test("ddsvd - naive - q=2") {
+    dssvdNaive(q = 2)
+  }
+
+
+  def dssvdNaive(q: Int) {
+    val inCoreA = dense(
+      (1, 2, 3, 4),
+      (2, 3, 4, 5),
+      (3, -4, 5, 6),
+      (4, 5, 6, 7),
+      (8, 6, 7, 8)
+    )
+    val drmA = drmParallelize(inCoreA, numPartitions = 2)
+
+    val (drmU, drmV, s) = dssvd(drmA, k = 4, q = q)
+    val (inCoreU, inCoreV) = (drmU.collect, drmV.collect)
+
+    printf("U:\n%s\n", inCoreU)
+    printf("V:\n%s\n", inCoreV)
+    printf("Sigma:\n%s\n", s)
+
+    (inCoreA - (inCoreU %*%: diagv(s)) %*% inCoreV.t).norm should be < 1E-5
+  }
+
+  test("dspca") {
+
+    import math._
+
+    val rnd = RandomUtils.getRandom
+
+    // Number of points
+    val m =  500
+    // Length of actual spectrum
+    val spectrumLen = 40
+
+    val spectrum = dvec((0 until spectrumLen).map(x => 300.0 * exp(-x) max 1e-3))
+    printf("spectrum:%s\n", spectrum)
+
+    val (u, _) = qr(new SparseRowMatrix(m, spectrumLen) :=
+        ((r, c, v) => if (rnd.nextDouble() < 0.2) 0 else rnd.nextDouble() + 5.0))
+
+    // PCA Rotation matrix -- should also be orthonormal.
+    val (tr, _) = qr(Matrices.symmetricUniformView(spectrumLen, spectrumLen, rnd.nextInt) - 10.0)
+
+    val input = (u %*%: diagv(spectrum)) %*% tr.t
+    val drmInput = drmParallelize(m = input, numPartitions = 2)
+
+    // Calculate just first 10 principal factors and reduce dimensionality.
+    // Since we assert just validity of the s-pca, not stochastic error, we bump p parameter to
+    // ensure to zero stochastic error and assert only functional correctness of the method's pca-
+    // specific additions.
+    val k = 10
+
+    // Calculate just first 10 principal factors and reduce dimensionality.
+    var (drmPCA, _, s) = dspca(A = drmInput, k = 10, p = spectrumLen, q = 1)
+    // Un-normalized pca data:
+    drmPCA = drmPCA %*% diagv(s)
+
+    val pca = drmPCA.checkpoint(sLevel = StorageLevel.NONE).collect
+
+    // Of course, once we calculated the pca, the spectrum is going to be different since our originally
+    // generated input was not centered. So here, we'd just brute-solve pca to verify
+    val xi = input.colMeans()
+    for (r <- 0 until input.nrow) input(r, ::) -= xi
+    var (pcaControl, _, sControl) = svd(m = input)
+    pcaControl = (pcaControl %*%: diagv(sControl))(::, 0 until k)
+
+    printf("pca:\n%s\n", pca(0 until 10, 0 until 10))
+    printf("pcaControl:\n%s\n", pcaControl(0 until 10, 0 until 10))
+
+    (pca(0 until 10, 0 until 10).norm - pcaControl(0 until 10, 0 until 10).norm).abs should be < 1E-5
+
+  }
+
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOpsSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOpsSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,92 @@
+package org.apache.mahout.sparkbindings.drm
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import RLikeDrmOps._
+import org.scalatest.{Matchers, FunSuite}
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+
+/** Tests for DrmLikeOps */
+class DrmLikeOpsSuite extends FunSuite with MahoutLocalContext {
+
+  test("mapBlock") {
+
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = A.mapBlock(/* Inherit width */) {
+      case (keys, block) => keys -> (block += 1.0)
+    }
+
+    val inCoreB = B.collect
+    val inCoreBControl = inCoreA + 1.0
+
+    println(inCoreB)
+
+    // Assert they are the same
+    (inCoreB - inCoreBControl).norm should be < 1E-10
+
+  }
+
+  test("col range") {
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = A(::, 1 to 2)
+    val inCoreB = B.collect
+    val inCoreBControl = inCoreA(::, 1 to 2)
+
+    println(inCoreB)
+
+    // Assert they are the same
+    (inCoreB - inCoreBControl).norm should be < 1E-10
+
+  }
+
+  test("row range") {
+
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = A(1 to 2, ::)
+    val inCoreB = B.collect
+    val inCoreBControl = inCoreA(1 to 2, ::)
+
+    println(inCoreB)
+
+    // Assert they are the same
+    (inCoreB - inCoreBControl).norm should be < 1E-10
+
+  }
+
+  test("col, row range") {
+
+    val inCoreA = dense((1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+    val B = A(1 to 2, 1 to 2)
+    val inCoreB = B.collect
+    val inCoreBControl = inCoreA(1 to 2, 1 to 2)
+
+    println(inCoreB)
+
+    // Assert they are the same
+    (inCoreB - inCoreBControl).norm should be < 1E-10
+
+  }
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import org.scalatest.FunSuite
+import org.apache.log4j.{Level, Logger, BasicConfigurator}
+import org.apache.mahout.math.scalabindings._
+import RLikeOps._
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.apache.spark.SparkContext._
+
+
+/**
+ * DRMLike tests
+ */
+class DrmLikeSuite extends FunSuite with MahoutLocalContext {
+
+
+  test("DRM DFS i/o (local)") {
+
+    val uploadPath = "UploadedDRM"
+
+    val inCoreA = dense((1, 2, 3), (3, 4, 5))
+    val drmA = drmParallelize(inCoreA)
+
+    drmA.writeDRM(path = uploadPath)
+
+    println(inCoreA)
+
+    // Load back from hdfs
+    val drmB = drmFromHDFS(path = uploadPath)
+
+    // Collect back into in-core
+    val inCoreB = drmB.collect
+
+    // Print out to see what it is we collected:
+    println(inCoreB)
+
+  }
+
+  test("DRM parallelizeEmpty") {
+
+    val drmEmpty = drmParallelizeEmpty(100, 50)
+
+    // collect back into in-core
+    val inCoreEmpty = drmEmpty.collect
+
+    //print out to see what it is we collected:
+    println(inCoreEmpty)
+    printf("drm nrow:%d, ncol:%d\n", drmEmpty.nrow, drmEmpty.ncol)
+    printf("in core nrow:%d, ncol:%d\n", inCoreEmpty.nrow, inCoreEmpty.ncol)
+
+
+  }
+
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/RLikeDrmOpsSuite.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,427 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.mahout.sparkbindings.drm
+
+import org.scalatest.{Matchers, FunSuite}
+import org.apache.mahout.sparkbindings.test.MahoutLocalContext
+import org.apache.mahout.math.scalabindings._
+import org.apache.mahout.sparkbindings.drm._
+import RLikeDrmOps._
+import org.apache.mahout.sparkbindings.drm.plan.{OpAtx, OpAtB, OpAtA, CheckpointAction}
+import org.apache.spark.storage.StorageLevel
+import org.apache.spark.SparkContext
+import scala.collection.mutable.ArrayBuffer
+import org.apache.mahout.math.Matrices
+import org.apache.mahout.sparkbindings.blas
+
+/** R-like DRM DSL operation tests */
+class RLikeDrmOpsSuite extends FunSuite with Matchers with MahoutLocalContext {
+
+  import RLikeOps._
+
+  val epsilon = 1E-5
+
+  test("A.t") {
+
+    val inCoreA = dense((1, 2, 3), (3, 4, 5))
+
+    val A = drmParallelize(inCoreA)
+
+    val inCoreAt = A.t.collect
+
+    // Assert first norm of difference is less than error margin.
+    (inCoreAt - inCoreA.t).norm should be < epsilon
+
+  }
+
+  test("C = A %*% B") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+
+    // Actual
+    val inCoreCControl = inCoreA %*% inCoreB
+
+    // Distributed operation
+    val C = A %*% B
+    val inCoreC = C.collect
+    println(inCoreC)
+
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+
+    // We also should be able to collect via implicit checkpoint
+    val inCoreC2 = C.collect
+    println(inCoreC2)
+
+    (inCoreC2 - inCoreCControl).norm should be < 1E-10
+
+  }
+
+  test("C = A %*% B mapBlock {}") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2).checkpoint()
+    val B = drmParallelize(inCoreB, numPartitions = 2).checkpoint()
+
+    // Actual
+    val inCoreCControl = inCoreA %*% inCoreB
+
+    A.colSums()
+    B.colSums()
+
+
+    val x = drmBroadcast(dvec(0,0))
+    val x2 = drmBroadcast(dvec(0,0))
+    // Distributed operation
+    val C = (B.t %*% A.t).t.mapBlock() {
+      case (keys, block) =>
+        for (row <- 0 until block.nrow) block(row,::) += x.value + x2
+        keys -> block
+    }
+
+    val inCoreC = C checkpoint StorageLevel.NONE collect;
+    println(inCoreC)
+
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+
+    // We also should be able to collect via implicit checkpoint
+    val inCoreC2 = C.collect
+    println(inCoreC2)
+
+    (inCoreC2 - inCoreCControl).norm should be < 1E-10
+
+    val inCoreQ = dqrThin(C)._1.collect
+
+    printf("Q=\n%s\n", inCoreQ)
+
+    // Assert unit-orthogonality
+    ((inCoreQ(::, 0) dot inCoreQ(::, 0)) - 1.0).abs should be < 1e-10
+    (inCoreQ(::, 0) dot inCoreQ(::, 1)).abs should be < 1e-10
+
+  }
+
+  test("C = A %*% B incompatible B keys") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+        // Re-key B into DrmLike[String] instead of [Int]
+        .mapBlock()({
+      case (keys,block) => keys.map(_.toString) -> block
+    })
+
+    val C = A %*% B
+
+    intercept[IllegalArgumentException] {
+      // This plan must not compile
+      C.checkpoint()
+    }
+  }
+
+  test("C = At %*% B , join") {
+
+    val inCoreA = dense((1, 2), (3, 4),(-3, -5))
+    val inCoreB = dense((3, 5), (4, 6), (0, 1))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+
+    val C = A.t %*% B
+
+    CheckpointAction.optimize(C) should equal (OpAtB[Int](A,B))
+
+    val inCoreC = C.collect
+    val inCoreControlC = inCoreA.t %*% inCoreB
+
+    (inCoreC - inCoreControlC).norm should be < 1E-10
+
+  }
+
+  test("C = At %*% B , join, String-keyed") {
+
+    val inCoreA = dense((1, 2), (3, 4),(-3, -5))
+    val inCoreB = dense((3, 5), (4, 6), (0, 1))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+        .mapBlock()({
+      case (keys, block) => keys.map(_.toString) -> block
+    })
+
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+        .mapBlock()({
+      case (keys, block) => keys.map(_.toString) -> block
+    })
+
+    val C = A.t %*% B
+
+    CheckpointAction.optimize(C) should equal (OpAtB[String](A,B))
+
+    val inCoreC = C.collect
+    val inCoreControlC = inCoreA.t %*% inCoreB
+
+    (inCoreC - inCoreControlC).norm should be < 1E-10
+
+  }
+
+  test("C = At %*% B , zippable, String-keyed") {
+
+    val inCoreA = dense((1, 2), (3, 4),(-3, -5))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+        .mapBlock()({
+      case (keys, block) => keys.map(_.toString) -> block
+    })
+
+    val B =  A + 1.0
+
+    val C = A.t %*% B
+
+    CheckpointAction.optimize(C) should equal (OpAtB[String](A,B))
+
+    val inCoreC = C.collect
+    val inCoreControlC = inCoreA.t %*% (inCoreA + 1.0)
+
+    (inCoreC - inCoreControlC).norm should be < 1E-10
+
+  }
+
+  test("C = A %*% inCoreB") {
+
+    val inCoreA = dense((1, 2, 3), (3, 4, 5), (4, 5, 6), (5, 6, 7))
+    val inCoreB = dense((3, 5, 7, 10), (4, 6, 9, 10), (5, 6, 7, 7))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val C = A %*% inCoreB
+
+    val inCoreC = C.collect
+    val inCoreCControl = inCoreA %*% inCoreB
+
+    println(inCoreC)
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+
+  }
+
+  test("C = inCoreA %*%: B") {
+
+    val inCoreA = dense((1, 2, 3), (3, 4, 5), (4, 5, 6), (5, 6, 7))
+    val inCoreB = dense((3, 5, 7, 10), (4, 6, 9, 10), (5, 6, 7, 7))
+
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+    val C = inCoreA %*%: B
+
+    val inCoreC = C.collect
+    val inCoreCControl = inCoreA %*% inCoreB
+
+    println(inCoreC)
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+
+  }
+
+  test("C = A.t %*% A") {
+    val inCoreA = dense((1, 2, 3), (3, 4, 5), (4, 5, 6), (5, 6, 7))
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+
+    val AtA = A.t %*% A
+
+    // Assert optimizer detects square
+    CheckpointAction.optimize(action = AtA) should equal(OpAtA(A))
+
+    val inCoreAtA = AtA.collect
+    val inCoreAtAControl = inCoreA.t %*% inCoreA
+
+    (inCoreAtA - inCoreAtAControl).norm should be < 1E-10
+  }
+
+  test("C = A.t %*% A fat non-graph") {
+    // Hack the max in-mem size for this test
+    System.setProperty(blas.AtA.PROPERTY_ATA_MAXINMEMNCOL, "540")
+
+    val inCoreA = Matrices.uniformView(400, 550, 1234)
+    val A = drmParallelize(m = inCoreA, numPartitions = 2)
+
+    val AtA = A.t %*% A
+
+    // Assert optimizer detects square
+    CheckpointAction.optimize(action = AtA) should equal(OpAtA(A))
+
+    val inCoreAtA = AtA.collect
+    val inCoreAtAControl = inCoreA.t %*% inCoreA
+
+    (inCoreAtA - inCoreAtAControl).norm should be < 1E-10
+    log.debug("test done.")
+  }
+
+
+  test("C = A.t %*% A non-int key") {
+    val inCoreA = dense((1, 2, 3), (3, 4, 5), (4, 5, 6), (5, 6, 7))
+    val AintKeyd = drmParallelize(m = inCoreA, numPartitions = 2)
+    val A = AintKeyd.mapBlock() {
+      case (keys, block) => keys.map(_.toString) -> block
+    }
+
+    val AtA = A.t %*% A
+
+    // Assert optimizer detects square
+    CheckpointAction.optimize(action = AtA) should equal(OpAtA(A))
+
+    val inCoreAtA = AtA.collect
+    val inCoreAtAControl = inCoreA.t %*% inCoreA
+
+    (inCoreAtA - inCoreAtAControl).norm should be < 1E-10
+  }
+
+  test("C = A + B") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+
+    val C = A + B
+    val inCoreC = C.collect
+
+    // Actual
+    val inCoreCControl = inCoreA + inCoreB
+
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+  }
+
+  test("C = A + B side test 1") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2)
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+
+    val C = A + B
+    val inCoreC = C.collect
+
+    val inCoreD = (A + B).collect
+
+    // Actual
+    val inCoreCControl = inCoreA + inCoreB
+
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+    (inCoreD - inCoreCControl).norm should be < 1E-10
+  }
+
+  test("C = A + B side test 2") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val A = drmParallelize(inCoreA, numPartitions = 2).checkpoint()
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+
+    val C = A + B
+    val inCoreC = C.collect
+
+    val inCoreD = (A + B).collect
+
+    // Actual
+    val inCoreCControl = inCoreA + inCoreB
+
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+    (inCoreD - inCoreCControl).norm should be < 1E-10
+  }
+
+  test("C = A + B side test 3") {
+
+    val inCoreA = dense((1, 2), (3, 4))
+    val inCoreB = dense((3, 5), (4, 6))
+
+    val B = drmParallelize(inCoreB, numPartitions = 2)
+//    val A = (drmParallelize(inCoreA, numPartitions = 2) + B).checkpoint(StorageLevel.MEMORY_ONLY_SER)
+    val A = (drmParallelize(inCoreA, numPartitions = 2) + B).checkpoint(StorageLevel.MEMORY_ONLY)
+
+    val C = A + B
+    val inCoreC = C.collect
+
+    val inCoreD = (A + B).collect
+
+    // Actual
+    val inCoreCControl = inCoreA +  inCoreB * 2.0
+
+    (inCoreC - inCoreCControl).norm should be < 1E-10
+    (inCoreD - inCoreCControl).norm should be < 1E-10
+  }
+
+  test ("general side")  {
+    val sc = implicitly[SparkContext]
+    val k1 = sc.parallelize(Seq(ArrayBuffer(0,1,2,3)))
+//      .persist(StorageLevel.MEMORY_ONLY)   // -- this will demonstrate immutability side effect!
+      .persist(StorageLevel.MEMORY_ONLY_SER)
+
+    println(k1.map(_ += 4).collect.head)
+    println(k1.map(_ += 4).collect.head)
+  }
+
+  test("Ax") {
+    val inCoreA = dense(
+      (1, 2),
+      (3, 4),
+      (20, 30)
+    )
+    val x = dvec(10, 3)
+
+    val drmA = drmParallelize(inCoreA, numPartitions = 2)
+
+    val ax = (drmA %*% x).collect(::, 0)
+
+    ax should equal(inCoreA %*% x)
+  }
+
+  test("A'x") {
+    val inCoreA = dense(
+      (1, 2),
+      (3, 4),
+      (20, 30)
+    )
+    val x = dvec(10, 3, 4)
+
+    val drmA = drmParallelize(inCoreA, numPartitions = 2)
+
+    CheckpointAction.optimize(drmA.t %*% x) should equal (OpAtx(drmA, x))
+
+    val atx = (drmA.t %*% x).collect(::, 0)
+
+    atx should equal(inCoreA.t %*% x)
+  }
+
+  test("colSums, colMeans") {
+    val inCoreA = dense(
+      (1, 2),
+      (3, 4),
+      (20, 30)
+    )
+    val drmA = drmParallelize(inCoreA, numPartitions = 2)
+
+    drmA.colSums() should equal (inCoreA.colSums())
+    drmA.colMeans() should equal (inCoreA.colMeans())
+  }
+
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/LoggerConfiguration.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/LoggerConfiguration.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/LoggerConfiguration.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/LoggerConfiguration.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,16 @@
+package org.apache.mahout.sparkbindings.test
+
+import org.scalatest.Suite
+import org.apache.log4j.{Level, Logger, BasicConfigurator}
+
+/**
+ * @author dmitriy
+ */
+trait LoggerConfiguration extends org.apache.mahout.test.LoggerConfiguration {
+  this: Suite =>
+
+  override protected def beforeAll(): Unit = {
+    super.beforeAll()
+    Logger.getLogger("org.apache.mahout.sparkbindings").setLevel(Level.DEBUG)
+  }
+}
diff -uNar -x .git mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/MahoutLocalContext.scala mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/MahoutLocalContext.scala
--- mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/MahoutLocalContext.scala	1969-12-31 16:00:00.000000000 -0800
+++ mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/test/MahoutLocalContext.scala	2014-03-29 01:03:14.000000000 -0700
@@ -0,0 +1,40 @@
+package org.apache.mahout.sparkbindings.test
+
+import org.scalatest.Suite
+import org.apache.spark.SparkContext
+import org.apache.mahout.sparkbindings._
+import org.apache.mahout.test.MahoutSuite
+
+/**
+ * @author dmitriy
+ */
+trait MahoutLocalContext extends MahoutSuite with LoggerConfiguration {
+  this: Suite =>
+
+  protected implicit var mahoutCtx: SparkContext = _
+
+  // Additional jars?
+  protected val buildJars = Traversable.empty[String]
+
+  override protected def beforeEach() {
+    super.beforeEach()
+
+    System.setProperty("spark.kryoserializer.buffer.mb","15")
+    System.setProperty("spark.akka.frameSize","30")
+    mahoutCtx = mahoutSparkContext(masterUrl = "local[3]",
+      appName = "MahoutLocalContext",
+      customJars = buildJars
+    )
+  }
+
+  override protected def afterEach() {
+    if (mahoutCtx != null) {
+      try {
+        mahoutCtx.stop()
+      } finally {
+        mahoutCtx = null
+      }
+    }
+    super.afterEach()
+  }
+}
diff -uNar -x .git mahout/src/conf/dirichlet.props mahout/src/conf/dirichlet.props
--- mahout/src/conf/dirichlet.props	2014-03-29 01:04:48.000000000 -0700
+++ mahout/src/conf/dirichlet.props	1969-12-31 16:00:00.000000000 -0800
@@ -1,19 +0,0 @@
-# The following parameters must be specified
-#i|input = /path/to/input
-#o|output = /path/to/output
-#x|maxIter = <number of iterations>
-#k|k = <number of models>
-
-# The following parameters all have default values if not specified
-#ow|overwrite = <clear output directory if present>
-#m|alpha = <Dirichlet alpha_0 value. Default: 1.0>
-#md|modelDistClass = <model distribution class name. Default: NormalModelDistribution>
-#mp|modelPrototypeClass = <vector class name for models. Default: RandomAccessSparseVector>
-#r|maxRed = <number of reducers. Default: 1>
-#cl|clustering = <cluster points if present>
-#e|emitMostLikely = <emit most likely cluster if clustering. Default: true>
-#t|threshold = <threshold if clustering and not emitMostLikely. Default: 0.0>
-
-
-
-
diff -uNar -x .git mahout/src/conf/driver.classes.default.props mahout/src/conf/driver.classes.default.props
--- mahout/src/conf/driver.classes.default.props	2014-03-29 01:04:48.000000000 -0700
+++ mahout/src/conf/driver.classes.default.props	2014-03-29 01:03:14.000000000 -0700
@@ -17,6 +17,7 @@
 org.apache.mahout.utils.ConcatenateVectorsJob = concatmatrices : Concatenates 2 matrices of same cardinality into a single matrix
 org.apache.mahout.clustering.streaming.tools.ResplitSequenceFiles = resplit : Splits a set of SequenceFiles into a number of equal splits
 org.apache.mahout.clustering.streaming.tools.ClusterQualitySummarizer = qualcluster : Runs clustering experiments and summarizes results in a CSV
+org.apache.mahout.classifier.df.tools.Describe = describe : Describe the fields and target variable in a data set
 
 #Math
 org.apache.mahout.math.hadoop.TransposeJob = transpose : Take the transpose of a matrix
@@ -26,26 +27,24 @@
 org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob = rowsimilarity : Compute the pairwise similarities of the rows of a matrix
 org.apache.mahout.math.hadoop.similarity.VectorDistanceSimilarityJob =  vecdist : Compute the distances between a set of Vectors (or Cluster or Canopy, they must fit in memory) and a list of Vectors
 org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli = ssvd : Stochastic SVD
+
 #Clustering
 org.apache.mahout.clustering.kmeans.KMeansDriver = kmeans : K-means clustering
 org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver = fkmeans : Fuzzy K-means clustering
-org.apache.mahout.clustering.minhash.MinHashDriver = minhash : Run Minhash clustering
 org.apache.mahout.clustering.lda.cvb.CVB0Driver = cvb : LDA via Collapsed Variation Bayes (0th deriv. approx)
 org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0 = cvb0_local : LDA via Collapsed Variation Bayes, in memory locally.
-org.apache.mahout.clustering.dirichlet.DirichletDriver = dirichlet : Dirichlet Clustering
-org.apache.mahout.clustering.meanshift.MeanShiftCanopyDriver = meanshift : Mean Shift clustering
 org.apache.mahout.clustering.canopy.CanopyDriver = canopy : Canopy clustering
-org.apache.mahout.clustering.spectral.eigencuts.EigencutsDriver = eigencuts : Eigencuts spectral clustering
 org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver = spectralkmeans : Spectral k-means clustering
 org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver = clusterpp : Groups Clustering Output In Clusters
 org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver = streamingkmeans : Streaming k-means clustering
 
-#Freq. Itemset Mining
-org.apache.mahout.fpm.pfpgrowth.FPGrowthDriver = fpg : Frequent Pattern Growth
 #Classification
 #new bayes
 org.apache.mahout.classifier.naivebayes.training.TrainNaiveBayesJob = trainnb : Train the Vector-based Bayes classifier
 org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver = testnb : Test the Vector-based Bayes classifier
+org.apache.mahout.classifier.df.mapreduce.BuildForest = buildforest : Build the random forest classifier
+org.apache.mahout.classifier.df.mapreduce.TestForest = testforest : Test the random forest classifier
+
 #SGD
 org.apache.mahout.classifier.sgd.TrainLogistic = trainlogistic : Train a logistic regression using stochastic gradient descent
 org.apache.mahout.classifier.sgd.RunLogistic = runlogistic : Run a logistic regression model against CSV data
diff -uNar -x .git mahout/src/conf/meanshift.props mahout/src/conf/meanshift.props
--- mahout/src/conf/meanshift.props	2014-03-29 01:04:48.000000000 -0700
+++ mahout/src/conf/meanshift.props	1969-12-31 16:00:00.000000000 -0800
@@ -1,13 +0,0 @@
-# The following parameters must be specified
-#i|input = /path/to/input
-#o|output = /path/to/output
-#t1|t1 = <T1 threshold value>
-#t2|t2 = <T2 threshold value >
-#cd|convergenceDelta = <the convergence threshold to halt iteration>
-#x|maxIter = <the maximum number of iterations to attempt>
-
-# The following parameters all have default values if not specified
-#ow|overwrite = <clear output directory if present>
-#cl|clustering = <cluster points if present>
-#ic|inputIsCanopies = <if present, input directory contains MeanShiftCanopies (vs.VectorWritable)>
-#dm|distanceMeasure = <distance measure class name. Default: SquaredEuclideanDistanceMeasure>
